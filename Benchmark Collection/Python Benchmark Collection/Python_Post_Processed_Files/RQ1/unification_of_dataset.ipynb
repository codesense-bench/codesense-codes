{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from pprint import pp\n",
    "from tree_sitter_languages import get_parser\n",
    "import os\n",
    "\n",
    "parser = get_parser(\"python\")\n",
    "\n",
    "processed_count = 0\n",
    "\n",
    "\n",
    "projects_path = \"/home/XXX/Traces/Python/All/tried\"\n",
    "\n",
    "\n",
    "projects_list = os.listdir(projects_path)\n",
    "print(len(projects_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping dlukes+corpy: Unterminated string starting at: line 201671126 column 11 (char 4419028848)\n"
     ]
    }
   ],
   "source": [
    "pydata = []\n",
    "\n",
    "for project in projects_list[500:545]:\n",
    "    project_path = os.path.join(projects_path, project)\n",
    "    trace_file_path = os.path.join(project_path, \"collected_traces.json\")\n",
    "    \n",
    "    if os.path.isfile(trace_file_path):\n",
    "        try:\n",
    "            with open(trace_file_path) as f:\n",
    "                project_data = json.load(f)\n",
    "            pydata.append((project, project_data))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Skipping {project}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for project_name, trace in pydata:\n",
    "    for k, v in trace.items():\n",
    "        code_files = {}\n",
    "        code_funcs = {}\n",
    "        fpath = v[0]\n",
    "        if \"/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/\" in fpath:\n",
    "            fpath = fpath.replace(\n",
    "                \"/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/\", \n",
    "                \"/home/XXX/Traces/Python/All/tried/\"\n",
    "            )\n",
    "        elif \"/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/\" in fpath:\n",
    "            fpath = fpath.replace(\n",
    "                \"/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/\", \n",
    "                \"/home/XXX/Traces/Python/All/tried/\"\n",
    "            )\n",
    "\n",
    "        \n",
    "        try:\n",
    "            with open(fpath, encoding='utf-8') as f:\n",
    "                code_files[fpath] = f.read()\n",
    "        except (FileNotFoundError, UnicodeDecodeError) as e:\n",
    "            print(f\"Error reading file {fpath}: {e}\")\n",
    "            try:\n",
    "                with open(fpath, encoding='unicode_escape') as f:\n",
    "                    code_files[fpath] = f.read()\n",
    "            except Exception as e2:\n",
    "                print(f\"Error reading file with fallback encoding {fpath}: {e2}\")\n",
    "                continue  # Skip to the next file if reading fails again\n",
    "        \n",
    "        try:\n",
    "            with open(fpath, \"rb\") as f:\n",
    "                def get_function_code():\n",
    "                    tree = parser.parse(f.read())\n",
    "                    queue = [tree.root_node]\n",
    "                    while queue:\n",
    "                        node = queue.pop()\n",
    "                        if node.type == 'function_definition':\n",
    "                            function_name = node.child_by_field_name(\"name\").text.decode()\n",
    "                            function_code = node.text.decode()\n",
    "                            yield function_name, function_code, node.start_point[0]\n",
    "                        queue.extend(node.children)\n",
    "                for func, func_code, func_line in get_function_code():\n",
    "                    code_funcs[func] = (func_code, func_line)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {fpath}\")\n",
    "            continue  # Skip to the next file if the current one is not found\n",
    "        \n",
    "        UNKNOWN = v[1]\n",
    "        pysteps = []\n",
    "        for step in v[2:]:\n",
    "            (\n",
    "                event, # 0\n",
    "                timestamp, # 1\n",
    "                line_no, # 2\n",
    "                source_line, # 3\n",
    "                new_vars, # 4\n",
    "                modified_vars, # 5\n",
    "                ended_by_exception, # 6\n",
    "                return_value_repr, # 7\n",
    "                exception, # 8\n",
    "            ) = step\n",
    "            if event == \"call\":\n",
    "                m = re.match(r\"\\s*def\\s+([^()\\s]+)\\s*\\(([^)]*)\\)\", source_line)\n",
    "                func_name = None\n",
    "                func_code = None\n",
    "                func_line = None\n",
    "                if m:\n",
    "                    func_name = m.group(1)\n",
    "                    func_code, func_line = code_funcs.get(func_name, (None, None))\n",
    "\n",
    "                if len(pysteps) > 0:\n",
    "                    data.append({\n",
    "                        \"project_name\": project_name,\n",
    "                        \"function_name\": func_name,\n",
    "                        \"function_code\": func_code,\n",
    "                        \"function_line\": func_line,\n",
    "                        \"fpath\": str(fpath),\n",
    "                        \"steps\": pysteps,\n",
    "                        \"code\": code_files,\n",
    "                    })\n",
    "                pysteps = []\n",
    "            pysteps.append({\n",
    "                \"event\": event,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"line_no\": line_no,\n",
    "                \"source_line\": source_line,\n",
    "                \"new_vars\": new_vars,\n",
    "                \"modified_vars\": modified_vars,\n",
    "                \"ended_by_exception\": ended_by_exception,\n",
    "                \"return_value_repr\": return_value_repr,\n",
    "                \"exception\": exception,\n",
    "            })\n",
    "\n",
    "        # Ensure we don't append empty steps\n",
    "        if pysteps:\n",
    "            data.append({\n",
    "                \"fpath\": str(fpath),\n",
    "                \"steps\": pysteps,\n",
    "                \"code\": code_files,\n",
    "            })\n",
    "\n",
    "    # Check that all data entries have the 'steps' key\n",
    "    data = [d for d in data if \"steps\" in d and \"project_name\" in d]\n",
    "\n",
    "    # Create the DataFrame\n",
    "    pydf = pd.DataFrame(data)\n",
    "    pydf[\"language\"] = \"Python\"\n",
    "    # print(pydf.columns, len(pydf))\n",
    "    # print(pydf[\"steps\"].str.len().describe())\n",
    "    # print(pydf[\"steps\"])\n",
    "    # print(pydf[\"fpath\"].iloc[0])\n",
    "    # print(pydf[\"function_name\"].notna().mean(), pydf[\"function_code\"].notna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85034 Index(['language', 'project_name', 'function_name', 'function_code',\n",
      "       'function_line', 'trace_steps', 'function_input', 'function_output'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Format the Python traces into df.\n",
    "# Eventually we will have traces from all languages.\n",
    "\n",
    "df = pydf.copy()\n",
    "df = df[df[\"project_name\"].notna() & df[\"function_name\"].notna() & df[\"function_code\"].notna() & df[\"function_line\"].notna()]\n",
    "\n",
    "def format_steps(steps):\n",
    "    if not steps[0][\"source_line\"].lstrip().startswith(\"def\"):\n",
    "        return None\n",
    "\n",
    "    #function_input = \", \".join([f\"{k} = {v[0]}\" for k, v in steps[0][\"new_vars\"].items()])\n",
    "    function_input = {f\"{k}\": v[0] for k, v in steps[0][\"new_vars\"].items()}\n",
    "\n",
    "    function_output = steps[-1][\"return_value_repr\"]\n",
    "    \n",
    "    \n",
    "    trace_steps = []\n",
    "    for i, step in enumerate(steps[:-1]):\n",
    "        # TODO: Handle exception, jump, multiple assignments within a line, etc.\n",
    "        # TODO: Handle duplicate lines\n",
    "        if step[\"event\"] == \"exception\":\n",
    "            trace_steps.append({\n",
    "                \"line_no\": step[\"line_no\"],\n",
    "                \"values_updated\": [{\"name\": k, \"type\": v[1], \"value\": v[0]} for k, v in step[\"new_vars\"].items()],\n",
    "                \"exception\": step[\"exception\"]  \n",
    "            })\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        #values_updated = steps[i+1][\"new_vars\"]\n",
    "        updated_vars = {**steps[i+1].get(\"new_vars\", {}), **steps[i+1].get(\"modified_vars\", {})}\n",
    "        values_updated = [{\"name\": k, \"type\": v[1], \"value\": v[0]} for k, v in updated_vars.items()]\n",
    "        trace_steps.append({\"line_no\": step[\"line_no\"], \"values_updated\": values_updated})\n",
    "    return trace_steps, function_input, function_output\n",
    "\n",
    "\n",
    "# Use `apply` and `zip` to unpack the results from format_steps\n",
    "df[[\"trace_steps\", \"function_input\", \"function_output\"]] = df[\"steps\"].apply(lambda steps: pd.Series(format_steps(steps)))\n",
    "\n",
    "# Select relevant columns for final DataFrame\n",
    "df = df[[\"language\", \"project_name\",\"function_name\", \"function_code\", \"function_line\", \"trace_steps\", \"function_input\", \"function_output\"]]\n",
    "\n",
    "# df = df.drop(columns=[\"steps\"])\n",
    "# format_steps(df[\"steps\"].iloc[100])\n",
    "# df = df.rename(columns={\"function_name\": })\n",
    "print(len(df), df.columns)\n",
    "#print(df[\"trace_steps\"].iloc[0][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_code(code):\n",
    "    # Remove blank lines\n",
    "    code = \"\".join(line for line in code.splitlines(keepends=True) if line)\n",
    "    return code\n",
    "\n",
    "\n",
    "def source_code_formatting(function_code, function_name, function_input):\n",
    "    lines = function_code.splitlines(keepends=False)\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "from mccabe import PathGraphingAstVisitor\n",
    "from collections import defaultdict\n",
    "\n",
    "COMMENT_SYMBOL = \"#\"\n",
    "\n",
    "\n",
    "# Initialize data structures\n",
    "seen_sources = set()\n",
    "jsonl_data = []\n",
    "\n",
    "for tup in df.itertuples():\n",
    "    if not isinstance(tup.trace_steps, list):\n",
    "        continue\n",
    "    \n",
    "    # if not filter_complex_entries(tup):\n",
    "    #     continue\n",
    "    \n",
    "    # if tup.function_input is None or any(x in str(tup.function_input).lower() for x in [\"none\"]):\n",
    "    #     continue\n",
    "    \n",
    "    # if (\"tensor\" in tup.function_input or \"Sequential\" in tup.function_input or \"array\" in tup.function_input or\n",
    "    #             \"ReLU()\" in tup.function_input or \"Encoder()\" in tup.function_input) and len(tup.function_input) > 30:\n",
    "    #     continue\n",
    "    \n",
    "    # Collect value updates on each line\n",
    "    values_per_line = defaultdict(list)\n",
    "    repeated_values = defaultdict(list)\n",
    "    step_index = 1\n",
    "\n",
    "    variable_name_value_line_mapping = {}\n",
    "\n",
    "\n",
    "    for variable, value in tup.function_input.items():\n",
    "        if variable not in variable_name_value_line_mapping:\n",
    "            variable_name_value_line_mapping[variable] = [] \n",
    "        variable_name_value_line_mapping[variable].append((1, value))  \n",
    "\n",
    "\n",
    "    for step in tup.trace_steps:\n",
    "        if \"exception\" in step:\n",
    "            exception_message = f'EXCEPTION: {step[\"exception\"]}'\n",
    "            values_per_line[step[\"line_no\"]-tup.function_line].append((step_index, exception_message))\n",
    "            step_index +=1\n",
    "        else:\n",
    "            for value in step[\"values_updated\"]:\n",
    "                state = f'{value[\"name\"]} = {str(value[\"value\"])}'\n",
    "                line_position = step[\"line_no\"] - tup.function_line\n",
    "\n",
    "                if value[\"name\"] in variable_name_value_line_mapping:\n",
    "                    variable_name_value_line_mapping[value[\"name\"]].append((line_position, value[\"value\"]))\n",
    "                else:\n",
    "                    variable_name_value_line_mapping[value[\"name\"]] = [(line_position, value[\"value\"])]\n",
    "\n",
    "                if state not in repeated_values[line_position]:\n",
    "                    repeated_values[line_position].append(state)\n",
    "\n",
    "        \n",
    "            if len(repeated_values[step[\"line_no\"]-tup.function_line]) > 0:\n",
    "                state_values = repeated_values[step[\"line_no\"]-tup.function_line]\n",
    "                values_per_line[step[\"line_no\"]-tup.function_line].append((step_index, state_values))\n",
    "                step_index += 1\n",
    "    \n",
    "\n",
    "    if len(values_per_line) == 0:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    source_code_with_input = source_code_formatting(tup.function_code, tup.function_name, tup.function_input)\n",
    "\n",
    "\n",
    "    lines = tup.function_code.splitlines(keepends=False)\n",
    "    annotated_lines = []\n",
    "\n",
    "    function_def_line = lines[0]\n",
    "    annotated_lines.append(function_def_line)\n",
    "\n",
    "    for lineno, line in enumerate(lines[1:], start=2):\n",
    "        states_and_line_values = values_per_line[lineno]\n",
    "        line = line.rstrip()\n",
    "\n",
    "        if len(states_and_line_values) > 0 and len(states_and_line_values) == 1:\n",
    "            for i, (index, values) in enumerate(states_and_line_values):\n",
    "                for state_index, value in enumerate(values, start=1):\n",
    "                    line += f\" {COMMENT_SYMBOL} [STATE] \" + \"\".join(value) + \" [/STATE]\"\n",
    "        annotated_lines.append(line)\n",
    "        \n",
    "\n",
    "    source_code_with_input = source_code_with_input.split(\"\\n\")\n",
    "    function_call_with_input = f\"{tup.function_name}({', '.join([f'{k}={v}' for k, v in tup.function_input.items()])})\"\n",
    "    annotated_lines.append(\"\\n\" + function_call_with_input)\n",
    "    source_code_with_input.append(\"\\n\" + function_call_with_input)\n",
    "\n",
    "    annotated_function_code = \"\\n\".join(annotated_lines)\n",
    "    source_code_with_input = \"\\n\".join(source_code_with_input)\n",
    "    formatted_code = format_code(annotated_function_code)\n",
    "\n",
    "\n",
    "    if tup.function_name in seen_sources:\n",
    "        continue\n",
    "\n",
    "    seen_sources.add(tup.function_name )\n",
    "\n",
    "    jsonl_data.append({\n",
    "        \"Language\": tup.language,\n",
    "        \"Project_Name\": tup.project_name,\n",
    "        \"Source Code\": source_code_with_input,\n",
    "        \"input\" : tup.function_input,\n",
    "        \"output\" : tup.function_output,\n",
    "        \"scratchpad_format\": formatted_code,\n",
    "        \"variable_values\": variable_name_value_line_mapping,\n",
    "    })\n",
    "\n",
    "\n",
    "# Write to JSONL file\n",
    "with open(\"dataset_all.jsonl\", \"a\", encoding='utf-8') as file:\n",
    "    for entry in jsonl_data:\n",
    "        file.write(json.dumps(entry) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curating again assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tiktoken\n",
    "\n",
    "def contains_complex_type(value):\n",
    "    \"\"\"Check if a string contains complex types.\"\"\"\n",
    "    complex_patterns = [\n",
    "        r\"<class '.*'>\",               # e.g., \"<class 'peewee.SqliteDatabase'>\"\n",
    "        r\"<.*object at 0x[\\da-f]+>\",    # e.g., \"<peewee.PrimaryKeyField object at 0x7f9ad1c4d550>\"\n",
    "        r\"at 0x[\\da-f]+\",               # any memory address\n",
    "        r\"^\\s*<\",                       # any string starting with \"<\"\n",
    "    ]\n",
    "    \n",
    "    if isinstance(value, str):\n",
    "        for pattern in complex_patterns:\n",
    "            if re.search(pattern, value):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def token_filtering(source_code):\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\") \n",
    "    MAX_TOKENS = 2048  \n",
    "\n",
    "\n",
    "    token_count = len(tokenizer.encode(source_code))\n",
    "\n",
    "    if token_count <= MAX_TOKENS:  \n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import tree_sitter\n",
    "from tree_sitter_languages import get_parser\n",
    "import re\n",
    "\n",
    "parser = get_parser(\"python\")\n",
    "\n",
    "\n",
    "\"Having assignments with only <STATE> comments\"\n",
    "def filtering_assignments(assignments, annotated_lines):\n",
    "    filtered_assingments = []\n",
    "    for assignment in assignments:\n",
    "        line_no = assignment.start_point[0]\n",
    "\n",
    "        if \"# [STATE]\" in annotated_lines[line_no]:\n",
    "            #comment = \"# \"+annotated_lines[line_no].split(\"#\")[1]\n",
    "            filtered_assingments.append(assignment)\n",
    "    return filtered_assingments\n",
    "\n",
    "\n",
    "def get_last_value(var_name, variable_values, current_line):\n",
    "    \"\"\"Fetch the last recorded value of a variable before a given line.\"\"\"\n",
    "    if var_name in variable_values:\n",
    "        values = variable_values[var_name]\n",
    "        for line, value in reversed(values):\n",
    "            if line < current_line:\n",
    "                return value\n",
    "    return None\n",
    "\n",
    "def find_assignments(node):\n",
    "    \"\"\"Recursively find all assignment nodes in the AST.\"\"\"\n",
    "    assignments = []\n",
    "    for child in node.children:\n",
    "        if child.type == \"assignment\" and node.type == \"expression_statement\":\n",
    "            assignments.append(child)\n",
    "        assignments.extend(find_assignments(child))\n",
    "    \n",
    "    return assignments\n",
    "\n",
    "def is_immediate_value(rhs_text):\n",
    "    \"\"\"Check if the RHS is an immediate value like int, float, string, list, tuple, dict.\"\"\"\n",
    "    try:\n",
    "        eval_rhs = eval(rhs_text)  # Evaluate safely (controlled usage)\n",
    "        return isinstance(eval_rhs, (int, float, str, list, tuple, dict, bool))\n",
    "    except:\n",
    "        return False  # If eval fails, it's not an immediate value\n",
    "\n",
    "def process_assignment(node, variable_values):\n",
    "    \"\"\"Process an assignment expression and generate comments/statements.\"\"\"\n",
    "    lhs = node.child_by_field_name(\"left\")\n",
    "    rhs = node.child_by_field_name(\"right\")\n",
    "    if lhs is None or rhs is None:\n",
    "        return None, None, None\n",
    "    \n",
    "    rhs_text = rhs.text.decode()\n",
    "    current_line = node.start_point[0] + 1  # Tree-sitter lines start from 0\n",
    "    \n",
    "    if \"(\" in rhs_text or is_immediate_value(rhs_text):  \n",
    "        return None, None, None\n",
    "    \n",
    "    identifiers = [child.text.decode() for child in rhs.children if child.type == \"identifier\"]\n",
    "    values = {var: get_last_value(var, variable_values, current_line) for var in identifiers}\n",
    "\n",
    "\n",
    "    if len(identifiers) == 0:\n",
    "        return None, None, None\n",
    "    \n",
    "    if all(v is not None for v in values.values()):\n",
    "        comment = f\"# {' '.join(f'{k} = {v}' for k, v in values.items())}\"\n",
    "        statement = f\"The value of '{', '.join(values.keys())}' is '{', '.join(map(str, values.values()))}' before the execution of statement \\\"{lhs.text.decode()} = {rhs_text}\\\". Can you predict the value of \\\"{lhs.text.decode()}\\\" after the execution of the statement?\"\n",
    "        return comment, statement, values\n",
    "    \n",
    "    return None, None, None\n",
    "\n",
    "def annotate_code(example):\n",
    "    \"\"\"Parse and annotate the code with extracted variable values.\"\"\"\n",
    "    code = example[\"Source Code\"]\n",
    "    source_code = example[\"scratchpad_format\"]\n",
    "    variable_values = example[\"variable_values\"]\n",
    "    tree = parser.parse(source_code.encode())\n",
    "\n",
    "    code_lines = code.split(\"\\n\")\n",
    "    annotated_lines = source_code.split(\"\\n\")\n",
    "    statements = []\n",
    "\n",
    "    assignments = find_assignments(tree.root_node)\n",
    "\n",
    "    filtered_assignments = filtering_assignments(assignments, annotated_lines)\n",
    "\n",
    "\n",
    "    if not filtered_assignments:\n",
    "        return None, None, None, None  # No assignments found\n",
    "    \n",
    "    # Try multiple assignments until we find one that is valid\n",
    "    valid_assignment = None\n",
    "    for assignment in assignments:\n",
    "        comment, statement, values = process_assignment(assignment, variable_values)\n",
    "        if comment and statement:\n",
    "            valid_assignment = assignment\n",
    "            break \n",
    "    if valid_assignment:\n",
    "        line_number = valid_assignment.start_point[0]\n",
    "\n",
    "\n",
    "        match_item = re.search(r'\\# \\[STATE\\](.*?)\\[/STATE\\]', annotated_lines[line_number], re.DOTALL)\n",
    "\n",
    "        ground_truth = None\n",
    "        if match_item:\n",
    "            ground_truth = match_item.group(1).strip()\n",
    "        else:\n",
    "            ground_truth = None   \n",
    "\n",
    "        return \"\\n\".join(code_lines), code_lines[line_number], ground_truth, values\n",
    "    \n",
    "    return None, None, None, None\n",
    "\n",
    "# Example usage\n",
    "with open(\"dataset_all.jsonl\", \"r\") as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "filtered_results = []\n",
    "\n",
    "for example in dataset:\n",
    "    if any(contains_complex_type(val) for val in example[\"input\"].values()):\n",
    "        continue\n",
    "\n",
    "\n",
    "    annotated_code, statements, ground_truth, values = annotate_code(example)\n",
    "    if annotated_code and statements and ground_truth: \n",
    "        if contains_complex_type(ground_truth) or not token_filtering(annotated_code):\n",
    "            continue\n",
    "        if \"=\" in ground_truth:\n",
    "            filtered_results.append(\n",
    "                {\"Programming Language\":example[\"Language\"],\n",
    "                \"Statement Type\": \"Assignment\",\n",
    "                \"Source Code\": annotated_code, \n",
    "                \"Selected Statement\": statements.strip(),\n",
    "                \"Function Input\": example[\"input\"],\n",
    "                \"Variable Values Before Statement\": values, \n",
    "                \"Value After Statement Execution\": ground_truth.split(\"=\")[1].strip(),\n",
    "                \"Variable States During Runtime\": example[\"variable_values\"],\n",
    "                \"Program Information\": f\"Project Name: {example['Project_Name']}\"\n",
    "                })\n",
    "\n",
    "\n",
    "with open(\"Unified_dataset_assignment.jsonl\", \"w\") as f:\n",
    "    for result in filtered_results:\n",
    "        f.write(json.dumps(result) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import tree_sitter\n",
    "from tree_sitter_languages import get_parser\n",
    "import re\n",
    "\n",
    "parser = get_parser(\"python\")\n",
    "\n",
    "\n",
    "\"Having assignments with only <STATE> comments\"\n",
    "def filtering_assignments(assignments, annotated_lines):\n",
    "    filtered_assingments = []\n",
    "    for assignment in assignments:\n",
    "        line_no = assignment.start_point[0]\n",
    "\n",
    "        if \"# [STATE]\" in annotated_lines[line_no]:\n",
    "            #comment = \"# \"+annotated_lines[line_no].split(\"#\")[1]\n",
    "            filtered_assingments.append(assignment)\n",
    "    return filtered_assingments\n",
    "\n",
    "\n",
    "def get_last_value(var_name, variable_values, current_line):\n",
    "    \"\"\"Fetch the last recorded value of a variable before a given line.\"\"\"\n",
    "    if var_name in variable_values:\n",
    "        values = variable_values[var_name]\n",
    "        for line, value in reversed(values):\n",
    "            if line < current_line:\n",
    "                return value\n",
    "    return None\n",
    "\n",
    "def find_assignments(node):\n",
    "    \"\"\"Recursively find all assignment nodes in the AST.\"\"\"\n",
    "    assignments = []\n",
    "    for child in node.children:\n",
    "        if child.type == \"assignment\" and node.type == \"expression_statement\":\n",
    "            assignments.append(child)\n",
    "        assignments.extend(find_assignments(child))\n",
    "    \n",
    "    return assignments\n",
    "\n",
    "\n",
    "def is_constant_value(rhs_text):\n",
    "    \"\"\"Check if the RHS is a constant value like int, float, string, bool.\"\"\"\n",
    "    try:\n",
    "        eval_rhs = eval(rhs_text)  # Evaluate safely (controlled usage)\n",
    "        return isinstance(eval_rhs, (int, float, str, bool))\n",
    "    except:\n",
    "        return False  # If eval fails, it's not a constant value\n",
    "\n",
    "def process_assignment(node, variable_values):\n",
    "    \"\"\"Process an assignment expression and generate comments/statements.\"\"\"\n",
    "    lhs = node.child_by_field_name(\"left\")\n",
    "    rhs = node.child_by_field_name(\"right\")\n",
    "    if lhs is None or rhs is None:\n",
    "        return None, None, None\n",
    "    \n",
    "    rhs_text = rhs.text.decode()\n",
    "    current_line = node.start_point[0] + 1  # Tree-sitter lines start from 0\n",
    "    \n",
    "    # Only process if RHS is a constant value\n",
    "    if not is_constant_value(rhs_text):\n",
    "        return None, None, None\n",
    "    \n",
    "    # For constant assignments, we don't need to track variable values\n",
    "    comment = f\"# Constant assignment: {lhs.text.decode()} = {rhs_text}\"\n",
    "    statement = f\"The value of '{lhs.text.decode()}' is assigned the constant value '{rhs_text}'.\"\n",
    "    values = {\"Constant\": rhs_text}\n",
    "    \n",
    "    return comment, statement, values\n",
    "\n",
    "def annotate_code(example):\n",
    "    \"\"\"Parse and annotate the code with extracted variable values.\"\"\"\n",
    "    code = example[\"Source Code\"]\n",
    "    source_code = example[\"scratchpad_format\"]\n",
    "    variable_values = example[\"variable_values\"]\n",
    "    tree = parser.parse(source_code.encode())\n",
    "\n",
    "    code_lines = code.split(\"\\n\")\n",
    "    annotated_lines = source_code.split(\"\\n\")\n",
    "    statements = []\n",
    "\n",
    "    assignments = find_assignments(tree.root_node)\n",
    "\n",
    "    filtered_assignments = filtering_assignments(assignments, annotated_lines)\n",
    "\n",
    "    if not filtered_assignments:\n",
    "        return None, None, None, None  # No assignments found\n",
    "    \n",
    "    # Try multiple assignments until we find one that is valid\n",
    "    valid_assignment = None\n",
    "    for assignment in assignments:\n",
    "        comment, statement, values = process_assignment(assignment, variable_values)\n",
    "        if comment and statement:\n",
    "            valid_assignment = assignment\n",
    "            break \n",
    "    if valid_assignment:\n",
    "        line_number = valid_assignment.start_point[0]\n",
    "\n",
    "        match_item = re.search(r'\\# \\[STATE\\](.*?)\\[/STATE\\]', annotated_lines[line_number], re.DOTALL)\n",
    "\n",
    "        ground_truth = None\n",
    "        if match_item:\n",
    "            ground_truth = match_item.group(1).strip()\n",
    "        else:\n",
    "            ground_truth = None   \n",
    "\n",
    "        return \"\\n\".join(code_lines), code_lines[line_number], ground_truth, values\n",
    "    \n",
    "    return None, None, None, None\n",
    "\n",
    "# Example usage\n",
    "with open(\"dataset_all.jsonl\", \"r\") as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "filtered_results = []\n",
    "\n",
    "for example in dataset:\n",
    "    if any(contains_complex_type(val) for val in example[\"input\"].values()):\n",
    "        continue\n",
    "\n",
    "    annotated_code, statements, ground_truth, values = annotate_code(example)\n",
    "    if annotated_code and statements and ground_truth: \n",
    "        if contains_complex_type(ground_truth) or not token_filtering(annotated_code):\n",
    "            continue\n",
    "        if \"=\" in ground_truth:\n",
    "            filtered_results.append(\n",
    "                {\"Programming Language\": example[\"Language\"],\n",
    "                 \"Statement Type\": \"Constant Assignment\",\n",
    "                 \"Source Code\": annotated_code, \n",
    "                 \"Selected Statement\": statements.strip(),\n",
    "                 \"Function Input\": example[\"input\"],\n",
    "                 \"Variable Values Before Statement\": values, \n",
    "                 \"Value After Statement Execution\": values[\"Constant\"],  # Constant value as string\n",
    "                 \"Variable States During Runtime\": example[\"variable_values\"],\n",
    "                 \"Program Information\": f\"Project Name: {example['Project_Name']}\"\n",
    "                })\n",
    "\n",
    "with open(\"Unified_dataset_constant_assignment.jsonl\", \"w\") as f:\n",
    "    for result in filtered_results:\n",
    "        f.write(json.dumps(result) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arithmatic Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import tree_sitter\n",
    "from tree_sitter_languages import get_parser\n",
    "import re\n",
    "\n",
    "parser = get_parser(\"python\")\n",
    "\n",
    "\n",
    "\"Having assignments with only <STATE> comments\"\n",
    "def filtering_assignments(assignments, annotated_lines):\n",
    "    filtered_assingments = []\n",
    "    for assignment in assignments:\n",
    "        line_no = assignment.start_point[0]\n",
    "\n",
    "        if \"# [STATE]\" in annotated_lines[line_no]:\n",
    "            #comment = \"# \"+annotated_lines[line_no].split(\"#\")[1]\n",
    "            filtered_assingments.append(assignment)\n",
    "    return filtered_assingments\n",
    "\n",
    "\n",
    "def get_last_value(var_name, variable_values, current_line):\n",
    "    \"\"\"Fetch the last recorded value of a variable before a given line.\"\"\"\n",
    "    if var_name in variable_values:\n",
    "        values = variable_values[var_name]\n",
    "        for line, value in reversed(values):\n",
    "            if line < current_line:\n",
    "                return value\n",
    "    return None\n",
    "\n",
    "def find_assignments(node):\n",
    "    \"\"\"Recursively find all assignment nodes in the AST.\"\"\"\n",
    "    assignments = []\n",
    "    for child in node.children:\n",
    "        if child.type == \"assignment\" and node.type == \"expression_statement\":\n",
    "            assignments.append(child)\n",
    "        assignments.extend(find_assignments(child))\n",
    "    \n",
    "    return assignments\n",
    "\n",
    "def is_arithmetic_expression(rhs_node):\n",
    "    \"\"\"Check if the RHS is an arithmetic expression.\"\"\"\n",
    "    arithmetic_operators = {\"+\", \"-\", \"*\", \"/\", \"//\", \"%\", \"**\"}  # Supported arithmetic operators\n",
    "    if rhs_node.type == \"binary_operator\":\n",
    "        operator = rhs_node.child_by_field_name(\"operator\").text.decode()\n",
    "        return operator in arithmetic_operators\n",
    "    return False\n",
    "\n",
    "def process_arithmetic_assignment(node, variable_values):\n",
    "    \"\"\"Process an arithmetic assignment expression and generate comments/statements.\"\"\"\n",
    "    lhs = node.child_by_field_name(\"left\")\n",
    "    rhs = node.child_by_field_name(\"right\")\n",
    "    if lhs is None or rhs is None:\n",
    "        return None, None, None\n",
    "    \n",
    "    rhs_text = rhs.text.decode()\n",
    "    current_line = node.start_point[0] + 1  # Tree-sitter lines start from 0\n",
    "    \n",
    "    # Only process if RHS is an arithmetic expression\n",
    "    if not is_arithmetic_expression(rhs):\n",
    "        return None, None, None\n",
    "    \n",
    "    # Extract variables used in the arithmetic expression\n",
    "    identifiers = [child.text.decode() for child in rhs.children if child.type == \"identifier\"]\n",
    "    values = {var: get_last_value(var, variable_values, current_line) for var in identifiers}\n",
    "\n",
    "    # Skip if any variable value is missing\n",
    "    if any(v is None for v in values.values()):\n",
    "        return None, None, None\n",
    "    \n",
    "    # Generate comment and statement\n",
    "    comment = f\"# Arithmetic assignment: {lhs.text.decode()} = {rhs_text}\"\n",
    "    statement = f\"The value of '{lhs.text.decode()}' is assigned the result of the arithmetic expression '{rhs_text}'.\"\n",
    "    \n",
    "    return comment, statement, values\n",
    "\n",
    "def annotate_code(example):\n",
    "    \"\"\"Parse and annotate the code with extracted variable values.\"\"\"\n",
    "    code = example[\"Source Code\"]\n",
    "    source_code = example[\"scratchpad_format\"]\n",
    "    variable_values = example[\"variable_values\"]\n",
    "    tree = parser.parse(source_code.encode())\n",
    "\n",
    "    code_lines = code.split(\"\\n\")\n",
    "    annotated_lines = source_code.split(\"\\n\")\n",
    "    statements = []\n",
    "\n",
    "    assignments = find_assignments(tree.root_node)\n",
    "\n",
    "    filtered_assignments = filtering_assignments(assignments, annotated_lines)\n",
    "\n",
    "    if not filtered_assignments:\n",
    "        return None, None, None, None  # No assignments found\n",
    "    \n",
    "    # Try multiple assignments until we find one that is valid\n",
    "    valid_assignment = None\n",
    "    for assignment in assignments:\n",
    "        comment, statement, values = process_arithmetic_assignment(assignment, variable_values)\n",
    "        if comment and statement:\n",
    "            valid_assignment = assignment\n",
    "            break \n",
    "    if valid_assignment:\n",
    "        line_number = valid_assignment.start_point[0]\n",
    "\n",
    "        match_item = re.search(r'\\# \\[STATE\\](.*?)\\[/STATE\\]', annotated_lines[line_number], re.DOTALL)\n",
    "\n",
    "        ground_truth = None\n",
    "        if match_item:\n",
    "            ground_truth = match_item.group(1).strip()\n",
    "        else:\n",
    "            ground_truth = None   \n",
    "\n",
    "        return \"\\n\".join(code_lines), code_lines[line_number], ground_truth, values\n",
    "    \n",
    "    return None, None, None, None\n",
    "\n",
    "# Example usage\n",
    "with open(\"dataset_all.jsonl\", \"r\") as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "filtered_results = []\n",
    "\n",
    "for example in dataset:\n",
    "    if any(contains_complex_type(val) for val in example[\"input\"].values()):\n",
    "        continue\n",
    "\n",
    "    annotated_code, statements, ground_truth, values = annotate_code(example)\n",
    "    if values is None:\n",
    "        continue\n",
    "    \n",
    "    if annotated_code and statements and ground_truth: \n",
    "        if contains_complex_type(ground_truth) or not token_filtering(annotated_code):\n",
    "            continue\n",
    "        if \"=\" in ground_truth:\n",
    "            filtered_results.append(\n",
    "                {\"Programming Language\": example[\"Language\"],\n",
    "                 \"Statement Type\": \"Arithmetic Assignment\",\n",
    "                 \"Source Code\": annotated_code, \n",
    "                 \"Selected Statement\": statements.strip(),\n",
    "                 \"Function Input\": example[\"input\"],\n",
    "                 \"Variable Values Before Statement\": values, \n",
    "                 \"Value After Statement Execution\": ground_truth.split(\"=\")[1].strip(),\n",
    "                 \"Variable States During Runtime\": example[\"variable_values\"],\n",
    "                 \"Program Information\": f\"Project Name: {example['Project_Name']}\"\n",
    "                })\n",
    "\n",
    "with open(\"Unified_dataset_arithmetic_assignment.jsonl\", \"w\") as f:\n",
    "    for result in filtered_results:\n",
    "        f.write(json.dumps(result) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import tree_sitter\n",
    "from tree_sitter_languages import get_parser\n",
    "import re\n",
    "\n",
    "parser = get_parser(\"python\")\n",
    "\n",
    "def filtering_branches(branches, annotated_lines):\n",
    "    yes_branches = []\n",
    "    no_branches = []\n",
    "    for branch in branches:\n",
    "        line_no = branch.start_point[0]\n",
    "\n",
    "        if \"# [STATE]\" in annotated_lines[line_no + 1]:\n",
    "            yes_branches.append(branch)\n",
    "        else:\n",
    "            no_branches.append(branch)\n",
    "    return yes_branches, no_branches\n",
    "\n",
    "\n",
    "def get_last_value(var_name, variable_values, current_line):\n",
    "    \"\"\"Fetch the last recorded value of a variable before a given line.\"\"\"\n",
    "    if var_name in variable_values:\n",
    "        values = variable_values[var_name]\n",
    "        for line, value in reversed(values):\n",
    "            if line < current_line:\n",
    "                return value\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_branches(node):\n",
    "    \"\"\"Recursively find all if-statements in the AST.\"\"\"\n",
    "    branches = []\n",
    "\n",
    "    if node.type == \"if_statement\":\n",
    "        branches.append(node)\n",
    "\n",
    "    for child in node.children:\n",
    "        branches.extend(find_branches(child))\n",
    "\n",
    "    return branches\n",
    "\n",
    "\n",
    "def process_branches(node, variable_values):\n",
    "    \"\"\"Process branch expression and generate comments/statements.\"\"\"\n",
    "\n",
    "    current_line = node.start_point[0] + 1  # Tree-sitter lines start from 0\n",
    "\n",
    "    if len(node.children) > 1:\n",
    "        condition_node = node.children[1]\n",
    "        condition_text = condition_node.text.decode()\n",
    "        current_line = condition_node.start_point[0]\n",
    "    else:\n",
    "        condition_text = node.text.decode()\n",
    "        current_line = node.start_point[0]\n",
    "\n",
    "    identifiers = [\n",
    "        child.text.decode()\n",
    "        for child in condition_node.children\n",
    "        if child.type == \"identifier\"\n",
    "    ]\n",
    "    values = {var: get_last_value(var, variable_values, current_line) for var in identifiers}\n",
    "\n",
    "    if not identifiers:\n",
    "        return None, None, None, None\n",
    "\n",
    "    if all(v is not None for v in values.values()):\n",
    "        comment = f\"# {' '.join(f'{k} = {v}' for k, v in values.items())}\"\n",
    "        statement = (\n",
    "            f\"The value of '{', '.join(values.keys())}' is '{', '.join(map(str, values.values()))}' \"\n",
    "            f\"before evaluating 'if {condition_text}'. Can you predict whether this branch will be executed?\"\n",
    "        )\n",
    "        return comment, statement, current_line, values\n",
    "\n",
    "    return None, None, None, None\n",
    "\n",
    "\n",
    "def annotate_code(example, yes_count, no_count, max_per_category):\n",
    "    \"\"\"Parse and annotate the code with extracted variable values while ensuring balance.\"\"\"\n",
    "    code = example[\"Source Code\"]\n",
    "    source_code = example[\"scratchpad_format\"]\n",
    "    variable_values = example[\"variable_values\"]\n",
    "    tree = parser.parse(source_code.encode())\n",
    "\n",
    "    code_lines = code.split(\"\\n\")\n",
    "    annotated_lines = source_code.split(\"\\n\")\n",
    "\n",
    "    branches = find_branches(tree.root_node)\n",
    "    yes_branches, no_branches = filtering_branches(branches, annotated_lines)\n",
    "\n",
    "    chosen_branch_type = None\n",
    "    chosen_branch_list = None\n",
    "    ground_truth = None\n",
    "\n",
    "    if yes_branches and (yes_count < max_per_category or not no_branches):\n",
    "        chosen_branch_type = \"Yes\"\n",
    "        chosen_branch_list = yes_branches\n",
    "    elif no_branches and (no_count < max_per_category or not yes_branches):\n",
    "        chosen_branch_type = \"No\"\n",
    "        chosen_branch_list = no_branches\n",
    "\n",
    "    if not chosen_branch_list:\n",
    "        return None, None, None, yes_count, no_count, None  \n",
    "\n",
    "\n",
    "    branch = random.choice(chosen_branch_list)\n",
    "    comment, statement, line_number, values = process_branches(branch, variable_values)\n",
    "\n",
    "    if comment and statement and line_number:\n",
    "\n",
    "        if chosen_branch_type == \"Yes\":\n",
    "            yes_count += 1\n",
    "        else:\n",
    "            no_count += 1\n",
    "\n",
    "        return \"\\n\".join(code_lines), code_lines[line_number] , chosen_branch_type, yes_count, no_count, values\n",
    "\n",
    "    return None, None, None, yes_count, no_count, None\n",
    "\n",
    "\n",
    "\n",
    "with open(\"dataset_all.jsonl\", \"r\") as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "filtered_results = []\n",
    "yes_count = 0\n",
    "no_count = 0\n",
    "max_per_category = len(dataset) // 2  # Let's make the dataset balanced\n",
    "\n",
    "for example in dataset:\n",
    "    if any(contains_complex_type(val) for val in example[\"input\"].values()):\n",
    "        continue\n",
    "    annotated_code, statements, ground_truth, yes_count, no_count, values = annotate_code(\n",
    "        example, yes_count, no_count, max_per_category\n",
    "    )\n",
    "    if annotated_code and statements and ground_truth:\n",
    "        if contains_complex_type(ground_truth) or not token_filtering(annotated_code):\n",
    "            continue\n",
    "        filtered_results.append(\n",
    "            {\"Programming Language\":example[\"Language\"],\n",
    "            \"Statement Type\": \"Branch\",\n",
    "            \"Source Code\": annotated_code, \n",
    "            \"Selected Statement\": statements.strip(),\n",
    "            \"Function Input\": example[\"input\"],\n",
    "            \"Variable Values Before Statement\": values, \n",
    "            \"Value After Statement Execution\": ground_truth,\n",
    "            \"Variable States During Runtime\": example[\"variable_values\"],\n",
    "            \"Program Information\": f\"Project Name: {example['Project_Name']}\"\n",
    "            })\n",
    "\n",
    "\n",
    "with open(\"Unified_dataset_branch.jsonl\", \"w\") as f:\n",
    "    for result in filtered_results:\n",
    "        f.write(json.dumps(result) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def secrets_dir(env=os.getenv('D2_ENVIRONMENT', None),\n",
      "                basedir=os.getenv('D2_SECRETS_BASEDIR', None)):\n",
      "    if env is not None:\n",
      "        env_str = str(env)\n",
      "    else:\n",
      "        cwd = os.getcwd()\n",
      "        default_file = os.path.join(cwd, '.python_secrets_environment')\n",
      "        if os.path.exists(default_file):\n",
      "            with open(default_file, 'r') as f:\n",
      "                env_str = f.read().strip()\n",
      "        else:\n",
      "            env_str = os.path.basename(cwd)\n",
      "    if basedir is None:\n",
      "        basedir = os.path.join(\n",
      "                HOME,\n",
      "                'secrets' if sys.platform.startswith('win') else '.secrets')\n",
      "    return os.path.join(basedir, env_str)\n",
      "\n",
      "secrets_dir(env=None, basedir=None)\n",
      "default_file = os.path.join(cwd, '.python_secrets_environment')\n",
      "{'cwd': \"'/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/davedittrich+python_secrets/davedittrich+python_secrets'\"}\n",
      "default_file = '/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/davedittrich+python_secrets/davedittrich+python_secrets/.python_secrets_environment'\n",
      "def _identify_environment(environment=None):\n",
      "    \"\"\"\n",
      "    Returns the environment identifier.\n",
      "\n",
      "    There are multiple ways to define the default environment (in order\n",
      "    of priority):\n",
      "\n",
      "    1. The --environment command line option;\n",
      "    2. The content of the file .python_secrets_environment in the current\n",
      "       working directory;\n",
      "    3. The value specified by environment variable D2_ENVIRONMENT; or\n",
      "    4. The basename of the current working directory.\n",
      "    \"\"\"\n",
      "    cwd = os.getcwd()\n",
      "    if environment is None:\n",
      "        env_file = os.path.join(cwd, '.python_secrets_environment')\n",
      "        if os.path.exists(env_file):\n",
      "            with open(env_file, 'r') as f:\n",
      "                environment = f.read().replace('\\n', '')\n",
      "        else:\n",
      "            environment = os.getenv('D2_ENVIRONMENT',\n",
      "                                    os.path.basename(cwd))\n",
      "    return environment\n",
      "\n",
      "_identify_environment(environment=None)\n",
      "env_file = os.path.join(cwd, '.python_secrets_environment')\n",
      "{'cwd': \"'/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/davedittrich+python_secrets/davedittrich+python_secrets'\"}\n",
      "env_file = '/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/davedittrich+python_secrets/davedittrich+python_secrets/.python_secrets_environment'\n",
      "def parse_route_template(template):\n",
      "    rbuilder = [\"^\"]\n",
      "    fbuilder = []\n",
      "    position = 0\n",
      "    schema = {}\n",
      "\n",
      "    for match in template_var_re_finditer(template):\n",
      "        param_name = match.group(\"name\")\n",
      "        param_type = match.group(\"type\") or \"id\"\n",
      "        # TODO: Handle KeyError, maybe we want to use a custom error here.\n",
      "        param_formatchar, param_re, param_schema = _schema_map[param_type]\n",
      "        schema[param_name] = param_schema\n",
      "\n",
      "        rbuilder.append(re.escape(template[position:match.start()]))\n",
      "        rbuilder.append(param_re.format(param_name))\n",
      "\n",
      "        fbuilder.append(template[position:match.start()])\n",
      "        fbuilder.append(\"{\")\n",
      "        fbuilder.append(param_name)\n",
      "        fbuilder.append(\":\")\n",
      "        fbuilder.append(param_formatchar)\n",
      "        fbuilder.append(\"}\")\n",
      "\n",
      "        position = match.end()\n",
      "\n",
      "    rbuilder.append(re.escape(template[position:]))\n",
      "    rbuilder.append(\"$\")\n",
      "    fbuilder.append(template[position:])\n",
      "\n",
      "    return (valid.Schema(schema),\n",
      "            re.compile(\"\".join(rbuilder)),\n",
      "            u\"\".join(fbuilder).format)\n",
      "\n",
      "parse_route_template(template='get/{variable}')\n",
      "fbuilder.append(param_name)\n",
      "{'param_name': \"'variable'\"}\n",
      "fbuilder = ['get/', '{', 'variable']\n",
      "def get_generation(ba_name, **kwargs):\n",
      "    # get data\n",
      "    c = client_factory(ba_name)\n",
      "    data = c.get_generation(**kwargs)\n",
      "    \n",
      "    # log\n",
      "    if len(data) == 0:\n",
      "        msg = '%s: No generation data at %s with args %s' % (ba_name, datetime.utcnow().isoformat(),\n",
      "                                                    kwargs)\n",
      "        logger.warn(msg)\n",
      "    \n",
      "    # return\n",
      "    return data\n",
      "\n",
      "get_generation(ba_name='CAISO', kwargs={'latest': True})\n",
      "c = client_factory(ba_name)\n",
      "{'ba_name': \"'CAISO'\"}\n",
      "c = {options={}, NAME='CAISO'}\n",
      "def get_load(ba_name, **kwargs):\n",
      "    # get data\n",
      "    c = client_factory(ba_name)\n",
      "    data = c.get_load(**kwargs)\n",
      "    \n",
      "    # log\n",
      "    if len(data) == 0:\n",
      "        msg = '%s: No load data at %s with args %s' % (ba_name, datetime.utcnow().isoformat(),\n",
      "                                                    kwargs)\n",
      "        logger.warn(msg)\n",
      "    \n",
      "    # return\n",
      "    return data\n",
      "\n",
      "get_load(ba_name='PJM', kwargs={'latest': True})\n",
      "c = client_factory(ba_name)\n",
      "{'ba_name': \"'PJM'\"}\n",
      "c = {options={}, NAME='PJM'}\n",
      "def format_decimal(self, altitude=None):\n",
      "        \"\"\"\n",
      "        Format decimal degrees with altitude::\n",
      "\n",
      "            >>> p = Point(41.5, -81.0, 12.3)\n",
      "            >>> p.format_decimal()\n",
      "            '41.5, -81.0, 12.3km'\n",
      "            >>> p = Point(41.5, 0, 0)\n",
      "            >>> p.format_decimal()\n",
      "            '41.5, 0.0'\n",
      "\n",
      "        :param bool altitude: Whether to include ``altitude`` value.\n",
      "            By default it is automatically included if it is non-zero.\n",
      "        \"\"\"\n",
      "        coordinates = [str(self.latitude), str(self.longitude)]\n",
      "\n",
      "        if altitude is None:\n",
      "            altitude = bool(self.altitude)\n",
      "        if altitude:\n",
      "            if not isinstance(altitude, str):\n",
      "                altitude = 'km'\n",
      "            coordinates.append(self.format_altitude(altitude))\n",
      "\n",
      "        return \", \".join(coordinates)\n",
      "\n",
      "format_decimal(self=Point(41.5, 81.0, 2.5), altitude=None, self.altitude=2.5, self.latitude=41.5, self.longitude=81.0)\n",
      "coordinates.append(self.format_altitude(altitude))\n",
      "{'altitude': \"'km'\"}\n",
      "coordinates = ['41.5', '81.0', '2.5km']\n",
      "def _global_import(name):\n",
      "    p = __import__(name, globals(), locals(), level=1)\n",
      "    lst = p.__all__ if '__all__' in dir(p) else dir(p)\n",
      "    if lst:\n",
      "        globals().pop(name, None)\n",
      "        for k in lst:\n",
      "            if not k.startswith('__'):\n",
      "                globals()[k] = p.__dict__[k]\n",
      "                __all__.append(k)\n",
      "\n",
      "_global_import(name='base')\n",
      "p = __import__(name, globals(), locals(), level=1)\n",
      "{'name': \"'base'\"}\n",
      "p = <module 'tensorpack.dataflow.base' from '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/tensorpack+tensorpack/tensorpack+tensorpack/tensorpack/dataflow/base.py'>\n",
      "def global_import(name):\n",
      "    p = __import__(name, globals(), locals(), level=1)\n",
      "    lst = p.__all__ if '__all__' in dir(p) else []\n",
      "    del globals()[name]\n",
      "    for k in lst:\n",
      "        if not k.startswith('__'):\n",
      "            globals()[k] = p.__dict__[k]\n",
      "            __all__.append(k)\n",
      "\n",
      "global_import(name='model_desc')\n",
      "p = __import__(name, globals(), locals(), level=1)\n",
      "{'name': \"'model_desc'\"}\n",
      "p = <module 'tensorpack.graph_builder.model_desc' from '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/tensorpack+tensorpack/tensorpack+tensorpack/tensorpack/graph_builder/model_desc.py'>\n",
      "def xldate_as_tuple(xldate, datemode):\n",
      "    if datemode not in (0, 1):\n",
      "        raise XLDateBadDatemode(datemode)\n",
      "    if xldate == 0.00:\n",
      "        return (0, 0, 0, 0, 0, 0)\n",
      "    if xldate < 0.00:\n",
      "        raise XLDateNegative(xldate)\n",
      "    xldays = int(xldate)\n",
      "    frac = xldate - xldays\n",
      "    seconds = int(round(frac * 86400.0))\n",
      "    assert 0 <= seconds <= 86400\n",
      "    if seconds == 86400:\n",
      "        hour = minute = second = 0\n",
      "        xldays += 1\n",
      "    else:\n",
      "        # second = seconds % 60; minutes = seconds // 60\n",
      "        minutes, second = divmod(seconds, 60)\n",
      "        # minute = minutes % 60; hour    = minutes // 60\n",
      "        hour, minute = divmod(minutes, 60)\n",
      "    if xldays >= _XLDAYS_TOO_LARGE[datemode]:\n",
      "        raise XLDateTooLarge(xldate)\n",
      "\n",
      "    if xldays == 0:\n",
      "        return (0, 0, 0, hour, minute, second)\n",
      "\n",
      "    if xldays < 61 and datemode == 0:\n",
      "        raise XLDateAmbiguous(xldate)\n",
      "\n",
      "    jdn = xldays + _JDN_delta[datemode]\n",
      "    yreg = (ifd(ifd(jdn * 4 + 274277, 146097) * 3, 4) + jdn + 1363) * 4 + 3\n",
      "    mp = ifd(yreg % 1461, 4) * 535 + 333\n",
      "    d = ifd(mp % 16384, 535) + 1\n",
      "    # mp /= 16384\n",
      "    mp >>= 14\n",
      "    if mp >= 10:\n",
      "        return (ifd(yreg, 1461) - 4715, mp - 9, d, hour, minute, second)\n",
      "    else:\n",
      "        return (ifd(yreg, 1461) - 4716, mp + 3, d, hour, minute, second)\n",
      "\n",
      "xldate_as_tuple(xldate=2741.0, datemode=0)\n",
      "minutes, second = divmod(seconds, 60)\n",
      "{'seconds': '0'}\n",
      "second = 0\n",
      "def cast_tuple(val, length = None):\n",
      "    if isinstance(val, list):\n",
      "        val = tuple(val)\n",
      "\n",
      "    output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n",
      "\n",
      "    if exists(length):\n",
      "        assert len(output) == length\n",
      "\n",
      "    return output\n",
      "\n",
      "cast_tuple(val=1, length=4)\n",
      "output = val if isinstance(val, tuple) else ((val,) * default(length, 1))\n",
      "{'length': '4'}\n",
      "output = (1, 1, 1, 1)\n",
      "def init_conv_(self, conv):\n",
      "        o, i, h, w = conv.weight.shape\n",
      "        conv_weight = torch.empty(o // 4, i, h, w)\n",
      "        nn.init.kaiming_uniform_(conv_weight)\n",
      "        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n",
      "\n",
      "        conv.weight.data.copy_(conv_weight)\n",
      "        nn.init.zeros_(conv.bias.data)\n",
      "\n",
      "init_conv_(self=PixelShuffleUpsample(  (net): Sequential(    (0): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))    (1): SiLU()    (2): PixelShuffle(upscale_factor=2)  )), conv=Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1)), self._backward_hooks=OrderedDict(), self._backward_pre_hooks=OrderedDict(), self._buffers=OrderedDict(), self._forward_hooks=OrderedDict(), self._forward_hooks_always_called=OrderedDict(), self._forward_hooks_with_kwargs=OrderedDict(), self._forward_pre_hooks=OrderedDict(), self._forward_pre_hooks_with_kwargs=OrderedDict(), self._is_full_backward_hook=None, self._load_state_dict_post_hooks=OrderedDict(), self._load_state_dict_pre_hooks=OrderedDict(), self._modules=OrderedDict([('net', Sequential(  (0): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))  (1): SiLU()  (2): PixelShuffle(upscale_factor=2)))]), self._non_persistent_buffers_set=set(), self._parameters=OrderedDict(), self._state_dict_hooks=OrderedDict(), self._state_dict_pre_hooks=OrderedDict(), self.training=True)\n",
      "nn.init.kaiming_uniform_(conv_weight)\n",
      "{'conv_weight': 'tensor([[[[ 1.4422e+00]],         [[ 1.0425e+00]],         [[-4.1854e-01]],         [[ 6.1111e-02]],         [[ 1.7011e-01]],         [[ 7.3859e-01]],         [[ 1.1234e-01]],         [[ 7.9434e-01]]],        [[[-5.1286e-02]],         [[ 6.4548e-01]],         [[-9.1563e-01]],         [[-1.3698e+00]],         [[-4.4227e-01]],         [[-3.2611e-01]],         [[-1.2114e+00]],         [[ 5.9611e-01]]],        [[[-1.5431e-01]],         [[-1.6676e-01]],         [[ 1.2692e+00]],         [[-2.8656e+00]],         [[ 2.2421e-43]],         [[ 0.0000e+00]],         [[ 3.6013e-43]],         [[ 0.0000e+00]]],        [[[ 2.1569e-07]],         [[ 3.0774e-41]],         [[ 2.1415e-07]],         [[ 3.0774e-41]],         [[ 1.4013e-45]],         [[ 0.0000e+00]],         [[ 1.4013e-45]],         [[ 0.0000e+00]]],        [[[ 1.4013e-45]],         [[ 0.0000e+00]],         [[ 1.4013e-45]],         [[ 0.0000e+00]],         [[ 1.4013e-45]],         [[ 0.0000e+00]],         [[ 1.4013e-45]],         [[ 0.0000e+00]]],        [[[ 1.4013e-45]],         [[ 0.0000e+00]],         [[ 1.4013e-45]],         [[ 0.0000e+00]],         [[ 1.4013e-45]],         [[ 0.0000e+00]],         [[ 1.4013e-45]],         [[ 0.0000e+00]]],        [[[ 1.4013e-45]],         [[ 0.0000e+00]],         [[ 2.0319e-43]],         [[ 0.0000e+00]],         [[ 2.1136e-07]],         [[ 3.0774e-41]],         [[ 2.0801e-07]],         [[ 3.0774e-41]]],        [[[ 4.4842e-44]],         [[ 0.0000e+00]],         [[ 1.5695e-43]],         [[ 0.0000e+00]],         [[ 7.7116e+24]],         [[ 3.0778e-41]],         [[ 0.0000e+00]],         [[ 0.0000e+00]]]])'}\n",
      "conv_weight = tensor([[[[ 0.2046]],         [[ 0.7383]],         [[-0.2354]],         [[ 0.7482]],         [[ 0.3718]],         [[-0.1113]],         [[ 0.5291]],         [[ 0.5602]]],        [[[-0.4435]],         [[-0.5901]],         [[ 0.3579]],         [[-0.2002]],         [[-0.1366]],         [[ 0.8172]],         [[-0.4366]],         [[ 0.8074]]],        [[[ 0.5757]],         [[ 0.6216]],         [[-0.0433]],         [[ 0.6816]],         [[-0.2335]],         [[-0.8362]],         [[-0.6477]],         [[-0.1428]]],        [[[ 0.6086]],         [[-0.8312]],         [[ 0.7359]],         [[ 0.4631]],         [[-0.2967]],         [[-0.5708]],         [[ 0.5318]],         [[-0.2615]]],        [[[ 0.2776]],         [[-0.6416]],         [[-0.1069]],         [[-0.3409]],         [[ 0.6007]],         [[ 0.7782]],         [[-0.4220]],         [[ 0.4654]]],        [[[-0.1207]],         [[-0.6514]],         [[-0.5793]],         [[-0.5897]],         [[-0.5506]],         [[-0.6959]],         [[ 0.8602]],         [[ 0.5812]]],        [[[-0.8445]],         [[ 0.3486]],         [[-0.7968]],         [[ 0.3068]],         [[-0.2670]],         [[ 0.4851]],         [[ 0.8124]],         [[ 0.8039]]],        [[[-0.2703]],         [[-0.2185]],         [[ 0.5634]],         [[ 0.8424]],         [[-0.8168]],         [[-0.2052]],         [[ 0.6885]],         [[ 0.8138]]]])\n",
      "def test_start_seconds(start_seconds):\n",
      "    parser_zero = GenericSubtitleParser(start_seconds=0)\n",
      "    parser_zero.fit(BytesIO(fake_srt))\n",
      "    parser = GenericSubtitleParser(start_seconds=start_seconds)\n",
      "    parser.fit(BytesIO(fake_srt))\n",
      "    expected = [\n",
      "        sub\n",
      "        for sub in parser_zero.subs_\n",
      "        if sub.start >= timedelta(seconds=start_seconds)\n",
      "    ]\n",
      "    assert all(esub == psub for esub, psub in zip(expected, parser.subs_))\n",
      "\n",
      "test_start_seconds(start_seconds=0)\n",
      "assert all(esub == psub for esub, psub in zip(expected, parser.subs_))\n",
      "{'expected': '[<ffsubsync.generic_subtitles.GenericSubtitle object at 0x7fb0b6c79520>, <ffsubsync.generic_subtitles.GenericSubtitle object at 0x7fb015502430>, <ffsubsync.generic_subtitles.GenericSubtitle object at 0x7fb01543e190>]'}\n",
      "@py_assert1 = None\n",
      "def git_versions_from_vcs(tag_prefix, root, verbose=False):\n",
      "    # this runs 'git' from the root of the source tree. This only gets called\n",
      "    # if the git-archive 'subst' keywords were *not* expanded, and\n",
      "    # _version.py hasn't already been rewritten with a short version string,\n",
      "    # meaning we're inside a checked out source tree.\n",
      "\n",
      "    if not os.path.exists(os.path.join(root, \".git\")):\n",
      "        if verbose:\n",
      "            print(\"no .git in %s\" % root)\n",
      "        return {}\n",
      "\n",
      "    GITS = [\"git\"]\n",
      "    if sys.platform == \"win32\":\n",
      "        GITS = [\"git.cmd\", \"git.exe\"]\n",
      "    stdout = run_command(GITS, [\"describe\", \"--tags\", \"--dirty\", \"--always\"],\n",
      "                         cwd=root)\n",
      "    if stdout is None:\n",
      "        return {}\n",
      "    if not stdout.startswith(tag_prefix):\n",
      "        if verbose:\n",
      "            print(\"tag '%s' doesn't start with prefix '%s'\"\n",
      "                  % (stdout, tag_prefix))\n",
      "        return {}\n",
      "    tag = stdout[len(tag_prefix):]\n",
      "    stdout = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n",
      "    if stdout is None:\n",
      "        return {}\n",
      "    full = stdout.strip()\n",
      "    if tag.endswith(\"-dirty\"):\n",
      "        full += \"-dirty\"\n",
      "    return {\"version\": tag, \"full\": full}\n",
      "\n",
      "git_versions_from_vcs(tag_prefix='v', root='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/andsor+pydevs/andsor+pydevs', verbose=False)\n",
      "stdout = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n",
      "{'GITS': \"['git']\"}\n",
      "stdout = '1ef835aee49f536a5a499db71927deac87f4152e'\n",
      "def count_messages_tokens(messages=[], model=None):\n",
      "    \"\"\"\n",
      "    Count the number of tokens in a list of messages\n",
      "    \"\"\"\n",
      "    try:\n",
      "        tokens_used = 0\n",
      "\n",
      "        for message in messages:\n",
      "            if isinstance(message, str):\n",
      "                tokens_used += count_tokens(message, model=model)\n",
      "            elif \"message\" in message:\n",
      "                tokens_used += count_tokens(message[\"message\"], model=model)\n",
      "\n",
      "                if \"code\" in message:\n",
      "                    tokens_used += count_tokens(message[\"code\"], model=model)\n",
      "\n",
      "                if \"output\" in message:\n",
      "                    tokens_used += count_tokens(message[\"output\"], model=model)\n",
      "\n",
      "        prompt_cost = token_cost(tokens_used, model=model)\n",
      "\n",
      "        return (tokens_used, prompt_cost)\n",
      "    except:\n",
      "        # Non-essential feature\n",
      "        return (0, 0)\n",
      "\n",
      "count_messages_tokens(messages=[{'role': 'system', 'message': 'You are Open Interpreter, a world-class programmer that can complete any goal by executing code.\\nFirst, write a plan. **Always recap the plan between each code block** (you have extreme short-term memory loss, so you need to recap the plan between each message block to retain it).\\nWhen you execute code, it will be executed **on the user\\'s machine**. The user has given you **full and complete permission** to execute any code necessary to complete the task. Execute the code.\\nIf you want to send data between programming languages, save the data to a txt or json.\\nYou can access the internet. Run **any code** to achieve the goal, and if at first you don\\'t succeed, try again and again.\\nYou can install new packages.\\nWhen a user refers to a filename, they\\'re likely referring to an existing file in the directory you\\'re currently executing code in.\\nWrite messages to the user in Markdown.\\nIn general, try to **make plans** with as few steps as possible. As for actually executing code to carry out that plan, for *stateful* languages (like python, javascript, shell, but NOT for html which starts from 0 every time) **it\\'s critical not to try to do everything in one code block.** You should try something, print information about it, then continue from there in tiny, informed steps. You will never get it on the first try, and attempting it in one go will often lead to errors you cant see.\\nYou are capable of **any** task.\\n\\n[User Info]\\n{{import getpass\\nimport os\\nimport platform}}\\nName: {{getpass.getuser()}}\\nCWD: {{os.getcwd()}}\\nSHELL: {{os.environ.get(\\'SHELL\\')}}\\nOS: {{platform.system()}}\"'}], model='gpt-3.5-turbo')\n",
      "prompt_cost = token_cost(tokens_used, model=model)\n",
      "{'tokens_used': '360'}\n",
      "prompt_cost = 0.00054\n",
      "def _co2mpas_info2df(start_time, main_flags=None):\n",
      "    import socket\n",
      "    import datetime\n",
      "    from co2mpas import __version__\n",
      "    from ..load.schema import define_flags_schema\n",
      "    time_elapsed = (datetime.datetime.today() - start_time).total_seconds()\n",
      "    hostname = socket.gethostname()\n",
      "    info = [\n",
      "        ('CO2MPAS version', __version__),\n",
      "        ('Simulation started', start_time.strftime('%Y/%m/%d-%H:%M:%S')),\n",
      "        ('Time elapsed', '%.3f sec' % time_elapsed),\n",
      "        ('Hostname', hostname),\n",
      "    ]\n",
      "\n",
      "    if main_flags:\n",
      "        main_flags = define_flags_schema(read=False).validate(main_flags)\n",
      "        info.extend(sorted(main_flags.items()))\n",
      "    import pandas as pd\n",
      "    df = pd.DataFrame(info, columns=['Parameter', 'Value'])\n",
      "    df.set_index(['Parameter'], inplace=True)\n",
      "    setattr(df, 'name', 'info')\n",
      "    return df\n",
      "\n",
      "_co2mpas_info2df(start_time=datetime.datetime(2024, 4, 3, 16, 28, 9, 62286), main_flags=None)\n",
      "df = pd.DataFrame(info, columns=['Parameter', 'Value'])\n",
      "{'info': \"[('CO2MPAS version', '4.1.6'), ('Simulation started', '2024/04/03-16:28:09'), ('Time elapsed', '103.530 sec'), ('Hostname', 'thor')]\"}\n",
      "df =             Parameter                Value0     CO2MPAS version                4.1.61  Simulation started  2024/04/03-16:28:092        Time elapsed          103.530 sec3            Hostname                 thor\n",
      "def dump(self, file, default_flow_style=False, **kw):\n",
      "        import yaml\n",
      "        kw['Dumper'] = kw.get('Dumper', yaml.CDumper)\n",
      "        with open(file, 'w') as f:\n",
      "            yaml.dump(\n",
      "                self.to_dict(), f, default_flow_style=default_flow_style, **kw\n",
      "            )\n",
      "\n",
      "dump(self={}, file='./conf.yaml', default_flow_style=False, kw={})\n",
      "with open(file, 'w') as f:\n",
      "{'file': \"'./conf.yaml'\"}\n",
      "f = <_io.TextIOWrapper name='./conf.yaml' mode='w' encoding='UTF-8'>\n",
      "def article_missing(field: str):\n",
      "    article = Article(\n",
      "        title=None, authors=[\"L, Robertson\"], year=1999, journal=\"Science\"\n",
      "    )\n",
      "    setattr(article, field, None)\n",
      "    return ArticleWrapper(article=article)\n",
      "\n",
      "article_missing(field='year')\n",
      "setattr(article, field, None)\n",
      "{'article': \"{title=None, authors=['L, Robertson'], keywords=[], year=1999, journal='Science', volume=None, issue=None, page=None, doi=None, references=[], sources=set(), extra={}}\", 'field': \"'year'\"}\n",
      "article = {title=None, authors=['L, Robertson'], keywords=[], year=None, journal='Science', volume=None, issue=None, page=None, doi=None, references=[], sources=set(), extra={}}\n",
      "def create_dummy_class(klass, dependency, message=\"\"):\n",
      "    \"\"\"\n",
      "    When a dependency of a class is not available, create a dummy class which throws ImportError\n",
      "    when used.\n",
      "\n",
      "    Args:\n",
      "        klass (str): name of the class.\n",
      "        dependency (str): name of the dependency.\n",
      "        message: extra message to print\n",
      "    Returns:\n",
      "        class: a class object\n",
      "    \"\"\"\n",
      "    err = \"Cannot import '{}', therefore '{}' is not available.\".format(dependency, klass)\n",
      "    if message:\n",
      "        err = err + \" \" + message\n",
      "\n",
      "    class _DummyMetaClass(type):\n",
      "        # throw error on class attribute access\n",
      "        def __getattr__(_, __):  # noqa: B902\n",
      "            raise ImportError(err)\n",
      "\n",
      "    class _Dummy(object, metaclass=_DummyMetaClass):\n",
      "        # throw error on constructor\n",
      "        def __init__(self, *args, **kwargs):\n",
      "            raise ImportError(err)\n",
      "\n",
      "    return _Dummy\n",
      "\n",
      "create_dummy_class(klass='DeformConv', dependency='detectron2._C', message='detectron2 is not compiled successfully, please build following the instructions!')\n",
      "err = \"Cannot import '{}', therefore '{}' is not available.\".format(dependency, klass)\n",
      "{'dependency': \"'detectron2._C'\", 'klass': \"'DeformConv'\"}\n",
      "err = \"Cannot import 'detectron2._C', therefore 'DeformConv' is not available.\"\n",
      "def create_dummy_func(func, dependency, message=\"\"):\n",
      "    \"\"\"\n",
      "    When a dependency of a function is not available, create a dummy function which throws\n",
      "    ImportError when used.\n",
      "\n",
      "    Args:\n",
      "        func (str): name of the function.\n",
      "        dependency (str or list[str]): name(s) of the dependency.\n",
      "        message: extra message to print\n",
      "    Returns:\n",
      "        function: a function object\n",
      "    \"\"\"\n",
      "    err = \"Cannot import '{}', therefore '{}' is not available.\".format(dependency, func)\n",
      "    if message:\n",
      "        err = err + \" \" + message\n",
      "\n",
      "    if isinstance(dependency, (list, tuple)):\n",
      "        dependency = \",\".join(dependency)\n",
      "\n",
      "    def _dummy(*args, **kwargs):\n",
      "        raise ImportError(err)\n",
      "\n",
      "    return _dummy\n",
      "\n",
      "create_dummy_func(func='deform_conv', dependency='detectron2._C', message='detectron2 is not compiled successfully, please build following the instructions!')\n",
      "err = \"Cannot import '{}', therefore '{}' is not available.\".format(dependency, func)\n",
      "{'dependency': \"'detectron2._C'\", 'func': \"'deform_conv'\"}\n",
      "err = \"Cannot import 'detectron2._C', therefore 'deform_conv' is not available.\"\n",
      "def register_mesh(mesh_info: MeshInfo, base_path: Optional[str]) -> None:\n",
      "    geodists, symmetry, texcoords = mesh_info.geodists, mesh_info.symmetry, mesh_info.texcoords\n",
      "    if geodists:\n",
      "        geodists = maybe_prepend_base_path(base_path, geodists)\n",
      "    if symmetry:\n",
      "        symmetry = maybe_prepend_base_path(base_path, symmetry)\n",
      "    if texcoords:\n",
      "        texcoords = maybe_prepend_base_path(base_path, texcoords)\n",
      "    MeshCatalog[mesh_info.name] = MeshInfo(\n",
      "        name=mesh_info.name,\n",
      "        data=maybe_prepend_base_path(base_path, mesh_info.data),\n",
      "        geodists=geodists,\n",
      "        symmetry=symmetry,\n",
      "        texcoords=texcoords,\n",
      "    )\n",
      "\n",
      "register_mesh(mesh_info=MeshInfo(name='smpl_27554', data='smpl_27554.pkl', geodists='geodists/geodists_smpl_27554.pkl', symmetry='symmetry/symmetry_smpl_27554.pkl', texcoords='texcoords/texcoords_smpl_27554.pkl'), base_path='https://dl.fbaipublicfiles.com/densepose/meshes/')\n",
      "geodists = maybe_prepend_base_path(base_path, geodists)\n",
      "{'base_path': \"'https://dl.fbaipublicfiles.com/densepose/meshes/'\", 'geodists': \"'geodists/geodists_smpl_27554.pkl'\"}\n",
      "geodists = 'https://dl.fbaipublicfiles.com/densepose/meshes/geodists/geodists_smpl_27554.pkl'\n",
      "def get_model(self):\n",
      "        from yolox.models import YOLOX, YOLOPAFPN, YOLOXHead\n",
      "\n",
      "        def init_yolo(M):\n",
      "            for m in M.modules():\n",
      "                if isinstance(m, nn.BatchNorm2d):\n",
      "                    m.eps = 1e-3\n",
      "                    m.momentum = 0.03\n",
      "\n",
      "        if getattr(self, \"model\", None) is None:\n",
      "            in_channels = [256, 512, 1024]\n",
      "            backbone = YOLOPAFPN(self.depth, self.width, in_channels=in_channels, act=self.act)\n",
      "            head = YOLOXHead(self.num_classes, self.width, in_channels=in_channels, act=self.act)\n",
      "            self.model = YOLOX(backbone, head)\n",
      "\n",
      "        self.model.apply(init_yolo)\n",
      "        self.model.head.initialize_biases(1e-2)\n",
      "        self.model.train()\n",
      "        return self.model\n",
      "\n",
      "get_model(self= keys               values                      seed               None                        output_dir         './YOLOX_outputs'           print_interval     10                          eval_interval      10                          dataset            None                        num_classes        80                          depth              0.33                        width              0.5                         act                'silu'                      data_num_workers   4                           input_size         (640, 640)                  multiscale_range   5                           data_dir           None                        train_ann          'instances_train2017.json'  val_ann            'instances_val2017.json'    test_ann           'instances_test2017.json'   mosaic_prob        1.0                         mixup_prob         1.0                         hsv_prob           1.0                         flip_prob          0.5                         degrees            10.0                        translate          0.1                         mosaic_scale       (0.1, 2)                    enable_mixup       True                        mixup_scale        (0.5, 1.5)                  shear              2.0                         warmup_epochs      5                           max_epoch          300                         warmup_lr          0                           min_lr_ratio       0.05                        basic_lr_per_img   0.00015625                  scheduler          'yoloxwarmcos'              no_aug_epochs      15                          ema                True                        weight_decay       0.0005                      momentum           0.9                         save_history_ckpt  True                        exp_name           'yolox_s'                   test_size          (640, 640)                  test_conf          0.01                        nmsthre            0.65                       , self.act='silu', self.basic_lr_per_img=0.00015625, self.data_dir=None, self.data_num_workers=4, self.dataset=None, self.degrees=10.0, self.depth=0.33, self.ema=True, self.enable_mixup=True, self.eval_interval=10, self.exp_name='yolox_s', self.flip_prob=0.5, self.hsv_prob=1.0, self.input_size=(640, 640), self.max_epoch=300, self.min_lr_ratio=0.05, self.mixup_prob=1.0, self.mixup_scale=(0.5, 1.5), self.momentum=0.9, self.mosaic_prob=1.0, self.mosaic_scale=(0.1, 2), self.multiscale_range=5, self.nmsthre=0.65, self.no_aug_epochs=15, self.num_classes=80, self.output_dir='./YOLOX_outputs', self.print_interval=10, self.save_history_ckpt=True, self.scheduler='yoloxwarmcos', self.seed=None, self.shear=2.0, self.test_ann='instances_test2017.json', self.test_conf=0.01, self.test_size=(640, 640), self.train_ann='instances_train2017.json', self.translate=0.1, self.val_ann='instances_val2017.json', self.warmup_epochs=5, self.warmup_lr=0, self.weight_decay=0.0005, self.width=0.5)\n",
      "self.model = YOLOX(backbone, head)\n",
      "{'backbone': 'YOLOPAFPN(  (backbone): CSPDarknet(    (stem): Focus(      (conv): BaseConv(        (conv): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (act): SiLU(inplace=True)      )    )    (dark2): Sequential(      (0): BaseConv(        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (act): SiLU(inplace=True)      )      (1): CSPLayer(        (conv1): BaseConv(          (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (conv2): BaseConv(          (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (conv3): BaseConv(          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (m): Sequential(          (0): Bottleneck(            (conv1): BaseConv(              (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)              (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)              (act): SiLU(inplace=True)            )            (conv2): BaseConv(              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)              (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)              (act): SiLU(inplace=True)            )          )        )      )    )    (dark3): Sequential(      (0): BaseConv(        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (act): SiLU(inplace=True)      )      (1): CSPLayer(        (conv1): BaseConv(          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (conv2): BaseConv(          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (conv3): BaseConv(          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (m): Sequential(          (0): Bottleneck(            (conv1): BaseConv(              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)              (act): SiLU(inplace=True)            )            (conv2): BaseConv(              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)              (act): SiLU(inplace=True)            )          )          (1): Bottleneck(            (conv1): BaseConv(              (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)              (act): SiLU(inplace=True)            )            (conv2): BaseConv(              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_runnin...ffine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (conv2): BaseConv(      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (conv3): BaseConv(      (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (m): Sequential(      (0): Bottleneck(        (conv1): BaseConv(          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (conv2): BaseConv(          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )    )  )  (bu_conv2): BaseConv(    (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    (act): SiLU(inplace=True)  )  (C3_n3): CSPLayer(    (conv1): BaseConv(      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (conv2): BaseConv(      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (conv3): BaseConv(      (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (m): Sequential(      (0): Bottleneck(        (conv1): BaseConv(          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (conv2): BaseConv(          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )    )  )  (bu_conv1): BaseConv(    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    (act): SiLU(inplace=True)  )  (C3_n4): CSPLayer(    (conv1): BaseConv(      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (conv2): BaseConv(      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (conv3): BaseConv(      (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (m): Sequential(      (0): Bottleneck(        (conv1): BaseConv(          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (conv2): BaseConv(          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )    )  ))', 'head': 'YOLOXHead(  (cls_convs): ModuleList(    (0-2): 3 x Sequential(      (0): BaseConv(        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (act): SiLU(inplace=True)      )      (1): BaseConv(        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (act): SiLU(inplace=True)      )    )  )  (reg_convs): ModuleList(    (0-2): 3 x Sequential(      (0): BaseConv(        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (act): SiLU(inplace=True)      )      (1): BaseConv(        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (act): SiLU(inplace=True)      )    )  )  (cls_preds): ModuleList(    (0-2): 3 x Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))  )  (reg_preds): ModuleList(    (0-2): 3 x Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))  )  (obj_preds): ModuleList(    (0-2): 3 x Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))  )  (stems): ModuleList(    (0): BaseConv(      (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (1): BaseConv(      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (2): BaseConv(      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )  )  (l1_loss): L1Loss()  (bcewithlog_loss): BCEWithLogitsLoss()  (iou_loss): IOUloss())'}\n",
      "self.model = YOLOX(  (backbone): YOLOPAFPN(    (backbone): CSPDarknet(      (stem): Focus(        (conv): BaseConv(          (conv): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )      (dark2): Sequential(        (0): BaseConv(          (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (1): CSPLayer(          (conv1): BaseConv(            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)            (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)            (act): SiLU(inplace=True)          )          (conv2): BaseConv(            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)            (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)            (act): SiLU(inplace=True)          )          (conv3): BaseConv(            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)            (act): SiLU(inplace=True)          )          (m): Sequential(            (0): Bottleneck(              (conv1): BaseConv(                (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)                (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)                (act): SiLU(inplace=True)              )              (conv2): BaseConv(                (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)                (act): SiLU(inplace=True)              )            )          )        )      )      (dark3): Sequential(        (0): BaseConv(          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (1): CSPLayer(          (conv1): BaseConv(            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)            (act): SiLU(inplace=True)          )          (conv2): BaseConv(            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)            (act): SiLU(inplace=True)          )          (conv3): BaseConv(            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)            (act): SiLU(inplace=True)          )          (m): Sequential(            (0): Bottleneck(              (conv1): BaseConv(                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)                (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)                (act): SiLU(inplace=True)              )              (conv2): BaseConv(                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)                (act): SiLU(inplace=True)              )            )            (1): Bottleneck(              (conv1): BaseConv(                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)                (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)                (act): SiLU(inplace=True)              )              (conv2): BaseConv(                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)                (act): SiLU(inplace=True)              )            )            (2): Bottleneck(              (conv1): BaseConv(                (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)                (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)                (act): SiLU(inplace=True)              )              (conv2): BaseConv(                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)                (act): SiLU(inplace=True)              )            )          )        )      )      (dark4): Sequential(        (0): BaseConv(          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (1): CSPLayer(          (conv1): BaseConv(            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)            (act): SiLU(inplace=True)          )          (conv2): BaseConv(            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)            (act): SiLU(inplace=True)          )          (conv3): BaseConv(            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)            (act): SiLU(inplace=True)          )          (m): Sequential(            (0): Bottleneck(              (conv1): BaseConv(                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)                (act): SiLU(inplace=True)              )              (conv2): BaseConv(                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)                (act): SiLU(inplace=True)              )            )            (1): Bottleneck(              (conv1): BaseConv(                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)                (act): SiLU(inplace=True)              )              (conv2): BaseConv(                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)                (act): SiLU(inplace=True)              )            )            (2): Bottleneck(              (conv1): BaseConv(                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)                (act): SiLU(inplace=True)              )              (conv2): BaseConv(                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)                (act): SiLU(inplace=True)              )            )          )        )      )      (dark5): Sequential(        (0): BaseConv(          (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (1): SPPBottleneck(          (conv1): BaseConv(            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)            (act): SiLU(inplace=True)          )          (m): ModuleList(            (0): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)            (1): MaxPool2d(kernel_size=9, stride=1, padding=4, dilation=1, ceil_mode=False)            (2): MaxPool2d(kernel_size=13, stride=1, padding=6, dilation=1, ceil_mode=False)          )          (conv2): BaseConv(            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)            (act): SiLU(inplace=True)          )        )        (2): CSPLayer(          (conv1): BaseConv(            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)            (act): SiLU(inplace=True)          )          (conv2): BaseConv(            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)            (act): SiLU(inplace=True)          )          (conv3): BaseConv(            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)            (act): SiLU(inplace=True)          )          (m): Sequential(            (0): Bottleneck(              (conv1): BaseConv(                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)                (act): SiLU(inplace=True)              )              (conv2): BaseConv(                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)                (act): SiLU(inplace=True)              )            )          )        )      )    )    (upsample): Upsample(scale_factor=2.0, mode='nearest')    (lateral_conv0): BaseConv(      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (C3_p4): CSPLayer(      (conv1): BaseConv(        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (act): SiLU(inplace=True)      )      (conv2): BaseConv(        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (act): SiLU(inplace=True)      )      (conv3): BaseConv(        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (act): SiLU(inplace=True)      )      (m): Sequential(        (0): Bottleneck(          (conv1): BaseConv(            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)            (act): SiLU(inplace=True)          )          (conv2): BaseConv(            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)            (act): SiLU(inplace=True)          )        )      )    )    (reduce_conv1): BaseConv(      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (C3_p3): CSPLayer(      (conv1): BaseConv(        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (act): SiLU(inplace=True)      )      (conv2): BaseConv(        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (act): SiLU(inplace=True)      )      (conv3): BaseConv(        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (act): SiLU(inplace=True)      )      (m): Sequential(        (0): Bottleneck(          (conv1): BaseConv(            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)            (act): SiLU(inplace=True)          )          (conv2): BaseConv(            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)            (act): SiLU(inplace=True)          )        )      )    )    (bu_conv2): BaseConv(      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (C3_n3): CSPLayer(      (conv1): BaseConv(        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (act): SiLU(inplace=True)      )      (conv2): BaseConv(        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (act): SiLU(inplace=True)      )      (conv3): BaseConv(        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (act): SiLU(inplace=True)      )      (m): Sequential(        (0): Bottleneck(          (conv1): BaseConv(            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)            (act): SiLU(inplace=True)          )          (conv2): BaseConv(            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)            (act): SiLU(inplace=True)          )        )      )    )    (bu_conv1): BaseConv(      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (C3_n4): CSPLayer(      (conv1): BaseConv(        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (act): SiLU(inplace=True)      )      (conv2): BaseConv(        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)        (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (act): SiLU(inplace=True)      )      (conv3): BaseConv(        (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)        (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (act): SiLU(inplace=True)      )      (m): Sequential(        (0): Bottleneck(          (conv1): BaseConv(            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)            (act): SiLU(inplace=True)          )          (conv2): BaseConv(            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)            (act): SiLU(inplace=True)          )        )      )    )  )  (head): YOLOXHead(    (cls_convs): ModuleList(      (0-2): 3 x Sequential(        (0): BaseConv(          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (1): BaseConv(          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )    )    (reg_convs): ModuleList(      (0-2): 3 x Sequential(        (0): BaseConv(          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (1): BaseConv(          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )    )    (cls_preds): ModuleList(      (0-2): 3 x Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))    )    (reg_preds): ModuleList(      (0-2): 3 x Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))    )    (obj_preds): ModuleList(      (0-2): 3 x Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))    )    (stems): ModuleList(      (0): BaseConv(        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (act): SiLU(inplace=True)      )      (1): BaseConv(        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (act): SiLU(inplace=True)      )      (2): BaseConv(        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)        (act): SiLU(inplace=True)      )    )    (l1_loss): L1Loss()    (bcewithlog_loss): BCEWithLogitsLoss()    (iou_loss): IOUloss()  ))\n",
      "def fmt_dtype(hdf_dt):\n",
      "    \"\"\"Get a (preferably short) string describing an HDF5 datatype\"\"\"\n",
      "    size = hdf_dt.get_size()\n",
      "\n",
      "    if isinstance(hdf_dt, h5t.TypeIntegerID):\n",
      "        # Check normal int & uint dtypes first\n",
      "        for candidate, descr in int_types_by_size().get(size, ()):\n",
      "            if hdf_dt == candidate:\n",
      "                return descr\n",
      "        un = 'un' if hdf_dt.get_sign() == h5t.SGN_NONE else ''\n",
      "        return \"{}-byte {}signed integer\".format(size, un)\n",
      "    elif isinstance(hdf_dt, h5t.TypeFloatID):\n",
      "        # Check normal float dtypes first\n",
      "        for candidate, descr in float_types_by_size().get(size, ()):\n",
      "            if hdf_dt == candidate:\n",
      "                return descr\n",
      "        return \"custom {}-byte float\".format(size)\n",
      "    elif isinstance(hdf_dt, h5t.TypeBitfieldID):\n",
      "        return \"{}-byte bitfield\".format(size)\n",
      "    elif isinstance(hdf_dt, h5t.TypeTimeID):\n",
      "        return \"time\"  # NB. time datatype is deprecated\n",
      "    elif isinstance(hdf_dt, h5t.TypeOpaqueID):\n",
      "        s = \"{}-byte opaque\".format(size)\n",
      "        tag = hdf_dt.get_tag()\n",
      "        if tag:\n",
      "            s += ' ({})'.format(tag.decode('utf-8', 'replace'))\n",
      "        return s\n",
      "    elif isinstance(hdf_dt, h5t.TypeStringID):\n",
      "        cset = cset_names.get(hdf_dt.get_cset(), '?cset')\n",
      "        if hdf_dt.is_variable_str():\n",
      "            return \"{} string\".format(cset)\n",
      "        else:\n",
      "            return \"{}-byte {} string\".format(size, cset)\n",
      "    elif isinstance(hdf_dt, h5t.TypeVlenID):\n",
      "        return \"vlen array of \" + fmt_dtype(hdf_dt.get_super())\n",
      "    elif isinstance(hdf_dt, h5t.TypeArrayID):\n",
      "        shape = fmt_shape(hdf_dt.get_array_dims())\n",
      "        return \"{} array of {}\".format(shape, fmt_dtype(hdf_dt.get_super()))\n",
      "\n",
      "    elif isinstance(hdf_dt, h5t.TypeCompoundID):\n",
      "        return \"({})\".format(\", \".join(\n",
      "            \"{}: {}\".format(\n",
      "                hdf_dt.get_member_name(i).decode('utf-8', 'replace'),\n",
      "                fmt_dtype(hdf_dt.get_member_type(i))\n",
      "            )\n",
      "            for i in range(hdf_dt.get_nmembers())\n",
      "        ))\n",
      "    elif isinstance(hdf_dt, h5t.TypeEnumID):\n",
      "        nmembers = hdf_dt.get_nmembers()\n",
      "        if nmembers >= 5:\n",
      "            return \"enum ({} options)\".format(nmembers)\n",
      "        else:\n",
      "            return \"enum ({})\".format(\", \".join(\n",
      "                hdf_dt.get_member_name(i).decode('utf-8', 'replace')\n",
      "                for i in range(nmembers)\n",
      "            ))\n",
      "    elif isinstance(hdf_dt, h5t.TypeReferenceID):\n",
      "        return \"region ref\" if hdf_dt == h5t.STD_REF_DSETREG else \"object ref\"\n",
      "\n",
      "    return \"unrecognised {}-byte datatype\".format(size)\n",
      "\n",
      "fmt_dtype(hdf_dt=REPR FAILED)\n",
      "for candidate, descr in float_types_by_size().get(size, ()):\n",
      "{'size': '4'}\n",
      "candidate = REPR FAILED\n",
      "def dtype_description(hdf_dt):\n",
      "    \"\"\"A slightly longer description, suitable for a tooltip\n",
      "\n",
      "    Can return None\n",
      "    \"\"\"\n",
      "    size = hdf_dt.get_size()\n",
      "\n",
      "    if isinstance(hdf_dt, h5t.TypeIntegerID):\n",
      "        # Check normal int & uint dtypes first\n",
      "        for candidate, descr in int_types_by_size().get(size, ()):\n",
      "            if hdf_dt == candidate:\n",
      "                un = 'un' if hdf_dt.get_sign() == h5t.SGN_NONE else ''\n",
      "                return '{}-bit {}signed integer'.format(size * 8, un)\n",
      "\n",
      "    elif isinstance(hdf_dt, h5t.TypeFloatID):\n",
      "        # Check normal float dtypes first\n",
      "        for candidate, descr in float_types_by_size().get(size, ()):\n",
      "            if hdf_dt == candidate:\n",
      "                return '{}-bit floating point'.format(size * 8)\n",
      "\n",
      "    return None\n",
      "\n",
      "dtype_description(hdf_dt=REPR FAILED)\n",
      "for candidate, descr in float_types_by_size().get(size, ()):\n",
      "{'size': '4'}\n",
      "candidate = REPR FAILED\n",
      "def _prepare_references_in_schema(schema: Dict[str, Any]) -> Dict[str, Any]:\n",
      "    # Create a copy so that $ref is not modified in the original schema in case\n",
      "    # that it would still reference a dictionary which might be attached to\n",
      "    # an Altair class _schema attribute\n",
      "    schema = copy.deepcopy(schema)\n",
      "\n",
      "    def _prepare_refs(d: Dict[str, Any]) -> Dict[str, Any]:\n",
      "        \"\"\"Add _VEGA_LITE_ROOT_URI in front of all $ref values. This function\n",
      "        recursively iterates through the whole dictionary.\"\"\"\n",
      "        for key, value in d.items():\n",
      "            if key == \"$ref\":\n",
      "                d[key] = _VEGA_LITE_ROOT_URI + d[key]\n",
      "            else:\n",
      "                # $ref values can only be nested in dictionaries or lists\n",
      "                # as the passed in `d` dictionary comes from the Vega-Lite json schema\n",
      "                # and in json we only have arrays (-> lists in Python) and objects\n",
      "                # (-> dictionaries in Python) which we need to iterate through.\n",
      "                if isinstance(value, dict):\n",
      "                    d[key] = _prepare_refs(value)\n",
      "                elif isinstance(value, list):\n",
      "                    prepared_values = []\n",
      "                    for v in value:\n",
      "                        if isinstance(v, dict):\n",
      "                            v = _prepare_refs(v)\n",
      "                        prepared_values.append(v)\n",
      "                    d[key] = prepared_values\n",
      "        return d\n",
      "\n",
      "    schema = _prepare_refs(schema)\n",
      "    return schema\n",
      "\n",
      "_prepare_references_in_schema(schema={'$ref': '#/definitions/ExprRef'})\n",
      "schema = _prepare_refs(schema)\n",
      "{'schema': \"{'$ref': '#/definitions/ExprRef'}\"}\n",
      "schema = {'$ref': 'urn:vega-lite-schema#/definitions/ExprRef'}\n",
      "def eval_block(code, namespace=None, filename=\"<string>\"):\n",
      "    \"\"\"\n",
      "    Execute a multi-line block of code in the given namespace\n",
      "\n",
      "    If the final statement in the code is an expression, return\n",
      "    the result of the expression.\n",
      "    \"\"\"\n",
      "    tree = ast.parse(code, filename=\"<ast>\", mode=\"exec\")\n",
      "    if namespace is None:\n",
      "        namespace = {}\n",
      "    catch_display = _CatchDisplay()\n",
      "\n",
      "    if isinstance(tree.body[-1], ast.Expr):\n",
      "        to_exec, to_eval = tree.body[:-1], tree.body[-1:]\n",
      "    else:\n",
      "        to_exec, to_eval = tree.body, []\n",
      "\n",
      "    for node in to_exec:\n",
      "        compiled = compile(ast.Module([node], []), filename=filename, mode=\"exec\")\n",
      "        exec(compiled, namespace)\n",
      "\n",
      "    with catch_display:\n",
      "        for node in to_eval:\n",
      "            compiled = compile(\n",
      "                ast.Interactive([node]), filename=filename, mode=\"single\"\n",
      "            )\n",
      "            exec(compiled, namespace)\n",
      "\n",
      "    return catch_display.output\n",
      "\n",
      "eval_block(code=b'\"\"\"\\nConnections Among U.S. Airports Interactive\\n-------------------------------------------\\nThis example shows all the connections between major U.S. airports. Lookup transformations \\nare used to find the coordinates of each airport and connecting airports. Connections \\nare displayed on mouseover via a single selection.\\n\"\"\"\\n# category: case studies\\nimport altair as alt\\nfrom vega_datasets import data\\n\\n# Since these data are each more than 5,000 rows we\\'ll import from the URLs\\nairports = data.airports.url\\nflights_airport = data.flights_airport.url\\n\\nstates = alt.topo_feature(data.us_10m.url, feature=\"states\")\\n\\n# Create mouseover selection\\nselect_city = alt.selection_point(\\n    on=\"mouseover\", nearest=True, fields=[\"origin\"], empty=False\\n)\\n\\n# Define which attributes to lookup from airports.csv\\nlookup_data = alt.LookupData(\\n    airports, key=\"iata\", fields=[\"state\", \"latitude\", \"longitude\"]\\n)\\n\\nbackground = alt.Chart(states).mark_geoshape(\\n    fill=\"lightgray\", \\n    stroke=\"white\"\\n).properties(\\n    width=750, \\n    height=500\\n).project(\"albersUsa\")\\n\\nconnections = alt.Chart(flights_airport).mark_rule(opacity=0.35).encode(\\n    latitude=\"latitude:Q\",\\n    longitude=\"longitude:Q\",\\n    latitude2=\"lat2:Q\",\\n    longitude2=\"lon2:Q\"\\n).transform_lookup(\\n    lookup=\"origin\", \\n    from_=lookup_data\\n).transform_lookup(\\n    lookup=\"destination\", \\n    from_=lookup_data, \\n    as_=[\"state\", \"lat2\", \"lon2\"]\\n).transform_filter(\\n    select_city\\n)\\n\\npoints = alt.Chart(flights_airport).mark_circle().encode(\\n    latitude=\"latitude:Q\",\\n    longitude=\"longitude:Q\",\\n    size=alt.Size(\"routes:Q\", scale=alt.Scale(range=[0, 1000]), legend=None),\\n    order=alt.Order(\"routes:Q\", sort=\"descending\"),\\n    tooltip=[\"origin:N\", \"routes:Q\"]\\n).transform_aggregate(\\n    routes=\"count()\", \\n    groupby=[\"origin\"]\\n).transform_lookup(\\n    lookup=\"origin\", \\n    from_=lookup_data\\n).transform_filter(\\n    (alt.datum.state != \"PR\") & (alt.datum.state != \"VI\")\\n).add_params(\\n    select_city\\n)\\n\\n(background + connections + points).configure_view(stroke=None)\\n', namespace=None, filename='<string>')\n",
      "exec(compiled, namespace)\n",
      "{'compiled': '<code object <module> at 0x7fc2e9a60b30, file \"<string>\", line 70>', 'namespace': '{\\'__builtins__\\': {\\'__name__\\': \\'builtins\\', \\'__doc__\\': \"Built-in functions, exceptions, and other objects.\\\\n\\\\nNoteworthy: None is the `nil\\' object; Ellipsis represents `...\\' in slices.\", \\'__package__\\': \\'\\', \\'__loader__\\': <class \\'_frozen_importlib.BuiltinImporter\\'>, \\'__spec__\\': ModuleSpec(name=\\'builtins\\', loader=<class \\'_frozen_importlib.BuiltinImporter\\'>, origin=\\'built-in\\'), \\'__build_class__\\': <built-in function __build_class__>, \\'__import__\\': <built-in function __import__>, \\'abs\\': <built-in function abs>, \\'all\\': <built-in function all>, \\'any\\': <built-in function any>, \\'ascii\\': <built-in function ascii>, \\'bin\\': <built-in function bin>, \\'breakpoint\\': <built-in function breakpoint>, \\'callable\\': <built-in function callable>, \\'chr\\': <built-in function chr>, \\'compile\\': <built-in function compile>, \\'delattr\\': <built-in function delattr>, \\'dir\\': <built-in function dir>, \\'divmod\\': <built-in function divmod>, \\'eval\\': <built-in function eval>, \\'exec\\': <built-in function exec>, \\'format\\': <built-in function format>, \\'getattr\\': <built-in function getattr>, \\'globals\\': <built-in function globals>, \\'hasattr\\': <built-in function hasattr>, \\'hash\\': <built-in function hash>, \\'hex\\': <built-in function hex>, \\'id\\': <built-in function id>, \\'input\\': <built-in function input>, \\'isinstance\\': <built-in function isinstance>, \\'issubclass\\': <built-in function issubclass>, \\'iter\\': <built-in function iter>, \\'len\\': <built-in function len>, \\'locals\\': <built-in function locals>, \\'max\\': <built-in function max>, \\'min\\': <built-in function min>, \\'next\\': <built-in function next>, \\'oct\\': <built-in function oct>, \\'ord\\': <built-in function ord>, \\'pow\\': <built-in function pow>, \\'print\\': <built-in function print>, \\'repr\\': <built-in function repr>, \\'round\\': <built-in function round>, \\'setattr\\': <built-in function setattr>, \\'sorted\\': <built-in function sorted>, \\'sum\\': <built-in function sum>, \\'vars\\': <built-in function vars>, \\'None\\': None, \\'Ellipsis\\': Ellipsis, \\'NotImplemented\\': NotImplemented, \\'False\\': False, \\'True\\': True, \\'bool\\': <class \\'bool\\'>, \\'memoryview\\': <class \\'memoryview\\'>, \\'bytearray\\': <class \\'bytearray\\'>, \\'bytes\\': <class \\'bytes\\'>, \\'classmethod\\': <class \\'classmethod\\'>, \\'complex\\': <class \\'complex\\'>, \\'dict\\': <class \\'dict\\'>, \\'enumerate\\': <class \\'enumerate\\'>, \\'filter\\': <class \\'filter\\'>, \\'float\\': <class \\'float\\'>, \\'frozenset\\': <class \\'frozenset\\'>, \\'property\\': <class \\'property\\'>, \\'int\\': <class \\'int\\'>, \\'list\\': <class \\'list\\'>, \\'map\\': <class \\'map\\'>, \\'object\\': <class \\'object\\'>, \\'range\\': <class \\'range\\'>, \\'reversed\\': <class \\'reversed\\'>, \\'set\\': <class \\'set\\'>, \\'slice\\': <class \\'slice\\'>, \\'staticmethod\\': <class \\'staticmethod\\'>, \\'str\\': <class \\'str\\'>, \\'super\\': <class \\'super\\'>, \\'tuple\\': <class \\'tuple\\'>, \\'type\\': <class \\'type\\'>, \\'zip\\': <class \\'zip\\'>, \\'__debug__\\': True, \\'BaseException\\': <class \\'BaseException\\'>, \\'Exception\\': <class \\'Exception\\'>, \\'TypeError\\': <class \\'TypeError\\'>, \\'StopAsyncIteration\\': <class \\'StopAsyncIteration\\'>, \\'StopIteration\\': <class \\'StopIteration\\'>, \\'GeneratorExit\\': <class \\'GeneratorExit\\'>, \\'SystemExit\\': <class \\'SystemExit\\'>, \\'KeyboardInterrupt\\': <class \\'KeyboardInterrupt\\'>, \\'ImportError\\': <class \\'ImportError\\'>, \\'ModuleNotFoundError\\': <class \\'ModuleNotFoundError\\'>, \\'OSError\\': <class \\'OSError\\'>, \\'EnvironmentError\\': <class \\'OSError\\'>, \\'IOError\\': <class \\'OSError\\'>, \\'EOFError\\': <class \\'EOFError\\'>, \\'RuntimeError\\': <class \\'RuntimeError\\'>, \\'RecursionError\\': <class \\'RecursionError\\'>, \\'NotImplementedError\\': <class \\'NotImplementedError\\'>, \\'NameError\\': <class \\'NameError\\'>, \\'UnboundLocalError\\': <class \\'UnboundLocalError\\'>, \\'AttributeError\\': <class \\'AttributeError\\'>, \\'SyntaxError\\': <class \\'SyntaxError\\'>, \\'IndentationError\\': <class \\'IndentationError\\'>, \\'TabError\\': <class \\'TabError\\'>, \\'LookupError\\': <class \\'LookupError\\'>, \\'IndexError\\': <class \\'IndexError\\'>, \\'KeyError\\': <class \\'KeyError\\'>, \\'ValueError\\': <class \\'ValueError\\'>, \\'UnicodeError\\': <class \\'UnicodeError\\'>, \\'UnicodeEncodeError\\': <class \\'UnicodeEncodeError\\'>, \\'UnicodeDecodeError\\': <class \\'UnicodeDecodeError\\'>, \\'UnicodeTranslateError\\': <class \\'UnicodeTranslateError\\'>, \\'AssertionError\\': <class \\'AssertionError\\'>, \\'ArithmeticError\\': <class \\'ArithmeticError\\'>, \\'FloatingPointError\\': <class \\'FloatingPointError\\'>, \\'OverflowError\\': <class \\'OverflowError\\'>, \\'ZeroDivisionError\\': <class \\'ZeroDivisionError\\'>, \\'SystemError\\': <class \\'SystemError\\'>, \\'ReferenceError\\': <class \\'ReferenceError\\'>, \\'MemoryError\\': <class \\'MemoryError\\'>, \\'BufferError\\': <class \\'BufferError\\'>, \\'Warning\\': <class \\'Warning\\'>, \\'UserWarning\\': <class \\'UserWarning\\'>, \\'DeprecationWarning\\': <class \\'DeprecationWarning\\'>, \\'PendingDeprecationWarning\\': <class \\'PendingDeprecationWarning\\'>, \\'SyntaxWarning\\': <class \\'SyntaxWarning\\'>, \\'RuntimeWarning\\': <class \\'RuntimeWarning\\'>, \\'FutureWarning\\': <class \\'FutureWarning\\'>, \\'ImportWarning\\': <class \\'ImportWarning\\'>, \\'UnicodeWarning\\': <class \\'UnicodeWarning\\'>, \\'BytesWarning\\': <class \\'BytesWarning\\'>, \\'ResourceWarning\\': <class \\'ResourceWarning\\'>, \\'ConnectionError\\': <class \\'ConnectionError\\'>, \\'BlockingIOError\\': <class \\'BlockingIOError\\'>, \\'BrokenPipeError\\': <class \\'BrokenPipeError\\'>, \\'ChildProcessError\\': <class \\'ChildProcessError\\'>, \\'ConnectionAbortedError\\': <class \\'ConnectionAbortedError\\'>, \\'ConnectionRefusedError\\': <class \\'ConnectionRefusedError\\'>, \\'ConnectionResetError\\': <class \\'ConnectionResetError\\'>, \\'FileExistsError\\': <class \\'FileExistsError\\'>, \\'FileNotFoundError\\': <class \\'FileNotFoundError\\'>, \\'IsADirectoryError\\': <class \\'IsADirectoryError\\'>, \\'NotADirectoryError\\': <class \\'NotADirectoryError\\'>, \\'InterruptedError\\': <class \\'InterruptedError\\'>, \\'PermissionError\\': <class \\'PermissionError\\'>, \\'ProcessLookupError\\': <class \\'ProcessLookupError\\'>, \\'TimeoutError\\': <class \\'TimeoutError\\'>, \\'open\\': <built-in function open>, \\'copyright\\': Copyright (c) 2001-2023 Python Software Foundation.All Rights Reserved.Copyright (c) 2000 BeOpen.com.All Rights Reserved.Copyright (c) 1995-2001 Corporation for National Research Initiatives.All Rights Reserved.Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.All Rights Reserved., \\'credits\\':     Thanks to CWI, CNRI, BeOpen.com, Zope Corporation and a cast of thousands    for supporting Python development.  See www.python.org for more information., \\'license\\': Type license() to see the full license text, \\'help\\': Type help() for interactive help, or help(object) for help about object., \\'__IPYTHON__\\': True, \\'display\\': <function display at 0x7fc2eb22bf70>, \\'exit\\': Use exit() or Ctrl-D (i.e. EOF) to exit, \\'quit\\': Use quit() or Ctrl-D (i.e. EOF) to exit}, \\'__doc__\\': \\'\\\\nConnections Among U.S. Airports Interactive\\\\n-------------------------------------------\\\\nThis example shows all the connections between major U.S. airports. Lookup transformations \\\\nare used to find the coordinates of each airport and connecting airports. Connections \\\\nare displayed on mouseover via a single selection.\\\\n\\', \\'alt\\': <module \\'altair\\' from \\'/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/altair-viz+altair/altair-viz+altair/altair/__init__.py\\'>, \\'data\\': <vega_datasets.core.DataLoader object at 0x7fc2ea693370>, \\'airports\\': \\'https://cdn.jsdelivr.net/npm/vega-datasets@v1.29.0/data/airports.csv\\', \\'flights_airport\\': \\'https://cdn.jsdelivr.net/npm/vega-datasets@v1.29.0/data/flights-airport.csv\\', \\'states\\': UrlData({  format: TopoDataFormat({    feature: \\'states\\',    type: \\'topojson\\'  }),  url: \\'https://cdn.jsdelivr.net/npm/vega-datasets@v1.29.0/data/us-10m.json\\'}), \\'select_city\\': Parameter(\\'param_1\\', SelectionParameter({  name: \\'param_1\\',  select: PointSelectionConfig({    fields: [\\'origin\\'],    nearest: True,    on: \\'mouseover\\',    type: \\'point\\'  })})), \\'lookup_data\\': LookupData({  data: \\'https://cdn.jsdelivr.net/npm/vega-datasets@v1.29.0/data/airports.csv\\',  fields: [\\'state\\', \\'latitude\\', \\'longitude\\'],  key: \\'iata\\'}), \\'background\\': alt.Chart(...), \\'connections\\': alt.Chart(...), \\'points\\': alt.Chart(...)}'}\n",
      "catch_display = {output=alt.LayerChart(...), old_hook=<built-in function displayhook>}\n",
      "def to_values(data: DataType) -> ToValuesReturnType:\n",
      "    \"\"\"Replace a DataFrame by a data model with values.\"\"\"\n",
      "    check_data_type(data)\n",
      "    if hasattr(data, \"__geo_interface__\"):\n",
      "        if isinstance(data, pd.DataFrame):\n",
      "            data = sanitize_dataframe(data)\n",
      "        # Maybe the type could be further clarified here that it is\n",
      "        # SupportGeoInterface and then the ignore statement is not needed?\n",
      "        data_sanitized = sanitize_geo_interface(data.__geo_interface__)  # type: ignore[arg-type]\n",
      "        return {\"values\": data_sanitized}\n",
      "    elif isinstance(data, pd.DataFrame):\n",
      "        data = sanitize_dataframe(data)\n",
      "        return {\"values\": data.to_dict(orient=\"records\")}\n",
      "    elif isinstance(data, dict):\n",
      "        if \"values\" not in data:\n",
      "            raise KeyError(\"values expected in data dict, but not present.\")\n",
      "        return data\n",
      "    elif hasattr(data, \"__dataframe__\"):\n",
      "        # experimental interchange dataframe support\n",
      "        pi = import_pyarrow_interchange()\n",
      "        pa_table = sanitize_arrow_table(pi.from_dataframe(data))\n",
      "        return {\"values\": pa_table.to_pylist()}\n",
      "    else:\n",
      "        # Should never reach this state as tested by check_data_type\n",
      "        raise ValueError(\"Unrecognized data type: {}\".format(type(data)))\n",
      "\n",
      "to_values(data=           date  precipitation  temp_max  temp_min  wind  weather0    2012-01-01            0.0      12.8       5.0   4.7  drizzle1    2012-01-02           10.9      10.6       2.8   4.5     rain2    2012-01-03            0.8      11.7       7.2   2.3     rain3    2012-01-04           20.3      12.2       5.6   4.7     rain4    2012-01-05            1.3       8.9       2.8   6.1     rain...         ...            ...       ...       ...   ...      ...1456 2015-12-27            8.6       4.4       1.7   2.9      fog1457 2015-12-28            1.5       5.0       1.7   1.3      fog1458 2015-12-29            0.0       7.2       0.6   2.6      fog1459 2015-12-30            0.0       5.6      -1.0   3.4      sun1460 2015-12-31            0.0       5.6      -2.1   3.5      sun[1461 rows x 6 columns])\n",
      "data = sanitize_dataframe(data)\n",
      "{'data': '           date  precipitation  temp_max  temp_min  wind  weather0    2012-01-01            0.0      12.8       5.0   4.7  drizzle1    2012-01-02           10.9      10.6       2.8   4.5     rain2    2012-01-03            0.8      11.7       7.2   2.3     rain3    2012-01-04           20.3      12.2       5.6   4.7     rain4    2012-01-05            1.3       8.9       2.8   6.1     rain...         ...            ...       ...       ...   ...      ...1456 2015-12-27            8.6       4.4       1.7   2.9      fog1457 2015-12-28            1.5       5.0       1.7   1.3      fog1458 2015-12-29            0.0       7.2       0.6   2.6      fog1459 2015-12-30            0.0       5.6      -1.0   3.4      sun1460 2015-12-31            0.0       5.6      -2.1   3.5      sun[1461 rows x 6 columns]'}\n",
      "data =                      date precipitation temp_max temp_min wind  weather0     2012-01-01T00:00:00           0.0     12.8      5.0  4.7  drizzle1     2012-01-02T00:00:00          10.9     10.6      2.8  4.5     rain2     2012-01-03T00:00:00           0.8     11.7      7.2  2.3     rain3     2012-01-04T00:00:00          20.3     12.2      5.6  4.7     rain4     2012-01-05T00:00:00           1.3      8.9      2.8  6.1     rain...                   ...           ...      ...      ...  ...      ...1456  2015-12-27T00:00:00           8.6      4.4      1.7  2.9      fog1457  2015-12-28T00:00:00           1.5      5.0      1.7  1.3      fog1458  2015-12-29T00:00:00           0.0      7.2      0.6  2.6      fog1459  2015-12-30T00:00:00           0.0      5.6     -1.0  3.4      sun1460  2015-12-31T00:00:00           0.0      5.6     -2.1  3.5      sun[1461 rows x 6 columns]\n",
      "def sanitize_dataframe(df: pd.DataFrame) -> pd.DataFrame:  # noqa: C901\n",
      "    \"\"\"Sanitize a DataFrame to prepare it for serialization.\n",
      "\n",
      "    * Make a copy\n",
      "    * Convert RangeIndex columns to strings\n",
      "    * Raise ValueError if column names are not strings\n",
      "    * Raise ValueError if it has a hierarchical index.\n",
      "    * Convert categoricals to strings.\n",
      "    * Convert np.bool_ dtypes to Python bool objects\n",
      "    * Convert np.int dtypes to Python int objects\n",
      "    * Convert floats to objects and replace NaNs/infs with None.\n",
      "    * Convert DateTime dtypes into appropriate string representations\n",
      "    * Convert Nullable integers to objects and replace NaN with None\n",
      "    * Convert Nullable boolean to objects and replace NaN with None\n",
      "    * convert dedicated string column to objects and replace NaN with None\n",
      "    * Raise a ValueError for TimeDelta dtypes\n",
      "    \"\"\"\n",
      "    df = df.copy()\n",
      "\n",
      "    if isinstance(df.columns, pd.RangeIndex):\n",
      "        df.columns = df.columns.astype(str)\n",
      "\n",
      "    for col_name in df.columns:\n",
      "        if not isinstance(col_name, str):\n",
      "            raise ValueError(\n",
      "                \"Dataframe contains invalid column name: {0!r}. \"\n",
      "                \"Column names must be strings\".format(col_name)\n",
      "            )\n",
      "\n",
      "    if isinstance(df.index, pd.MultiIndex):\n",
      "        raise ValueError(\"Hierarchical indices not supported\")\n",
      "    if isinstance(df.columns, pd.MultiIndex):\n",
      "        raise ValueError(\"Hierarchical indices not supported\")\n",
      "\n",
      "    def to_list_if_array(val):\n",
      "        if isinstance(val, np.ndarray):\n",
      "            return val.tolist()\n",
      "        else:\n",
      "            return val\n",
      "\n",
      "    for dtype_item in df.dtypes.items():\n",
      "        # We know that the column names are strings from the isinstance check\n",
      "        # further above but mypy thinks it is of type Hashable and therefore does not\n",
      "        # let us assign it to the col_name variable which is already of type str.\n",
      "        col_name = cast(str, dtype_item[0])\n",
      "        dtype = dtype_item[1]\n",
      "        dtype_name = str(dtype)\n",
      "        if dtype_name == \"category\":\n",
      "            # Work around bug in to_json for categorical types in older versions\n",
      "            # of pandas as they do not properly convert NaN values to null in to_json.\n",
      "            # We can probably remove this part once we require pandas >= 1.0\n",
      "            col = df[col_name].astype(object)\n",
      "            df[col_name] = col.where(col.notnull(), None)\n",
      "        elif dtype_name == \"string\":\n",
      "            # dedicated string datatype (since 1.0)\n",
      "            # https://pandas.pydata.org/pandas-docs/version/1.0.0/whatsnew/v1.0.0.html#dedicated-string-data-type\n",
      "            col = df[col_name].astype(object)\n",
      "            df[col_name] = col.where(col.notnull(), None)\n",
      "        elif dtype_name == \"bool\":\n",
      "            # convert numpy bools to objects; np.bool is not JSON serializable\n",
      "            df[col_name] = df[col_name].astype(object)\n",
      "        elif dtype_name == \"boolean\":\n",
      "            # dedicated boolean datatype (since 1.0)\n",
      "            # https://pandas.io/docs/user_guide/boolean.html\n",
      "            col = df[col_name].astype(object)\n",
      "            df[col_name] = col.where(col.notnull(), None)\n",
      "        elif dtype_name.startswith(\"datetime\") or dtype_name.startswith(\"timestamp\"):\n",
      "            # Convert datetimes to strings. This needs to be a full ISO string\n",
      "            # with time, which is why we cannot use ``col.astype(str)``.\n",
      "            # This is because Javascript parses date-only times in UTC, but\n",
      "            # parses full ISO-8601 dates as local time, and dates in Vega and\n",
      "            # Vega-Lite are displayed in local time by default.\n",
      "            # (see https://github.com/altair-viz/altair/issues/1027)\n",
      "            df[col_name] = (\n",
      "                df[col_name].apply(lambda x: x.isoformat()).replace(\"NaT\", \"\")\n",
      "            )\n",
      "        elif dtype_name.startswith(\"timedelta\"):\n",
      "            raise ValueError(\n",
      "                'Field \"{col_name}\" has type \"{dtype}\" which is '\n",
      "                \"not supported by Altair. Please convert to \"\n",
      "                \"either a timestamp or a numerical value.\"\n",
      "                \"\".format(col_name=col_name, dtype=dtype)\n",
      "            )\n",
      "        elif dtype_name.startswith(\"geometry\"):\n",
      "            # geopandas >=0.6.1 uses the dtype geometry. Continue here\n",
      "            # otherwise it will give an error on np.issubdtype(dtype, np.integer)\n",
      "            continue\n",
      "        elif (\n",
      "            dtype_name\n",
      "            in {\n",
      "                \"Int8\",\n",
      "                \"Int16\",\n",
      "                \"Int32\",\n",
      "                \"Int64\",\n",
      "                \"UInt8\",\n",
      "                \"UInt16\",\n",
      "                \"UInt32\",\n",
      "                \"UInt64\",\n",
      "                \"Float32\",\n",
      "                \"Float64\",\n",
      "            }\n",
      "        ):  # nullable integer datatypes (since 24.0) and nullable float datatypes (since 1.2.0)\n",
      "            # https://pandas.pydata.org/pandas-docs/version/0.25/whatsnew/v0.24.0.html#optional-integer-na-support\n",
      "            col = df[col_name].astype(object)\n",
      "            df[col_name] = col.where(col.notnull(), None)\n",
      "        elif numpy_is_subtype(dtype, np.integer):\n",
      "            # convert integers to objects; np.int is not JSON serializable\n",
      "            df[col_name] = df[col_name].astype(object)\n",
      "        elif numpy_is_subtype(dtype, np.floating):\n",
      "            # For floats, convert to Python float: np.float is not JSON serializable\n",
      "            # Also convert NaN/inf values to null, as they are not JSON serializable\n",
      "            col = df[col_name]\n",
      "            bad_values = col.isnull() | np.isinf(col)\n",
      "            df[col_name] = col.astype(object).where(~bad_values, None)\n",
      "        elif dtype == object:\n",
      "            # Convert numpy arrays saved as objects to lists\n",
      "            # Arrays are not JSON serializable\n",
      "            col = df[col_name].astype(object).apply(to_list_if_array)\n",
      "            df[col_name] = col.where(col.notnull(), None)\n",
      "    return df\n",
      "\n",
      "sanitize_dataframe(df=           date  precipitation  temp_max  temp_min  wind  weather0    2012-01-01            0.0      12.8       5.0   4.7  drizzle1    2012-01-02           10.9      10.6       2.8   4.5     rain2    2012-01-03            0.8      11.7       7.2   2.3     rain3    2012-01-04           20.3      12.2       5.6   4.7     rain4    2012-01-05            1.3       8.9       2.8   6.1     rain...         ...            ...       ...       ...   ...      ...1456 2015-12-27            8.6       4.4       1.7   2.9      fog1457 2015-12-28            1.5       5.0       1.7   1.3      fog1458 2015-12-29            0.0       7.2       0.6   2.6      fog1459 2015-12-30            0.0       5.6      -1.0   3.4      sun1460 2015-12-31            0.0       5.6      -2.1   3.5      sun[1461 rows x 6 columns])\n",
      "col = df[col_name].astype(object).apply(to_list_if_array)\n",
      "{'to_list_if_array': '<function sanitize_dataframe.<locals>.to_list_if_array at 0x7fc2dcd75700>'}\n",
      "col = 0       drizzle1          rain2          rain3          rain4          rain         ...   1456        fog1457        fog1458        fog1459        sun1460        sunName: weather, Length: 1461, dtype: object\n",
      "def lineAfterContext(line, prefix):\n",
      "    if line.startswith(prefix):\n",
      "        line = line[len(prefix):]\n",
      "\n",
      "    toks = line.split(' in ', 1)\n",
      "    if len(toks) == 2:\n",
      "        rest = toks[1].split(' ')\n",
      "        line = ' '.join(rest[1:])\n",
      "\n",
      "    return line\n",
      "\n",
      "lineAfterContext(line='ic| eins: zwei', prefix='ic| ')\n",
      "line = line[len(prefix):]\n",
      "{'prefix': \"'ic| '\"}\n",
      "line = 'eins: zwei'\n",
      "def argumentToString(obj):\n",
      "    s = DEFAULT_ARG_TO_STRING_FUNCTION(obj)\n",
      "    s = s.replace('\\\\n', '\\n')  # Preserve string newlines in output.\n",
      "    return s\n",
      "\n",
      "argumentToString(obj=1)\n",
      "s = DEFAULT_ARG_TO_STRING_FUNCTION(obj)\n",
      "{'obj': '1'}\n",
      "s = '1'\n",
      "def lineIsContextAndTime(line):\n",
      "    line = stripPrefix(line)  # ic| f.py:33 in foo() at 08:08:51.389\n",
      "    context, time = line.split(' at ')\n",
      "\n",
      "    return (\n",
      "        lineIsContext(context) and\n",
      "        len(time.split(':')) == 3 and\n",
      "        len(time.split('.')) == 2)\n",
      "\n",
      "lineIsContextAndTime(line='ic| test_icecream.py:229 in testAsArgument() at 01:15:24.779')\n",
      "line = stripPrefix(line)  # ic| f.py:33 in foo() at 08:08:51.389\n",
      "{'line': \"'ic| test_icecream.py:229 in testAsArgument() at 01:15:24.779'\"}\n",
      "line = 'test_icecream.py:229 in testAsArgument() at 01:15:24.779'\n",
      "def lineIsContext(line):\n",
      "    line = stripPrefix(line)  # ic| f.py:33 in foo()\n",
      "    sourceLocation, function = line.split(' in ')  # f.py:33 in foo()\n",
      "    filename, lineNumber = sourceLocation.split(':')  # f.py:33\n",
      "    name, ext = splitext(filename)\n",
      "\n",
      "    return (\n",
      "        int(lineNumber) > 0 and\n",
      "        ext in ['.py', '.pyc', '.pyo'] and\n",
      "        name == splitext(MY_FILENAME)[0] and\n",
      "        (function == '<module>' or function.endswith('()')))\n",
      "\n",
      "lineIsContext(line='test_icecream.py:229 in testAsArgument()')\n",
      "name, ext = splitext(filename)\n",
      "{'filename': \"'test_icecream.py'\"}\n",
      "name = 'test_icecream'\n",
      "def format_pair(prefix, arg, value):\n",
      "    if arg is _arg_source_missing:\n",
      "        arg_lines = []\n",
      "        value_prefix = prefix\n",
      "    else:\n",
      "        arg_lines = indented_lines(prefix, arg)\n",
      "        value_prefix = arg_lines[-1] + ': '\n",
      "\n",
      "    looksLikeAString = value[0] + value[-1] in [\"''\", '\"\"']\n",
      "    if looksLikeAString:  # Align the start of multiline strings.\n",
      "        value = prefixLinesAfterFirst(' ', value)\n",
      "\n",
      "    value_lines = indented_lines(value_prefix, value)\n",
      "    lines = arg_lines[:-1] + value_lines\n",
      "    return '\\n'.join(lines)\n",
      "\n",
      "format_pair(prefix='    ', arg='multilineStr', value=\"'line1\\nline2'\")\n",
      "arg_lines = indented_lines(prefix, arg)\n",
      "{'prefix': \"'    '\", 'arg': \"'multilineStr'\"}\n",
      "arg_lines = ['    multilineStr']\n",
      "def appendTo(s):\n",
      "            lst.append(s)\n",
      "\n",
      "appendTo(s='ic| a: 1', lst=[])\n",
      "lst.append(s)\n",
      "{'s': \"'ic| a: 1'\"}\n",
      "lst = ['ic| a: 1']\n",
      "def call_on_comment(*args, **kwargs):\n",
      "    global events\n",
      "    repo_full_name = kwargs[\"repo_full_name\"]\n",
      "    pr_id = kwargs[\"pr_number\"]\n",
      "    key = f\"{repo_full_name}-{pr_id}\"\n",
      "\n",
      "    # Check if a previous process exists for the same key, cancel it\n",
      "    thread = events.get(key, None)\n",
      "    if thread:\n",
      "        terminate_thread(thread)\n",
      "\n",
      "    thread = threading.Thread(target=run_comment, args=args, kwargs=kwargs)\n",
      "    events[key] = thread\n",
      "    thread.start()\n",
      "\n",
      "call_on_comment(args=(), kwargs={'repo_full_name': 'exampleRepo', 'pr_number': 1})\n",
      "thread = events.get(key, None)\n",
      "{'key': \"'exampleRepo-1'\"}\n",
      "thread = None\n",
      "def hmac_digest(secret, message, encoding=\"utf-8\"):\n",
      "    \"\"\"Return hex digest of a message HMAC using secret\"\"\"\n",
      "    if isinstance(secret, str):\n",
      "        secret = secret.encode(encoding)\n",
      "    return hmac.new(secret, message.encode(encoding), hashlib.sha256).hexdigest()\n",
      "\n",
      "hmac_digest(secret='secret_hmac_for_userids', message='mat:secret', encoding='utf-8')\n",
      "secret = secret.encode(encoding)\n",
      "{'encoding': \"'utf-8'\"}\n",
      "secret = b'secret_hmac_for_userids'\n",
      "def parse_csv(csv):\n",
      "    lines = csv.strip().split('\\n')\n",
      "    lines = [line for line in lines if line.strip()]\n",
      "    csv = '\\n'.join(lines)\n",
      "    if autoeq_pattern.match(csv):  # Matches AutoEq produced CSV files\n",
      "        columns = lines[0].split(',')\n",
      "        return {column: [float(line.split(',')[i]) for line in lines[1:]] for i, column in enumerate(columns)}\n",
      "\n",
      "    if rew_pattern.match(csv) or crinacle_pattern.match(csv):\n",
      "        # These two have all sort of junk in them but the first column is frequency and the second SPL, so all good\n",
      "        csv = '\\n'.join([re.sub(r'(?:, ?| |\\t)', '\\t', line) for line in lines if numeric_start.match(line) and '?' not in line])\n",
      "        lines = csv.split('\\n')\n",
      "\n",
      "    column_separator, decimal_separator = find_csv_separators(csv)\n",
      "    columns = find_csv_columns(csv, column_separator)\n",
      "\n",
      "    # Find indexes of frequency and raw columns\n",
      "    if columns is None:\n",
      "        # No header, assume first column is frequency and the second is SPL\n",
      "        ixs = {'frequency': 0, 'raw': 1}\n",
      "    else:\n",
      "        ixs = {'frequency': None, 'raw': None}\n",
      "        for i, column in enumerate(columns):\n",
      "            if re.match(r'^freq', column, flags=re.IGNORECASE):\n",
      "                ixs['frequency'] = i\n",
      "            if re.match(r'^(?:spl|gain|ampl|raw)', column, flags=re.IGNORECASE):\n",
      "                ixs['raw'] = i\n",
      "        if ixs['frequency'] is None:\n",
      "            if len(columns) == 2:  # Can't find proper columns but there's only two, assuming freq + raw\n",
      "                ixs = {'frequency': 0, 'raw': 1}\n",
      "            else:\n",
      "                raise CsvParseError('Failed to find frequency column')\n",
      "        if ixs['raw'] is None:\n",
      "            raise CsvParseError('Failed to find SPL column')\n",
      "\n",
      "    # Read and parse data lines\n",
      "    data_line_pattern = re.compile(rf'^-?\\d+(?:{column_separator}\\d+)?')\n",
      "    data = {'frequency': [], 'raw': []}\n",
      "    for line in lines:\n",
      "        if not data_line_pattern.match(line):\n",
      "            continue\n",
      "        cells = line.split(column_separator)\n",
      "        if decimal_separator == ',':\n",
      "            cells = [float(cell.replace(',', '.')) for cell in cells]\n",
      "        else:\n",
      "            cells = [float(cell) for cell in cells]\n",
      "        for column, ix in ixs.items():\n",
      "            data[column].append(cells[ix])\n",
      "    return data\n",
      "\n",
      "parse_csv(csv='frequency,raw\\n20,0\\n1000,3\\n20000,0\\n')\n",
      "csv = '\\n'.join(lines)\n",
      "{'lines': \"['frequency,raw', '20,0', '1000,3', '20000,0']\"}\n",
      "csv = 'frequency,raw\\n20,0\\n1000,3\\n20000,0'\n",
      "def _sort(a, aux, lo, hi):\n",
      "        if lo >= hi:\n",
      "            return\n",
      "\n",
      "        if hi - lo + 1 < MergeSort.CUTOFF:\n",
      "            InsertionSort.sort(a, lo, hi)\n",
      "            return\n",
      "\n",
      "        mid = lo + (hi - lo) // 2\n",
      "        MergeSort._sort(a, aux, lo, mid)\n",
      "        MergeSort._sort(a, aux, mid + 1, hi)\n",
      "        MergeSort._merge(a, aux, lo, mid, hi)\n",
      "\n",
      "_sort(a=[4, 2, 1, 23, 4, 5, 6, 7, 8, 9, 20, 11, 13, 34, 66], aux=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], lo=0, hi=7)\n",
      "MergeSort._merge(a, aux, lo, mid, hi)\n",
      "{'a': '[1, 2, 4, 4, 5, 6, 7, 23, 8, 9, 20, 11, 13, 34, 66]', 'aux': '[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]', 'lo': '0', 'mid': '3', 'hi': '7'}\n",
      "aux = [1, 2, 4, 4, 5, 6, 7, 23, 0, 0, 0, 0, 0, 0, 0]\n",
      "def partition(a, lo, hi):\n",
      "        i = lo\n",
      "        j = hi\n",
      "\n",
      "        while True:\n",
      "            while not util.less(a[lo], a[i]):\n",
      "                i += 1\n",
      "                if i >= hi:\n",
      "                    break\n",
      "\n",
      "            while util.less(a[lo], a[j]):\n",
      "                j -= 1\n",
      "                if j <= lo:\n",
      "                    break\n",
      "\n",
      "            if i >= j:\n",
      "                break\n",
      "\n",
      "            util.exchange(a, i, j)\n",
      "\n",
      "        util.exchange(a, lo, j)\n",
      "        return j\n",
      "\n",
      "partition(a=[4, 2, 1, 23, 4, 5, 6, 7, 8, 9, 20, 11, 13, 34, 66], lo=0, hi=14)\n",
      "util.exchange(a, i, j)\n",
      "{'a': '[4, 2, 1, 23, 4, 5, 6, 7, 8, 9, 20, 11, 13, 34, 66]', 'i': '4', 'j': '3'}\n",
      "a = [4, 2, 1, 4, 23, 5, 6, 7, 8, 9, 20, 11, 13, 34, 66]\n",
      "def get_google_drive_folder_location():\n",
      "    \"\"\"\n",
      "    Try to locate the Google Drive folder.\n",
      "\n",
      "    Returns:\n",
      "        (str) Full path to the current Google Drive folder\n",
      "    \"\"\"\n",
      "    gdrive_db_path = \"Library/Application Support/Google/Drive/sync_config.db\"\n",
      "    yosemite_gdrive_db_path = (\n",
      "        \"Library/Application Support/Google/Drive/\" \"user_default/sync_config.db\"\n",
      "    )\n",
      "    yosemite_gdrive_db = os.path.join(os.environ[\"HOME\"], yosemite_gdrive_db_path)\n",
      "    if os.path.isfile(yosemite_gdrive_db):\n",
      "        gdrive_db_path = yosemite_gdrive_db\n",
      "\n",
      "    googledrive_home = None\n",
      "\n",
      "    gdrive_db = os.path.join(os.environ[\"HOME\"], gdrive_db_path)\n",
      "    if os.path.isfile(gdrive_db):\n",
      "        con = sqlite3.connect(gdrive_db)\n",
      "        if con:\n",
      "            cur = con.cursor()\n",
      "            query = (\n",
      "                \"SELECT data_value \"\n",
      "                \"FROM data \"\n",
      "                \"WHERE entry_key = 'local_sync_root_path';\"\n",
      "            )\n",
      "            cur.execute(query)\n",
      "            data = cur.fetchone()\n",
      "            googledrive_home = str(data[0])\n",
      "            con.close()\n",
      "\n",
      "    if not googledrive_home:\n",
      "        error(\n",
      "            constants.ERROR_UNABLE_TO_FIND_STORAGE.format(\n",
      "                provider=\"Google Drive install\"\n",
      "            )\n",
      "        )\n",
      "\n",
      "    return googledrive_home\n",
      "\n",
      "get_google_drive_folder_location()\n",
      "con = sqlite3.connect(gdrive_db)\n",
      "{'gdrive_db': \"'/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/lra+mackup/lra+mackup/tests/fixtures/Library/Application Support/Google/Drive/sync_config.db'\"}\n",
      "con = REPR FAILED\n",
      "def can_file_be_synced_on_current_platform(path):\n",
      "    \"\"\"\n",
      "    Check if the given path can be synced locally.\n",
      "\n",
      "    Check if it makes sense to sync the file at the given path on the current\n",
      "    platform.\n",
      "    For now we don't sync any file in the ~/Library folder on GNU/Linux.\n",
      "    There might be other exceptions in the future.\n",
      "\n",
      "    Args:\n",
      "        (str): Path to the file or folder to check. If relative, prepend it\n",
      "               with the home folder.\n",
      "               'abc' becomes '~/abc'\n",
      "               '/def' stays '/def'\n",
      "\n",
      "    Returns:\n",
      "        (bool): True if given file can be synced\n",
      "    \"\"\"\n",
      "    can_be_synced = True\n",
      "\n",
      "    # If the given path is relative, prepend home\n",
      "    fullpath = os.path.join(os.environ[\"HOME\"], path)\n",
      "\n",
      "    # Compute the ~/Library path on macOS\n",
      "    # End it with a slash because we are looking for this specific folder and\n",
      "    # not any file/folder named LibrarySomething\n",
      "    library_path = os.path.join(os.environ[\"HOME\"], \"Library/\")\n",
      "\n",
      "    if platform.system() == constants.PLATFORM_LINUX:\n",
      "        if fullpath.startswith(library_path):\n",
      "            can_be_synced = False\n",
      "\n",
      "    return can_be_synced\n",
      "\n",
      "can_file_be_synced_on_current_platform(path='some/file')\n",
      "fullpath = os.path.join(os.environ[\"HOME\"], path)\n",
      "{'path': \"'some/file'\"}\n",
      "fullpath = '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/lra+mackup/lra+mackup/tests/fixtures/some/file'\n",
      "def _compare_parts(self, a, b, decimal):\n",
      "        if decimal:\n",
      "            if a == \"\": a = \"0\"\n",
      "            if b == \"\": b = \"0\"\n",
      "            return int(a) - int(b)\n",
      "        else:\n",
      "            i = 0\n",
      "            while i < (min(len(a), len(b)) + 1):\n",
      "                res = self._order(self._get_empty_str_on_index_error(a, i)) \\\n",
      "                        - self._order(self._get_empty_str_on_index_error(b, i))\n",
      "                if res != 0:\n",
      "                    return res\n",
      "                i += 1\n",
      "            else:\n",
      "                return 0\n",
      "\n",
      "_compare_parts(self=2.2.0~rc5, a='', b='', decimal=False, self.epoch=0, self.revision='0', self.revision_allowed_chars=('.', '+', '~'), self.upstream_version='2.2.0~rc5', self.upstream_version_allowed_chars=('.', '+', '~', '-', ':'), self.version='2.2.0~rc5')\n",
      "res = self._order(self._get_empty_str_on_index_error(a, i)) \\\n",
      "{'a': \"''\", 'i': '0'}\n",
      "res = 0\n",
      "def getorcreatesubcontext(self, path):\n",
      "        for name in path:\n",
      "            that = self.resolvables.get(name)\n",
      "            if that is None:\n",
      "                self.resolvables[name] = that = Context(self)\n",
      "            self = that\n",
      "        return self\n",
      "\n",
      "getorcreatesubcontext(self=Context(SuperContext(), False), path=('woo',), self.islist=False, self.parent=SuperContext(), self.resolvables=OrderedDict())\n",
      "self.resolvables[name] = that = Context(self)\n",
      "{'self': 'Context(SuperContext(), False)'}\n",
      "that = Context(Context(SuperContext(), False), False)\n",
      "def execute(self, entry):\n",
      "        directives = []\n",
      "        for i, word in enumerate(entry.words()):\n",
      "            def append(resolvable):\n",
      "                directives.append((resolvable.directivevalue, i))\n",
      "                raise self.Enough\n",
      "            try:\n",
      "                self.getresolvables(word.cat(), append)\n",
      "            except (AttributeError, CatNotSupportedException, self.Enough):\n",
      "                pass\n",
      "        if 1 != len(directives):\n",
      "            raise UnsupportedEntryException(\"Expected 1 directive but %s found: %s\" % (directives, entry))\n",
      "        d, i = directives[0]\n",
      "        d(entry.subentry(0, i), entry.phrase(i + 1), self)\n",
      "\n",
      "execute(self=Context(SuperContext(), False), entry=Entry([Text('ns'), Text('woo'), Blank(' '), Text('='), Blank(' '), Text('yay')]), self.islist=False, self.parent=SuperContext(), self.resolvables=OrderedDict([('here', Text('/tmp'))]))\n",
      "d(entry.subentry(0, i), entry.phrase(i + 1), self)\n",
      "{'i': '2'}\n",
      "self.resolvables = OrderedDict([('here', Text('/tmp')), ('ns', Context(Context(SuperContext(), False), False))])\n",
      "def static_bool_env(varname: str, default: bool) -> bool:\n",
      "  \"\"\"Read an environment variable and interpret it as a boolean.\n",
      "\n",
      "  This is deprecated. Please use bool_flag() unless your flag\n",
      "  will be used in a static method and does not require runtime updates.\n",
      "\n",
      "  True values are (case insensitive): 'y', 'yes', 't', 'true', 'on', and '1';\n",
      "  false values are 'n', 'no', 'f', 'false', 'off', and '0'.\n",
      "  Args:\n",
      "    varname: the name of the variable\n",
      "    default: the default boolean value\n",
      "  Returns:\n",
      "    boolean return value derived from defaults and environment.\n",
      "  Raises: ValueError if the environment variable is anything else.\n",
      "  \"\"\"\n",
      "  val = os.getenv(varname, str(default))\n",
      "  val = val.lower()\n",
      "  if val in ('y', 'yes', 't', 'true', 'on', '1'):\n",
      "    return True\n",
      "  elif val in ('n', 'no', 'f', 'false', 'off', '0'):\n",
      "    return False\n",
      "  else:\n",
      "    raise ValueError(\n",
      "      'invalid truth value {!r} for environment {!r}'.format(val, varname)\n",
      "    )\n",
      "\n",
      "static_bool_env(varname='FLAX_LAZY_RNG', default=True)\n",
      "val = os.getenv(varname, str(default))\n",
      "{'varname': \"'FLAX_LAZY_RNG'\"}\n",
      "val = 'True'\n",
      "def splitdrive(path):\n",
      "    \"\"\"\n",
      "    Split the path into a pair (drive, tail) where drive is either a\n",
      "    mount point or the empty string. On systems which do not use drive\n",
      "    specifications, drive will always be the empty string.\n",
      "\n",
      "    In all cases, drive + tail will be the same as path.\n",
      "\n",
      "    Equivalent to \"os.path.splitdrive\".\n",
      "\n",
      "    Args:\n",
      "        path (path-like object): Path or URL.\n",
      "\n",
      "    Returns:\n",
      "        tuple of str: drive, tail.\n",
      "    \"\"\"\n",
      "    relative = get_instance(path).relpath(path)\n",
      "    drive = path.rsplit(relative, 1)[0]\n",
      "    if drive and not drive[-2:] == '//':\n",
      "        # Keep \"/\" tail side\n",
      "        relative = '/' + relative\n",
      "        drive = drive.rstrip('/')\n",
      "    return drive, relative\n",
      "\n",
      "splitdrive(path='dummy://dir1/dir2/dir3')\n",
      "relative = get_instance(path).relpath(path)\n",
      "{'path': \"'dummy://dir1/dir2/dir3'\"}\n",
      "relative = 'dir1/dir2/dir3'\n",
      "def _copy(src, dst, src_is_storage, dst_is_storage):\n",
      "    \"\"\"\n",
      "    Copies file from source to destination\n",
      "\n",
      "    Args:\n",
      "        src (str or file-like object): Source file.\n",
      "        dst (str or file-like object): Destination file.\n",
      "        src_is_storage (bool): Source is storage.\n",
      "        dst_is_storage (bool): Destination is storage.\n",
      "    \"\"\"\n",
      "    # If both storage: Tries to perform same storage direct copy\n",
      "    if src_is_storage and dst_is_storage:\n",
      "        system = get_instance(src)\n",
      "        if system is get_instance(dst):\n",
      "\n",
      "            # Checks if same file\n",
      "            if system.relpath(src) == system.relpath(dst):\n",
      "                raise same_file_error(\n",
      "                    \"'%s' and '%s' are the same file\" % (src, dst))\n",
      "\n",
      "            # Tries to copy\n",
      "            try:\n",
      "                return system.copy(src, dst)\n",
      "            except (UnsupportedOperation, ObjectException):\n",
      "                pass\n",
      "\n",
      "    # At least one storage object: copies streams\n",
      "    with cos_open(src, 'rb') as fsrc:\n",
      "        with cos_open(dst, 'wb') as fdst:\n",
      "\n",
      "            # Get stream buffer size\n",
      "            for stream in (fdst, fsrc):\n",
      "                try:\n",
      "                    buffer_size = getattr(stream, '_buffer_size')\n",
      "                    break\n",
      "                except AttributeError:\n",
      "                    continue\n",
      "            else:\n",
      "                buffer_size = 16384\n",
      "\n",
      "            # Read and write\n",
      "            copyfileobj(fsrc, fdst, buffer_size)\n",
      "\n",
      "_copy(src='dummy_read://file.txt', dst='/tmp/pytest-of-XXX/pytest-198/test_cos_open0/file_dst.txt', src_is_storage=True, dst_is_storage=False)\n",
      "with cos_open(dst, 'wb') as fdst:\n",
      "{'dst': \"'/tmp/pytest-of-XXX/pytest-198/test_cos_open0/file_dst.txt'\"}\n",
      "fdst = <_io.BufferedWriter name='/tmp/pytest-of-XXX/pytest-198/test_cos_open0/file_dst.txt'>\n",
      "def _getmtime_from_header(header):\n",
      "        \"\"\"\n",
      "        Return the time from header\n",
      "\n",
      "        Args:\n",
      "            header (dict): Object header.\n",
      "\n",
      "        Returns:\n",
      "            float: The number of seconds since the epoch\n",
      "        \"\"\"\n",
      "        # By default, assumes that information are in a standard HTTP header\n",
      "        for key in ('Last-Modified', 'last-modified'):\n",
      "            try:\n",
      "                return mktime(parsedate(header.pop(key)))\n",
      "            except KeyError:\n",
      "                continue\n",
      "        else:\n",
      "            raise UnsupportedOperation('getmtime')\n",
      "\n",
      "_getmtime_from_header(header={'Accept-Ranges': 'bytes', 'Content-Length': '100', 'Last-Modified': 'Wed, 03 Apr 2024 20:18:10 GMT'})\n",
      "return mktime(parsedate(header.pop(key)))\n",
      "{'key': \"'Last-Modified'\"}\n",
      "header = {'Accept-Ranges': 'bytes', 'Content-Length': '100'}\n",
      "def _getsize_from_header(header):\n",
      "        \"\"\"\n",
      "        Return the size from header\n",
      "\n",
      "        Args:\n",
      "            header (dict): Object header.\n",
      "\n",
      "        Returns:\n",
      "            int: Size in bytes.\n",
      "        \"\"\"\n",
      "        # By default, assumes that information are in a standard HTTP header\n",
      "        for key in ('Content-Length', 'content-length'):\n",
      "            try:\n",
      "                return int(header.pop(key))\n",
      "            except KeyError:\n",
      "                continue\n",
      "        else:\n",
      "            raise UnsupportedOperation('getsize')\n",
      "\n",
      "_getsize_from_header(header={'Accept-Ranges': 'bytes', 'Content-Length': '100'})\n",
      "return int(header.pop(key))\n",
      "{'key': \"'Content-Length'\"}\n",
      "header = {'Accept-Ranges': 'bytes'}\n",
      "def put_object(key=None, data=None, **_):\n",
      "            \"\"\"Check arguments and returns fake value\"\"\"\n",
      "            assert key.startswith(key_value)\n",
      "            if key[-1] == '/':\n",
      "                assert data == b''\n",
      "            else:\n",
      "                assert len(data) == len(ossobject._write_buffer)\n",
      "            put_object_called.append(1)\n",
      "\n",
      "put_object(key='key/', data=b'', _={}, key_value='key', ossobject=None, put_object_called=[])\n",
      "assert key.startswith(key_value)\n",
      "{'key_value': \"'key'\"}\n",
      "@py_assert1 = None\n",
      "def readHarFileInfo(filename: str) -> HarFileInfoObj:\n",
      "        \"\"\"\n",
      "        :param filename: Filename.\n",
      "        :return: An `dict`, with the key-value pairs:\n",
      "\n",
      "        \"\"\"\n",
      "        hfiObj = HarFileInfoObj(file=filename)\n",
      "\n",
      "        with open(filename, \"rb\") as f:\n",
      "            f.seek(0)\n",
      "            while True:\n",
      "                # Read all header names\n",
      "                pos, name, end_pos = HarFileIO._readHeaderPosName(f)\n",
      "                if not name:\n",
      "                    break\n",
      "                hfiObj.addHAInfo(name, pos, end_pos)\n",
      "                hfi=hfiObj.ha_infos[name]\n",
      "                (hfi.version, hfi.data_type, hfi.storage_type,  hfi.long_name, hfi.file_dims) = HarFileIO._getHeaderInfo(f, name)\n",
      "        return hfiObj\n",
      "\n",
      "readHarFileInfo(filename='test_remove_header_array.har')\n",
      "with open(filename, \"rb\") as f:\n",
      "{'filename': \"'test_remove_header_array.har'\"}\n",
      "f = <_io.BufferedReader name='test_remove_header_array.har'>\n",
      "def _unpack_args(args, nargs_spec):\n",
      "    \"\"\"Given an iterable of arguments and an iterable of nargs specifications,\n",
      "    it returns a tuple with all the unpacked arguments at the first index\n",
      "    and all remaining arguments as the second.\n",
      "\n",
      "    The nargs specification is the number of arguments that should be consumed\n",
      "    or `-1` to indicate that this position should eat up all the remainders.\n",
      "\n",
      "    Missing items are filled with `None`.\n",
      "    \"\"\"\n",
      "    args = deque(args)\n",
      "    nargs_spec = deque(nargs_spec)\n",
      "    rv = []\n",
      "    spos = None\n",
      "\n",
      "    def _fetch(c):\n",
      "        try:\n",
      "            if spos is None:\n",
      "                return c.popleft()\n",
      "            else:\n",
      "                return c.pop()\n",
      "        except IndexError:\n",
      "            return None\n",
      "\n",
      "    while nargs_spec:\n",
      "        nargs = _fetch(nargs_spec)\n",
      "        if nargs == 1:\n",
      "            rv.append(_fetch(args))\n",
      "        elif nargs > 1:\n",
      "            x = [_fetch(args) for _ in range(nargs)]\n",
      "            # If we're reversed, we're pulling in the arguments in reverse,\n",
      "            # so we need to turn them around.\n",
      "            if spos is not None:\n",
      "                x.reverse()\n",
      "            rv.append(tuple(x))\n",
      "        elif nargs < 0:\n",
      "            if spos is not None:\n",
      "                raise TypeError('Cannot have two nargs < 0')\n",
      "            spos = len(rv)\n",
      "            rv.append(None)\n",
      "\n",
      "    # spos is the position of the wildcard (star).  If it's not `None`,\n",
      "    # we fill it with the remainder.\n",
      "    if spos is not None:\n",
      "        rv[spos] = tuple(args)\n",
      "        args = []\n",
      "        rv[spos + 1:] = reversed(rv[spos + 1:])\n",
      "\n",
      "    return tuple(rv), list(args)\n",
      "\n",
      "_unpack_args(args=['foo.txt', 'bar.txt', 'dir'], nargs_spec=[-1, 1])\n",
      "rv[spos] = tuple(args)\n",
      "{'args': \"deque(['foo.txt', 'bar.txt'])\"}\n",
      "rv = [('foo.txt', 'bar.txt'), 'dir']\n",
      "def join_options(options):\n",
      "    \"\"\"Given a list of option strings this joins them in the most appropriate\n",
      "    way and returns them in the form ``(formatted_string,\n",
      "    any_prefix_is_slash)`` where the second item in the tuple is a flag that\n",
      "    indicates if any of the option prefixes was a slash.\n",
      "    \"\"\"\n",
      "    rv = []\n",
      "    any_prefix_is_slash = False\n",
      "    for opt in options:\n",
      "        prefix = split_opt(opt)[0]\n",
      "        if prefix == '/':\n",
      "            any_prefix_is_slash = True\n",
      "        rv.append((len(prefix), opt))\n",
      "\n",
      "    rv.sort(key=lambda x: x[0])\n",
      "\n",
      "    rv = ', '.join(x[1] for x in rv)\n",
      "    return rv, any_prefix_is_slash\n",
      "\n",
      "join_options(options=['--help'])\n",
      "rv.append((len(prefix), opt))\n",
      "{'prefix': \"'--'\"}\n",
      "rv = [(2, '--help')]\n",
      "def format_filename(filename, shorten=False):\n",
      "    \"\"\"Formats a filename for user display.  The main purpose of this\n",
      "    function is to ensure that the filename can be displayed at all.  This\n",
      "    will decode the filename to unicode if necessary in a way that it will\n",
      "    not fail.  Optionally, it can shorten the filename to not include the\n",
      "    full path to the filename.\n",
      "\n",
      "    :param filename: formats a filename for UI display.  This will also convert\n",
      "                     the filename into unicode without failing.\n",
      "    :param shorten: this optionally shortens the filename to strip of the\n",
      "                    path that leads up to it.\n",
      "    \"\"\"\n",
      "    if shorten:\n",
      "        filename = os.path.basename(filename)\n",
      "    return filename_to_ui(filename)\n",
      "\n",
      "format_filename(filename='/x/foo.txt', shorten=True)\n",
      "filename = os.path.basename(filename)\n",
      "{'filename': \"'/x/foo.txt'\"}\n",
      "filename = 'foo.txt'\n",
      "def test_echo_via_pager(monkeypatch, capfd, cat):\n",
      "    monkeypatch.setitem(os.environ, 'PAGER', cat)\n",
      "    monkeypatch.setattr(click._termui_impl, 'isatty', lambda x: True)\n",
      "    click.echo_via_pager('haha')\n",
      "    out, err = capfd.readouterr()\n",
      "    assert out == 'haha\\n'\n",
      "\n",
      "test_echo_via_pager(monkeypatch={_setattr=[], _setitem=[], _cwd=None, _savesyspath=None}, capfd={request=<SubRequest 'capfd' for <Function test_echo_via_pager[cat]>>, _capture=<MultiCapture out=<FDCapture 1 oldfd=6 _state='started' tmpfile=<_io.TextIOWrapper name=\"<_io.FileIO name=7 mode='rb+' closefd=True>\" mode='r+' encoding='utf-8'>> err=<FDCapture 2 oldfd=8 _state='started' tmpfile=<_io.TextIOWrapper name=\"<_io.FileIO name=9 mode='rb+' closefd=True>\" mode='r+' encoding='utf-8'>> in_=None _state='started' _in_suspended=False>, _captured_out='', _captured_err=''}, cat='cat')\n",
      "monkeypatch.setitem(os.environ, 'PAGER', cat)\n",
      "{'cat': \"'cat'\"}\n",
      "monkeypatch = {_setattr=[], _setitem=[(environ({'SHELL': '/bin/bash', 'LSCOLORS': 'Gxfxcxdxbxegedabagacad', 'USER_ZDOTDIR': '/home/XXX', 'COLORTERM': 'truecolor', 'LESS': '-R', 'TERM_PROGRAM_VERSION': '3.2a', 'GVM_VERSION': '1.0.22', 'CONDA_EXE': '/local/rcs/XXX/miniforge3/bin/conda', '_CE_M': '', 'TMUX': '/tmp/tmux-19200/default,59951,3', 'PKG_CONFIG_PATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib/pkgconfig:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib/pkgconfig:', '_P9K_TTY': '/dev/pts/20', 'GVM_PATH_BACKUP': '/home/XXX/.gvm/bin:/local/rcs/XXX/miniforge3/envs/mal/bin:/local/rcs/XXX/miniforge3/condabin:/home/XXX/.gdrive-downloader:/local/arise/XXX/miniforge3/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/bin:/home/XXX/.gvm/gos/go1.19.1/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/bin:/home/XXX/.gvm/bin:/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/XXX/.local/bin:/home/XXX/.local/bin:/home/XXX/.local/bin', 'P9K_TTY': 'old', 'LC_FIG_SET_PARENT': '4c022497-5122-4b80-b325-c89bab32302a', 'PWD': '/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/MrTango+click/MrTango+click', 'LOGNAME': 'XXX', 'XDG_SESSION_TYPE': 'tty', 'CONDA_PREFIX': '/local/rcs/XXX/miniforge3/envs/MrTango+click', 'VSCODE_GIT_ASKPASS_NODE': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/node', 'MOTD_SHOWN': 'pam', 'VSCODE_INJECTION': '1', 'GVM_OVERLAY_PREFIX': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay', 'HOME': '/home/XXX', 'LANG': 'en_US.UTF-8', 'DYLD_LIBRARY_PATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'gvm_pkgset_name': 'global', 'SSL_CERT_DIR': '/usr/lib/ssl/certs', 'CONDA_PROMPT_MODIFIER': '(MrTango+click) ', 'GIT_ASKPASS': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/extensions/git/dist/askpass.sh', 'GVM_ROOT': '/home/XXX/.gvm', 'SSH_CONNECTION': '127.0.0.1 39996 127.0.0.1 22', 'GOROOT': '/home/XXX/.gvm/gos/go1.19.1', 'NVM_DIR': '/local/rcs/XXX/.nvm', 'VSCODE_GIT_ASKPASS_EXTRA_ARGS': '', 'XDG_SESSION_CLASS': 'user', 'PYTHONPATH': ':/local/rcs/XXX/code/pytrace-collector:/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/MrTango+click/MrTango+click', 'TERM': 'screen', 'ZSH': '/home/XXX/.oh-my-zsh', '_CE_CONDA': '', 'VSCODE_NONCE': 'd0bc7031-48a3-4719-8bb5-ef236ddd0016', 'ZDOTDIR': '/home/XXX', 'USER': 'XXX', 'TMUX_PANE': '%3', 'VSCODE_GIT_IPC_HANDLE': '/run/user/19200/vscode-git-13d67c6199.sock', 'CONDA_SHLVL': '3', 'SHLVL': '3', 'PAGER': 'cat', '_P9K_SSH_TTY': '/dev/pts/20', 'XDG_SESSION_ID': '43', 'CONDA_PYTHON_EXE': '/local/rcs/XXX/miniforge3/bin/python', 'LD_LIBRARY_PATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib', 'XDG_RUNTIME_DIR': '/run/user/19200', 'SSL_CERT_FILE': '/usr/lib/ssl/certs/ca-certificates.crt', 'SSH_CLIENT': '127.0.0.1 46946 22', 'CONDA_DEFAULT_ENV': 'MrTango+click', 'P9K_SSH': '1', 'LC_ALL': 'en_US.UTF-8', 'VSCODE_GIT_ASKPASS_MAIN': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/extensions/git/dist/askpass-main.js', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'BROWSER': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/helpers/browser.sh', 'PATH': '/home/XXX/.gdrive-downloader:/local/arise/XXX/miniforge3/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/bin:/home/XXX/.gvm/gos/go1.19.1/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/bin:/home/XXX/.gvm/bin:/local/rcs/XXX/miniforge3/envs/MrTango+click/bin:/local/rcs/XXX/miniforge3/condabin:/home/XXX/.gdrive-downloader:/local/arise/XXX/miniforge3/bin:/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/XXX/.local/bin:/home/XXX/.local/bin', 'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/19200/bus', 'gvm_go_name': 'go1.19.1', 'CONDA_PREFIX_1': '/local/rcs/XXX/miniforge3', 'CONDA_PREFIX_2': '/local/rcs/XXX/miniforge3/envs/mal', 'OLDPWD': '/local/rcs/XXX/code/pytrace-collector', 'GOPATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global', 'TERM_PROGRAM': 'tmux', 'VSCODE_IPC_HOOK_CLI': '/run/user/19200/vscode-ipc-518d6355-acaf-4714-a359-be3fe9f21e09.sock', '_': '/local/rcs/XXX/miniforge3/envs/MrTango+click/bin/python', 'PYTEST_CURRENT_TEST': 'tests/test_utils.py::test_echo_via_pager[cat] (call)'}), 'PAGER', 'less')], _cwd=None, _savesyspath=None}\n",
      "def _pipepager(text, cmd, color):\n",
      "    \"\"\"Page through text by feeding it to another program.  Invoking a\n",
      "    pager through this might support colors.\n",
      "    \"\"\"\n",
      "    import subprocess\n",
      "    env = dict(os.environ)\n",
      "\n",
      "    # If we're piping to less we might support colors under the\n",
      "    # condition that\n",
      "    cmd_detail = cmd.rsplit('/', 1)[-1].split()\n",
      "    if color is None and cmd_detail[0] == 'less':\n",
      "        less_flags = os.environ.get('LESS', '') + ' '.join(cmd_detail[1:])\n",
      "        if not less_flags:\n",
      "            env['LESS'] = '-R'\n",
      "            color = True\n",
      "        elif 'r' in less_flags or 'R' in less_flags:\n",
      "            color = True\n",
      "\n",
      "    if not color:\n",
      "        text = strip_ansi(text)\n",
      "\n",
      "    c = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE,\n",
      "                         env=env)\n",
      "    encoding = get_best_encoding(c.stdin)\n",
      "    try:\n",
      "        c.stdin.write(text.encode(encoding, 'replace'))\n",
      "        c.stdin.close()\n",
      "    except (IOError, KeyboardInterrupt):\n",
      "        pass\n",
      "\n",
      "    # Less doesn't respect ^C, but catches it for its own UI purposes (aborting\n",
      "    # search or other commands inside less).\n",
      "    #\n",
      "    # That means when the user hits ^C, the parent process (click) terminates,\n",
      "    # but less is still alive, paging the output and messing up the terminal.\n",
      "    #\n",
      "    # If the user wants to make the pager exit on ^C, they should set\n",
      "    # `LESS='-K'`. It's not our decision to make.\n",
      "    while True:\n",
      "        try:\n",
      "            c.wait()\n",
      "        except KeyboardInterrupt:\n",
      "            pass\n",
      "        else:\n",
      "            break\n",
      "\n",
      "_pipepager(text='haha\\n', cmd='cat', color=None)\n",
      "c = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE,\n",
      "{'cmd': \"'cat'\"}\n",
      "c = <Popen: returncode: None args: 'cat'>\n",
      "def encode_request(*args):\n",
      "    \"\"\"Pack a series of arguments into a RESP array of bulk strings.\"\"\"\n",
      "    result = [\"*\" + str(len(args)) + CRLF]\n",
      "    \n",
      "    for arg in args:\n",
      "        if arg is None:\n",
      "            result.append('$-1' + CRLF)\n",
      "        else:\n",
      "            s = str(arg)\n",
      "            result.append('$' + str(len(s)) + CRLF + s + CRLF)\n",
      "\n",
      "    return \"\".join(result)\n",
      "\n",
      "encode_request(args=('ping',))\n",
      "s = str(arg)\n",
      "{'arg': \"'ping'\"}\n",
      "s = 'ping'\n",
      "def bar_factory(chars=None, *, tip=None, background=None, borders=None, errors=None):\n",
      "    \"\"\"Create a factory of a bar with the given styling parameters.\n",
      "    Supports unicode grapheme clusters and emoji chars (those that has length one but when on\n",
      "    screen occupies two cells).\n",
      "\n",
      "    Now supports transparent fills! Just send a tip, and leave `chars` as None.\n",
      "    Also tips are now considered for the 100%, which means it smoothly enters and exits the\n",
      "    frame to get to 100%!! The effect is super cool, use a multi-char tip to see.\n",
      "\n",
      "    Args:\n",
      "        chars (Optional[str]): the sequence of increasing glyphs to fill the bar\n",
      "            can be None for a transparent fill, unless tip is also None.\n",
      "        tip (Optional[str): the tip in front of the bar\n",
      "            can be None, unless chars is also None.\n",
      "        background (Optional[str]): the pattern to be used underneath the bar\n",
      "        borders (Optional[Union[str, Tuple[str, str]]): the pattern or patterns to be used\n",
      "            before and after the bar\n",
      "        errors (Optional[Union[str, Tuple[str, str]]): the pattern or patterns to be used\n",
      "            when an underflow or overflow occurs\n",
      "\n",
      "    Returns:\n",
      "        a styled bar factory\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    @bar_controller\n",
      "    def inner_bar_factory(length, spinner_factory=None):\n",
      "        if chars:\n",
      "            if is_wide(chars[-1]):  # previous chars can be anything.\n",
      "                def fill_style(complete, filling):  # wide chars fill.\n",
      "                    odd = bool(complete % 2)\n",
      "                    fill = (None,) if odd != bool(filling) else ()  # odd XOR filling.\n",
      "                    fill += (chars[-1], None) * int(complete / 2)  # already marked wide chars.\n",
      "                    if filling and odd:\n",
      "                        fill += mark_graphemes((chars[filling - 1],))\n",
      "                    return fill\n",
      "            else:  # previous chars cannot be wide.\n",
      "                def fill_style(complete, filling):  # narrow chars fill.\n",
      "                    fill = (chars[-1],) * complete  # unneeded marks here.\n",
      "                    if filling:\n",
      "                        fill += (chars[filling - 1],)  # no widies here.\n",
      "                    return fill\n",
      "        else:\n",
      "            def fill_style(complete, filling):  # invisible fill.\n",
      "                return fix_cells(padding[:complete + bool(filling)])\n",
      "\n",
      "        def running(fill):\n",
      "            return None, (fix_cells(padding[len(fill) + len_tip:]),)  # this is a 1-tuple.\n",
      "\n",
      "        def ended(fill):\n",
      "            border = None if len(fill) + len(underflow) <= length else underflow\n",
      "            texts = *(() if border else (underflow,)), blanks\n",
      "            return border, texts\n",
      "\n",
      "        @bordered(borders, '||')\n",
      "        def draw_known(apply_state, percent):\n",
      "            virtual_fill = round(virtual_length * max(0., min(1., percent)))\n",
      "            fill = fill_style(*divmod(virtual_fill, num_graphemes))\n",
      "            border, texts = apply_state(fill)\n",
      "            border = overflow if percent > 1. else None if percent == 1. else border\n",
      "            return fix_cells(combine_cells(fill, tip, *texts)[len_tip:length + len_tip]), border\n",
      "\n",
      "        if spinner_factory:\n",
      "            @bordered(borders, '||')\n",
      "            def draw_unknown(_percent=None):\n",
      "                return next(player), None\n",
      "\n",
      "            player = spinner_player(spinner_factory(length))\n",
      "        else:\n",
      "            draw_unknown = None\n",
      "\n",
      "        padding = (' ',) * len_tip + background * math.ceil((length + len_tip) / len(background))\n",
      "        virtual_length, blanks = num_graphemes * (length + len_tip), (' ',) * length\n",
      "        return draw_known, running, ended, draw_unknown\n",
      "\n",
      "    assert chars or tip, 'tip is mandatory for transparent bars'\n",
      "    assert not (chars and not is_wide(chars[-1]) and has_wide(chars)), \\\n",
      "        'cannot use grapheme with a narrow last char'\n",
      "\n",
      "    chars = split_graphemes(chars or '')  # the only one not yet marked.\n",
      "    tip, background = (to_cells(x) for x in (tip, background or ' '))\n",
      "    underflow, overflow = extract_fill_graphemes(errors, (f'{VS_15}', f'{VS_15}'))\n",
      "    num_graphemes, len_tip = len(chars) or 1, len(tip)\n",
      "    return inner_bar_factory\n",
      "\n",
      "bar_factory(chars='', tip=None, background=None, borders=None, errors=None)\n",
      "underflow, overflow = extract_fill_graphemes(errors, (f'{VS_15}', f'{VS_15}'))\n",
      "{'errors': 'None'}\n",
      "overflow = ('',)\n",
      "def static_sliding_window(sep, gap, contents, length, right, initial):\n",
      "    \"\"\"Implement a sliding window over some content interspersed with a separator.\n",
      "    It is very efficient, storing data in only one string.\n",
      "\n",
      "    Note that the implementation is \"static\" in the sense that the content is pre-\n",
      "    calculated and maintained static, but actually when the window slides both the\n",
      "    separator and content seem to be moved.\n",
      "    Also keep in mind that `right` is for the content, not the window.\n",
      "    \"\"\"\n",
      "\n",
      "    def sliding_window():\n",
      "        pos = initial\n",
      "        while True:\n",
      "            if pos < 0:\n",
      "                pos += original\n",
      "            elif pos >= original:\n",
      "                pos -= original\n",
      "            yield content[pos:pos + length]\n",
      "            pos += step\n",
      "\n",
      "    adjusted_sep = fix_cells((sep * math.ceil(gap / len(sep)))[:gap]) if gap else ''\n",
      "    content = tuple(chain.from_iterable(chain.from_iterable(zip(repeat(adjusted_sep), contents))))\n",
      "    original, step = len(content), -1 if right else 1\n",
      "    assert length <= original, f'window slides inside content, {length} must be <= {original}'\n",
      "    content += content[:length]\n",
      "    return sliding_window()\n",
      "\n",
      "static_sliding_window(sep=(' ',), gap=3, contents=(('a', 'b', 'c'),), length=3, right=True, initial=0)\n",
      "original, step = len(content), -1 if right else 1\n",
      "{'content': \"(' ', ' ', ' ', 'a', 'b', 'c')\"}\n",
      "original = 6\n",
      "def test_split_options(param, expected):\n",
      "    if expected is SAME:\n",
      "        expected = param\n",
      "    assert split_options(param) == expected\n",
      "\n",
      "test_split_options(param=None, expected=(None, None))\n",
      "assert split_options(param) == expected\n",
      "{'param': 'None'}\n",
      "@py_assert2 = None\n",
      "def calibrated_fps(calibrate):\n",
      "    \"\"\"Calibration of the dynamic frames per second engine.\n",
      "\n",
      "    I've started with the equation y = log10(x + m) * k + n, where:\n",
      "      y is the desired fps, m and n are horizontal and vertical translation,\n",
      "      k is a calibration factor, computed from some user input c (see readme for details).\n",
      "\n",
      "    Considering minfps and maxfps as given constants, I came to:\n",
      "      fps = log10(x + 1) * k + minfps, which must be equal to maxfps for x = c,\n",
      "    so the factor k = (maxfps - minfps) / log10(c + 1), and\n",
      "      fps = log10(x + 1) * (maxfps - minfps) / log10(c + 1) + minfps\n",
      "\n",
      "    Neat! ;)\n",
      "\n",
      "    Args:\n",
      "        calibrate (float): user provided\n",
      "\n",
      "    Returns:\n",
      "        a callable to calculate the fps\n",
      "\n",
      "    \"\"\"\n",
      "    min_fps, max_fps = 2., 60.\n",
      "    calibrate = max(1e-6, calibrate)\n",
      "    adjust_log_curve = 100. / min(calibrate, 100.)  # adjust the curve for small numbers\n",
      "    factor = (max_fps - min_fps) / math.log10((calibrate * adjust_log_curve) + 1.)\n",
      "\n",
      "    def fps(rate):\n",
      "        if rate <= 0:\n",
      "            return 10.  # bootstrap speed\n",
      "        if rate < calibrate:\n",
      "            return math.log10((rate * adjust_log_curve) + 1.) * factor + min_fps\n",
      "        return max_fps\n",
      "\n",
      "    return fps\n",
      "\n",
      "calibrated_fps(calibrate=-5.0)\n",
      "calibrate = max(1e-6, calibrate)\n",
      "{'calibrate': '-5.0'}\n",
      "calibrate = 1e-06\n",
      "def elapsed_text(seconds, precise, prefix=''):\n",
      "    seconds = round(seconds, 1 if precise else 0)\n",
      "    if seconds < 60.:\n",
      "        return '{}{:{}f}s'.format(prefix, seconds, .1 if precise else .0)\n",
      "\n",
      "    minutes, seconds = divmod(seconds, 60.)\n",
      "    if minutes < 60.:\n",
      "        return '{}{:.0f}:{:0{}f}'.format(prefix, minutes, seconds, 4.1 if precise else 2.0)\n",
      "\n",
      "    hours, minutes = divmod(minutes, 60.)\n",
      "    return '{}{:.0f}:{:02.0f}:{:0{}f}'.format(prefix, hours, minutes, seconds,\n",
      "                                              4.1 if precise else 2.0)\n",
      "\n",
      "elapsed_text(seconds=1.23, precise=True, prefix='')\n",
      "seconds = round(seconds, 1 if precise else 0)\n",
      "{'seconds': '1.23'}\n",
      "seconds = 1.2\n",
      "def getblockimpl(lines, first, last, pilcrow):\n",
      "    max = len(lines) - 1\n",
      "    first -= 1\n",
      "    last -= 1\n",
      "    i = first\n",
      "    while i < max and not hastext(lines[i]):\n",
      "        if i >= last and istoplevel(lines[i + 1]):\n",
      "            return None, None, '# Nothing to send.' + pilcrow + eol\n",
      "        i += 1\n",
      "    while last < max and not istoplevel(lines[last + 1]):\n",
      "        last += 1\n",
      "    while first < last and not hastext(lines[first]):\n",
      "        first += 1\n",
      "    while first and not istoplevel(lines[first]):\n",
      "        first -= 1\n",
      "    lines[last] # Check for out of range.\n",
      "    return first, last, eol.join(l for l in lines[first:last + 1] if hastext(l)) + pilcrow + eol\n",
      "\n",
      "getblockimpl(lines=['', 'hello', 'function with', ' indented block', 'class with', '', '  block after blank', '  \\tand its own indented block', '\\t', '  and back again after a wrong blank', '', 'something else', '\\t', ' \\t', 'last'], first=10, last=11, pilcrow='')\n",
      "max = len(lines) - 1\n",
      "{'lines': \"['', 'hello', 'function with', ' indented block', 'class with', '', '  block after blank', '  \\\\tand its own indented block', '\\\\t', '  and back again after a wrong blank', '', 'something else', '\\\\t', ' \\\\t', 'last']\"}\n",
      "max = 14\n",
      "def parse(text, *, strip=False, strict=True):\n",
      "    parser = loguru._colorizer.AnsiParser()\n",
      "    parser.feed(text)\n",
      "    tokens = parser.done(strict=strict)\n",
      "\n",
      "    if strip:\n",
      "        return parser.strip(tokens)\n",
      "    return parser.colorize(tokens, \"\")\n",
      "\n",
      "parse(text='<red>Foo</red>\\n', strip=False, strict=True)\n",
      "parser.feed(text)\n",
      "{'text': \"'<red>Foo</red>\\\\n'\"}\n",
      "parser = {_tokens=[(1, ''), (2, '\\x1b[31m'), (1, 'Foo'), (4, '\\x1b[0m'), (1, '\\n')], _tags=[], _color_tokens=[]}\n",
      "def random_port():\n",
      "        port = helper.random_port()\n",
      "        while port in generated_ports:\n",
      "            port = helper.random_port()\n",
      "        generated_ports.add(port)\n",
      "        return port\n",
      "\n",
      "random_port(generated_ports=set())\n",
      "generated_ports.add(port)\n",
      "{'port': '56663'}\n",
      "generated_ports = {56663}\n",
      "def test_split_img_txt_da(inputs):\n",
      "    txt_da = DocumentArray()\n",
      "    img_da = DocumentArray()\n",
      "    for doc in inputs[0]:\n",
      "        split_img_txt_da(doc, img_da, txt_da)\n",
      "    assert len(txt_da) == inputs[1][0]\n",
      "    assert len(img_da) == inputs[1][1]\n",
      "\n",
      "test_split_img_txt_da(inputs=(<DocumentArray (length=4) at 140538627068688>, (3, 1)))\n",
      "assert len(txt_da) == inputs[1][0]\n",
      "{'txt_da': '<DocumentArray (length=3) at 140537876329136>'}\n",
      "@py_assert2 = None\n",
      "def test_server_preprocess_ndarray_image(image_uri, size):\n",
      "    d1 = Document(uri=image_uri)\n",
      "    d1.load_uri_to_blob()\n",
      "    d2 = Document(uri=image_uri)\n",
      "    d2.load_uri_to_image_tensor()\n",
      "\n",
      "    t1 = _transform_blob(size)(d1.blob).numpy()\n",
      "    t2 = _transform_ndarray(size)(d2.tensor).numpy()\n",
      "    assert t1.shape == t2.shape\n",
      "\n",
      "test_server_preprocess_ndarray_image(image_uri='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/jina-ai+clip-as-service/jina-ai+clip-as-service/tests/img/00000.jpg', size=224)\n",
      "t2 = _transform_ndarray(size)(d2.tensor).numpy()\n",
      "{'size': '224'}\n",
      "t2 = array([[[-1.8293927 ,  0.99326   ,  1.218778  , ..., -0.76831955,         -0.69819516, -0.6261791 ],        [-1.871253  ,  0.83835363,  1.1161801 , ..., -0.71867406,         -0.66353786, -0.5779885 ],        [-1.8495831 ,  0.3691529 ,  1.0339135 , ..., -0.60506105,         -0.5474743 , -0.5001206 ],        ...,        [-1.8324506 ,  0.46106532,  0.7589506 , ..., -1.3229892 ,         -1.4584173 , -1.509373  ],        [-1.8534997 ,  0.37634084,  0.6575078 , ..., -1.3002565 ,         -1.4684434 , -1.5584292 ],        [-1.8524398 ,  0.32740185,  0.56791097, ..., -1.275775  ,         -1.4632285 , -1.5475469 ]],       [[-1.8205297 ,  1.0893543 ,  1.31915   , ..., -0.27816093,         -0.18627533, -0.11378706],        [-1.8304325 ,  0.9543234 ,  1.2362387 , ..., -0.30391544,         -0.20329967, -0.11902631],        [-1.8111343 ,  0.4697153 ,  1.1526138 , ..., -0.29344058,         -0.21467075, -0.16618787],        ...,        [-1.6441311 ,  0.7576508 ,  1.1757797 , ..., -1.7395262 ,         -1.7533139 , -1.7488168 ],        [-1.5793352 ,  0.84063566,  1.2314364 , ..., -1.6973344 ,         -1.7601075 , -1.7529469 ],        [-1.4979699 ,  0.8915888 ,  1.2598896 , ..., -1.6417253 ,         -1.7217588 , -1.75764   ]],       [[-1.3964468 ,  1.3654916 ,  1.5833169 , ...,  0.11189864,          0.14282744,  0.18943931],        [-1.4222968 ,  1.2280223 ,  1.4979748 , ...,  0.10506461,          0.13882811,  0.19719559],        [-1.3639748 ,  0.79668385,  1.4436656 , ...,  0.11625384,          0.14236891,  0.16509578],        ...,        [-1.539558  ,  0.6254223 ,  0.88010895, ..., -1.4571475 ,         -1.4812293 , -1.4754997 ],        [-1.5454704 ,  0.67148876,  0.92816603, ..., -1.4164418 ,         -1.4880763 , -1.4814891 ],        [-1.4934982 ,  0.72418857,  0.94967735, ..., -1.371076  ,         -1.4606231 , -1.4841652 ]]], dtype=float32)\n",
      "def _from_timetuple(self, t):\n",
      "        self.days_from_epoch = calendar.timegm(t) // Date.DAY\n",
      "\n",
      "_from_timetuple(self=Date(0), t=time.struct_time(tm_year=2024, tm_mon=4, tm_mday=3, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=2, tm_yday=94, tm_isdst=-1))\n",
      "self.days_from_epoch = calendar.timegm(t) // Date.DAY\n",
      "{'t': 'time.struct_time(tm_year=2024, tm_mon=4, tm_mday=3, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=2, tm_yday=94, tm_isdst=-1)'}\n",
      "self.days_from_epoch = 19816\n",
      "def _insert(self, key, value):\n",
      "        flat_key = self._serialize_key(key)\n",
      "        i = self._index.get(flat_key, -1)\n",
      "        if i >= 0:\n",
      "            self._items[i] = (key, value)\n",
      "        else:\n",
      "            self._items.append((key, value))\n",
      "            self._index[flat_key] = len(self._items) - 1\n",
      "\n",
      "_insert(self=OrderedMapSerializedKey([]), key='bob', value=199)\n",
      "flat_key = self._serialize_key(key)\n",
      "{'key': \"'bob'\"}\n",
      "flat_key = b'\\xe3\\x81\\xbfbob'\n",
      "def stringified_dict_contains_value(key, value, str_dict):\n",
      "    \"\"\"Checks if dict in for of string like \"{'test': 5}\" contains\n",
      "    key/value pair. This works faster, then creating actual dict\n",
      "    from string since this operation is called for each task in case\n",
      "    of kwargs search.\"\"\"\n",
      "    if not str_dict:\n",
      "        return False\n",
      "    value = str(value)\n",
      "    try:\n",
      "        # + 3 for key right quote, one for colon and one for space\n",
      "        key_index = str_dict.index(key) + len(key) + 3\n",
      "    except ValueError:\n",
      "        return False\n",
      "    try:\n",
      "        comma_index = str_dict.index(',', key_index)\n",
      "    except ValueError:\n",
      "        # last value in dict\n",
      "        comma_index = str_dict.index('}', key_index)\n",
      "    return str(value) == str_dict[key_index:comma_index].strip('\"\\'')\n",
      "\n",
      "stringified_dict_contains_value(key='test', value=5, str_dict=\"{'test': 5}\")\n",
      "comma_index = str_dict.index('}', key_index)\n",
      "{'key_index': '9'}\n",
      "comma_index = 10\n",
      "def abs_path(path):\n",
      "    path = os.path.expanduser(path)\n",
      "    if not os.path.isabs(path):\n",
      "        cwd = os.environ.get('PWD') or os.getcwd()\n",
      "        path = os.path.join(cwd, path)\n",
      "    return path\n",
      "\n",
      "abs_path(path='~/file.txt')\n",
      "path = os.path.expanduser(path)\n",
      "{'path': \"'~/file.txt'\"}\n",
      "path = '/home/XXX/file.txt'\n",
      "def authenticate(pattern, email):\n",
      "    if '|' in pattern:\n",
      "        return email in pattern.split('|')\n",
      "    if '*' in pattern:\n",
      "        pattern = re.escape(pattern).replace(r'\\.\\*', r\"[A-Za-z0-9!#$%&'*+/=?^_`{|}~.\\-]*\")\n",
      "        return re.fullmatch(pattern, email)\n",
      "    return pattern == email\n",
      "\n",
      "authenticate(pattern='.*@example.com', email='one@example.com')\n",
      "pattern = re.escape(pattern).replace(r'\\.\\*', r\"[A-Za-z0-9!#$%&'*+/=?^_`{|}~.\\-]*\")\n",
      "{'pattern': \"'.*@example.com'\"}\n",
      "pattern = \"[A-Za-z0-9!#$%&'*+/=?^_`{|}~.\\\\-]*@example\\\\.com\"\n",
      "def _add_committer_to_committers(all_committers: List[str], initials: str, name: str, email: str):\n",
      "    committer_formatted = f'{initials},{name},{email}\\n'\n",
      "    committer_position = _position_of_committer_with_initials(all_committers, initials)\n",
      "    if committer_position is _COMMITTER_NOT_PRESENT:\n",
      "        all_committers.append(committer_formatted)\n",
      "    else:\n",
      "        all_committers[committer_position] = committer_formatted\n",
      "\n",
      "_add_committer_to_committers(all_committers=[], initials='initials3', name='name3', email='email3')\n",
      "committer_position = _position_of_committer_with_initials(all_committers, initials)\n",
      "{'all_committers': '[]', 'initials': \"'initials3'\"}\n",
      "committer_position = -1\n",
      "def add_committer(initials: str, name: str, email: str, *, file_path: str = _GLOBAL) -> None:\n",
      "    all_committers = _read_all_committers_from_file(file_path)\n",
      "    _add_committer_to_committers(all_committers, initials, name, email)\n",
      "    _write_committers_to_file(all_committers, file_path)\n",
      "\n",
      "add_committer(initials='initials3', name='name3', email='email3', file_path='/home/XXX/.guet/committers')\n",
      "_add_committer_to_committers(all_committers, initials, name, email)\n",
      "{'all_committers': \"['initials1,name1,email1\\\\n', 'initials2,name2,email2\\\\n']\", 'initials': \"'initials3'\", 'name': \"'name3'\", 'email': \"'email3'\"}\n",
      "all_committers = ['initials1,name1,email1\\n', 'initials2,name2,email2\\n', 'initials3,name3,email3\\n']\n",
      "def _load_global_committers(path: str) -> List[Committer]:\n",
      "    lines = read_lines(path)\n",
      "    committers = []\n",
      "    for line in lines:\n",
      "        initials, name, email = line.rstrip().split(',')\n",
      "        committers.append(GlobalCommitter(initials=initials, name=name, email=email))\n",
      "    return committers\n",
      "\n",
      "_load_global_committers(path='/home/XXX/.guet/committers')\n",
      "lines = read_lines(path)\n",
      "{'path': \"'/home/XXX/.guet/committers'\"}\n",
      "lines = ['initials,name,email\\n']\n",
      "def _load_local_committers(path: str, path_to_project_root: str) -> List[Committer]:\n",
      "    lines = read_lines(path)\n",
      "    committers = []\n",
      "    for line in lines:\n",
      "        initials, name, email = line.rstrip().split(',')\n",
      "        committers.append(LocalCommitter(initials=initials, name=name, email=email, project_root=path_to_project_root))\n",
      "    return committers\n",
      "\n",
      "_load_local_committers(path='/path/to/project/root/.guet/committers', path_to_project_root='/path/to/project/root')\n",
      "lines = read_lines(path)\n",
      "{'path': \"'/path/to/project/root/.guet/committers'\"}\n",
      "lines = ['initials1,othername1,otheremail1\\n']\n",
      "def _add_to_current_set_lines(current_set, formatted_set_committers_information):\n",
      "    git_path = git_path_from_cwd()\n",
      "    line_with_git_path = next((line for line in current_set if line.endswith(git_path)), None)\n",
      "    if line_with_git_path:\n",
      "        index = current_set.index(line_with_git_path)\n",
      "        current_set[index] = formatted_set_committers_information\n",
      "    else:\n",
      "        current_set.append(formatted_set_committers_information)\n",
      "\n",
      "_add_to_current_set_lines(current_set=['initials3,initials4,1000000000000,/absolute/path/to/other/.git'], formatted_set_committers_information='initials1,initials2,1000000000000,/absolute/path/to/.git')\n",
      "line_with_git_path = next((line for line in current_set if line.endswith(git_path)), None)\n",
      "{'git_path': \"'/absolute/path/to/.git'\"}\n",
      "line_with_git_path = None\n",
      "def _parse_file_content(create, path_to_hook):\n",
      "        _content = Hook._get_file_content(path_to_hook, create)\n",
      "        if _content != GUET_HOOK_FILE:\n",
      "            _content = Hook._handle_mismatched_content(_content, create)\n",
      "        return _content\n",
      "\n",
      "_parse_file_content(create=True, path_to_hook='/path/to/.git/hooks/name')\n",
      "_content = Hook._handle_mismatched_content(_content, create)\n",
      "{'_content': \"['Other', 'Content']\", 'create': 'True'}\n",
      "_content = ['#! /usr/bin/env python3', 'from guet.hooks import manage', 'import sys', 'manage(sys.argv[0])']\n",
      "def which(exe=None):\n",
      "    \"\"\"\n",
      "    Python clone of /usr/bin/which\n",
      "    \"\"\"\n",
      "\n",
      "    if not exe:\n",
      "        log.error(\"No executable was passed to be searched by salt.utils.path.which()\")\n",
      "        return None\n",
      "\n",
      "    ## define some utilities (we use closures here because our predecessor used them)\n",
      "    def is_executable_common(path):\n",
      "        \"\"\"\n",
      "        This returns truth if posixy semantics (which python simulates on\n",
      "        windows) states that this is executable.\n",
      "        \"\"\"\n",
      "        return os.path.isfile(path) and os.access(path, os.X_OK)\n",
      "\n",
      "    def resolve(path):\n",
      "        \"\"\"\n",
      "        This will take a path and recursively follow the link until we get to a\n",
      "        real file.\n",
      "        \"\"\"\n",
      "        while os.path.islink(path):\n",
      "            res = readlink(path)\n",
      "\n",
      "            # if the link points to a relative target, then convert it to an\n",
      "            # absolute path relative to the original path\n",
      "            if not os.path.isabs(res):\n",
      "                directory, _ = os.path.split(path)\n",
      "                res = join(directory, res)\n",
      "            path = res\n",
      "        return path\n",
      "\n",
      "    # windows-only\n",
      "    def has_executable_ext(path, ext_membership):\n",
      "        \"\"\"\n",
      "        Extract the extension from the specified path, lowercase it so we\n",
      "        can be insensitive, and then check it against the available exts.\n",
      "        \"\"\"\n",
      "        p, ext = os.path.splitext(path)\n",
      "        return ext.lower() in ext_membership\n",
      "\n",
      "    ## prepare related variables from the environment\n",
      "    res = salt.utils.stringutils.to_unicode(os.environ.get(\"PATH\", \"\"))\n",
      "    system_path = res.split(os.pathsep)\n",
      "\n",
      "    # add some reasonable defaults in case someone's PATH is busted\n",
      "    if not salt.utils.platform.is_windows():\n",
      "        res = set(system_path)\n",
      "        extended_path = [\n",
      "            \"/sbin\",\n",
      "            \"/bin\",\n",
      "            \"/usr/sbin\",\n",
      "            \"/usr/bin\",\n",
      "            \"/usr/local/sbin\",\n",
      "            \"/usr/local/bin\",\n",
      "        ]\n",
      "        system_path.extend([p for p in extended_path if p not in res])\n",
      "\n",
      "    ## now to define the semantics of what's considered executable on a given platform\n",
      "    if salt.utils.platform.is_windows():\n",
      "        # executable semantics on windows requires us to search PATHEXT\n",
      "        res = salt.utils.stringutils.to_str(os.environ.get(\"PATHEXT\", \".EXE\"))\n",
      "\n",
      "        # generate two variables, one of them for O(n) searches (but ordered)\n",
      "        # and another for O(1) searches. the previous guy was trying to use\n",
      "        # memoization with a function that has no arguments, this provides\n",
      "        # the exact same benefit\n",
      "        pathext = res.split(os.pathsep)\n",
      "        res = {ext.lower() for ext in pathext}\n",
      "\n",
      "        # check if our caller already specified a valid extension as then we don't need to match it\n",
      "        _, ext = os.path.splitext(exe)\n",
      "        if ext.lower() in res:\n",
      "            pathext = [\"\"]\n",
      "\n",
      "            is_executable = is_executable_common\n",
      "\n",
      "        # The specified extension isn't valid, so we just assume it's part of the\n",
      "        # filename and proceed to walk the pathext list\n",
      "        else:\n",
      "            is_executable = lambda path, membership=res: is_executable_common(\n",
      "                path\n",
      "            ) and has_executable_ext(path, membership)\n",
      "\n",
      "    else:\n",
      "        # in posix, there's no such thing as file extensions..only zuul\n",
      "        pathext = [\"\"]\n",
      "\n",
      "        # executable semantics are pretty simple on reasonable platforms...\n",
      "        is_executable = is_executable_common\n",
      "\n",
      "    ## search for the executable\n",
      "\n",
      "    # check to see if the full path was specified as then we don't need\n",
      "    # to actually walk the system_path for any reason\n",
      "    if is_executable(exe):\n",
      "        return exe\n",
      "\n",
      "    # now to search through our system_path\n",
      "    for path in system_path:\n",
      "        p = join(path, exe)\n",
      "\n",
      "        # iterate through all extensions to see which one is executable\n",
      "        for ext in pathext:\n",
      "            pext = p + ext\n",
      "            rp = resolve(pext)\n",
      "            if is_executable(rp):\n",
      "                return p + ext\n",
      "            continue\n",
      "        continue\n",
      "\n",
      "    ## if something was executable, we should've found it already...\n",
      "    log.trace(\n",
      "        \"'%s' could not be found in the following search path: '%s'\", exe, system_path\n",
      "    )\n",
      "    return None\n",
      "\n",
      "which(exe='true')\n",
      "res = set(system_path)\n",
      "{'system_path': \"['/home/XXX/.gdrive-downloader', '/local/arise/XXX/miniforge3/bin', '/home/XXX/.gvm/pkgsets/go1.19.1/global/bin', '/home/XXX/.gvm/gos/go1.19.1/bin', '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/bin', '/home/XXX/.gvm/bin', '/local/rcs/XXX/miniforge3/envs/saltstack+salt/bin', '/local/rcs/XXX/miniforge3/condabin', '/home/XXX/.gdrive-downloader', '/local/arise/XXX/miniforge3/bin', '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/remote-cli', '/usr/local/sbin', '/usr/local/bin', '/usr/sbin', '/usr/bin', '/sbin', '/bin', '/usr/games', '/usr/local/games', '/snap/bin', '/home/XXX/.local/bin', '/home/XXX/.local/bin']\"}\n",
      "res = {'/usr/games', '/sbin', '/home/XXX/.gvm/gos/go1.19.1/bin', '/home/XXX/.gvm/bin', '/home/XXX/.gvm/pkgsets/go1.19.1/global/bin', '/home/XXX/.local/bin', '/snap/bin', '/usr/bin', '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/remote-cli', '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/bin', '/bin', '/local/rcs/XXX/miniforge3/envs/saltstack+salt/bin', '/usr/local/games', '/usr/local/sbin', '/home/XXX/.gdrive-downloader', '/local/rcs/XXX/miniforge3/condabin', '/local/arise/XXX/miniforge3/bin', '/usr/sbin', '/usr/local/bin'}\n",
      "def join(*parts, **kwargs):\n",
      "    \"\"\"\n",
      "    This functions tries to solve some issues when joining multiple absolute\n",
      "    paths on both *nix and windows platforms.\n",
      "\n",
      "    See tests/unit/utils/path_join_test.py for some examples on what's being\n",
      "    talked about here.\n",
      "\n",
      "    The \"use_posixpath\" kwarg can be be used to force joining using poxixpath,\n",
      "    which is useful for Salt fileserver paths on Windows masters.\n",
      "    \"\"\"\n",
      "    parts = [salt.utils.stringutils.to_str(part) for part in parts]\n",
      "\n",
      "    kwargs = salt.utils.args.clean_kwargs(**kwargs)\n",
      "    use_posixpath = kwargs.pop(\"use_posixpath\", False)\n",
      "    if kwargs:\n",
      "        salt.utils.args.invalid_kwargs(kwargs)\n",
      "\n",
      "    pathlib = posixpath if use_posixpath else os.path\n",
      "\n",
      "    # Normalize path converting any os.sep as needed\n",
      "    parts = [pathlib.normpath(p) for p in parts]\n",
      "\n",
      "    try:\n",
      "        root = parts.pop(0)\n",
      "    except IndexError:\n",
      "        # No args passed to func\n",
      "        return \"\"\n",
      "\n",
      "    root = salt.utils.stringutils.to_unicode(root)\n",
      "    if not parts:\n",
      "        ret = root\n",
      "    else:\n",
      "        stripped = [p.lstrip(os.sep) for p in parts]\n",
      "        ret = pathlib.join(root, *salt.utils.data.decode(stripped))\n",
      "    return pathlib.normpath(ret)\n",
      "\n",
      "join(parts=('/home/XXX/.gdrive-downloader', 'true'), kwargs={})\n",
      "ret = pathlib.join(root, *salt.utils.data.decode(stripped))\n",
      "{'stripped': \"['true']\"}\n",
      "ret = '/home/XXX/.gdrive-downloader/true'\n",
      "def parse(theme_file):\n",
      "    \"\"\"Parse the theme file.\"\"\"\n",
      "    data = util.read_file_json(theme_file)\n",
      "\n",
      "    if \"wallpaper\" not in data:\n",
      "        data[\"wallpaper\"] = \"None\"\n",
      "\n",
      "    if \"alpha\" not in data:\n",
      "        data[\"alpha\"] = util.Color.alpha_num\n",
      "\n",
      "    # Terminal.sexy format.\n",
      "    if \"color\" in data:\n",
      "        data = terminal_sexy_to_wal(data)\n",
      "\n",
      "    return data\n",
      "\n",
      "parse(theme_file='tests/test_files/test_file.json')\n",
      "data = util.read_file_json(theme_file)\n",
      "{'theme_file': \"'tests/test_files/test_file.json'\"}\n",
      "data = {'wallpaper': '5.png', 'alpha': '100', 'special': {'background': '#1F211E', 'foreground': '#F5F1F4', 'cursor': '#F5F1F4'}, 'colors': {'color0': '#1F211E', 'color1': '#4B7A85', 'color2': '#CC6A93', 'color3': '#5C9894', 'color4': '#A0A89B', 'color5': '#D1B9A9', 'color6': '#E3D6D8', 'color7': '#F5F1F4', 'color8': '#666666', 'color9': '#4B7A85', 'color10': '#CC6A93', 'color11': '#5C9894', 'color12': '#A0A89B', 'color13': '#D1B9A9', 'color14': '#E3D6D8', 'color15': '#F5F1F4'}}\n",
      "def get(img, cache_dir=CACHE_DIR, iterative=False, recursive=False):\n",
      "    \"\"\"Validate image input.\"\"\"\n",
      "    if os.path.isfile(img):\n",
      "        wal_img = img\n",
      "\n",
      "    elif os.path.isdir(img):\n",
      "        if iterative:\n",
      "            wal_img = get_next_image(img, recursive)\n",
      "\n",
      "        else:\n",
      "            wal_img = get_random_image(img, recursive)\n",
      "\n",
      "    else:\n",
      "        logging.error(\"No valid image file found.\")\n",
      "        sys.exit(1)\n",
      "\n",
      "    wal_img = os.path.abspath(wal_img)\n",
      "\n",
      "    # Cache the image file path.\n",
      "    util.save_file(wal_img, os.path.join(cache_dir, \"wal\"))\n",
      "\n",
      "    logging.info(\"Using image \\033[1;37m%s\\033[0m.\", os.path.basename(wal_img))\n",
      "    return wal_img\n",
      "\n",
      "get(img='tests/test_files/test.jpg', cache_dir='/home/XXX/.cache/wal', iterative=False, recursive=False)\n",
      "wal_img = os.path.abspath(wal_img)\n",
      "{'wal_img': \"'tests/test_files/test.jpg'\"}\n",
      "wal_img = '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/dylanaraps+pywal/dylanaraps+pywal/tests/test_files/test.jpg'\n",
      "def get_random_image(img_dir, recursive):\n",
      "    \"\"\"Pick a random image file from a directory.\"\"\"\n",
      "    if recursive:\n",
      "        images, current_wall = get_image_dir_recursive(img_dir)\n",
      "    else:\n",
      "        images, current_wall = get_image_dir(img_dir)\n",
      "\n",
      "    if len(images) > 2 and current_wall in images:\n",
      "        images.remove(current_wall)\n",
      "\n",
      "    elif not images:\n",
      "        logging.error(\"No images found in directory.\")\n",
      "        sys.exit(1)\n",
      "\n",
      "    random.shuffle(images)\n",
      "    return os.path.join(img_dir if not recursive else \"\", images[0])\n",
      "\n",
      "get_random_image(img_dir='tests/test_files', recursive=False)\n",
      "random.shuffle(images)\n",
      "{'images': \"['test2.jpg', 'test.png']\"}\n",
      "images = ['test.png', 'test2.jpg']\n",
      "def get_image_dir(img_dir):\n",
      "    \"\"\"Get all images in a directory.\"\"\"\n",
      "    current_wall = wallpaper.get()\n",
      "    current_wall = os.path.basename(current_wall)\n",
      "\n",
      "    file_types = (\".png\", \".jpg\", \".jpeg\", \".jpe\", \".gif\")\n",
      "\n",
      "    return [img.name for img in os.scandir(img_dir)\n",
      "            if img.name.lower().endswith(file_types)], current_wall\n",
      "\n",
      "get_image_dir(img_dir='tests/test_files')\n",
      "current_wall = os.path.basename(current_wall)\n",
      "{'current_wall': \"'/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/dylanaraps+pywal/dylanaraps+pywal/tests/test_files/test.jpg'\"}\n",
      "current_wall = 'test.jpg'\n",
      "def check_impl_detail(**guards):\n",
      "    \"\"\"This function returns True or False depending on the host platform.\n",
      "       Examples:\n",
      "          if check_impl_detail():               # only on CPython (default)\n",
      "          if check_impl_detail(jython=True):    # only on Jython\n",
      "          if check_impl_detail(cpython=False):  # everywhere except on CPython\n",
      "    \"\"\"\n",
      "    guards, default = _parse_guards(guards)\n",
      "    return guards.get(sys.implementation.name, default)\n",
      "\n",
      "check_impl_detail(guards={})\n",
      "guards, default = _parse_guards(guards)\n",
      "{'guards': '{}'}\n",
      "default = False\n",
      "def _find_and_replace_patterns(content, patterns_and_insertions):\n",
      "  r\"\"\"content: str\n",
      "\n",
      "  patterns_and_insertions: List[Dict]\n",
      "\n",
      "  Example for patterns_and_insertions:\n",
      "\n",
      "      [\n",
      "          {\n",
      "              \"pattern\" :\n",
      "              r\"(?:\\\\figcompfigures{\\s*)(?P<first>.*?)\\s*}\\s*{\\s*(?P<second>.*?)\\s*}\\s*{\\s*(?P<third>.*?)\\s*}\",\n",
      "              \"insertion\" :\n",
      "              r\"\\parbox[c]{{{second}\\linewidth}}{{\\includegraphics[width={third}\\linewidth]{{figures/{first}}}}}}\",\n",
      "              \"description\": \"Replace figcompfigures\"\n",
      "          },\n",
      "      ]\n",
      "  \"\"\"\n",
      "  for pattern_and_insertion in patterns_and_insertions:\n",
      "    pattern = pattern_and_insertion['pattern']\n",
      "    insertion = pattern_and_insertion['insertion']\n",
      "    description = pattern_and_insertion['description']\n",
      "    logging.info('Processing pattern: %s.', description)\n",
      "    p = regex.compile(pattern)\n",
      "    m = p.search(content)\n",
      "    while m is not None:\n",
      "      local_insertion = insertion.format(**m.groupdict())\n",
      "      if pattern_and_insertion.get('strip_whitespace', True):\n",
      "        local_insertion = strip_whitespace(local_insertion)\n",
      "      logging.info(f'Found {content[m.start():m.end()]:<70}')\n",
      "      logging.info(f'Replacing with {local_insertion:<30}')\n",
      "      content = content[: m.start()] + local_insertion + content[m.end() :]\n",
      "      m = p.search(content)\n",
      "    logging.info('Finished pattern: %s.', description)\n",
      "  return content\n",
      "\n",
      "_find_and_replace_patterns(content='& \\\\figcompfigures{\\n\\timage1.jpg\\n}{\\n\\t\\\\ww\\n}{\\n\\t1.0\\n\\t}\\n& \\\\figcompfigures{image2.jpg}{\\\\ww}{1.0}', patterns_and_insertions=[{'pattern': '(?:\\\\\\\\figcompfigures{\\\\s*)(?P<first>.*?)\\\\s*}\\\\s*{\\\\s*(?P<second>.*?)\\\\s*}\\\\s*{\\\\s*(?P<third>.*?)\\\\s*}', 'insertion': '\\\\parbox[c]{{\\n            {second}\\\\linewidth\\n        }}{{\\n            \\\\includegraphics[\\n                width={third}\\\\linewidth\\n            ]{{\\n                figures/{first}\\n            }}\\n        }} ', 'description': 'Replace figcompfigures'}])\n",
      "p = regex.compile(pattern)\n",
      "{'pattern': \"'(?:\\\\\\\\\\\\\\\\figcompfigures{\\\\\\\\s*)(?P<first>.*?)\\\\\\\\s*}\\\\\\\\s*{\\\\\\\\s*(?P<second>.*?)\\\\\\\\s*}\\\\\\\\s*{\\\\\\\\s*(?P<third>.*?)\\\\\\\\s*}'\"}\n",
      "p = regex.Regex('(?:\\\\\\\\figcompfigures{\\\\s*)(?P<first>.*?)\\\\s*}\\\\s*{\\\\s*(?P<second>.*?)\\\\s*}\\\\s*{\\\\s*(?P<third>.*?)\\\\s*}', flags=regex.V0)\n",
      "def strip_whitespace(text):\n",
      "  \"\"\"Strips all whitespace characters.\n",
      "\n",
      "  https://stackoverflow.com/questions/8270092/remove-all-whitespace-in-a-string\n",
      "  \"\"\"\n",
      "  pattern = regex.compile(r'\\s+')\n",
      "  text = regex.sub(pattern, '', text)\n",
      "  return text\n",
      "\n",
      "strip_whitespace(text='\\\\parbox[c]{\\n            \\\\ww\\\\linewidth\\n        }{\\n            \\\\includegraphics[\\n                width=1.0\\\\linewidth\\n            ]{\\n                figures/image1.jpg\\n            }\\n        } ')\n",
      "text = regex.sub(pattern, '', text)\n",
      "{'pattern': \"regex.Regex('\\\\\\\\s+', flags=regex.V0)\", 'text': \"'\\\\\\\\parbox[c]{\\\\n            \\\\\\\\ww\\\\\\\\linewidth\\\\n        }{\\\\n            \\\\\\\\includegraphics[\\\\n                width=1.0\\\\\\\\linewidth\\\\n            ]{\\\\n                figures/image1.jpg\\\\n            }\\\\n        } '\"}\n",
      "text = '\\\\parbox[c]{\\\\ww\\\\linewidth}{\\\\includegraphics[width=1.0\\\\linewidth]{figures/image1.jpg}}'\n",
      "def merge_args_into_config(args, config_params):\n",
      "  final_args = copy.deepcopy(config_params)\n",
      "  config_keys = config_params.keys()\n",
      "  for key, value in args.items():\n",
      "    if key in config_keys:\n",
      "      if any([isinstance(value, t) for t in [str, bool, float, int]]):\n",
      "        # Overwrites config value with args value.\n",
      "        final_args[key] = value\n",
      "      elif isinstance(value, list):\n",
      "        # Appends args values to config values.\n",
      "        final_args[key] = value + config_params[key]\n",
      "      elif isinstance(value, dict):\n",
      "        # Updates config params with args params.\n",
      "        final_args[key].update(**value)\n",
      "    else:\n",
      "      final_args[key] = value\n",
      "  return final_args\n",
      "\n",
      "merge_args_into_config(args={'input_folder': 'foo/bar', 'resize_images': False, 'im_size': 500, 'compress_pdf': False, 'pdf_im_resolution': 500, 'images_allowlist': {'path1/': 1000}, 'commands_to_delete': ['\\\\todo1'], 'use_external_tikz': 'foo/bar/tikz'}, config_params={'input_folder': 'foo_/bar_', 'resize_images': True, 'im_size': 1000, 'compress_pdf': True, 'pdf_im_resolution': 1000, 'images_allowlist': {'path2/': 1000}, 'commands_to_delete': ['\\\\todo2'], 'use_external_tikz': 'foo_/bar_/tikz_'})\n",
      "final_args = copy.deepcopy(config_params)\n",
      "{'config_params': \"{'input_folder': 'foo_/bar_', 'resize_images': True, 'im_size': 1000, 'compress_pdf': True, 'pdf_im_resolution': 1000, 'images_allowlist': {'path2/': 1000}, 'commands_to_delete': ['\\\\\\\\todo2'], 'use_external_tikz': 'foo_/bar_/tikz_'}\"}\n",
      "final_args = {'input_folder': 'foo_/bar_', 'resize_images': True, 'im_size': 1000, 'compress_pdf': True, 'pdf_im_resolution': 1000, 'images_allowlist': {'path2/': 1000}, 'commands_to_delete': ['\\\\todo2'], 'use_external_tikz': 'foo_/bar_/tikz_'}\n",
      "def _remove_comments_inline(text):\n",
      "  \"\"\"Removes the comments from the string 'text' and ignores % inside \\\\url{}.\"\"\"\n",
      "  if 'auto-ignore' in text:\n",
      "    return text\n",
      "  if text.lstrip(' ').lstrip('\\t').startswith('%'):\n",
      "    return ''\n",
      "\n",
      "  url_pattern = r'\\\\url\\{(?>[^{}]|(?R))*\\}'\n",
      "\n",
      "  def remove_comments(segment):\n",
      "    \"\"\"Remove comments from a segment of text.\"\"\"\n",
      "    if segment.lstrip().startswith('%'):\n",
      "      return ''\n",
      "    match = regex.search(r'(?<!\\\\)%', segment)\n",
      "    if match:\n",
      "      return segment[: match.end()] + '\\n'\n",
      "    else:\n",
      "      return segment\n",
      "\n",
      "  # split the text into segments based on \\url{} tags\n",
      "  segments = regex.split(f'({url_pattern})', text)\n",
      "\n",
      "  for i in range(len(segments)):\n",
      "    # only process segments that are not part of a \\url{} tag\n",
      "    if not regex.match(url_pattern, segments[i]):\n",
      "      segments[i] = remove_comments(segments[i])\n",
      "\n",
      "  final_text = ''.join(segments)\n",
      "  return (\n",
      "      final_text\n",
      "      if final_text.endswith('\\n') or final_text.endswith('\\\\n')\n",
      "      else final_text + '\\n'\n",
      "  )\n",
      "\n",
      "_remove_comments_inline(text='Foo %Comment\\n')\n",
      "segments = regex.split(f'({url_pattern})', text)\n",
      "{'text': \"'Foo %Comment\\\\n'\"}\n",
      "segments = ['Foo %Comment\\n']\n",
      "def _replace_includesvg(content, svg_inkscape_files):\n",
      "  def repl_svg(matchobj):\n",
      "    svg_path = matchobj.group(2)\n",
      "    svg_filename = os.path.basename(svg_path)\n",
      "    # search in svg_inkscape split if pdf_tex file is available\n",
      "    matching_pdf_tex_files = _keep_pattern(\n",
      "        svg_inkscape_files, ['/' + svg_filename + '-tex.pdf_tex']\n",
      "    )\n",
      "    if len(matching_pdf_tex_files) == 1:\n",
      "      options = '' if matchobj.group(1) is None else matchobj.group(1)\n",
      "      return f'\\\\includeinkscape{options}{{{matching_pdf_tex_files[0]}}}'\n",
      "    else:\n",
      "      return matchobj.group(0)\n",
      "\n",
      "  content = regex.sub(r'\\\\includesvg(\\[.*?\\])?{(.*?)}', repl_svg, content)\n",
      "\n",
      "  return content\n",
      "\n",
      "_replace_includesvg(content='Foo\\\\includesvg{test2}\\nFoo', svg_inkscape_files=['ext_svg/test1-tex.pdf_tex', 'ext_svg/test2-tex.pdf_tex'])\n",
      "content = regex.sub(r'\\\\includesvg(\\[.*?\\])?{(.*?)}', repl_svg, content)\n",
      "{'repl_svg': '<function _replace_includesvg.<locals>.repl_svg at 0x7f1855304550>', 'content': \"'Foo\\\\\\\\includesvg{test2}\\\\nFoo'\"}\n",
      "content = 'Foo\\\\includeinkscape{ext_svg/test2-tex.pdf_tex}\\nFoo'\n",
      "def _search_reference(filename, contents, strict=False):\n",
      "  \"\"\"Returns a match object if filename is referenced in contents, and None otherwise.\n",
      "\n",
      "  If not strict mode, path prefix and extension are optional.\n",
      "  \"\"\"\n",
      "  if strict:\n",
      "    # regex pattern for strict=True for path/to/img.ext:\n",
      "    # \\{[\\s%]*path/to/img\\.ext[\\s%]*\\}\n",
      "    filename_regex = filename.replace('.', r'\\.')\n",
      "  else:\n",
      "    filename_path = Path(filename)\n",
      "\n",
      "    # make extension optional\n",
      "    root, extension = filename_path.stem, filename_path.suffix\n",
      "    basename_regex = '{}({})?'.format(\n",
      "        regex.escape(root), regex.escape(extension)\n",
      "    )\n",
      "\n",
      "    # iterate through parent fragments to make path prefix optional\n",
      "    path_prefix_regex = ''\n",
      "    for fragment in reversed(filename_path.parents):\n",
      "      if fragment.name == '.':\n",
      "        continue\n",
      "      fragment = regex.escape(fragment.name)\n",
      "      path_prefix_regex = '({}{}{})?'.format(\n",
      "          path_prefix_regex, fragment, os.sep\n",
      "      )\n",
      "\n",
      "    # Regex pattern for strict=True for path/to/img.ext:\n",
      "    # \\{[\\s%]*(<path_prefix>)?<basename>(<ext>)?[\\s%]*\\}\n",
      "    filename_regex = path_prefix_regex + basename_regex\n",
      "\n",
      "  # Some files 'path/to/file' are referenced in tex as './path/to/file' thus\n",
      "  # adds prefix for relative paths starting with './' or '.\\' to regex search.\n",
      "  filename_regex = r'(.' + os.sep + r')?' + filename_regex\n",
      "\n",
      "  # Pads with braces and optional whitespace/comment characters.\n",
      "  patn = r'\\{{[\\s%]*{}[\\s%]*\\}}'.format(filename_regex)\n",
      "  # Picture references in LaTeX are allowed to be in different cases.\n",
      "  return regex.search(patn, contents, regex.IGNORECASE)\n",
      "\n",
      "_search_reference(filename='to/img.ext', contents='{img.ext}', strict=False)\n",
      "filename_path = Path(filename)\n",
      "{'filename': \"'to/img.ext'\"}\n",
      "filename_path = PosixPath('to/img.ext')\n",
      "def run_arxiv_cleaner(parameters):\n",
      "  \"\"\"Core of the code, runs the actual arXiv cleaner.\"\"\"\n",
      "\n",
      "  files_to_delete = [\n",
      "      r'\\.aux$',\n",
      "      r'\\.sh$',\n",
      "      r'\\.blg$',\n",
      "      r'\\.brf$',\n",
      "      r'\\.log$',\n",
      "      r'\\.out$',\n",
      "      r'\\.ps$',\n",
      "      r'\\.dvi$',\n",
      "      r'\\.synctex.gz$',\n",
      "      '~$',\n",
      "      r'\\.backup$',\n",
      "      r'\\.gitignore$',\n",
      "      r'\\.DS_Store$',\n",
      "      r'\\.svg$',\n",
      "      r'^\\.idea',\n",
      "      r'\\.dpth$',\n",
      "      r'\\.md5$',\n",
      "      r'\\.dep$',\n",
      "      r'\\.auxlock$',\n",
      "      r'\\.fls$',\n",
      "      r'\\.fdb_latexmk$',\n",
      "  ]\n",
      "\n",
      "  if not parameters['keep_bib']:\n",
      "    files_to_delete.append(r'\\.bib$')\n",
      "\n",
      "  parameters.update({\n",
      "      'to_delete': files_to_delete,\n",
      "      'figures_to_copy_if_referenced': [\n",
      "          r'\\.png$',\n",
      "          r'\\.jpg$',\n",
      "          r'\\.jpeg$',\n",
      "          r'\\.pdf$',\n",
      "      ],\n",
      "  })\n",
      "\n",
      "  logging.info('Collecting file structure.')\n",
      "  parameters['output_folder'] = _create_out_folder(parameters['input_folder'])\n",
      "\n",
      "  from_zip = parameters['input_folder'].endswith('.zip')\n",
      "  tempdir_context = (\n",
      "      tempfile.TemporaryDirectory() if from_zip else contextlib.suppress()\n",
      "  )\n",
      "\n",
      "  with tempdir_context as tempdir:\n",
      "\n",
      "    if from_zip:\n",
      "      logging.info('Unzipping input folder.')\n",
      "      shutil.unpack_archive(parameters['input_folder'], tempdir)\n",
      "      parameters['input_folder'] = tempdir\n",
      "\n",
      "    splits = _split_all_files(parameters)\n",
      "\n",
      "    logging.info('Reading all tex files')\n",
      "    tex_contents = _read_all_tex_contents(\n",
      "        splits['tex_in_root'] + splits['tex_not_in_root'], parameters\n",
      "    )\n",
      "\n",
      "    for tex_file in tex_contents:\n",
      "      logging.info('Removing comments in file %s.', tex_file)\n",
      "      tex_contents[tex_file] = _remove_comments_and_commands_to_delete(\n",
      "          tex_contents[tex_file], parameters\n",
      "      )\n",
      "\n",
      "    for tex_file in tex_contents:\n",
      "      logging.info('Replacing \\\\includesvg calls in file %s.', tex_file)\n",
      "      tex_contents[tex_file] = _replace_includesvg(\n",
      "          tex_contents[tex_file], splits['svg_inkscape']\n",
      "      )\n",
      "\n",
      "    for tex_file in tex_contents:\n",
      "      logging.info('Replacing Tikz Pictures in file %s.', tex_file)\n",
      "      content = _replace_tikzpictures(\n",
      "          tex_contents[tex_file], splits['external_tikz_figures']\n",
      "      )\n",
      "      # If file ends with '\\n' already, the split in last line would add an extra\n",
      "      # '\\n', so we remove it.\n",
      "      tex_contents[tex_file] = content.split('\\n')\n",
      "\n",
      "    _keep_only_referenced_tex(tex_contents, splits)\n",
      "    _add_root_tex_files(splits)\n",
      "\n",
      "    for tex_file in splits['tex_to_copy']:\n",
      "      logging.info('Replacing patterns in file %s.', tex_file)\n",
      "      content = '\\n'.join(tex_contents[tex_file])\n",
      "      content = _find_and_replace_patterns(\n",
      "          content, parameters.get('patterns_and_insertions', list())\n",
      "      )\n",
      "      tex_contents[tex_file] = content\n",
      "      new_path = os.path.join(parameters['output_folder'], tex_file)\n",
      "      logging.info('Writing modified contents to %s.', new_path)\n",
      "      _write_file_content(\n",
      "          content,\n",
      "          new_path,\n",
      "      )\n",
      "\n",
      "    full_content = '\\n'.join(\n",
      "        ''.join(tex_contents[fn]) for fn in splits['tex_to_copy']\n",
      "    )\n",
      "    _copy_only_referenced_non_tex_not_in_root(parameters, full_content, splits)\n",
      "    for non_tex_file in splits['non_tex_in_root']:\n",
      "      logging.info('Copying non-tex file %s.', non_tex_file)\n",
      "      _copy_file(non_tex_file, parameters)\n",
      "\n",
      "    _resize_and_copy_figures_if_referenced(parameters, full_content, splits)\n",
      "    logging.info('Outputs written to %s', parameters['output_folder'])\n",
      "\n",
      "run_arxiv_cleaner(parameters={'input_folder': 'tex', 'images_allowlist': {'images/im2_included.jpg': 200, 'images/im3_included.png': 400}, 'resize_images': True, 'im_size': 100, 'compress_pdf': False, 'pdf_im_resolution': 500, 'commands_to_delete': ['mytodo'], 'commands_only_to_delete': ['red'], 'environments_to_delete': ['mynote'], 'use_external_tikz': 'ext_tikz', 'keep_bib': False})\n",
      "_keep_only_referenced_tex(tex_contents, splits)\n",
      "{'tex_contents': \"{'main.tex': ['\\\\\\\\begin{document}', 'Text', '', 'Text%', '', '', 'This is a percent \\\\\\\\%.', '\\\\\\\\includegraphics{images/im1_included.png}', '\\\\\\\\includegraphics{images/im3_included.png}', '\\\\\\\\includegraphics{%', '  images/im4_included.png%', '  }', '\\\\\\\\includegraphics[width=.5\\\\\\\\linewidth]{%', '  images/im5_included.jpg}', '', '\\\\\\\\includegraphics{./images/im3_included.png}', '', 'This line should not be separated', '%', 'from this one.', '', '', '', '\\\\\\\\newif\\\\\\\\ifvar', '', '\\\\\\\\ifvar', '\\\\\\\\fi', '', '\\\\\\\\newcommand{\\\\\\\\red}[1]{{\\\\\\\\color{red} #1}}', 'hello test hello', 'test hello', 'test', '', '', '\\\\\\\\input{figures/figure_included.tex}', '', '\\\\\\\\includegraphics{ext_tikz/test1.pdf}', '', '\\\\\\\\input{figures/figure_included.tikz}', '', '\\\\\\\\begin{tikzpicture}', '    \\\\\\\\node (test) at (0,0) {Test3};', '\\\\\\\\end{tikzpicture}', '', '\\\\\\\\tikzsetnextfilename{test_no_match}', '\\\\\\\\begin{tikzpicture}', '    \\\\\\\\node (test) at (0,0) {Test4};', '\\\\\\\\end{tikzpicture}', '', '\\\\\\\\end{document}', ''], 'figures/figure_not_included.tex': ['\\\\\\\\addplot{figures/data_not_included.txt}', '\\\\\\\\input{figures/figure_not_included_2.tex}', ''], 'figures/figure_not_included_2.tex': [''], 'figures/figure_included.tikz': ['\\\\ufeff\\\\\\\\includegraphics{ext_tikz/test2.pdf}', ''], 'figures/figure_included.tex': ['\\\\\\\\includegraphics{images/im2_included.jpg}', '\\\\\\\\addplot{figures/data_included.txt}', '']}\", 'splits': \"{'all': ['main.bib', 'main.bbl', 'main.tex', 'main.aux', 'ext_tikz/test2.pdf', 'ext_tikz/test1.pdf', 'ext_tikz/figure_not_included.pdf', 'figures/data_not_included.txt', 'figures/figure_not_included.tex', 'figures/figure_not_included_2.tex', 'figures/figure_included.tikz', 'figures/figure_included.tex', 'figures/data_included.txt', 'not_included/figures/data_included.txt', 'images/im4_included.png', 'images/im1.png', 'images/im4_not_included.png', 'images/im3_included.png', 'images/im2_included.jpg', 'images/im5_not_included.jpg', 'images/im5_included.jpg', 'images/im1_included.png', 'images/im_not_included.png', 'images/include/images/im3_included.png'], 'in_root': ['main.bib', 'main.bbl', 'main.tex', 'main.aux'], 'not_in_root': ['ext_tikz/test2.pdf', 'ext_tikz/test1.pdf', 'ext_tikz/figure_not_included.pdf', 'figures/data_not_included.txt', 'figures/figure_not_included.tex', 'figures/figure_not_included_2.tex', 'figures/figure_included.tikz', 'figures/figure_included.tex', 'figures/data_included.txt', 'not_included/figures/data_included.txt', 'images/im4_included.png', 'images/im1.png', 'images/im4_not_included.png', 'images/im3_included.png', 'images/im2_included.jpg', 'images/im5_not_included.jpg', 'images/im5_included.jpg', 'images/im1_included.png', 'images/im_not_included.png', 'images/include/images/im3_included.png'], 'to_copy_in_root': ['main.bbl', 'main.tex'], 'to_copy_not_in_root': ['figures/data_not_included.txt', 'figures/figure_not_included.tex', 'figures/figure_not_included_2.tex', 'figures/figure_included.tikz', 'figures/figure_included.tex', 'figures/data_included.txt', 'not_included/figures/data_included.txt'], 'figures': ['ext_tikz/test2.pdf', 'ext_tikz/test1.pdf', 'ext_tikz/figure_not_included.pdf', 'images/im4_included.png', 'images/im1.png', 'images/im4_not_included.png', 'images/im3_included.png', 'images/im2_included.jpg', 'images/im5_not_included.jpg', 'images/im5_included.jpg', 'images/im1_included.png', 'images/im_not_included.png', 'images/include/images/im3_included.png'], 'tex_in_root': ['main.tex'], 'tex_not_in_root': ['figures/figure_not_included.tex', 'figures/figure_not_included_2.tex', 'figures/figure_included.tikz', 'figures/figure_included.tex'], 'non_tex_in_root': ['main.bbl'], 'non_tex_not_in_root': ['figures/data_not_included.txt', 'figures/data_included.txt', 'not_included/figures/data_included.txt'], 'external_tikz_figures': ['ext_tikz/test2.pdf', 'ext_tikz/test1.pdf', 'ext_tikz/figure_not_included.pdf'], 'svg_inkscape': []}\"}\n",
      "splits = {'all': ['main.bib', 'main.bbl', 'main.tex', 'main.aux', 'ext_tikz/test2.pdf', 'ext_tikz/test1.pdf', 'ext_tikz/figure_not_included.pdf', 'figures/data_not_included.txt', 'figures/figure_not_included.tex', 'figures/figure_not_included_2.tex', 'figures/figure_included.tikz', 'figures/figure_included.tex', 'figures/data_included.txt', 'not_included/figures/data_included.txt', 'images/im4_included.png', 'images/im1.png', 'images/im4_not_included.png', 'images/im3_included.png', 'images/im2_included.jpg', 'images/im5_not_included.jpg', 'images/im5_included.jpg', 'images/im1_included.png', 'images/im_not_included.png', 'images/include/images/im3_included.png'], 'in_root': ['main.bib', 'main.bbl', 'main.tex', 'main.aux'], 'not_in_root': ['ext_tikz/test2.pdf', 'ext_tikz/test1.pdf', 'ext_tikz/figure_not_included.pdf', 'figures/data_not_included.txt', 'figures/figure_not_included.tex', 'figures/figure_not_included_2.tex', 'figures/figure_included.tikz', 'figures/figure_included.tex', 'figures/data_included.txt', 'not_included/figures/data_included.txt', 'images/im4_included.png', 'images/im1.png', 'images/im4_not_included.png', 'images/im3_included.png', 'images/im2_included.jpg', 'images/im5_not_included.jpg', 'images/im5_included.jpg', 'images/im1_included.png', 'images/im_not_included.png', 'images/include/images/im3_included.png'], 'to_copy_in_root': ['main.bbl', 'main.tex'], 'to_copy_not_in_root': ['figures/data_not_included.txt', 'figures/figure_not_included.tex', 'figures/figure_not_included_2.tex', 'figures/figure_included.tikz', 'figures/figure_included.tex', 'figures/data_included.txt', 'not_included/figures/data_included.txt'], 'figures': ['ext_tikz/test2.pdf', 'ext_tikz/test1.pdf', 'ext_tikz/figure_not_included.pdf', 'images/im4_included.png', 'images/im1.png', 'images/im4_not_included.png', 'images/im3_included.png', 'images/im2_included.jpg', 'images/im5_not_included.jpg', 'images/im5_included.jpg', 'images/im1_included.png', 'images/im_not_included.png', 'images/include/images/im3_included.png'], 'tex_in_root': ['main.tex'], 'tex_not_in_root': ['figures/figure_not_included.tex', 'figures/figure_not_included_2.tex', 'figures/figure_included.tikz', 'figures/figure_included.tex'], 'non_tex_in_root': ['main.bbl'], 'non_tex_not_in_root': ['figures/data_not_included.txt', 'figures/data_included.txt', 'not_included/figures/data_included.txt'], 'external_tikz_figures': ['ext_tikz/test2.pdf', 'ext_tikz/test1.pdf', 'ext_tikz/figure_not_included.pdf'], 'svg_inkscape': [], 'tex_to_copy': ['figures/figure_included.tex', 'figures/figure_included.tikz', 'main.tex']}\n",
      "def _read_file_content(filename):\n",
      "  with open(filename, 'r', encoding='utf-8') as fp:\n",
      "    lines = fp.readlines()\n",
      "    lines = _strip_tex_contents(lines, '\\\\end{document}')\n",
      "    return lines\n",
      "\n",
      "_read_file_content(filename='tex/main.tex')\n",
      "with open(filename, 'r', encoding='utf-8') as fp:\n",
      "{'filename': \"'tex/main.tex'\"}\n",
      "fp = <_io.TextIOWrapper name='tex/main.tex' mode='r' encoding='utf-8'>\n",
      "def _keep_only_referenced_tex(contents, splits):\n",
      "  \"\"\"Returns the filenames referenced from the tex files themselves.\n",
      "\n",
      "  It needs various iterations in case one file is referenced from an\n",
      "  unreferenced file.\n",
      "  \"\"\"\n",
      "  old_referenced = set(splits['tex_in_root'] + splits['tex_not_in_root'])\n",
      "  while True:\n",
      "    referenced = set(splits['tex_in_root'])\n",
      "    for fn in old_referenced:\n",
      "      for fn2 in old_referenced:\n",
      "        if regex.search(\n",
      "            r'(' + os.path.splitext(fn)[0] + r'[.}])', '\\n'.join(contents[fn2])\n",
      "        ):\n",
      "          referenced.add(fn)\n",
      "\n",
      "    if referenced == old_referenced:\n",
      "      splits['tex_to_copy'] = list(referenced)\n",
      "      return\n",
      "\n",
      "    old_referenced = referenced.copy()\n",
      "\n",
      "_keep_only_referenced_tex(contents={'main.tex': ['\\\\begin{document}', 'Text', '', 'Text%', '', '', 'This is a percent \\\\%.', '\\\\includegraphics{images/im1_included.png}', '\\\\includegraphics{images/im3_included.png}', '\\\\includegraphics{%', '  images/im4_included.png%', '  }', '\\\\includegraphics[width=.5\\\\linewidth]{%', '  images/im5_included.jpg}', '', '\\\\includegraphics{./images/im3_included.png}', '', 'This line should not be separated', '%', 'from this one.', '', '', '', '\\\\newif\\\\ifvar', '', '\\\\ifvar', '\\\\fi', '', '\\\\newcommand{\\\\red}[1]{{\\\\color{red} #1}}', 'hello test hello', 'test hello', 'test', '', '', '\\\\input{figures/figure_included.tex}', '', '\\\\includegraphics{ext_tikz/test1.pdf}', '', '\\\\input{figures/figure_included.tikz}', '', '\\\\begin{tikzpicture}', '    \\\\node (test) at (0,0) {Test3};', '\\\\end{tikzpicture}', '', '\\\\tikzsetnextfilename{test_no_match}', '\\\\begin{tikzpicture}', '    \\\\node (test) at (0,0) {Test4};', '\\\\end{tikzpicture}', '', '\\\\end{document}', ''], 'figures/figure_not_included.tex': ['\\\\addplot{figures/data_not_included.txt}', '\\\\input{figures/figure_not_included_2.tex}', ''], 'figures/figure_not_included_2.tex': [''], 'figures/figure_included.tikz': ['\\ufeff\\\\includegraphics{ext_tikz/test2.pdf}', ''], 'figures/figure_included.tex': ['\\\\includegraphics{images/im2_included.jpg}', '\\\\addplot{figures/data_included.txt}', '']}, splits={'all': ['main.bib', 'main.bbl', 'main.tex', 'main.aux', 'ext_tikz/test2.pdf', 'ext_tikz/test1.pdf', 'ext_tikz/figure_not_included.pdf', 'figures/data_not_included.txt', 'figures/figure_not_included.tex', 'figures/figure_not_included_2.tex', 'figures/figure_included.tikz', 'figures/figure_included.tex', 'figures/data_included.txt', 'not_included/figures/data_included.txt', 'images/im4_included.png', 'images/im1.png', 'images/im4_not_included.png', 'images/im3_included.png', 'images/im2_included.jpg', 'images/im5_not_included.jpg', 'images/im5_included.jpg', 'images/im1_included.png', 'images/im_not_included.png', 'images/include/images/im3_included.png'], 'in_root': ['main.bib', 'main.bbl', 'main.tex', 'main.aux'], 'not_in_root': ['ext_tikz/test2.pdf', 'ext_tikz/test1.pdf', 'ext_tikz/figure_not_included.pdf', 'figures/data_not_included.txt', 'figures/figure_not_included.tex', 'figures/figure_not_included_2.tex', 'figures/figure_included.tikz', 'figures/figure_included.tex', 'figures/data_included.txt', 'not_included/figures/data_included.txt', 'images/im4_included.png', 'images/im1.png', 'images/im4_not_included.png', 'images/im3_included.png', 'images/im2_included.jpg', 'images/im5_not_included.jpg', 'images/im5_included.jpg', 'images/im1_included.png', 'images/im_not_included.png', 'images/include/images/im3_included.png'], 'to_copy_in_root': ['main.bbl', 'main.tex'], 'to_copy_not_in_root': ['figures/data_not_included.txt', 'figures/figure_not_included.tex', 'figures/figure_not_included_2.tex', 'figures/figure_included.tikz', 'figures/figure_included.tex', 'figures/data_included.txt', 'not_included/figures/data_included.txt'], 'figures': ['ext_tikz/test2.pdf', 'ext_tikz/test1.pdf', 'ext_tikz/figure_not_included.pdf', 'images/im4_included.png', 'images/im1.png', 'images/im4_not_included.png', 'images/im3_included.png', 'images/im2_included.jpg', 'images/im5_not_included.jpg', 'images/im5_included.jpg', 'images/im1_included.png', 'images/im_not_included.png', 'images/include/images/im3_included.png'], 'tex_in_root': ['main.tex'], 'tex_not_in_root': ['figures/figure_not_included.tex', 'figures/figure_not_included_2.tex', 'figures/figure_included.tikz', 'figures/figure_included.tex'], 'non_tex_in_root': ['main.bbl'], 'non_tex_not_in_root': ['figures/data_not_included.txt', 'figures/data_included.txt', 'not_included/figures/data_included.txt'], 'external_tikz_figures': ['ext_tikz/test2.pdf', 'ext_tikz/test1.pdf', 'ext_tikz/figure_not_included.pdf'], 'svg_inkscape': []})\n",
      "splits['tex_to_copy'] = list(referenced)\n",
      "{'referenced': \"{'figures/figure_included.tex', 'figures/figure_included.tikz', 'main.tex'}\"}\n",
      "splits = {'all': ['main.bib', 'main.bbl', 'main.tex', 'main.aux', 'ext_tikz/test2.pdf', 'ext_tikz/test1.pdf', 'ext_tikz/figure_not_included.pdf', 'figures/data_not_included.txt', 'figures/figure_not_included.tex', 'figures/figure_not_included_2.tex', 'figures/figure_included.tikz', 'figures/figure_included.tex', 'figures/data_included.txt', 'not_included/figures/data_included.txt', 'images/im4_included.png', 'images/im1.png', 'images/im4_not_included.png', 'images/im3_included.png', 'images/im2_included.jpg', 'images/im5_not_included.jpg', 'images/im5_included.jpg', 'images/im1_included.png', 'images/im_not_included.png', 'images/include/images/im3_included.png'], 'in_root': ['main.bib', 'main.bbl', 'main.tex', 'main.aux'], 'not_in_root': ['ext_tikz/test2.pdf', 'ext_tikz/test1.pdf', 'ext_tikz/figure_not_included.pdf', 'figures/data_not_included.txt', 'figures/figure_not_included.tex', 'figures/figure_not_included_2.tex', 'figures/figure_included.tikz', 'figures/figure_included.tex', 'figures/data_included.txt', 'not_included/figures/data_included.txt', 'images/im4_included.png', 'images/im1.png', 'images/im4_not_included.png', 'images/im3_included.png', 'images/im2_included.jpg', 'images/im5_not_included.jpg', 'images/im5_included.jpg', 'images/im1_included.png', 'images/im_not_included.png', 'images/include/images/im3_included.png'], 'to_copy_in_root': ['main.bbl', 'main.tex'], 'to_copy_not_in_root': ['figures/data_not_included.txt', 'figures/figure_not_included.tex', 'figures/figure_not_included_2.tex', 'figures/figure_included.tikz', 'figures/figure_included.tex', 'figures/data_included.txt', 'not_included/figures/data_included.txt'], 'figures': ['ext_tikz/test2.pdf', 'ext_tikz/test1.pdf', 'ext_tikz/figure_not_included.pdf', 'images/im4_included.png', 'images/im1.png', 'images/im4_not_included.png', 'images/im3_included.png', 'images/im2_included.jpg', 'images/im5_not_included.jpg', 'images/im5_included.jpg', 'images/im1_included.png', 'images/im_not_included.png', 'images/include/images/im3_included.png'], 'tex_in_root': ['main.tex'], 'tex_not_in_root': ['figures/figure_not_included.tex', 'figures/figure_not_included_2.tex', 'figures/figure_included.tikz', 'figures/figure_included.tex'], 'non_tex_in_root': ['main.bbl'], 'non_tex_not_in_root': ['figures/data_not_included.txt', 'figures/data_included.txt', 'not_included/figures/data_included.txt'], 'external_tikz_figures': ['ext_tikz/test2.pdf', 'ext_tikz/test1.pdf', 'ext_tikz/figure_not_included.pdf'], 'svg_inkscape': [], 'tex_to_copy': ['figures/figure_included.tex', 'figures/figure_included.tikz', 'main.tex']}\n",
      "def apply_changes(file_path: str, changes: List, confirm: bool = False):\n",
      "    \"\"\"\n",
      "    Pass changes as loaded json (list of dicts)\n",
      "    \"\"\"\n",
      "    with open(file_path) as f:\n",
      "        original_file_lines = f.readlines()\n",
      "\n",
      "    # Filter out explanation elements\n",
      "    operation_changes = [change for change in changes if \"operation\" in change]\n",
      "    explanations = [\n",
      "        change[\"explanation\"] for change in changes if \"explanation\" in change\n",
      "    ]\n",
      "\n",
      "    # Sort the changes in reverse line order\n",
      "    operation_changes.sort(key=lambda x: x[\"line\"], reverse=True)\n",
      "\n",
      "    file_lines = original_file_lines.copy()\n",
      "    for change in operation_changes:\n",
      "        operation = change[\"operation\"]\n",
      "        line = change[\"line\"]\n",
      "        content = change[\"content\"]\n",
      "\n",
      "        if operation == \"Replace\":\n",
      "            file_lines[line - 1] = content + \"\\n\"\n",
      "        elif operation == \"Delete\":\n",
      "            del file_lines[line - 1]\n",
      "        elif operation == \"InsertAfter\":\n",
      "            file_lines.insert(line, content + \"\\n\")\n",
      "\n",
      "    # Print explanations\n",
      "    cprint(\"Explanations:\", \"blue\")\n",
      "    for explanation in explanations:\n",
      "        cprint(f\"- {explanation}\", \"blue\")\n",
      "\n",
      "    # Display changes diff\n",
      "    print(\"\\nChanges to be made:\")\n",
      "    diff = difflib.unified_diff(original_file_lines, file_lines, lineterm=\"\")\n",
      "    for line in diff:\n",
      "        if line.startswith(\"+\"):\n",
      "            cprint(line, \"green\", end=\"\")\n",
      "        elif line.startswith(\"-\"):\n",
      "            cprint(line, \"red\", end=\"\")\n",
      "        else:\n",
      "            print(line, end=\"\")\n",
      "\n",
      "    if confirm:\n",
      "        # check if user wants to apply changes or exit\n",
      "        confirmation = input(\"Do you want to apply these changes? (y/n): \")\n",
      "        if confirmation.lower() != \"y\":\n",
      "            print(\"Changes not applied\")\n",
      "            sys.exit(0)\n",
      "\n",
      "    with open(file_path, \"w\") as f:\n",
      "        f.writelines(file_lines)\n",
      "    print(\"Changes applied.\")\n",
      "\n",
      "apply_changes(file_path='/tmp/tmp6qrrn_j9', changes=[{'operation': 'Replace', 'line': 2, 'content': 'new second line'}], confirm=False)\n",
      "with open(file_path, \"w\") as f:\n",
      "{'file_path': \"'/tmp/tmp6qrrn_j9'\"}\n",
      "f = <_io.TextIOWrapper name='/tmp/tmp6qrrn_j9' mode='w' encoding='UTF-8'>\n",
      "def return_logger(name: str) -> logging.Logger:\n",
      "    # Set log message format\n",
      "\n",
      "    logger = logging.getLogger(name)\n",
      "\n",
      "    if not len(logger.handlers):\n",
      "        log_formatter = logging.Formatter('%(asctime)-s: %(levelname)-s %(message)s')\n",
      "        # set logging level\n",
      "        logger.setLevel(logging.DEBUG)\n",
      "\n",
      "        # Direct logs to stderr\n",
      "        console_handler = logging.StreamHandler()\n",
      "        console_handler.setFormatter(log_formatter)\n",
      "        logger.addHandler(console_handler)\n",
      "\n",
      "    return logger\n",
      "\n",
      "return_logger(name='imagededup.handlers.metrics.classification')\n",
      "logger = logging.getLogger(name)\n",
      "{'name': \"'imagededup.handlers.metrics.classification'\"}\n",
      "logger = <Logger imagededup.handlers.metrics.classification (WARNING)>\n",
      "def transform_path_to_dotted(sys_path, module_path):\n",
      "    \"\"\"\n",
      "    Returns the dotted path inside a sys.path as a list of names. e.g.\n",
      "\n",
      "    >>> transform_path_to_dotted([str(Path(\"/foo\").absolute())], Path('/foo/bar/baz.py').absolute())\n",
      "    (('bar', 'baz'), False)\n",
      "\n",
      "    Returns (None, False) if the path doesn't really resolve to anything.\n",
      "    The second return part is if it is a package.\n",
      "    \"\"\"\n",
      "    # First remove the suffix.\n",
      "    module_path = remove_python_path_suffix(module_path)\n",
      "    if module_path.name.startswith('.'):\n",
      "        return None, False\n",
      "\n",
      "    # Once the suffix was removed we are using the files as we know them. This\n",
      "    # means that if someone uses an ending like .vim for a Python file, .vim\n",
      "    # will be part of the returned dotted part.\n",
      "\n",
      "    is_package = module_path.name == '__init__'\n",
      "    if is_package:\n",
      "        module_path = module_path.parent\n",
      "\n",
      "    def iter_potential_solutions():\n",
      "        for p in sys_path:\n",
      "            if str(module_path).startswith(p):\n",
      "                # Strip the trailing slash/backslash\n",
      "                rest = str(module_path)[len(p):]\n",
      "                # On Windows a path can also use a slash.\n",
      "                if rest.startswith(os.path.sep) or rest.startswith('/'):\n",
      "                    # Remove a slash in cases it's still there.\n",
      "                    rest = rest[1:]\n",
      "\n",
      "                if rest:\n",
      "                    split = rest.split(os.path.sep)\n",
      "                    if not all(split):\n",
      "                        # This means that part of the file path was empty, this\n",
      "                        # is very strange and is probably a file that is called\n",
      "                        # `.py`.\n",
      "                        return\n",
      "                    # Stub folders for foo can end with foo-stubs. Just remove\n",
      "                    # it.\n",
      "                    yield tuple(re.sub(r'-stubs$', '', s) for s in split)\n",
      "\n",
      "    potential_solutions = tuple(iter_potential_solutions())\n",
      "    if not potential_solutions:\n",
      "        return None, False\n",
      "    # Try to find the shortest path, this makes more sense usually, because the\n",
      "    # user usually has venvs somewhere. This means that a path like\n",
      "    # .tox/py37/lib/python3.7/os.py can be normal for a file. However in that\n",
      "    # case we definitely want to return ['os'] as a path and not a crazy\n",
      "    # ['.tox', 'py37', 'lib', 'python3.7', 'os']. Keep in mind that this is a\n",
      "    # heuristic and there's now ay to \"always\" do it right.\n",
      "    return sorted(potential_solutions, key=lambda p: len(p))[0], is_package\n",
      "\n",
      "transform_path_to_dotted(sys_path=['/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi', '/local/rcs/XXX/code/pytrace-collector', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python39.zip', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/lib-dynload', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages/PySnooper-1.2.0-py3.9.egg'], module_path=PosixPath('/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi/example.py'))\n",
      "module_path = remove_python_path_suffix(module_path)\n",
      "{'module_path': \"PosixPath('/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi/example.py')\"}\n",
      "module_path = PosixPath('/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi/example')\n",
      "def infer_node(context, element):\n",
      "    if isinstance(context, CompForContext):\n",
      "        return _infer_node(context, element)\n",
      "\n",
      "    if_stmt = element\n",
      "    while if_stmt is not None:\n",
      "        if_stmt = if_stmt.parent\n",
      "        if if_stmt.type in ('if_stmt', 'for_stmt'):\n",
      "            break\n",
      "        if parser_utils.is_scope(if_stmt):\n",
      "            if_stmt = None\n",
      "            break\n",
      "    predefined_if_name_dict = context.predefined_names.get(if_stmt)\n",
      "    # TODO there's a lot of issues with this one. We actually should do\n",
      "    # this in a different way. Caching should only be active in certain\n",
      "    # cases and this all sucks.\n",
      "    if predefined_if_name_dict is None and if_stmt \\\n",
      "            and if_stmt.type == 'if_stmt' and context.inference_state.is_analysis:\n",
      "        if_stmt_test = if_stmt.children[1]\n",
      "        name_dicts = [{}]\n",
      "        # If we already did a check, we don't want to do it again -> If\n",
      "        # value.predefined_names is filled, we stop.\n",
      "        # We don't want to check the if stmt itself, it's just about\n",
      "        # the content.\n",
      "        if element.start_pos > if_stmt_test.end_pos:\n",
      "            # Now we need to check if the names in the if_stmt match the\n",
      "            # names in the suite.\n",
      "            if_names = get_names_of_node(if_stmt_test)\n",
      "            element_names = get_names_of_node(element)\n",
      "            str_element_names = [e.value for e in element_names]\n",
      "            if any(i.value in str_element_names for i in if_names):\n",
      "                for if_name in if_names:\n",
      "                    definitions = context.inference_state.infer(context, if_name)\n",
      "                    # Every name that has multiple different definitions\n",
      "                    # causes the complexity to rise. The complexity should\n",
      "                    # never fall below 1.\n",
      "                    if len(definitions) > 1:\n",
      "                        if len(name_dicts) * len(definitions) > 16:\n",
      "                            debug.dbg('Too many options for if branch inference %s.', if_stmt)\n",
      "                            # There's only a certain amount of branches\n",
      "                            # Jedi can infer, otherwise it will take to\n",
      "                            # long.\n",
      "                            name_dicts = [{}]\n",
      "                            break\n",
      "\n",
      "                        original_name_dicts = list(name_dicts)\n",
      "                        name_dicts = []\n",
      "                        for definition in definitions:\n",
      "                            new_name_dicts = list(original_name_dicts)\n",
      "                            for i, name_dict in enumerate(new_name_dicts):\n",
      "                                new_name_dicts[i] = name_dict.copy()\n",
      "                                new_name_dicts[i][if_name.value] = ValueSet([definition])\n",
      "\n",
      "                            name_dicts += new_name_dicts\n",
      "                    else:\n",
      "                        for name_dict in name_dicts:\n",
      "                            name_dict[if_name.value] = definitions\n",
      "        if len(name_dicts) > 1:\n",
      "            result = NO_VALUES\n",
      "            for name_dict in name_dicts:\n",
      "                with context.predefine_names(if_stmt, name_dict):\n",
      "                    result |= _infer_node(context, element)\n",
      "            return result\n",
      "        else:\n",
      "            return _infer_node_if_inferred(context, element)\n",
      "    else:\n",
      "        if predefined_if_name_dict:\n",
      "            return _infer_node(context, element)\n",
      "        else:\n",
      "            return _infer_node_if_inferred(context, element)\n",
      "\n",
      "infer_node(context=ModuleContext(<ModuleValue: __main__@2-8 is_stub=False>), element=PythonNode(test, [<Name: f@8,11>, <Keyword: if>, PythonNode(atom_expr, [<Name: random@8,16>, PythonNode(trailer, [<Operator: .>, <Name: choice@8,23>]), PythonNode(trailer, [<Operator: (>, PythonNode(atom, [<Operator: [>, PythonNode(testlist_comp, [<Number: 0>, <Operator: ,>, <Number: 1>]), <Operator: ]>]), <Operator: )>])]), <Keyword: else>, <Name: C@8,42>]))\n",
      "predefined_if_name_dict = context.predefined_names.get(if_stmt)\n",
      "{'if_stmt': 'None'}\n",
      "predefined_if_name_dict = None\n",
      "def _search_param_in_docstr(docstr, param_str):\n",
      "    \"\"\"\n",
      "    Search `docstr` for type(-s) of `param_str`.\n",
      "\n",
      "    >>> _search_param_in_docstr(':type param: int', 'param')\n",
      "    ['int']\n",
      "    >>> _search_param_in_docstr('@type param: int', 'param')\n",
      "    ['int']\n",
      "    >>> _search_param_in_docstr(\n",
      "    ...   ':type param: :class:`threading.Thread`', 'param')\n",
      "    ['threading.Thread']\n",
      "    >>> bool(_search_param_in_docstr('no document', 'param'))\n",
      "    False\n",
      "    >>> _search_param_in_docstr(':param int param: some description', 'param')\n",
      "    ['int']\n",
      "\n",
      "    \"\"\"\n",
      "    # look at #40 to see definitions of those params\n",
      "    patterns = [re.compile(p % re.escape(param_str))\n",
      "                for p in DOCSTRING_PARAM_PATTERNS]\n",
      "    for pattern in patterns:\n",
      "        match = pattern.search(docstr)\n",
      "        if match:\n",
      "            return [_strip_rst_role(match.group(1))]\n",
      "\n",
      "    return _search_param_in_numpydocstr(docstr, param_str)\n",
      "\n",
      "_search_param_in_docstr(docstr=':type param: int', param_str='param')\n",
      "patterns = [re.compile(p % re.escape(param_str))\n",
      "{'param_str': \"'param'\"}\n",
      "patterns = [re.compile('\\\\s*:type\\\\s+param:\\\\s*([^\\\\n]+)'), re.compile('\\\\s*:param\\\\s+(\\\\w+)\\\\s+param:[^\\\\n]*'), re.compile('\\\\s*@type\\\\s+param:\\\\s*([^\\\\n]+)')]\n",
      "def generate(resource_types=()):\n",
      "    resource_defs = {}\n",
      "    definitions = {\n",
      "        'resources': resource_defs,\n",
      "        'string_dict': {\n",
      "            \"type\": \"object\",\n",
      "            \"patternProperties\": {\n",
      "                \"\": {\"type\": \"string\"},\n",
      "            },\n",
      "        },\n",
      "        'basic_dict': {\n",
      "            \"type\": \"object\",\n",
      "            \"patternProperties\": {\n",
      "                \"\": {\n",
      "                    'oneOf': [\n",
      "                        {\"type\": \"string\"},\n",
      "                        {\"type\": \"boolean\"},\n",
      "                        {\"type\": \"number\"},\n",
      "                    ],\n",
      "                }\n",
      "            },\n",
      "        },\n",
      "        'iam-statement': {\n",
      "            'additionalProperties': False,\n",
      "            'type': 'object',\n",
      "            'properties': {\n",
      "                'Sid': {'type': 'string'},\n",
      "                'Effect': {'type': 'string', 'enum': ['Allow', 'Deny']},\n",
      "                'Principal': {'anyOf': [\n",
      "                    {'type': 'string'},\n",
      "                    {'type': 'object'}, {'type': 'array'}]},\n",
      "                'NotPrincipal': {'anyOf': [{'type': 'object'}, {'type': 'array'}]},\n",
      "                'Action': {'anyOf': [{'type': 'string'}, {'type': 'array'}]},\n",
      "                'NotAction': {'anyOf': [{'type': 'string'}, {'type': 'array'}]},\n",
      "                'Resource': {'anyOf': [{'type': 'string'}, {'type': 'array'}]},\n",
      "                'NotResource': {'anyOf': [{'type': 'string'}, {'type': 'array'}]},\n",
      "                'Condition': {'type': 'object'}\n",
      "            },\n",
      "            'required': ['Sid', 'Effect'],\n",
      "            'oneOf': [\n",
      "                {'required': ['Principal', 'Action', 'Resource']},\n",
      "                {'required': ['NotPrincipal', 'Action', 'Resource']},\n",
      "                {'required': ['Principal', 'NotAction', 'Resource']},\n",
      "                {'required': ['NotPrincipal', 'NotAction', 'Resource']},\n",
      "                {'required': ['Principal', 'Action', 'NotResource']},\n",
      "                {'required': ['NotPrincipal', 'Action', 'NotResource']},\n",
      "                {'required': ['Principal', 'NotAction', 'NotResource']},\n",
      "                {'required': ['NotPrincipal', 'NotAction', 'NotResource']}\n",
      "            ]\n",
      "        },\n",
      "        'actions': {},\n",
      "        'filters': {\n",
      "            'value': ValueFilter.schema,\n",
      "            'event': EventFilter.schema,\n",
      "            'age': AgeFilter.schema,\n",
      "            'reduce': ReduceFilter.schema,\n",
      "            # Shortcut form of value filter as k=v\n",
      "            'valuekv': {\n",
      "                'type': 'object',\n",
      "                'additionalProperties': {'oneOf': [{'type': 'number'}, {'type': 'null'},\n",
      "                    {'type': 'array', 'maxItems': 0}, {'type': 'string'}, {'type': 'boolean'}]},\n",
      "                'minProperties': 1,\n",
      "                'maxProperties': 1},\n",
      "        },\n",
      "        'filters_common': {\n",
      "            'list_item_attrs': _get_attr_schema(),\n",
      "            'comparison_operators': {\n",
      "                'enum': list(OPERATORS.keys())},\n",
      "            'value_types': {'enum': VALUE_TYPES},\n",
      "            'value_from': ValuesFrom.schema,\n",
      "            'value': {'oneOf': [\n",
      "                {'type': 'array'},\n",
      "                {'type': 'string'},\n",
      "                {'type': 'boolean'},\n",
      "                {'type': 'number'},\n",
      "                {'type': 'null'}]},\n",
      "        },\n",
      "        'policy': {\n",
      "            'type': 'object',\n",
      "            'required': ['name', 'resource'],\n",
      "            'additionalProperties': False,\n",
      "            'properties': {\n",
      "                'name': {\n",
      "                    'type': 'string',\n",
      "                    'pattern': \"^[A-z][A-z0-9]*(-[A-z0-9]+)*$\"},\n",
      "                'conditions': {\n",
      "                    'type': 'array',\n",
      "                    'items': {'anyOf': [\n",
      "                        {'type': 'object', 'additionalProperties': False,\n",
      "                         'properties': {'or': {\n",
      "                             '$ref': '#/definitions/policy/properties/conditions'}}},\n",
      "                        {'type': 'object', 'additionalProperties': False,\n",
      "                         'properties': {'not': {\n",
      "                             '$ref': '#/definitions/policy/properties/conditions'}}},\n",
      "                        {'type': 'object', 'additionalProperties': False,\n",
      "                         'properties': {'and': {\n",
      "                             '$ref': '#/definitions/policy/properties/conditions'}}},\n",
      "                        {'$ref': '#/definitions/filters/value'},\n",
      "                        {'$ref': '#/definitions/filters/event'},\n",
      "                        {'$ref': '#/definitions/filters/valuekv'}]}},\n",
      "                # these should be deprecated for conditions\n",
      "                'region': {'type': 'string'},\n",
      "                'tz': {'type': 'string'},\n",
      "                'start': {'format': 'date-time'},\n",
      "                'end': {'format': 'date-time'},\n",
      "                'resource': {'oneOf': [\n",
      "                    {'type': 'string'},\n",
      "                    {'type': 'array', 'items': {'type': 'string'}}]},\n",
      "                'max-resources': {'anyOf': [\n",
      "                    {'type': 'integer', 'minimum': 1},\n",
      "                    {'$ref': '#/definitions/max-resources-properties'}\n",
      "                ]},\n",
      "                'max-resources-percent': {'type': 'number', 'minimum': 0, 'maximum': 100},\n",
      "                'comment': {'type': 'string'},\n",
      "                'comments': {'type': 'string'},\n",
      "                'description': {'type': 'string'},\n",
      "                'tags': {'type': 'array', 'items': {'type': 'string'}},\n",
      "                'metadata': {'type': 'object'},\n",
      "                'mode': {'$ref': '#/definitions/policy-mode'},\n",
      "                'source': {'enum': list(sources.keys())},\n",
      "                'actions': {\n",
      "                    'type': 'array',\n",
      "                },\n",
      "                'filters': {\n",
      "                    'type': 'array'\n",
      "                },\n",
      "                #\n",
      "                # TODO: source queries should really move under\n",
      "                # source. This was initially used for describe sources\n",
      "                # to expose server side query mechanisms, however its\n",
      "                # important to note it also prevents resource cache\n",
      "                # utilization between policies that have different\n",
      "                # queries.\n",
      "                'query': {\n",
      "                    'type': 'array', 'items': {'type': 'object'}}\n",
      "\n",
      "            },\n",
      "        },\n",
      "        'policy-mode': {\n",
      "            'anyOf': [e.schema for _, e in execution.items()],\n",
      "        },\n",
      "        'max-resources-properties': {\n",
      "            'type': 'object',\n",
      "            'additionalProperties': False,\n",
      "            'properties': {\n",
      "                'amount': {\"type\": 'integer', 'minimum': 1},\n",
      "                'op': {'enum': ['or', 'and']},\n",
      "                'percent': {'type': 'number', 'minimum': 0, 'maximum': 100}\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "\n",
      "    resource_refs = []\n",
      "    for cloud_name, cloud_type in sorted(clouds.items()):\n",
      "        for type_name, resource_type in sorted(cloud_type.resources.items()):\n",
      "            r_type_name = \"%s.%s\" % (cloud_name, type_name)\n",
      "            if resource_types and r_type_name not in resource_types:\n",
      "                if not resource_type.type_aliases:\n",
      "                    continue\n",
      "                elif not {\"%s.%s\" % (cloud_name, ralias) for ralias\n",
      "                        in resource_type.type_aliases}.intersection(\n",
      "                        resource_types):\n",
      "                    continue\n",
      "\n",
      "            aliases = []\n",
      "            if resource_type.type_aliases:\n",
      "                aliases.extend([\"%s.%s\" % (cloud_name, a) for a in resource_type.type_aliases])\n",
      "                # aws gets legacy aliases with no cloud prefix\n",
      "                if cloud_name == 'aws':\n",
      "                    aliases.extend(resource_type.type_aliases)\n",
      "\n",
      "            # aws gets additional alias for default name\n",
      "            if cloud_name == 'aws':\n",
      "                aliases.append(type_name)\n",
      "\n",
      "            resource_refs.append(\n",
      "                process_resource(\n",
      "                    r_type_name,\n",
      "                    resource_type,\n",
      "                    resource_defs,\n",
      "                    aliases,\n",
      "                    definitions,\n",
      "                    cloud_name\n",
      "                ))\n",
      "\n",
      "    schema = {\n",
      "        \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
      "        'id': 'http://schema.cloudcustodian.io/v0/custodian.json',\n",
      "        'definitions': definitions,\n",
      "        'type': 'object',\n",
      "        'required': ['policies'],\n",
      "        'additionalProperties': False,\n",
      "        'properties': {\n",
      "            'vars': {'type': 'object'},\n",
      "            'policies': {\n",
      "                'type': 'array',\n",
      "                'additionalItems': False,\n",
      "                'items': {'anyOf': resource_refs}\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "\n",
      "    # allow empty policies with lazy load\n",
      "    if not resource_refs:\n",
      "        schema['properties']['policies']['items'] = {'type': 'object'}\n",
      "    return schema\n",
      "\n",
      "generate(resource_types=())\n",
      "process_resource(\n",
      "{'r_type_name': \"'gcp.region'\", 'resource_type': \"<class 'c7n_gcp.region.Region'>\", 'resource_defs': '{}', 'aliases': '[]', 'definitions': \"{'resources': {}, 'string_dict': {'type': 'object', 'patternProperties': {'': {'type': 'string'}}}, 'basic_dict': {'type': 'object', 'patternProperties': {'': {'oneOf': [{'type': 'string'}, {'type': 'boolean'}, {'type': 'number'}]}}}, 'iam-statement': {'additionalProperties': False, 'type': 'object', 'properties': {'Sid': {'type': 'string'}, 'Effect': {'type': 'string', 'enum': ['Allow', 'Deny']}, 'Principal': {'anyOf': [{'type': 'string'}, {'type': 'object'}, {'type': 'array'}]}, 'NotPrincipal': {'anyOf': [{'type': 'object'}, {'type': 'array'}]}, 'Action': {'anyOf': [{'type': 'string'}, {'type': 'array'}]}, 'NotAction': {'anyOf': [{'type': 'string'}, {'type': 'array'}]}, 'Resource': {'anyOf': [{'type': 'string'}, {'type': 'array'}]}, 'NotResource': {'anyOf': [{'type': 'string'}, {'type': 'array'}]}, 'Condition': {'type': 'object'}}, 'required': ['Sid', 'Effect'], 'oneOf': [{'required': ['Principal', 'Action', 'Resource']}, {'required': ['NotPrincipal', 'Action', 'Resource']}, {'required': ['Principal', 'NotAction', 'Resource']}, {'required': ['NotPrincipal', 'NotAction', 'Resource']}, {'required': ['Principal', 'Action', 'NotResource']}, {'required': ['NotPrincipal', 'Action', 'NotResource']}, {'required': ['Principal', 'NotAction', 'NotResource']}, {'required': ['NotPrincipal', 'NotAction', 'NotResource']}]}, 'actions': {}, 'filters': {'value': {'type': 'object', 'additionalProperties': False, 'required': ['type'], 'properties': {'type': {'enum': ['value']}, 'key': {'type': 'string'}, 'value_type': {'$ref': '#/definitions/filters_common/value_types'}, 'default': {'type': 'object'}, 'value_regex': {'type': 'string'}, 'value_from': {'$ref': '#/definitions/filters_common/value_from'}, 'value': {'$ref': '#/definitions/filters_common/value'}, 'op': {'$ref': '#/definitions/filters_common/comparison_operators'}, 'value_path': {'type': 'string'}}}, 'event': {'type': 'object', 'additionalProperties': False, 'required': ['type'], 'properties': {'type': {'enum': ['event']}, 'key': {'type': 'string'}, 'value_type': {'$ref': '#/definitions/filters_common/value_types'}, 'default': {'type': 'object'}, 'value_regex': {'type': 'string'}, 'value_from': {'$ref': '#/definitions/filters_common/value_from'}, 'value': {'$ref': '#/definitions/filters_common/value'}, 'op': {'$ref': '#/definitions/filters_common/comparison_operators'}, 'value_path': {'type': 'string'}}}, 'age': None, 'reduce': {'type': 'object', 'additionalProperties': False, 'required': ['type'], 'properties': {'type': {'enum': ['reduce']}, 'group-by': {'oneOf': [{'type': 'string'}, {'type': 'object', 'key': {'type': 'string'}, 'value_type': {'enum': ['string', 'number', 'date']}, 'value_regex': 'string'}]}, 'sort-by': {'oneOf': [{'type': 'string'}, {'type': 'object', 'key': {'type': 'string'}, 'value_type': {'enum': ['string', 'number', 'date']}, 'value_regex': 'string'}]}, 'order': {'enum': ['asc', 'desc', 'reverse', 'randomize']}, 'null-order': {'enum': ['first', 'last']}, 'limit': {'type': 'number', 'minimum': 0}, 'limit-percent': {'type': 'number', 'minimum': 0, 'maximum': 100}, 'discard': {'type': 'number', 'minimum': 0}, 'discard-percent': {'type': 'number', 'minimum': 0, 'maximum': 100}}}, 'valuekv': {'type': 'object', 'additionalProperties': {'oneOf': [{'type': 'number'}, {'type': 'null'}, {'type': 'array', 'maxItems': 0}, {'type': 'string'}, {'type': 'boolean'}]}, 'minProperties': 1, 'maxProperties': 1}}, 'filters_common': {'list_item_attrs': {'items': {'anyOf': [{'$ref': '#/definitions/filters/value'}, {'$ref': '#/definitions/filters/valuekv'}, {'additional_properties': False, 'properties': {'and': {'type': 'array', 'items': {'anyOf': [{'$ref': '#/definitions/filters/value'}, {'$ref': '#/definitions/filters/valuekv'}]}}}, 'type': 'object'}, {'additional_properties': False, 'properties': {'or': {'type': 'array', 'items': {'anyOf': [{'$ref': '#/definitions/filters/value'}, {'$ref': '#/definitions/filters/valuekv'}]}}}, 'type': 'object'}, {'additional_properties': False, 'properties': {'not': {'type': 'array', 'items': {'anyOf': [{'$ref': '#/definitions/filters/value...onalProperties': False, 'properties': {'execution-options': {'type': 'object'}, 'function-prefix': {'type': 'string'}, 'member-role': {'type': 'string'}, 'packages': {'type': 'array', 'items': {'type': 'string'}}, 'layers': {'type': 'array', 'items': {'type': 'string'}}, 'concurrency': {'type': 'integer'}, 'runtime': {'enum': ['python3.8', 'python3.9', 'python3.10', 'python3.11', 'python3.12']}, 'role': {'type': 'string'}, 'handler': {'type': 'string'}, 'pattern': {'type': 'object', 'minProperties': 1}, 'timeout': {'type': 'number'}, 'memory': {'type': 'number'}, 'environment': {'type': 'object'}, 'tags': {'type': 'object'}, 'dead_letter_config': {'type': 'object'}, 'kms_key_arn': {'type': 'string'}, 'tracing_config': {'type': 'object'}, 'security_groups': {'type': 'array'}, 'subnets': {'type': 'array'}, 'type': {'enum': ['asg-instance-state']}, 'events': {'type': 'array', 'items': {'enum': ['launch-success', 'launch-failure', 'terminate-success', 'terminate-failure']}}}, 'required': ['type']}, {'type': 'object', 'additionalProperties': False, 'properties': {'execution-options': {'type': 'object'}, 'function-prefix': {'type': 'string'}, 'member-role': {'type': 'string'}, 'packages': {'type': 'array', 'items': {'type': 'string'}}, 'layers': {'type': 'array', 'items': {'type': 'string'}}, 'concurrency': {'type': 'integer'}, 'runtime': {'enum': ['python3.8', 'python3.9', 'python3.10', 'python3.11', 'python3.12']}, 'role': {'type': 'string'}, 'handler': {'type': 'string'}, 'pattern': {'type': 'object', 'minProperties': 1}, 'timeout': {'type': 'number'}, 'memory': {'type': 'number'}, 'environment': {'type': 'object'}, 'tags': {'type': 'object'}, 'dead_letter_config': {'type': 'object'}, 'kms_key_arn': {'type': 'string'}, 'tracing_config': {'type': 'object'}, 'security_groups': {'type': 'array'}, 'subnets': {'type': 'array'}, 'type': {'enum': ['guard-duty']}}, 'required': ['type']}, {'type': 'object', 'additionalProperties': False, 'properties': {'execution-options': {'type': 'object'}, 'function-prefix': {'type': 'string'}, 'member-role': {'type': 'string'}, 'packages': {'type': 'array', 'items': {'type': 'string'}}, 'layers': {'type': 'array', 'items': {'type': 'string'}}, 'concurrency': {'type': 'integer'}, 'runtime': {'enum': ['python3.8', 'python3.9', 'python3.10', 'python3.11', 'python3.12']}, 'role': {'type': 'string'}, 'handler': {'type': 'string'}, 'pattern': {'type': 'object', 'minProperties': 1}, 'timeout': {'type': 'number'}, 'memory': {'type': 'number'}, 'environment': {'type': 'object'}, 'tags': {'type': 'object'}, 'dead_letter_config': {'type': 'object'}, 'kms_key_arn': {'type': 'string'}, 'tracing_config': {'type': 'object'}, 'security_groups': {'type': 'array'}, 'subnets': {'type': 'array'}, 'type': {'enum': ['config-poll-rule']}, 'schedule': {'enum': ['One_Hour', 'Three_Hours', 'Six_Hours', 'Twelve_Hours', 'TwentyFour_Hours']}, 'ignore-support-check': {'type': 'boolean'}}, 'required': ['type']}, {'type': 'object', 'additionalProperties': False, 'properties': {'execution-options': {'type': 'object'}, 'function-prefix': {'type': 'string'}, 'member-role': {'type': 'string'}, 'packages': {'type': 'array', 'items': {'type': 'string'}}, 'layers': {'type': 'array', 'items': {'type': 'string'}}, 'concurrency': {'type': 'integer'}, 'runtime': {'enum': ['python3.8', 'python3.9', 'python3.10', 'python3.11', 'python3.12']}, 'role': {'type': 'string'}, 'handler': {'type': 'string'}, 'pattern': {'type': 'object', 'minProperties': 1}, 'timeout': {'type': 'number'}, 'memory': {'type': 'number'}, 'environment': {'type': 'object'}, 'tags': {'type': 'object'}, 'dead_letter_config': {'type': 'object'}, 'kms_key_arn': {'type': 'string'}, 'tracing_config': {'type': 'object'}, 'security_groups': {'type': 'array'}, 'subnets': {'type': 'array'}, 'type': {'enum': ['config-rule']}}, 'required': ['type']}]}, 'max-resources-properties': {'type': 'object', 'additionalProperties': False, 'properties': {'amount': {'type': 'integer', 'minimum': 1}, 'op': {'enum': ['or', 'and']}, 'percent': {'type': 'number', 'minimum': 0, 'maximum': 100}}}}\", 'cloud_name': \"'gcp'\"}\n",
      "resource_defs = {'gcp.region': {'actions': {}, 'filters': {}, 'policy': {'allOf': [{'$ref': '#/definitions/policy'}, {'properties': {'resource': {'enum': ['gcp.region']}, 'filters': {'type': 'array', 'items': {'anyOf': [{'enum': []}, {'type': 'object', 'additionalProperties': False, 'properties': {'or': {'$ref': '#/definitions/resources/gcp.region/policy/allOf/1/properties/filters'}}}, {'type': 'object', 'additionalProperties': False, 'properties': {'and': {'$ref': '#/definitions/resources/gcp.region/policy/allOf/1/properties/filters'}}}, {'type': 'object', 'additionalProperties': False, 'properties': {'not': {'$ref': '#/definitions/resources/gcp.region/policy/allOf/1/properties/filters'}}}]}}, 'actions': {'type': 'array', 'items': {'anyOf': [{'enum': []}]}}}}]}}}\n",
      "def _get_attr_schema():\n",
      "    base_filters = [\n",
      "        {'$ref': '#/definitions/filters/value'},\n",
      "        {'$ref': '#/definitions/filters/valuekv'},\n",
      "    ]\n",
      "    any_of = []\n",
      "    any_of.extend(base_filters)\n",
      "\n",
      "    for op in ('and', 'or', 'not',):\n",
      "        any_of.append(\n",
      "            {\n",
      "                'additional_properties': False,\n",
      "                'properties': {\n",
      "                    op: {\n",
      "                        'type': 'array',\n",
      "                        'items': {\n",
      "                            'anyOf': base_filters\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                'type': 'object'\n",
      "            }\n",
      "        )\n",
      "\n",
      "    attr_schema = {\n",
      "        'items': {\n",
      "            'anyOf': any_of\n",
      "        },\n",
      "        'type': 'array',\n",
      "    }\n",
      "    return attr_schema\n",
      "\n",
      "_get_attr_schema()\n",
      "any_of.extend(base_filters)\n",
      "{'base_filters': \"[{'$ref': '#/definitions/filters/value'}, {'$ref': '#/definitions/filters/valuekv'}]\"}\n",
      "any_of = [{'$ref': '#/definitions/filters/value'}, {'$ref': '#/definitions/filters/valuekv'}]\n",
      "def _underscore(word: str) -> str:\n",
      "    # https://github.com/jpvanhal/inflection/blob/master/inflection.py\n",
      "    word = re.sub(r\"([A-Z]+)([A-Z][a-z])\", r'\\1_\\2', word)\n",
      "    word = re.sub(r\"([a-z\\d])([A-Z])\", r'\\1_\\2', word)\n",
      "    word = word.replace(\"-\", \"_\")\n",
      "    return word.lower()\n",
      "\n",
      "_underscore(word='TestKlass')\n",
      "word = re.sub(r\"([a-z\\d])([A-Z])\", r'\\1_\\2', word)\n",
      "{'word': \"'TestKlass'\"}\n",
      "word = 'Test_Klass'\n",
      "def timed_frames(records, min_frame_dur=1, max_frame_dur=None, last_frame_dur=1000):\n",
      "    \"\"\"Return a tuple made of the geometry of the screen and a generator of\n",
      "    instances of TimedFrame computed from asciicast records\n",
      "\n",
      "    Asciicast records are first coalesced so that the mininum duration between\n",
      "    two frames is at least `min_frame_dur` milliseconds. Events with a duration\n",
      "    greater than `max_frame_dur` will see their duration reduced to that value.\n",
      "\n",
      "    The duration of all frames lasting until the end of the animation\n",
      "    will be adjusted so that the last frame of the animation lasts\n",
      "    `last_frame_dur`\n",
      "\n",
      "    :param records: Terminal session record in Asciicast v2 format\n",
      "    :param min_frame_dur: Minimum frame duration in milliseconds (integer)\n",
      "    :param min_frame_dur: Minimum frame duration in milliseconds (integer)\n",
      "    :param max_frame_dur: Maximum frame duration in milliseconds (None or\n",
      "    integer)\n",
      "    :param last_frame_dur: Duration of the last frame of the animation\n",
      "    (integer)\n",
      "    \"\"\"\n",
      "    if not isinstance(records, Iterator):\n",
      "        records = iter(records)\n",
      "\n",
      "    header = next(records)\n",
      "    assert isinstance(header, AsciiCastV2Header)\n",
      "\n",
      "    if not max_frame_dur and header.idle_time_limit:\n",
      "        max_frame_dur = int(header.idle_time_limit * 1000)\n",
      "\n",
      "    def generator():\n",
      "        screen = pyte.Screen(header.width, header.height)\n",
      "        stream = pyte.Stream(screen)\n",
      "        timed_records = _group_by_time(records, min_frame_dur, max_frame_dur,\n",
      "                                       last_frame_dur)\n",
      "\n",
      "        for record_ in timed_records:\n",
      "            assert isinstance(record_, AsciiCastV2Event)\n",
      "            for char in record_.event_data:\n",
      "                stream.feed(char)\n",
      "            yield TimedFrame(int(1000 * record_.time),\n",
      "                             int(1000 * record_.duration),\n",
      "                             _screen_buffer(screen))\n",
      "\n",
      "    return (header.width, header.height), generator()\n",
      "\n",
      "timed_frames(records=[AsciiCastV2Header(version=2, width=80, height=24, theme=AsciiCastV2Theme(fg='#000000', bg='#FFFFFF', palette='#123456:#123456:#123456:#123456:#123456:#123456:#123456:#123456:#123456:#123456:#123456:#123456:#123456:#123456:#123456:#123456'), idle_time_limit=None), AsciiCastV2Event(time=0, event_type='o', event_data='0\\r\\n', duration=None), AsciiCastV2Event(time=1, event_type='o', event_data='1\\r\\n', duration=None)], min_frame_dur=1, max_frame_dur=None, last_frame_dur=42)\n",
      "records = iter(records)\n",
      "{'records': \"[AsciiCastV2Header(version=2, width=80, height=24, theme=AsciiCastV2Theme(fg='#000000', bg='#FFFFFF', palette='#123456:#123456:#123456:#123456:#123456:#123456:#123456:#123456:#123456:#123456:#123456:#123456:#123456:#123456:#123456:#123456'), idle_time_limit=None), AsciiCastV2Event(time=0, event_type='o', event_data='0\\\\r\\\\n', duration=None), AsciiCastV2Event(time=1, event_type='o', event_data='1\\\\r\\\\n', duration=None)]\"}\n",
      "records = REPR FAILED\n",
      "def __get_command_output(command, cwd=None):\n",
      "    \"\"\" Execute arbitrary commands.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    command : list\n",
      "        the command and its arguments\n",
      "\n",
      "    cwd : string\n",
      "        the directory where the command should be executed\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    output : string\n",
      "        the raw output of the command executed\n",
      "    \"\"\"\n",
      "\n",
      "    p = subprocess.Popen(command, stdout=subprocess.PIPE,\n",
      "            stderr=subprocess.PIPE, cwd=cwd)\n",
      "    p.wait()\n",
      "    return p.returncode, p.stdout.read(), p.stderr.read()\n",
      "\n",
      "__get_command_output(command=['git', 'rev-parse'], cwd='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/ASPP+pelita/ASPP+pelita/pelita')\n",
      "p = subprocess.Popen(command, stdout=subprocess.PIPE,\n",
      "{'command': \"['git', 'rev-parse']\"}\n",
      "p = <Popen: returncode: None args: ['git', 'rev-parse']>\n",
      "def parse_layout(layout_str):\n",
      "    \"\"\"Parse a layout string\n",
      "\n",
      "    Return a dict\n",
      "        {'walls': list_of_wall_coordinates,\n",
      "         'food' : list_of_food_coordinates,\n",
      "         'bot'  : list_of_4_bot_coordinate}\n",
      "\n",
      "    A layout string is composed of wall characters '#', food characters '.', and\n",
      "    bot characters '0', '1', '2', and '3'.\n",
      "\n",
      "    Valid layouts must be enclosed by walls and be of rectangular shape. Example:\n",
      "\n",
      "     ########\n",
      "     #0  .  #\n",
      "     #2    1#\n",
      "     #  .  3#\n",
      "     ########\n",
      "\n",
      "\n",
      "    If items are overlapping, several layout strings can be concateneted:\n",
      "     ########\n",
      "     #0  .  #\n",
      "     #     1#\n",
      "     #  .  3#\n",
      "     ########\n",
      "     ########\n",
      "     #2  .  #\n",
      "     #     1#\n",
      "     #  .  3#\n",
      "     ########\n",
      "\n",
      "     In this case, bot '0' and bot '2' are on top of each other at position (1,1)\n",
      "    \"\"\"\n",
      "    layout_list = []\n",
      "    start = False\n",
      "    for i, line in enumerate(layout_str.splitlines()):\n",
      "        row = line.strip()\n",
      "        if not row:\n",
      "            # ignore emptylines\n",
      "            continue\n",
      "        if not start:\n",
      "            # start a new layout\n",
      "            # check that row is a valid opening string\n",
      "            if row.count('#') != len(row):\n",
      "                raise ValueError(f\"Layout does not start with a row of walls (line: {i})!\")\n",
      "            current_layout = [row]\n",
      "            start = True\n",
      "            continue\n",
      "        # we are in the middle of a layout, just append to the current\n",
      "        # layout unless we detect the closing string\n",
      "        current_layout.append(row)\n",
      "        if row.count('#') == len(row):\n",
      "            # this is a closing string\n",
      "            # append the layout to tha layout list\n",
      "            layout_list.append('\\n'.join(current_layout))\n",
      "            start = False\n",
      "\n",
      "    if start:\n",
      "        # the last layout has not been closed, complain here!\n",
      "        raise ValueError(f\"Layout does not end with a row of walls (line: {i})!\")\n",
      "\n",
      "    # initialize walls, food and bots from the first layout\n",
      "    out = parse_single_layout(layout_list.pop(0))\n",
      "    for layout in layout_list:\n",
      "        items = parse_layout(layout)\n",
      "        # walls should always be the same\n",
      "        if items['walls'] != out['walls']:\n",
      "            raise ValueError('Walls are not equal in all layouts!')\n",
      "        # add the food, removing duplicates\n",
      "        out['food'] = list(set(out['food'] + items['food']))\n",
      "        # add the bots\n",
      "        for bot_idx, bot_pos in enumerate(items['bots']):\n",
      "            if bot_pos:\n",
      "                # this bot position is not None, overwrite whatever we had before\n",
      "                out['bots'][bot_idx] = bot_pos\n",
      "\n",
      "    return out\n",
      "\n",
      "parse_layout(layout_str='\\n        ##################\\n        #. ... .##.     3#\\n        # # #  .  .### #1#\\n        # # ##.   .      #\\n        #      .   .## # #\\n        #0# ###.  .  # # #\\n        #2     .##. ... .#\\n        ################## ')\n",
      "layout_list.append('\\n'.join(current_layout))\n",
      "{'current_layout': \"['##################', '#. ... .##.     3#', '# # #  .  .### #1#', '# # ##.   .      #', '#      .   .## # #', '#0# ###.  .  # # #', '#2     .##. ... .#', '##################']\"}\n",
      "layout_list = ['##################\\n#. ... .##.     3#\\n# # #  .  .### #1#\\n# # ##.   .      #\\n#      .   .## # #\\n#0# ###.  .  # # #\\n#2     .##. ... .#\\n##################']\n",
      "def parse_single_layout(layout_str):\n",
      "    \"\"\"Parse a single layout from a string\n",
      "\n",
      "    See parse_layout for details about valid layout strings.\n",
      "    \"\"\"\n",
      "    # width of the layout (x-axis)\n",
      "    width = None\n",
      "    # list of layout rows\n",
      "    rows = []\n",
      "    start = False\n",
      "    for i, line in enumerate(layout_str.splitlines()):\n",
      "        row = line.strip()\n",
      "        if not row:\n",
      "            # always ignore empty lines\n",
      "            continue\n",
      "        # a layout is always started by a full row of walls\n",
      "        if not start:\n",
      "            if row.count('#') != len(row):\n",
      "                raise ValueError(f\"Layout must be enclosed by walls (line: {i})!\")\n",
      "            else:\n",
      "                # start the layout parsing\n",
      "                start = True\n",
      "                # set width of layout\n",
      "                width = len(row)\n",
      "                # check that width is even\n",
      "                if width % 2:\n",
      "                    raise ValueError(f\"Layout width must be even (found {width})!\")\n",
      "                rows.append(row)\n",
      "                continue\n",
      "        # Here we are within the layout\n",
      "        # every row must have the same length\n",
      "        if len(row) != width:\n",
      "            raise ValueError(f\"Layout rows have differing widths (line: {i})!\")\n",
      "        # rows are always enclosed by walls\n",
      "        if row[0] != '#' or row[-1] != '#':\n",
      "            raise ValueError(f\"Layout must be enclosed by walls (line:{i})!\")\n",
      "        # append current row to the list of rows\n",
      "        rows.append(row)\n",
      "        # detect closing row and ignore whatever follows\n",
      "        if row.count('#') == len(row):\n",
      "            start = False\n",
      "            break\n",
      "\n",
      "    if start:\n",
      "        # layout has not been closed!\n",
      "        raise ValueError(f\"Layout must be enclosed by walls (line:{i})!\")\n",
      "\n",
      "    # height of the layout (y-axis)\n",
      "    height = len(rows)\n",
      "    walls = []\n",
      "    food = []\n",
      "    # bot positions (we assume 4 bots)\n",
      "    bots = [None]*4\n",
      "\n",
      "    # iterate through the grid of characters\n",
      "    for y, row in enumerate(rows):\n",
      "        for x, char in enumerate(row):\n",
      "            coord = (x, y)\n",
      "            # assign the char to the corresponding list\n",
      "            if char == '#':\n",
      "                # wall\n",
      "                walls.append(coord)\n",
      "            elif char == '.':\n",
      "                # food\n",
      "                food.append(coord)\n",
      "            elif char == ' ':\n",
      "                # empty\n",
      "                continue\n",
      "            else:\n",
      "                # bot\n",
      "                try:\n",
      "                    # we expect an 0<=index<=3\n",
      "                    bot_idx = int(char)\n",
      "                    if bot_idx >= len(bots):\n",
      "                        # reuse the except below\n",
      "                        raise ValueError\n",
      "                except ValueError:\n",
      "                    raise ValueError(f\"Unknown character {char} in maze!\")\n",
      "                bots[bot_idx] = coord\n",
      "    walls.sort()\n",
      "    food.sort()\n",
      "    return {'walls':walls, 'food':food, 'bots':bots}\n",
      "\n",
      "parse_single_layout(layout_str='##################\\n#. ... .##.     3#\\n# # #  .  .### #1#\\n# # ##.   .      #\\n#      .   .## # #\\n#0# ###.  .  # # #\\n#2     .##. ... .#\\n##################')\n",
      "rows.append(row)\n",
      "{'row': \"'##################'\"}\n",
      "rows = ['##################']\n",
      "def initial_positions(walls):\n",
      "    \"\"\"Calculate initial positions.\n",
      "\n",
      "    Given the list of walls, returns the free positions that are closest to the\n",
      "    bottom left and top right corner. The algorithm starts searching from\n",
      "    (1, height-2) and (width-2, 1) respectively and uses the Manhattan distance\n",
      "    for judging what is closest. On equal distances, a smaller distance in the\n",
      "    x value is preferred.\n",
      "    \"\"\"\n",
      "    width = max(walls)[0] + 1\n",
      "    height = max(walls)[1] + 1\n",
      "\n",
      "    left_start = (1, height - 2)\n",
      "    left = []\n",
      "    right_start = (width - 2, 1)\n",
      "    right = []\n",
      "\n",
      "    dist = 0\n",
      "    while len(left) < 2:\n",
      "        # iterate through all possible x distances (inclusive)\n",
      "        for x_dist in range(dist + 1):\n",
      "            y_dist = dist - x_dist\n",
      "            pos = (left_start[0] + x_dist, left_start[1] - y_dist)\n",
      "            # if both coordinates are out of bounds, we stop\n",
      "            if not (0 <= pos[0] < width) and not (0 <= pos[1] < height):\n",
      "                raise ValueError(\"Not enough free initial positions.\")\n",
      "            # if one coordinate is out of bounds, we just continue\n",
      "            if not (0 <= pos[0] < width) or not (0 <= pos[1] < height):\n",
      "                continue\n",
      "            # check if the new value is free\n",
      "            if pos not in walls:\n",
      "                left.append(pos)\n",
      "\n",
      "            if len(left) == 2:\n",
      "                break\n",
      "\n",
      "        dist += 1\n",
      "\n",
      "    dist = 0\n",
      "    while len(right) < 2:\n",
      "        # iterate through all possible x distances (inclusive)\n",
      "        for x_dist in range(dist + 1):\n",
      "            y_dist = dist - x_dist\n",
      "            pos = (right_start[0] - x_dist, right_start[1] + y_dist)\n",
      "            # if both coordinates are out of bounds, we stop\n",
      "            if not (0 <= pos[0] < width) and not (0 <= pos[1] < height):\n",
      "                raise ValueError(\"Not enough free initial positions.\")\n",
      "            # if one coordinate is out of bounds, we just continue\n",
      "            if not (0 <= pos[0] < width) or not (0 <= pos[1] < height):\n",
      "                continue\n",
      "            # check if the new value is free\n",
      "            if pos not in walls:\n",
      "                right.append(pos)\n",
      "\n",
      "            if len(right) == 2:\n",
      "                break\n",
      "\n",
      "        dist += 1\n",
      "\n",
      "    # lower indices start further away\n",
      "    left.reverse()\n",
      "    right.reverse()\n",
      "    return [left[0], right[0], left[1], right[1]]\n",
      "\n",
      "initial_positions(walls=[(0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 3), (2, 0), (2, 1), (2, 3), (3, 0), (3, 1), (3, 3), (4, 0), (4, 1), (4, 3), (5, 0), (5, 3), (6, 0), (6, 3), (7, 0), (7, 1), (7, 2), (7, 3)])\n",
      "height = max(walls)[1] + 1\n",
      "{'walls': '[(0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 3), (2, 0), (2, 1), (2, 3), (3, 0), (3, 1), (3, 3), (4, 0), (4, 1), (4, 3), (5, 0), (5, 3), (6, 0), (6, 3), (7, 0), (7, 1), (7, 2), (7, 3)]'}\n",
      "height = 4\n",
      "def deep_extend(*args):\n",
      "        \"\"\"\n",
      "        Deep copy of each item (\"extend\" makes swallow copy!)\n",
      "        \"\"\"\n",
      "        def clone_obj(item):\n",
      "            if isinstance(item, dict):\n",
      "                return dict(**item)\n",
      "            if isinstance(item, (list, tuple)):\n",
      "                return list(item)\n",
      "            return None\n",
      "\n",
      "        def iterator(item, i, iterable):\n",
      "            obj = clone_obj(item)\n",
      "            if obj is None:\n",
      "                iterable[i] = item\n",
      "            else:\n",
      "                if isinstance(obj, dict):\n",
      "                    iterable[i] = deep_extend({}, obj)\n",
      "                elif isinstance(obj, (list, tuple)):\n",
      "                    FuncFlow.each(obj, iterator)\n",
      "                    iterable[i] = obj\n",
      "                else:\n",
      "                    raise TypeError(\"deep_copy cannot handle this type: {}\".format(type(obj)))\n",
      "            \n",
      "        args = list(args)\n",
      "        dest = args.pop(0)\n",
      "\n",
      "        for source in args:\n",
      "            if source:\n",
      "                for k, v in source.items():\n",
      "                    obj = clone_obj(v)\n",
      "                    if obj is None:\n",
      "                        dest[k] = v\n",
      "                    else:\n",
      "                        FuncFlow.each(obj, iterator)\n",
      "                        dest[k] = obj\n",
      "        return dest\n",
      "\n",
      "deep_extend(args=(2,))\n",
      "args = list(args)\n",
      "{'args': '(2,)'}\n",
      "args = [2]\n",
      "def get_nested(d, path, delimiter=\"/\"):\n",
      "    \"\"\"\n",
      "    Address nested dicts via combined path\n",
      "    \"\"\"\n",
      "    def item_by_tag(d, tags):\n",
      "        # print(\">>>>>>>>>>>>>> running nested\", d, tags)\n",
      "        t = tags[-1]\n",
      "        if len(tags) == 1:\n",
      "            return d[t]\n",
      "        return item_by_tag(d[t], tags[:-1])\n",
      "\n",
      "    tags = path.split(delimiter)\n",
      "    tags.reverse()\n",
      "    # print(\">>>>>>>>>>>>>> splitted\", tags)\n",
      "    return item_by_tag(d, tags)\n",
      "\n",
      "get_nested(d={'nested': {'data': 'should be unchanged', 'event': None}}, path='nested/event', delimiter='/')\n",
      "tags = path.split(delimiter)\n",
      "{'delimiter': \"'/'\"}\n",
      "tags = ['nested', 'event']\n",
      "def extend(*args):\n",
      "        args = list(args)\n",
      "        dest = args.pop(0)\n",
      "        for source in args:\n",
      "            if source:\n",
      "                dest.update(source)\n",
      "        return dest\n",
      "\n",
      "extend(args=({}, {}))\n",
      "args = list(args)\n",
      "{'args': '({}, {})'}\n",
      "args = [{}, {}]\n",
      "def digit_version(version_str: str, length: int = 4):\n",
      "    \"\"\"Convert a version string into a tuple of integers.\n",
      "\n",
      "    This method is usually used for comparing two versions. For pre-release\n",
      "    versions: alpha < beta < rc.\n",
      "\n",
      "    Args:\n",
      "        version_str (str): The version string.\n",
      "        length (int): The maximum number of version levels. Default: 4.\n",
      "\n",
      "    Returns:\n",
      "        tuple[int]: The version info in digits (integers).\n",
      "    \"\"\"\n",
      "    assert 'parrots' not in version_str\n",
      "    version = parse(version_str)\n",
      "    assert version.release, f'failed to parse version {version_str}'\n",
      "    release = list(version.release)\n",
      "    release = release[:length]\n",
      "    if len(release) < length:\n",
      "        release = release + [0] * (length - len(release))\n",
      "    if version.is_prerelease:\n",
      "        mapping = {'a': -3, 'b': -2, 'rc': -1}\n",
      "        val = -4\n",
      "        # version.pre can be None\n",
      "        if version.pre:\n",
      "            if version.pre[0] not in mapping:\n",
      "                warnings.warn(f'unknown prerelease version {version.pre[0]}, '\n",
      "                              'version checking may go wrong')\n",
      "            else:\n",
      "                val = mapping[version.pre[0]]\n",
      "            release.extend([val, version.pre[-1]])\n",
      "        else:\n",
      "            release.extend([val, 0])\n",
      "\n",
      "    elif version.is_postrelease:\n",
      "        release.extend([1, version.post])\n",
      "    else:\n",
      "        release.extend([0, 0])\n",
      "    return tuple(release)\n",
      "\n",
      "digit_version(version_str='2.2.2+cu121', length=4)\n",
      "version = parse(version_str)\n",
      "{'version_str': \"'2.2.2+cu121'\"}\n",
      "version = <Version('2.2.2+cu121')>\n",
      "def from_file(filepath, delimiter=\"\", blanklines=False):\n",
      "    \"\"\"Imports userdata from a file.\n",
      "\n",
      "    :type filepath: string\n",
      "\n",
      "    :param filepath  The absolute path to the file.\n",
      "\n",
      "    :type delimiter: string\n",
      "\n",
      "    :param: delimiter  Delimiter to use with the troposphere.Join().\n",
      "\n",
      "    :type blanklines: boolean\n",
      "\n",
      "    :param blanklines  If blank lines should be ignored\n",
      "\n",
      "    rtype: troposphere.Base64\n",
      "    :return The base64 representation of the file.\n",
      "    \"\"\"\n",
      "    data = []\n",
      "\n",
      "    try:\n",
      "        with open(filepath, \"r\") as f:\n",
      "            for line in f:\n",
      "                if blanklines and line.strip(\"\\n\\r \") == \"\":\n",
      "                    continue\n",
      "\n",
      "                data.append(line)\n",
      "    except IOError:\n",
      "        raise IOError(\"Error opening or reading file: {}\".format(filepath))\n",
      "\n",
      "    return Base64(Join(delimiter, data))\n",
      "\n",
      "from_file(filepath='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/cloudtools+troposphere/cloudtools+troposphere/tests/userdata_test_scripts/char_escaping.sh', delimiter='', blanklines=False)\n",
      "with open(filepath, \"r\") as f:\n",
      "{'filepath': \"'/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/cloudtools+troposphere/cloudtools+troposphere/tests/userdata_test_scripts/char_escaping.sh'\"}\n",
      "f = <_io.TextIOWrapper name='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/cloudtools+troposphere/cloudtools+troposphere/tests/userdata_test_scripts/char_escaping.sh' mode='r' encoding='UTF-8'>\n",
      "def SeparateFlagArgs(args: list):\n",
      "  \"\"\"Splits a list of args into those for Flags and those for Fire.\n",
      "\n",
      "  If an isolated '--' arg is not present in the arg list, then all of the args\n",
      "  are for Fire. If there is an isolated '--', then the args after the final '--'\n",
      "  are flag args, and the rest of the args are fire args.\n",
      "\n",
      "  Args:\n",
      "    args: The list of arguments received by the Fire command.\n",
      "  Returns:\n",
      "    A tuple with the Fire args (a list), followed by the Flag args (a list).\n",
      "  \"\"\"\n",
      "  if len(args) > 0 and (args[-1] == '-h' or args[-1] == '--help') and '--' not in args:\n",
      "    args.pop()\n",
      "    args.append('--')\n",
      "    args.append('-h')\n",
      "\n",
      "  if '--' in args:\n",
      "    separator_index = len(args) - 1 - args[::-1].index('--')  # index of last --\n",
      "    flag_args = args[separator_index + 1:]\n",
      "    args = args[:separator_index]\n",
      "    return args, flag_args\n",
      "\n",
      "  return args, []\n",
      "\n",
      "SeparateFlagArgs(args=['a', 'b', '--'])\n",
      "separator_index = len(args) - 1 - args[::-1].index('--')  # index of last --\n",
      "{'args': \"['a', 'b', '--']\"}\n",
      "separator_index = 2\n",
      "def prepare_docstring_help(N):\n",
      "    \"\"\"Replace docstrings to include the parameters (schema)\"\"\"\n",
      "    # at this point, the params have not yet been populated\n",
      "\n",
      "    args = []\n",
      "    if hasattr(N, '__annotations__'):\n",
      "        for attr_name, cls in N.__annotations__.items():\n",
      "\n",
      "            filtered = filter_params(N)\n",
      "            parsed = parse_source_for_params(filtered)\n",
      "            attr = attr_map(parsed).get(attr_name)\n",
      "            if attr is None:\n",
      "                continue\n",
      "\n",
      "            args.append(argument_help(attr_name, attr))\n",
      "\n",
      "    return '\\n'.join(args)\n",
      "\n",
      "prepare_docstring_help(N={})\n",
      "parsed = parse_source_for_params(filtered)\n",
      "{'filtered': \"['            bar: int = 0\\\\n']\"}\n",
      "parsed = OrderedDict([('bar: int', '0')])\n",
      "def filter_params(N):\n",
      "    \"\"\"Filter source lines of the class\n",
      "    Returns:\n",
      "        fields as source lines\n",
      "    \"\"\"\n",
      "    filtered_source = []\n",
      "    for line in inspect.getsourcelines(N.__class__)[0][1:]:\n",
      "        # When parsing, post_init would bleed into the attributes without this hack\n",
      "        if line.strip().startswith('def '):\n",
      "            break\n",
      "        filtered_source.append(line)\n",
      "    return filtered_source\n",
      "\n",
      "filter_params(N={})\n",
      "filtered_source.append(line)\n",
      "{'line': \"'            bar: int = 0\\\\n'\"}\n",
      "filtered_source = ['            bar: int = 0\\n']\n",
      "def _ParseFn(args):\n",
      "    \"\"\"Parses the list of `args` into (varargs, kwargs), remaining_args.\"\"\"\n",
      "    kwargs, remaining_kwargs, remaining_args = _ParseKeywordArgs(\n",
      "        args, all_args, fn_spec.varkw)\n",
      "\n",
      "    # Note: _ParseArgs modifies kwargs.\n",
      "    parsed_args, kwargs, remaining_args, capacity = _ParseArgs(\n",
      "        fn_spec.args, fn_spec.defaults, num_required_args, kwargs,\n",
      "        remaining_args, metadata)\n",
      "\n",
      "    if fn_spec.varargs or fn_spec.varkw:\n",
      "      # If we're allowed *varargs or **kwargs, there's always capacity.\n",
      "      capacity = True\n",
      "\n",
      "    extra_kw = set(kwargs) - set(fn_spec.kwonlyargs)\n",
      "    if fn_spec.varkw is None and extra_kw:\n",
      "      raise FireError('Unexpected kwargs present:', extra_kw)\n",
      "\n",
      "    missing_kwonly = set(required_kwonly) - set(kwargs)\n",
      "    if missing_kwonly:\n",
      "      raise FireError('Missing required flags:', missing_kwonly)\n",
      "\n",
      "    # If we accept *varargs, then use all remaining arguments for *varargs.\n",
      "    if fn_spec.varargs is not None:\n",
      "      varargs, remaining_args = remaining_args, []\n",
      "    else:\n",
      "      varargs = []\n",
      "\n",
      "    for index, value in enumerate(varargs):\n",
      "      varargs[index] = _ParseValue(value, None, None, metadata)\n",
      "\n",
      "    varargs = parsed_args + varargs\n",
      "    remaining_args += remaining_kwargs\n",
      "\n",
      "    consumed_args = args[:len(args) - len(remaining_args)]\n",
      "    return (varargs, kwargs), consumed_args, remaining_args, capacity\n",
      "\n",
      "_ParseFn(args=['x'], all_args=[], fn_spec={args=[], varargs=None, varkw='cli_args', defaults=(), kwonlyargs=[], kwonlydefaults={}, annotations={}}, metadata={'ACCEPTS_POSITIONAL_ARGS': False}, num_required_args=0, required_kwonly=set())\n",
      "consumed_args = args[:len(args) - len(remaining_args)]\n",
      "{'args': \"['x']\"}\n",
      "consumed_args = []\n",
      "def config_dict(configuration_tuple):\n",
      "    config_dict = {}\n",
      "\n",
      "    config_file = configuration_tuple._asdict().get('CFG')\n",
      "    if config_file is None:\n",
      "        config_file = configfile.get_config_path(configuration_tuple)\n",
      "\n",
      "    if config_file is not None:\n",
      "        config_dict = utils.filter_fields(configfile.read_config(config_file), configuration_tuple)\n",
      "        config_dict = utils.type_correct_with(config_dict, configuration_tuple)\n",
      "\n",
      "    return config_dict\n",
      "\n",
      "config_dict(configuration_tuple={})\n",
      "config_dict = utils.type_correct_with(config_dict, configuration_tuple)\n",
      "{'config_dict': \"{'bar': '42'}\", 'configuration_tuple': '{}'}\n",
      "config_dict = {'bar': 42}\n",
      "def type_correct_with(cdict, cfg_tuple):\n",
      "    \"\"\"Use type hints of the cfg tuple to cast parameters i.e. attributes into their intended types\"\"\"\n",
      "    # TODO: This would be cleaner, if the config would use Schema or derivative in the\n",
      "    # first place and use its validation process\n",
      "    res = {}\n",
      "    for k, v in cdict.items():\n",
      "        typename = getattr(cfg_tuple, k)\n",
      "        res.update({k: type(typename)(v)})\n",
      "    return res\n",
      "\n",
      "type_correct_with(cdict={'bar': '42'}, cfg_tuple={})\n",
      "res.update({k: type(typename)(v)})\n",
      "{'typename': '0'}\n",
      "res = {'bar': 42}\n",
      "def is_eq(value, rep=None):\n",
      "    if rep is None:\n",
      "        rep = repr(value)\n",
      "\n",
      "    def is_valid(data, explain=False):\n",
      "        if not explain:\n",
      "            return data == value\n",
      "        return (\n",
      "            True, 'data is equal to {}'.format(rep)\n",
      "        ) if data == value else (\n",
      "            False, 'data is not equal to {}'.format(rep)\n",
      "        )\n",
      "    return is_valid\n",
      "\n",
      "is_eq(value=0, rep=None)\n",
      "rep = repr(value)\n",
      "{'value': '0'}\n",
      "rep = '0'\n",
      "def is_geq(value, rep=None):\n",
      "    if rep is None:\n",
      "        rep = repr(value)\n",
      "\n",
      "    def is_valid(data, explain=False):\n",
      "        if not explain:\n",
      "            return data >= value\n",
      "        return (\n",
      "            True, 'data is greater than or equal to {}'.format(rep)\n",
      "        ) if data >= value else (\n",
      "            False, 'data is not greater than or equal to {}'.format(rep)\n",
      "        )\n",
      "    return is_valid\n",
      "\n",
      "is_geq(value=0, rep=None)\n",
      "rep = repr(value)\n",
      "{'value': '0'}\n",
      "rep = '0'\n",
      "def is_lt(value, rep=None):\n",
      "    if rep is None:\n",
      "        rep = repr(value)\n",
      "\n",
      "    def is_valid(data, explain=False):\n",
      "        if not explain:\n",
      "            return data < value\n",
      "        return (\n",
      "            True, 'data is lower than {}'.format(rep)\n",
      "        ) if data < value else (\n",
      "            False, 'data is not lower than {}'.format(rep)\n",
      "        )\n",
      "    return is_valid\n",
      "\n",
      "is_lt(value=0, rep=None)\n",
      "rep = repr(value)\n",
      "{'value': '0'}\n",
      "rep = '0'\n",
      "def make_repo(path=None):\n",
      "    if not path:\n",
      "        path = \".\"\n",
      "    repo = git.Repo.init(path)\n",
      "    repo.config_writer().set_value(\"user\", \"name\", \"Test User\").release()\n",
      "    repo.config_writer().set_value(\"user\", \"email\", \"testuser@example.com\").release()\n",
      "\n",
      "    return repo\n",
      "\n",
      "make_repo(path='/tmp/tmpbbwa2p3f')\n",
      "repo = git.Repo.init(path)\n",
      "{'path': \"'/tmp/tmpbbwa2p3f'\"}\n",
      "repo = <git.repo.base.Repo '/tmp/tmpbbwa2p3f/.git'>\n",
      "def safe_abs_path(res):\n",
      "    \"Gives an abs path, which safely returns a full (not 8.3) windows path\"\n",
      "    res = Path(res).resolve()\n",
      "    return str(res)\n",
      "\n",
      "safe_abs_path(res='/tmp/tmpbbwa2p3f')\n",
      "res = Path(res).resolve()\n",
      "{'res': \"'/tmp/tmpbbwa2p3f'\"}\n",
      "res = PosixPath('/tmp/tmpbbwa2p3f')\n",
      "def is_image_file(file_name):\n",
      "    \"\"\"\n",
      "    Check if the given file name has an image file extension.\n",
      "    \n",
      "    :param file_name: The name of the file to check.\n",
      "    :return: True if the file is an image, False otherwise.\n",
      "    \"\"\"\n",
      "    file_name = str(file_name)  # Convert file_name to string\n",
      "    return any(file_name.endswith(ext) for ext in IMAGE_EXTENSIONS)\n",
      "\n",
      "is_image_file(file_name=PosixPath('/tmp/tmpcr1f5en7/.gitignore'))\n",
      "file_name = str(file_name)  # Convert file_name to string\n",
      "{'file_name': \"PosixPath('/tmp/tmpcr1f5en7/.gitignore')\"}\n",
      "file_name = '/tmp/tmpcr1f5en7/.gitignore'\n",
      "def find_original_update_blocks(content, fence=DEFAULT_FENCE):\n",
      "    # make sure we end with a newline, otherwise the regex will miss <<UPD on the last line\n",
      "    if not content.endswith(\"\\n\"):\n",
      "        content = content + \"\\n\"\n",
      "\n",
      "    pieces = re.split(split_re, content)\n",
      "\n",
      "    pieces.reverse()\n",
      "    processed = []\n",
      "\n",
      "    # Keep using the same filename in cases where GPT produces an edit block\n",
      "    # without a filename.\n",
      "    current_filename = None\n",
      "    try:\n",
      "        while pieces:\n",
      "            cur = pieces.pop()\n",
      "\n",
      "            if cur in (DIVIDER, UPDATED):\n",
      "                processed.append(cur)\n",
      "                raise ValueError(f\"Unexpected {cur}\")\n",
      "\n",
      "            if cur.strip() != HEAD:\n",
      "                processed.append(cur)\n",
      "                continue\n",
      "\n",
      "            processed.append(cur)  # original_marker\n",
      "\n",
      "            filename = strip_filename(processed[-2].splitlines()[-1], fence)\n",
      "            try:\n",
      "                if not filename:\n",
      "                    filename = strip_filename(processed[-2].splitlines()[-2], fence)\n",
      "                if not filename:\n",
      "                    if current_filename:\n",
      "                        filename = current_filename\n",
      "                    else:\n",
      "                        raise ValueError(missing_filename_err)\n",
      "            except IndexError:\n",
      "                if current_filename:\n",
      "                    filename = current_filename\n",
      "                else:\n",
      "                    raise ValueError(missing_filename_err)\n",
      "\n",
      "            current_filename = filename\n",
      "\n",
      "            original_text = pieces.pop()\n",
      "            processed.append(original_text)\n",
      "\n",
      "            divider_marker = pieces.pop()\n",
      "            processed.append(divider_marker)\n",
      "            if divider_marker.strip() != DIVIDER:\n",
      "                raise ValueError(f\"Expected `{DIVIDER}` not {divider_marker.strip()}\")\n",
      "\n",
      "            updated_text = pieces.pop()\n",
      "            processed.append(updated_text)\n",
      "\n",
      "            updated_marker = pieces.pop()\n",
      "            processed.append(updated_marker)\n",
      "            if updated_marker.strip() != UPDATED:\n",
      "                raise ValueError(f\"Expected `{UPDATED}` not `{updated_marker.strip()}\")\n",
      "\n",
      "            yield filename, original_text, updated_text\n",
      "    except ValueError as e:\n",
      "        processed = \"\".join(processed)\n",
      "        err = e.args[0]\n",
      "        raise ValueError(f\"{processed}\\n^^^ {err}\")\n",
      "    except IndexError:\n",
      "        processed = \"\".join(processed)\n",
      "        raise ValueError(f\"{processed}\\n^^^ Incomplete SEARCH/REPLACE block.\")\n",
      "    except Exception:\n",
      "        processed = \"\".join(processed)\n",
      "        raise ValueError(f\"{processed}\\n^^^ Error parsing SEARCH/REPLACE block.\")\n",
      "\n",
      "find_original_update_blocks(content='ok', fence=('<source>', '</source>'))\n",
      "processed.append(cur)\n",
      "{'cur': \"'ok\\\\n'\"}\n",
      "processed = ['ok\\n']\n",
      "def do_replace(fname, content, before_text, after_text, fence=None):\n",
      "    before_text = strip_quoted_wrapping(before_text, fname, fence)\n",
      "    after_text = strip_quoted_wrapping(after_text, fname, fence)\n",
      "    fname = Path(fname)\n",
      "\n",
      "    # does it want to make a new file?\n",
      "    if not fname.exists() and not before_text.strip():\n",
      "        fname.touch()\n",
      "        content = \"\"\n",
      "\n",
      "    if content is None:\n",
      "        return\n",
      "\n",
      "    if not before_text.strip():\n",
      "        # append to existing file, or start a new file\n",
      "        new_content = content + after_text\n",
      "    else:\n",
      "        new_content = replace_most_similar_chunk(content, before_text, after_text)\n",
      "\n",
      "    return new_content\n",
      "\n",
      "do_replace(fname='/tmp/tmp7g7a2csg/file.txt', content='two\\n', before_text='two\\n', after_text='three\\n', fence=('```', '```'))\n",
      "new_content = replace_most_similar_chunk(content, before_text, after_text)\n",
      "{'content': \"'two\\\\n'\", 'before_text': \"'two\\\\n'\", 'after_text': \"'three\\\\n'\"}\n",
      "new_content = 'three\\n'\n",
      "def strip_quoted_wrapping(res, fname=None, fence=DEFAULT_FENCE):\n",
      "    \"\"\"\n",
      "    Given an input string which may have extra \"wrapping\" around it, remove the wrapping.\n",
      "    For example:\n",
      "\n",
      "    filename.ext\n",
      "    ```\n",
      "    We just want this content\n",
      "    Not the filename and triple quotes\n",
      "    ```\n",
      "    \"\"\"\n",
      "    if not res:\n",
      "        return res\n",
      "\n",
      "    res = res.splitlines()\n",
      "\n",
      "    if fname and res[0].strip().endswith(Path(fname).name):\n",
      "        res = res[1:]\n",
      "\n",
      "    if res[0].startswith(fence[0]) and res[-1].startswith(fence[1]):\n",
      "        res = res[1:-1]\n",
      "\n",
      "    res = \"\\n\".join(res)\n",
      "    if res and res[-1] != \"\\n\":\n",
      "        res += \"\\n\"\n",
      "\n",
      "    return res\n",
      "\n",
      "strip_quoted_wrapping(res='two\\n', fname='/tmp/tmp7g7a2csg/file.txt', fence=('```', '```'))\n",
      "res = \"\\n\".join(res)\n",
      "{'res': \"['two']\"}\n",
      "res = 'two'\n",
      "def replace_most_similar_chunk(whole, part, replace):\n",
      "    \"\"\"Best efforts to find the `part` lines in `whole` and replace them with `replace`\"\"\"\n",
      "\n",
      "    whole, whole_lines = prep(whole)\n",
      "    part, part_lines = prep(part)\n",
      "    replace, replace_lines = prep(replace)\n",
      "\n",
      "    res = perfect_or_whitespace(whole_lines, part_lines, replace_lines)\n",
      "    if res:\n",
      "        return res\n",
      "\n",
      "    # drop leading empty line, GPT sometimes adds them spuriously (issue #25)\n",
      "    if len(part_lines) > 2 and not part_lines[0].strip():\n",
      "        skip_blank_line_part_lines = part_lines[1:]\n",
      "        res = perfect_or_whitespace(whole_lines, skip_blank_line_part_lines, replace_lines)\n",
      "        if res:\n",
      "            return res\n",
      "\n",
      "    # Try to handle when it elides code with ...\n",
      "    try:\n",
      "        res = try_dotdotdots(whole, part, replace)\n",
      "        if res:\n",
      "            return res\n",
      "    except ValueError:\n",
      "        pass\n",
      "\n",
      "    return\n",
      "    # Try fuzzy matching\n",
      "    res = replace_closest_edit_distance(whole_lines, part, part_lines, replace_lines)\n",
      "    if res:\n",
      "        return res\n",
      "\n",
      "replace_most_similar_chunk(whole='two\\n', part='two\\n', replace='three\\n')\n",
      "replace, replace_lines = prep(replace)\n",
      "{'replace': \"'three\\\\n'\"}\n",
      "replace_lines = ['three\\n']\n",
      "def perfect_or_whitespace(whole_lines, part_lines, replace_lines):\n",
      "    # Try for a perfect match\n",
      "    res = perfect_replace(whole_lines, part_lines, replace_lines)\n",
      "    if res:\n",
      "        return res\n",
      "\n",
      "    # Try being flexible about leading whitespace\n",
      "    res = replace_part_with_missing_leading_whitespace(whole_lines, part_lines, replace_lines)\n",
      "    if res:\n",
      "        return res\n",
      "\n",
      "perfect_or_whitespace(whole_lines=['two\\n'], part_lines=['two\\n'], replace_lines=['three\\n'])\n",
      "res = perfect_replace(whole_lines, part_lines, replace_lines)\n",
      "{'whole_lines': \"['two\\\\n']\", 'part_lines': \"['two\\\\n']\", 'replace_lines': \"['three\\\\n']\"}\n",
      "res = 'three\\n'\n",
      "def perfect_replace(whole_lines, part_lines, replace_lines):\n",
      "    part_tup = tuple(part_lines)\n",
      "    part_len = len(part_lines)\n",
      "\n",
      "    for i in range(len(whole_lines) - part_len + 1):\n",
      "        whole_tup = tuple(whole_lines[i : i + part_len])\n",
      "        if part_tup == whole_tup:\n",
      "            res = whole_lines[:i] + replace_lines + whole_lines[i + part_len :]\n",
      "            return \"\".join(res)\n",
      "\n",
      "perfect_replace(whole_lines=['two\\n'], part_lines=['two\\n'], replace_lines=['three\\n'])\n",
      "part_tup = tuple(part_lines)\n",
      "{'part_lines': \"['two\\\\n']\"}\n",
      "part_tup = ('two\\n',)\n",
      "def parse_quoted_filenames(args):\n",
      "    filenames = re.findall(r\"\\\"(.+?)\\\"|(\\S+)\", args)\n",
      "    filenames = [name for sublist in filenames for name in sublist if name]\n",
      "    return filenames\n",
      "\n",
      "parse_quoted_filenames(args='foo.txt bar.txt')\n",
      "filenames = re.findall(r\"\\\"(.+?)\\\"|(\\S+)\", args)\n",
      "{'args': \"'foo.txt bar.txt'\"}\n",
      "filenames = [('', 'foo.txt'), ('', 'bar.txt')]\n",
      "def match_but_for_leading_whitespace(whole_lines, part_lines):\n",
      "    num = len(whole_lines)\n",
      "\n",
      "    # does the non-whitespace all agree?\n",
      "    if not all(whole_lines[i].lstrip() == part_lines[i].lstrip() for i in range(num)):\n",
      "        return\n",
      "\n",
      "    # are they all offset the same?\n",
      "    add = set(\n",
      "        whole_lines[i][: len(whole_lines[i]) - len(part_lines[i])]\n",
      "        for i in range(num)\n",
      "        if whole_lines[i].strip()\n",
      "    )\n",
      "\n",
      "    if len(add) != 1:\n",
      "        return\n",
      "\n",
      "    return add.pop()\n",
      "\n",
      "match_but_for_leading_whitespace(whole_lines=['    line1\\n', '    line2\\n'], part_lines=['line1\\n', 'line2\\n'])\n",
      "num = len(whole_lines)\n",
      "{'whole_lines': \"['    line1\\\\n', '    line2\\\\n']\"}\n",
      "num = 2\n",
      "def check_gitignore(git_root, io, ask=True):\n",
      "    if not git_root:\n",
      "        return\n",
      "\n",
      "    try:\n",
      "        repo = git.Repo(git_root)\n",
      "        if repo.ignored(\".aider\"):\n",
      "            return\n",
      "    except git.exc.InvalidGitRepositoryError:\n",
      "        pass\n",
      "\n",
      "    pat = \".aider*\"\n",
      "\n",
      "    gitignore_file = Path(git_root) / \".gitignore\"\n",
      "    if gitignore_file.exists():\n",
      "        content = io.read_text(gitignore_file)\n",
      "        if content is None:\n",
      "            return\n",
      "        if pat in content.splitlines():\n",
      "            return\n",
      "    else:\n",
      "        content = \"\"\n",
      "\n",
      "    if ask and not io.confirm_ask(f\"Add {pat} to .gitignore (recommended)?\"):\n",
      "        return\n",
      "\n",
      "    if content and not content.endswith(\"\\n\"):\n",
      "        content += \"\\n\"\n",
      "    content += pat + \"\\n\"\n",
      "    io.write_text(gitignore_file, content)\n",
      "\n",
      "    io.tool_output(f\"Added {pat} to .gitignore\")\n",
      "\n",
      "check_gitignore(git_root=PosixPath('/tmp/tmpcr1f5en7'), io={user_input_color=None, tool_output_color=None, tool_error_color=None, input=None, output=None, pretty=False, yes=True, input_history_file=None, chat_history_file=None, encoding='utf-8', dry_run=False, console=<console width=105 None>}, ask=True)\n",
      "gitignore_file = Path(git_root) / \".gitignore\"\n",
      "{'git_root': \"PosixPath('/tmp/tmpcr1f5en7')\"}\n",
      "gitignore_file = PosixPath('/tmp/tmpcr1f5en7/.gitignore')\n",
      "def find_diffs(content):\n",
      "    # We can always fence with triple-quotes, because all the udiff content\n",
      "    # is prefixed with +/-/space.\n",
      "\n",
      "    if not content.endswith(\"\\n\"):\n",
      "        content = content + \"\\n\"\n",
      "\n",
      "    lines = content.splitlines(keepends=True)\n",
      "    line_num = 0\n",
      "    edits = []\n",
      "    while line_num < len(lines):\n",
      "        while line_num < len(lines):\n",
      "            line = lines[line_num]\n",
      "            if line.startswith(\"```diff\"):\n",
      "                line_num, these_edits = process_fenced_block(lines, line_num + 1)\n",
      "                edits += these_edits\n",
      "                break\n",
      "            line_num += 1\n",
      "\n",
      "    # For now, just take 1!\n",
      "    # edits = edits[:1]\n",
      "\n",
      "    return edits\n",
      "\n",
      "find_diffs(content='\\nSome text...\\n\\n```diff\\n--- /dev/null\\n+++ file.txt\\n@@ ... @@\\n-Original\\n+Modified\\n```\\n')\n",
      "line_num, these_edits = process_fenced_block(lines, line_num + 1)\n",
      "{'lines': \"['\\\\n', 'Some text...\\\\n', '\\\\n', '```diff\\\\n', '--- /dev/null\\\\n', '+++ file.txt\\\\n', '@@ ... @@\\\\n', '-Original\\\\n', '+Modified\\\\n', '```\\\\n']\"}\n",
      "these_edits = [('file.txt', ['-Original\\n', '+Modified\\n'])]\n",
      "def test_decorated_processors(partial_val):\n",
      "    class ExampleSchema(Schema):\n",
      "        \"\"\"Includes different ways to invoke decorators and set up methods\"\"\"\n",
      "\n",
      "        TAG = \"TAG\"\n",
      "\n",
      "        value = fields.Integer(as_string=True)\n",
      "\n",
      "        # Implicit default raw, pre dump, static method.\n",
      "        @pre_dump\n",
      "        def increment_value(self, item, **kwargs):\n",
      "            assert \"many\" in kwargs\n",
      "            item[\"value\"] += 1\n",
      "            return item\n",
      "\n",
      "        # Implicit default raw, post dump, class method.\n",
      "        @post_dump\n",
      "        def add_tag(self, item, **kwargs):\n",
      "            assert \"many\" in kwargs\n",
      "            item[\"value\"] = self.TAG + item[\"value\"]\n",
      "            return item\n",
      "\n",
      "        # Explicitly raw, post dump, instance method.\n",
      "        @post_dump(pass_many=True)\n",
      "        def add_envelope(self, data, many, **kwargs):\n",
      "            key = self.get_envelope_key(many)\n",
      "            return {key: data}\n",
      "\n",
      "        # Explicitly raw, pre load, instance method.\n",
      "        @pre_load(pass_many=True)\n",
      "        def remove_envelope(self, data, many, partial, **kwargs):\n",
      "            assert partial is partial_val\n",
      "            key = self.get_envelope_key(many)\n",
      "            return data[key]\n",
      "\n",
      "        @staticmethod\n",
      "        def get_envelope_key(many):\n",
      "            return \"data\" if many else \"datum\"\n",
      "\n",
      "        # Explicitly not raw, pre load, instance method.\n",
      "        @pre_load(pass_many=False)\n",
      "        def remove_tag(self, item, partial, **kwargs):\n",
      "            assert partial is partial_val\n",
      "            assert \"many\" in kwargs\n",
      "            item[\"value\"] = item[\"value\"][len(self.TAG) :]\n",
      "            return item\n",
      "\n",
      "        # Explicit default raw, post load, instance method.\n",
      "        @post_load()\n",
      "        def decrement_value(self, item, partial, **kwargs):\n",
      "            assert partial is partial_val\n",
      "            assert \"many\" in kwargs\n",
      "            item[\"value\"] -= 1\n",
      "            return item\n",
      "\n",
      "    schema = ExampleSchema(partial=partial_val)\n",
      "\n",
      "    # Need to re-create these because the processors will modify in place.\n",
      "    make_item = lambda: {\"value\": 3}\n",
      "    make_items = lambda: [make_item(), {\"value\": 5}]\n",
      "\n",
      "    item_dumped = schema.dump(make_item())\n",
      "    assert item_dumped == {\"datum\": {\"value\": \"TAG4\"}}\n",
      "    item_loaded = schema.load(item_dumped)\n",
      "    assert item_loaded == make_item()\n",
      "\n",
      "    items_dumped = schema.dump(make_items(), many=True)\n",
      "    assert items_dumped == {\"data\": [{\"value\": \"TAG4\"}, {\"value\": \"TAG6\"}]}\n",
      "    items_loaded = schema.load(items_dumped, many=True)\n",
      "    assert items_loaded == make_items()\n",
      "\n",
      "test_decorated_processors(partial_val=True)\n",
      "items_loaded = schema.load(items_dumped, many=True)\n",
      "{'items_dumped': \"{'data': [{'value': 'TAG4'}, {'value': 'TAG6'}]}\"}\n",
      "items_loaded = [{'value': 3}, {'value': 5}]\n",
      "def _addpoint(sol, i, distances, prec):\n",
      "    \"\"\"\n",
      "    Try to add point i to a given solution-approach.\n",
      "\n",
      "    gives all possibilties and a status about the solution:\n",
      "        state = 0: possibilities found\n",
      "        state = 1: no possibilities but no contradiction\n",
      "        state = 2: solution-approach has a contradiction with point i\n",
      "    \"\"\"\n",
      "    res = []\n",
      "\n",
      "    # if i is already part of the solution return it\n",
      "    if np.ndim(sol[i]) != 0:\n",
      "        return [sol], 0\n",
      "\n",
      "    # collect the points already present in the solution\n",
      "    solpnts = []\n",
      "    for n, m in enumerate(sol):\n",
      "        if np.ndim(m) != 0:\n",
      "            solpnts.append(n)\n",
      "\n",
      "    # number of present points\n",
      "    pntscount = len(solpnts)\n",
      "\n",
      "    # try to add the point in all possible constellations\n",
      "    for n in range(pntscount - 1):\n",
      "        for m in range(n + 1, pntscount):\n",
      "            tmppnt, state = _pntcoord(\n",
      "                sol, i, solpnts[n], solpnts[m], distances, prec\n",
      "            )\n",
      "\n",
      "            # if possiblities are found, add them (at most 2! (think about))\n",
      "            if state == 0:\n",
      "                for pnt in tmppnt:\n",
      "                    res.append(dcopy(sol))\n",
      "                    res[-1][i] = pnt\n",
      "\n",
      "            # if one possiblity or a contradiction is found, return the result\n",
      "            if state != 1:\n",
      "                return res, state\n",
      "\n",
      "    # if the state remaind 1, return empty result and no contradiction\n",
      "    return res, state\n",
      "\n",
      "_addpoint(sol=[array([0., 0.]), array([3., 0.]), 0, 0], i=2, distances=array([[ 0.,  3.,  4.,  1.],       [ 3.,  0.,  2.,  3.],       [ 4.,  2.,  0., -1.],       [ 1.,  3., -1.,  0.]]), prec=0.1)\n",
      "for m in range(n + 1, pntscount):\n",
      "{'pntscount': '2'}\n",
      "m = 1\n",
      "def _pntcoord(sol, i, n, m, distances, prec):\n",
      "    \"\"\"\n",
      "    Generate coordinates for point i in constellation to points m and n.\n",
      "\n",
      "    Check if these coordinates are valid with all other points in the solution.\n",
      "    \"\"\"\n",
      "    tmppnt = []\n",
      "\n",
      "    state = 1\n",
      "\n",
      "    pntscount = len(sol)\n",
      "\n",
      "    # if no distances known, return empty result and the unknown-state\n",
      "    if distances[i, n] < -0.5 or distances[i, m] < -0.5:\n",
      "        return tmppnt, state\n",
      "\n",
      "    # if the Triangle inequality is not fullfilled give a contradiction\n",
      "    if distances[i, n] + distances[i, m] < _dist(sol[n], sol[m]):\n",
      "        state = 2\n",
      "        return tmppnt, state\n",
      "\n",
      "    # generate the affine rotation to bring the points in the right place\n",
      "    g = _affinef(*_invtranmat(*_tranmat(sol[n], sol[m])))\n",
      "\n",
      "    # generate the coordinates\n",
      "    x = _xvalue(distances[i, n], distances[i, m], _dist(sol[n], sol[m]))\n",
      "    y1, y2 = _yvalue(distances[i, n], distances[i, m], _dist(sol[n], sol[m]))\n",
      "\n",
      "    # generate the possible positons\n",
      "    pos1 = g(np.array([x, y1]))\n",
      "    pos2 = g(np.array([x, y2]))\n",
      "\n",
      "    valid1 = True\n",
      "    valid2 = True\n",
      "\n",
      "    # check if the possible positions are valid\n",
      "    for k in range(pntscount):\n",
      "        if np.ndim(sol[k]) != 0 and distances[i, k] > -0.5:\n",
      "            valid1 &= abs(_dist(sol[k], pos1) - distances[i, k]) < prec\n",
      "            valid2 &= abs(_dist(sol[k], pos2) - distances[i, k]) < prec\n",
      "\n",
      "    # if any position is valid, add it to the result\n",
      "    if valid1 or valid2:\n",
      "        state = 0\n",
      "        same = abs(y1 - y2) < prec / 4.0\n",
      "        if valid1:\n",
      "            tmppnt.append(dcopy(pos1))\n",
      "        if valid2 and not same:\n",
      "            tmppnt.append(dcopy(pos2))\n",
      "    # if the positions are not valid, give a contradiction\n",
      "    else:\n",
      "        state = 2\n",
      "\n",
      "    return tmppnt, state\n",
      "\n",
      "_pntcoord(sol=[array([0., 0.]), array([3., 0.]), 0, 0], i=2, n=0, m=1, distances=array([[ 0.,  3.,  4.,  1.],       [ 3.,  0.,  2.,  3.],       [ 4.,  2.,  0., -1.],       [ 1.,  3., -1.,  0.]]), prec=0.1)\n",
      "tmppnt.append(dcopy(pos2))\n",
      "{'pos2': 'array([ 3.5       , -1.93649167])'}\n",
      "tmppnt = [array([3.5       , 1.93649167]), array([ 3.5       , -1.93649167])]\n",
      "def _tranmat(a, b):\n",
      "    \"\"\"\n",
      "    Get the coefficents for the affine-linear function f(x)=Ax+s.\n",
      "\n",
      "    Which fullfills that A is a rotation-matrix,\n",
      "    f(a) = [0,0] and f(b) = [|b-a|,0].\n",
      "    \"\"\"\n",
      "    A = np.zeros((2, 2))\n",
      "    A[0, 0] = b[0] - a[0]\n",
      "    A[1, 1] = b[0] - a[0]\n",
      "    A[1, 0] = -(b[1] - a[1])\n",
      "    A[0, 1] = +(b[1] - a[1])\n",
      "    A /= _dist(a, b)\n",
      "    s = -np.dot(A, a)\n",
      "    return A, s\n",
      "\n",
      "_tranmat(a=array([0., 0.]), b=array([3., 0.]))\n",
      "s = -np.dot(A, a)\n",
      "{'A': 'array([[ 1.,  0.],       [-0.,  1.]])', 'a': 'array([0., 0.])'}\n",
      "s = array([-0., -0.])\n",
      "def _yvalue(b, a, c):\n",
      "    \"\"\"\n",
      "    Get the two possible y-values for the upper point of a triangle.\n",
      "\n",
      "    where c is the length of the down side starting in the origin and\n",
      "    lying on the x-axes, a is the distance of the unknown point to the origen\n",
      "    and b is the distance of the unknown point to the righter given point\n",
      "    \"\"\"\n",
      "    # ckeck flatness to eliminate numerical errors when the triangle is flat\n",
      "    if a + b <= c or a + c <= b or b + c <= a:\n",
      "        return 0.0, -0.0\n",
      "\n",
      "    res = 2 * ((a * b) ** 2 + (a * c) ** 2 + (b * c) ** 2)\n",
      "    res -= a ** 4 + b ** 4 + c ** 4\n",
      "    # in case of numerical errors set res to 0 (hope you check validty before)\n",
      "    res = max(res, 0.0)\n",
      "    res = np.sqrt(res)\n",
      "    res /= 2 * c\n",
      "    return res, -res\n",
      "\n",
      "_yvalue(b=4.0, a=2.0, c=3.0)\n",
      "res = np.sqrt(res)\n",
      "{'res': '135.0'}\n",
      "res = 11.61895003862225\n",
      "def _solequal(sol1, sol2, prec):\n",
      "    \"\"\"\n",
      "    Compare two different solutions with a given precicion.\n",
      "\n",
      "    Return True if they equal.\n",
      "    \"\"\"\n",
      "    res = True\n",
      "\n",
      "    for sol_1, sol_2 in zip(sol1, sol2):\n",
      "        if np.ndim(sol_1) != 0 and np.ndim(sol_2) != 0:\n",
      "            res &= _dist(sol_1, sol_2) < prec\n",
      "        elif np.ndim(sol_1) != 0 and np.ndim(sol_2) == 0:\n",
      "            return False\n",
      "        elif np.ndim(sol_1) == 0 and np.ndim(sol_2) != 0:\n",
      "            return False\n",
      "\n",
      "    return res\n",
      "\n",
      "_solequal(sol1=[array([0., 0.]), array([3., 0.]), array([3.5       , 1.93649167]), array([ 0.16666667, -0.9860133 ])], sol2=[array([0., 0.]), array([3., 0.]), array([3.5       , 1.93649167]), array([0.16666667, 0.9860133 ])], prec=0.1)\n",
      "res &= _dist(sol_1, sol_2) < prec\n",
      "{'sol_1': 'array([ 0.16666667, -0.9860133 ])', 'sol_2': 'array([0.16666667, 0.9860133 ])'}\n",
      "res = False\n",
      "def _process_namespace(name, namespaces, ns_sep=':', attr_prefix='@'):\n",
      "    if not namespaces:\n",
      "        return name\n",
      "    try:\n",
      "        ns, name = name.rsplit(ns_sep, 1)\n",
      "    except ValueError:\n",
      "        pass\n",
      "    else:\n",
      "        ns_res = namespaces.get(ns.strip(attr_prefix))\n",
      "        name = '{}{}{}{}'.format(\n",
      "            attr_prefix if ns.startswith(attr_prefix) else '',\n",
      "            ns_res, ns_sep, name) if ns_res else name\n",
      "    return name\n",
      "\n",
      "_process_namespace(name='http://defaultns.com/:root', namespaces={'http://defaultns.com/': '', 'http://a.com/': 'a', 'http://b.com/': 'b'}, ns_sep=':', attr_prefix='@')\n",
      "ns, name = name.rsplit(ns_sep, 1)\n",
      "{'ns_sep': \"':'\"}\n",
      "ns = 'http://defaultns.com/'\n",
      "def response_error(code_status, message=None):\n",
      "    payload = {'error': HTTP_STATUS_CODES.get(code_status, \"something went wrong\")}\n",
      "    if message:\n",
      "        payload['message'] = message\n",
      "    response = jsonify(payload)\n",
      "    response.status_code = code_status\n",
      "    return response\n",
      "\n",
      "response_error(code_status=400, message='Invalid input')\n",
      "response = jsonify(payload)\n",
      "{'payload': \"{'error': 'Bad Request', 'message': 'Invalid input'}\"}\n",
      "response = <Response 50 bytes [200 OK]>\n",
      "def check_data(a, b):\n",
      "    if not isinstance(a, np.ndarray):\n",
      "        a = np.array(a)\n",
      "\n",
      "    if not isinstance(b, np.ndarray):\n",
      "        b = np.array(b)\n",
      "\n",
      "    if type(a) != type(b):\n",
      "        raise ValueError(\"Type mismatch: %s and %s\" % (type(a), type(b)))\n",
      "\n",
      "    if a.size != b.size:\n",
      "        raise ValueError(\"Arrays must be equal in length.\")\n",
      "    return a, b\n",
      "\n",
      "check_data(a=[], b=1)\n",
      "b = np.array(b)\n",
      "{'b': '1'}\n",
      "b = array(1)\n",
      "def clasifier(optimizer):\n",
      "    X, y = make_classification(\n",
      "        n_samples=1000, n_features=100, n_informative=75, random_state=1111, n_classes=2, class_sep=2.5\n",
      "    )\n",
      "    y = one_hot(y)\n",
      "\n",
      "    X -= np.mean(X, axis=0)\n",
      "    X /= np.std(X, axis=0)\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1111)\n",
      "\n",
      "    model = NeuralNet(\n",
      "        layers=[\n",
      "            Dense(128, Parameters(init=\"uniform\")),\n",
      "            Activation(\"relu\"),\n",
      "            Dropout(0.5),\n",
      "            Dense(64, Parameters(init=\"normal\")),\n",
      "            Activation(\"relu\"),\n",
      "            Dense(2),\n",
      "            Activation(\"softmax\"),\n",
      "        ],\n",
      "        loss=\"categorical_crossentropy\",\n",
      "        optimizer=optimizer,\n",
      "        metric=\"accuracy\",\n",
      "        batch_size=64,\n",
      "        max_epochs=10,\n",
      "    )\n",
      "    model.fit(X_train, y_train)\n",
      "    predictions = model.predict(X_test)\n",
      "    return roc_auc_score(y_test[:, 0], predictions[:, 0])\n",
      "\n",
      "clasifier(optimizer={rho=0.95, eps=1e-08, lr=1.0})\n",
      "predictions = model.predict(X_test)\n",
      "{'X_test': 'array([[-0.56004849,  0.34018962,  1.01535167, ..., -0.84997967,         2.09726856, -0.93988689],       [ 0.27941223,  0.49234387,  0.22399796, ...,  1.26554326,         0.00246477, -0.44601728],       [ 0.31621209,  0.07147743, -0.76119601, ...,  0.89775686,         0.76871988,  1.87124925],       ...,       [-0.14644015, -0.945898  , -1.7625167 , ...,  1.07582814,        -1.2681324 ,  0.89124254],       [ 2.11886151, -0.81978169,  0.9298373 , ..., -0.14101579,        -0.75876976, -1.79802492],       [-2.15698547, -0.11529033, -0.13576962, ...,  0.58981803,        -1.0836584 ,  0.09128275]])'}\n",
      "predictions = array([[1.35235888e-02, 9.86476411e-01],       [1.66726720e-02, 9.83327328e-01],       [9.99999994e-01, 5.90208114e-09],       [9.98244393e-01, 1.75560676e-03],       [7.40176791e-01, 2.59823209e-01],       [3.71724770e-05, 9.99962828e-01],       [9.99713566e-01, 2.86434297e-04],       [9.18564614e-01, 8.14353861e-02],       [8.86055945e-01, 1.13944055e-01],       [9.99811079e-01, 1.88920895e-04],       [4.07889501e-03, 9.95921105e-01],       [9.97496720e-01, 2.50328006e-03],       [8.84405282e-01, 1.15594718e-01],       [8.87751398e-01, 1.12248602e-01],       [9.99998858e-01, 1.14232539e-06],       [1.48689622e-02, 9.85131038e-01],       [1.39868786e-01, 8.60131214e-01],       [4.59735284e-04, 9.99540265e-01],       [8.10085291e-01, 1.89914709e-01],       [4.53483673e-04, 9.99546516e-01],       [2.54536929e-05, 9.99974546e-01],       [1.00000000e+00, 2.95234428e-10],       [9.99999994e-01, 5.67815508e-09],       [3.58133366e-04, 9.99641867e-01],       [9.99841916e-01, 1.58084197e-04],       [9.99916089e-01, 8.39110353e-05],       [2.87906452e-01, 7.12093548e-01],       [2.94657253e-03, 9.97053427e-01],       [1.29650123e-05, 9.99987035e-01],       [5.50214872e-05, 9.99944979e-01],       [9.95252480e-05, 9.99900475e-01],       [7.23698056e-05, 9.99927630e-01],       [9.96809259e-01, 3.19074119e-03],       [1.07959433e-06, 9.99998920e-01],       [9.88249217e-01, 1.17507825e-02],       [2.97550757e-05, 9.99970245e-01],       [8.57948547e-08, 9.99999914e-01],       [8.01493506e-01, 1.98506494e-01],       [1.09917929e-01, 8.90082071e-01],       [9.21897555e-04, 9.99078102e-01],       [9.99999987e-01, 1.25416177e-08],       [9.99999998e-01, 2.12806072e-09],       [8.99300702e-01, 1.00699298e-01],       [8.24316542e-02, 9.17568346e-01],       [5.67859947e-10, 9.99999999e-01],       [9.79143160e-01, 2.08568400e-02],       [1.08458105e-02, 9.89154189e-01],       [8.11828923e-01, 1.88171077e-01],       [9.99999997e-01, 3.48283036e-09],       [3.63447084e-03, 9.96365529e-01],       [9.42074990e-01, 5.79250098e-02],       [8.03211756e-04, 9.99196788e-01],       [9.99988213e-01, 1.17866694e-05],       [9.99998358e-01, 1.64225648e-06],       [9.72622763e-02, 9.02737724e-01],       [2.11943736e-01, 7.88056264e-01],       [1.62273056e-08, 9.99999984e-01],       [1.62848262e-06, 9.99998372e-01],       [1.44160831e-04, 9.99855839e-01],       [9.13952621e-01, 8.60473792e-02],       [9.99812916e-01, 1.87083698e-04],       [9.52723540e-01, 4.72764602e-02],       [9.99988339e-01, 1.16607282e-05],       [9.86051929e-01, 1.39480708e-02],       [9.99865345e-01, 1.34654685e-04],       [9.13378149e-03, 9.90866219e-01],       [9.97440609e-01, 2.55939080e-03],       [9.63875042e-01, 3.61249578e-02],       [1.48619796e-04, 9.99851380e-01],       [5.28682244e-01, 4.71317756e-01],       [9.99718892e-01, 2.81108349e-04],       [9.94642754e-01, 5.35724604e-03],       [6.75869852e-01, 3.24130148e-01],       [5.28322536e-01, 4.71677464e-01],       [2.30777115e-02, 9.76922289e-01],       [6.54557813e-01, 3.45442187e-01],       [9.93999608e-01, 6.00039239e-03],       [3.57095748e-01, 6.42904252e-01],       [8.79406665e-02, 9.12059333e-01],       [3.85768901e-01, 6.14231099e-01],       [9.98609603e-01, 1.39039719e-03],       [3.34849170e-02, 9.66515083e-01],       [3.98038731e-03, 9.96019613e-01],       [1.74548424e-02, 9.82545158e-01],       [9.00595452e-01, 9.94045476e-02],       [4.81287456e-02, 9.51871254e-01],       [8.31075240e-01, 1.68924760e-01],       [2.61540350e-02, 9.73845965e-01],       [1.86037392e-01, 8.13962608e-01],       [1.74708127e-04, 9.99825292e-01],       [1.02657966e-03, 9.98973420e-01],       [1.18972214e-06, 9.99998810e-01],       [5.85900774e-03, 9.94140992e-01],       [1.07627613e-06, 9.99998924e-01],       [3.40652630e-01, 6.59347370e-01],       [9.83588586e-01, 1.64114140e-02],       [9.97918350e-01, 2.08165016e-03],       [9.72075551e-01, 2.79244493e-02],       [8.92673320e-03, 9.91073267e-01],       [9.99997418e-01, 2.58224639e-06],       [9.36717962e-03, 9.90632820e-01],       [9.99845583e-01, 1.54416829e-04],       [7.07307425e-03, 9.92926926e-01],       [9.95053730e-01, 4.94627014e-03],       [1.41115970e-02, 9.85888403e-01],       [3.25889450e-04, 9.99674111e-01],       [9.81332495e-04, 9.99018668e-01],       [9.60401982e-01, 3.95980180e-02],       [9.99952843e-01, 4.71566138e-05],       [9.99999947e-01, 5.34044573e-08],       [6.00106898e-07, 9.99999400e-01],       [9.99129133e-01, 8.70867188e-04],       [9.94286308e-01, 5.71369203e-03],       [9.99981113e-01, 1.88874698e-05],       [7.92336020e-02, 9.20766398e-01],       [9.95091548e-01, 4.90845229e-03],       [2.88767388e-05, 9.99971123e-01],       [2.07836444e-01, 7.92163556e-01],       [9.92386903e-01, 7.61309722e-03],       [5.25658178e-06, 9.99994743e-01],       [1.88273783e-04, 9.99811726e-01],       [9.97402679e-01, 2.59732143e-03],       [1.33733905e-04, 9.99866266e-01],       [9.99999885e-01, 1.15261224e-07],       [5.70741250e-05, 9.99942926e-01],       [2.62490629e-03, 9.97375094e-01],       [8.76013445e-01, 1.23986555e-01],       [2.61776408e-04, 9.99738224e-01],       [2.14964444e-02, 9.78503556e-01],       [8.69490648e-01, 1.30509352e-01],       [3.07479580e-05, 9.99969252e-01],       [8.02048076e-02, 9.19795192e-01],       [9.99889581e-01, 1.10418878e-04],       [7.93254407e-03, 9.92067456e-01],       [1.92176072e-01, 8.07823928e-01],       [1.71791819e-03, 9.98282082e-01],       [7.20407537e-07, 9.99999280e-01],       [9.97076828e-01, 2.92317157e-03],       [2.17317964e-07, 9.99999783e-01],       [2.00199637e-02, 9.79980036e-01],       [7.18304676e-04, 9.99281695e-01],       [9.86816819e-01, 1.31831806e-02],       [9.99991335e-01, 8.66460511e-06],       [9.81203923e-01, 1.87960770e-02],       [9.99999989e-01, 1.09012882e-08],       [2.16560536e-01, 7.83439464e-01],       [9.97841732e-01, 2.15826773e-03],       [9.98825819e-01, 1.17418094e-03],       [9.99999183e-01, 8.17282702e-07],       [8.47976914e-01, 1.52023086e-01]])\n",
      "def str2datetime(string):\n",
      "    \"\"\"\n",
      "    Convert a normalized Benthos timestamp to a datetime object\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    string : str\n",
      "        String to convert\n",
      "    \"\"\"\n",
      "\n",
      "    if string == \"0000-00-00T24:60:60Z\":\n",
      "        return None\n",
      "\n",
      "    ms = string[20:-1]\n",
      "    ms += \"000000\"[:6 - len(ms)]\n",
      "    return datetime.datetime(\n",
      "        int(string[:4]),\n",
      "        int(string[5:7]),\n",
      "        int(string[8:10]),\n",
      "        int(string[11:13]),\n",
      "        int(string[14:16]),\n",
      "        int(string[17:19]),\n",
      "        int(ms))\n",
      "\n",
      "str2datetime(string='1970-01-01T00:00:00.0Z')\n",
      "ms += \"000000\"[:6 - len(ms)]\n",
      "{'ms': \"'0'\"}\n",
      "ms = '000000'\n",
      "def getLogger(name, stdout=None):\n",
      "    \"\"\" Return logger suitable for Cumulus \"\"\"\n",
      "    logger = logging.getLogger(name)\n",
      "    # clear existing handlers\n",
      "    logger.handlers = []\n",
      "    if (stdout is None):\n",
      "        logger.addHandler(logging.NullHandler())\n",
      "    if stdout is not None:\n",
      "        handler = logging.StreamHandler()\n",
      "        handler.setLevel(stdout['level'])\n",
      "        handler.setFormatter(CumulusFormatter())\n",
      "        logger.addHandler(handler)\n",
      "    # logging level\n",
      "    logger.setLevel(1)\n",
      "    return logger\n",
      "\n",
      "getLogger(name='cumulus.aws', stdout=None)\n",
      "logger = logging.getLogger(name)\n",
      "{'name': \"'cumulus.aws'\"}\n",
      "logger = <Logger cumulus.aws (WARNING)>\n",
      "def load_json_dict(filename, *args):\n",
      "    \"\"\"Checks if file exists. Returns {} if something fails.\"\"\"\n",
      "    data = {}\n",
      "    if os.path.exists(filename):\n",
      "        lock.acquire()\n",
      "        with open(filename, \"r\") as f:\n",
      "            try:\n",
      "                data = _json.load(f)\n",
      "                if not isinstance(data, dict):\n",
      "                    data = {}\n",
      "            except:\n",
      "                data = {}  # TODO: issue a warning and bubble it up\n",
      "        lock.release()\n",
      "        if args:\n",
      "            return {key: data[key] for key in args if key in data}\n",
      "    return data\n",
      "\n",
      "load_json_dict(filename='/home/XXX/.plotly/.config', args=())\n",
      "with open(filename, \"r\") as f:\n",
      "{'filename': \"'/home/XXX/.plotly/.config'\"}\n",
      "f = <_io.TextIOWrapper name='/home/XXX/.plotly/.config' mode='r' encoding='UTF-8'>\n",
      "def _get_logger(name):\n",
      "    \"\"\"Gets a logger by name, or creates and configures it for the first time.\"\"\"\n",
      "    logger = logging.getLogger(name)\n",
      "    logger.setLevel(logging.INFO)\n",
      "    # If the logger is configured, skip the configure\n",
      "    if not logger.handlers and not logging.getLogger().handlers:\n",
      "        handler = logging.StreamHandler(sys.stderr)\n",
      "        logger.addHandler(handler)\n",
      "    return logger\n",
      "\n",
      "_get_logger(name='hyperopt.utils')\n",
      "logger = logging.getLogger(name)\n",
      "{'name': \"'hyperopt.utils'\"}\n",
      "logger = <Logger hyperopt.utils (WARNING)>\n",
      "def delete(self, *, ip_parameter, cascade):\n",
      "\n",
      "        as_address = clean_address(ip_parameter)\n",
      "        as_network = clean_network(ip_parameter)\n",
      "\n",
      "        if as_address in self.__description:\n",
      "            return self.__remove_ip_object(as_address)\n",
      "        elif as_network in self.__description:\n",
      "            return self.__remove_ip_object(as_network)\n",
      "\n",
      "        raise IPObjectNotInSpaceError(\"cannot delete undescribed IP object\")\n",
      "\n",
      "delete(self=AddressSpace(_AddressSpace__strict=False, _AddressSpace__description={IPv4Address('203.0.113.128'): 'an IPv4 test net address'}, _AddressSpace__networks={}, _AddressSpace__addresses={4: {IPv4Address('203.0.113.128')}}, _AddressSpace__parent_supernet={IPv4Address('203.0.113.128'): None}, _AddressSpace__children_ip_object={None: {IPv4Address('203.0.113.128')}}), ip_parameter='203.0.113.128', cascade=True, self._AddressSpace__addresses={4: {IPv4Address('203.0.113.128')}}, self._AddressSpace__children_ip_object={None: {IPv4Address('203.0.113.128')}}, self._AddressSpace__description={IPv4Address('203.0.113.128'): 'an IPv4 test net address'}, self._AddressSpace__networks={}, self._AddressSpace__parent_supernet={IPv4Address('203.0.113.128'): None}, self._AddressSpace__strict=False)\n",
      "as_address = clean_address(ip_parameter)\n",
      "{'ip_parameter': \"'203.0.113.128'\"}\n",
      "as_address = IPv4Address('203.0.113.128')\n",
      "def __remove_ip_object(self, ip_object):\n",
      "\n",
      "        if ip_object not in self.__description:\n",
      "            raise IPObjectNotInSpaceError(\n",
      "                \"cannot remove undescribed IP object\"\n",
      "            )\n",
      "\n",
      "        if isinstance(ip_object, IPAddressTuple):\n",
      "\n",
      "            supernet = self.__parent_supernet[ip_object]\n",
      "            self.__children_ip_object[supernet].remove(ip_object)\n",
      "            del self.__parent_supernet[ip_object]\n",
      "            del self.__description[ip_object]\n",
      "\n",
      "        elif isinstance(ip_object, IPNetworkTuple):\n",
      "\n",
      "            supernet = self.__parent_supernet[ip_object]\n",
      "            children_of_supernet = (\n",
      "                self.__children_ip_object.setdefault(supernet, set())\n",
      "            )\n",
      "\n",
      "            for child in self.__children_ip_object[ip_object]:\n",
      "                self.__parent_supernet[child] = supernet\n",
      "                children_of_supernet.add(child)\n",
      "\n",
      "            del self.__children_ip_object[ip_object]\n",
      "            del self.__parent_supernet[ip_object]\n",
      "            del self.__description[ip_object]\n",
      "\n",
      "        else:\n",
      "\n",
      "            raise TypeError(\"ip_parameter must be a valid IP object\")\n",
      "\n",
      "        return True\n",
      "\n",
      "__remove_ip_object(self=AddressSpace(_AddressSpace__strict=False, _AddressSpace__description={IPv4Address('203.0.113.128'): 'an IPv4 test net address'}, _AddressSpace__networks={}, _AddressSpace__addresses={4: {IPv4Address('203.0.113.128')}}, _AddressSpace__parent_supernet={IPv4Address('203.0.113.128'): None}, _AddressSpace__children_ip_object={None: {IPv4Address('203.0.113.128')}}), ip_object=IPv4Address('203.0.113.128'), self._AddressSpace__addresses={4: {IPv4Address('203.0.113.128')}}, self._AddressSpace__children_ip_object={None: {IPv4Address('203.0.113.128')}}, self._AddressSpace__description={IPv4Address('203.0.113.128'): 'an IPv4 test net address'}, self._AddressSpace__networks={}, self._AddressSpace__parent_supernet={IPv4Address('203.0.113.128'): None}, self._AddressSpace__strict=False)\n",
      "self.__children_ip_object[supernet].remove(ip_object)\n",
      "{'ip_object': \"IPv4Address('203.0.113.128')\"}\n",
      "self = AddressSpace(_AddressSpace__strict=False, _AddressSpace__description={IPv4Address('203.0.113.128'): 'an IPv4 test net address'}, _AddressSpace__networks={}, _AddressSpace__addresses={4: {IPv4Address('203.0.113.128')}}, _AddressSpace__parent_supernet={IPv4Address('203.0.113.128'): None}, _AddressSpace__children_ip_object={None: set()})\n",
      "def _test_grad_nd(n, ndim):\n",
      "    coords = [np.arange(n)] * ndim\n",
      "    xc = np.meshgrid(*coords, indexing=\"ij\")\n",
      "\n",
      "    # u = sum_i(xc[i]**2)\n",
      "    u = reduce(lambda x,y: x+y**2, xc, 0.0)\n",
      "    ucopy = np.copy(u)\n",
      "\n",
      "    # check the gradient values\n",
      "    slices = tuple([slice(1,-1,None)] * ndim)\n",
      "    for i in range(ndim):\n",
      "        assert grad(u, axis=i) == pytest.approx(2*xc[i][slices])\n",
      "\n",
      "    # check if u is unchanged\n",
      "    assert np.all(u == ucopy)\n",
      "\n",
      "_test_grad_nd(n=92, ndim=1)\n",
      "assert grad(u, axis=i) == pytest.approx(2*xc[i][slices])\n",
      "{'u': 'array([0.000e+00, 1.000e+00, 4.000e+00, 9.000e+00, 1.600e+01, 2.500e+01,       3.600e+01, 4.900e+01, 6.400e+01, 8.100e+01, 1.000e+02, 1.210e+02,       1.440e+02, 1.690e+02, 1.960e+02, 2.250e+02, 2.560e+02, 2.890e+02,       3.240e+02, 3.610e+02, 4.000e+02, 4.410e+02, 4.840e+02, 5.290e+02,       5.760e+02, 6.250e+02, 6.760e+02, 7.290e+02, 7.840e+02, 8.410e+02,       9.000e+02, 9.610e+02, 1.024e+03, 1.089e+03, 1.156e+03, 1.225e+03,       1.296e+03, 1.369e+03, 1.444e+03, 1.521e+03, 1.600e+03, 1.681e+03,       1.764e+03, 1.849e+03, 1.936e+03, 2.025e+03, 2.116e+03, 2.209e+03,       2.304e+03, 2.401e+03, 2.500e+03, 2.601e+03, 2.704e+03, 2.809e+03,       2.916e+03, 3.025e+03, 3.136e+03, 3.249e+03, 3.364e+03, 3.481e+03,       3.600e+03, 3.721e+03, 3.844e+03, 3.969e+03, 4.096e+03, 4.225e+03,       4.356e+03, 4.489e+03, 4.624e+03, 4.761e+03, 4.900e+03, 5.041e+03,       5.184e+03, 5.329e+03, 5.476e+03, 5.625e+03, 5.776e+03, 5.929e+03,       6.084e+03, 6.241e+03, 6.400e+03, 6.561e+03, 6.724e+03, 6.889e+03,       7.056e+03, 7.225e+03, 7.396e+03, 7.569e+03, 7.744e+03, 7.921e+03,       8.100e+03, 8.281e+03])'}\n",
      "@py_assert3 = None\n",
      "def _test_grad2_nd(n, ndim):\n",
      "    coords = [np.arange(n)] * ndim\n",
      "    xc = np.meshgrid(*coords, indexing=\"ij\")\n",
      "\n",
      "    # u = sum_i(xc[i]**2)\n",
      "    u = reduce(lambda x,y: x+y**2, xc, 0.0)\n",
      "    ucopy = np.copy(u)\n",
      "\n",
      "    # check the gradient values\n",
      "    gu = np.zeros(tuple([n-2]*ndim))\n",
      "    gu2 = gu + 2.0\n",
      "    for i in range(ndim):\n",
      "        for j in range(ndim):\n",
      "            if i == j:\n",
      "                assert grad2(u, axes=(i,j)) == pytest.approx(gu2)\n",
      "            else:\n",
      "                assert grad2(u, axes=(i,j)) == pytest.approx(gu)\n",
      "\n",
      "    # check if u is unchanged\n",
      "    assert np.all(u == ucopy)\n",
      "\n",
      "_test_grad2_nd(n=32, ndim=1)\n",
      "coords = [np.arange(n)] * ndim\n",
      "{'n': '32'}\n",
      "coords = [array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])]\n",
      "def det_hess(u):\n",
      "    \"\"\"\n",
      "    Get the determinant of the Hessian matrix of `u`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    * `u` : numpy.ndarray\n",
      "        The ndarray input with shape (n0+2, n1+2, ..., nd1+2).\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    * numpy.ndarray\n",
      "        The ndarray of the second grad of `u` with shape (n0, n1, ..., nd1).\n",
      "    \"\"\"\n",
      "    ndim = np.ndim(u)\n",
      "    inshape = np.asarray(u.shape)\n",
      "    outshape = list(inshape - 2)\n",
      "\n",
      "    # obtain the second gradient per each pairs of axes\n",
      "    hess_unarranged = np.zeros([ndim, ndim] + outshape)\n",
      "    for i in range(ndim):\n",
      "        for j in range(i,ndim):\n",
      "            grad2_val = grad2(u, (i,j))\n",
      "            hess_unarranged[i,j] = grad2_val\n",
      "            hess_unarranged[j,i] = grad2_val\n",
      "\n",
      "    # rearrange hessian to have shape: outshape + [ndim, ndim]\n",
      "    perm_idx = list(range(2,ndim+2)) + list(range(2))\n",
      "    hess = np.transpose(hess_unarranged, perm_idx)\n",
      "\n",
      "    # calculate and return the determinant\n",
      "    return np.linalg.det(hess)\n",
      "\n",
      "det_hess(u=array([  0.,   1.,   4.,   9.,  16.,  25.,  36.,  49.,  64.,  81., 100.,       121., 144., 169., 196., 225., 256., 289., 324., 361., 400., 441.,       484., 529., 576., 625., 676., 729., 784., 841., 900., 961.]))\n",
      "hess = np.transpose(hess_unarranged, perm_idx)\n",
      "{'hess_unarranged': 'array([[[2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]]])', 'perm_idx': '[2, 0, 1]'}\n",
      "hess = array([[[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]],       [[2.]]])\n",
      "def forward(source, phi):\n",
      "    \"\"\"\n",
      "    Obtain the target density distribution given the source distribution and\n",
      "    the mapping potential, phi.\n",
      "    The mapping from source coordinate, $x$, to the target coordinate, $y$, is\n",
      "    given by:\n",
      "\n",
      "    $$\n",
      "    y = x + \\nabla phi(x).\n",
      "    $$\n",
      "\n",
      "    The coordinate in i-th dimension is given by `np.arange(source.shape[i])`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    * `source` : numpy.ndarray\n",
      "        The source density distribution in n-dimensional array.\n",
      "    * `phi` : numpy.ndarray\n",
      "        The mapping potential given above. It must have the same shape as\n",
      "        `source`.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    * numpy.ndarray\n",
      "        The target density distribution in n-dimensional array.\n",
      "    \"\"\"\n",
      "    # convert to np.ndarray\n",
      "    source = np.asarray(source)\n",
      "    phi = np.asarray(phi)\n",
      "    # check the shapes of inputs\n",
      "    if source.shape != phi.shape:\n",
      "        raise ValueError(\"The source and phi must have the same shape.\")\n",
      "\n",
      "    # calculate the total potential so that $y = \\nabla u(x)$\n",
      "    u0, u, phi_pad = _get_full_potential(phi)\n",
      "    ndim = np.ndim(phi)\n",
      "\n",
      "    # calculate the determinant of the hessian\n",
      "    det_hess_s = det_hess(u)\n",
      "\n",
      "    # get the displacement in (n x D) format\n",
      "    x = np.array([grad(u0, axis=i) for i in range(ndim)]).reshape((ndim,-1)).T\n",
      "    y = np.array([grad(u , axis=i) for i in range(ndim)]).reshape((ndim,-1)).T\n",
      "\n",
      "    # interpolate the values\n",
      "    interp = lambda s: griddata(y, s.flatten(), x, \"linear\").reshape(s.shape)\n",
      "    target_s = source / det_hess_s\n",
      "    target = interp(target_s)\n",
      "\n",
      "    # fill nan values with zeros\n",
      "    target[np.isnan(target)] = 0.0\n",
      "    return target\n",
      "\n",
      "forward(source=array([3.72665317e-06, 6.14389891e-06, 1.00262383e-05, 1.61957424e-05,       2.58959932e-05, 4.09857759e-05, 6.42099934e-05, 9.95728564e-05,       1.52843925e-04, 2.32233182e-04, 3.49276399e-04, 5.19975743e-04,       7.66241736e-04, 1.11767979e-03, 1.61375600e-03, 2.30636063e-03,       3.26276232e-03, 4.56890930e-03, 6.33298575e-03, 8.68907130e-03,       1.18006814e-02, 1.58638899e-02, 2.11096565e-02, 2.78049116e-02,       3.62518979e-02, 4.67852390e-02, 5.97662260e-02, 7.55738747e-02,       9.45924385e-02, 1.17195255e-01, 1.43725065e-01, 1.74471250e-01,       2.09644807e-01, 2.49352209e-01, 2.93569685e-01, 3.42119690e-01,       3.94651546e-01, 4.50628259e-01, 5.09321387e-01, 5.69815527e-01,       6.31023482e-01, 6.91712523e-01, 7.50541364e-01, 8.06106646e-01,       8.56996891e-01, 9.01851159e-01, 9.39419053e-01, 9.68618450e-01,       9.88587205e-01, 9.98725433e-01, 9.98725433e-01, 9.88587205e-01,       9.68618450e-01, 9.39419053e-01, 9.01851159e-01, 8.56996891e-01,       8.06106646e-01, 7.50541364e-01, 6.91712523e-01, 6.31023482e-01,       5.69815527e-01, 5.09321387e-01, 4.50628259e-01, 3.94651546e-01,       3.42119690e-01, 2.93569685e-01, 2.49352209e-01, 2.09644807e-01,       1.74471250e-01, 1.43725065e-01, 1.17195255e-01, 9.45924385e-02,       7.55738747e-02, 5.97662260e-02, 4.67852390e-02, 3.62518979e-02,       2.78049116e-02, 2.11096565e-02, 1.58638899e-02, 1.18006814e-02,       8.68907130e-03, 6.33298575e-03, 4.56890930e-03, 3.26276232e-03,       2.30636063e-03, 1.61375600e-03, 1.11767979e-03, 7.66241736e-04,       5.19975743e-04, 3.49276399e-04, 2.32233182e-04, 1.52843925e-04,       9.95728564e-05, 6.42099934e-05, 4.09857759e-05, 2.58959932e-05,       1.61957424e-05, 1.00262383e-05, 6.14389891e-06, 3.72665317e-06]), phi=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n",
      "x = np.array([grad(u0, axis=i) for i in range(ndim)]).reshape((ndim,-1)).T\n",
      "{'ndim': '1'}\n",
      "x = array([[  1.],       [  2.],       [  3.],       [  4.],       [  5.],       [  6.],       [  7.],       [  8.],       [  9.],       [ 10.],       [ 11.],       [ 12.],       [ 13.],       [ 14.],       [ 15.],       [ 16.],       [ 17.],       [ 18.],       [ 19.],       [ 20.],       [ 21.],       [ 22.],       [ 23.],       [ 24.],       [ 25.],       [ 26.],       [ 27.],       [ 28.],       [ 29.],       [ 30.],       [ 31.],       [ 32.],       [ 33.],       [ 34.],       [ 35.],       [ 36.],       [ 37.],       [ 38.],       [ 39.],       [ 40.],       [ 41.],       [ 42.],       [ 43.],       [ 44.],       [ 45.],       [ 46.],       [ 47.],       [ 48.],       [ 49.],       [ 50.],       [ 51.],       [ 52.],       [ 53.],       [ 54.],       [ 55.],       [ 56.],       [ 57.],       [ 58.],       [ 59.],       [ 60.],       [ 61.],       [ 62.],       [ 63.],       [ 64.],       [ 65.],       [ 66.],       [ 67.],       [ 68.],       [ 69.],       [ 70.],       [ 71.],       [ 72.],       [ 73.],       [ 74.],       [ 75.],       [ 76.],       [ 77.],       [ 78.],       [ 79.],       [ 80.],       [ 81.],       [ 82.],       [ 83.],       [ 84.],       [ 85.],       [ 86.],       [ 87.],       [ 88.],       [ 89.],       [ 90.],       [ 91.],       [ 92.],       [ 93.],       [ 94.],       [ 95.],       [ 96.],       [ 97.],       [ 98.],       [ 99.],       [100.]])\n",
      "def _get_default_expanded_coordinate(shape, ndim):\n",
      "    x_coords = []\n",
      "    for i in range(ndim):\n",
      "        idx = [None] * ndim\n",
      "        idx[i] = slice(None, None, None)\n",
      "        x_coords.append(np.arange(shape[i])[tuple(idx)])\n",
      "    return x_coords\n",
      "\n",
      "_get_default_expanded_coordinate(shape=array([102]), ndim=1)\n",
      "x_coords.append(np.arange(shape[i])[tuple(idx)])\n",
      "{'idx': '[slice(None, None, None)]'}\n",
      "x_coords = [array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101])]\n",
      "def _pad_conserve_grads(phi):\n",
      "    # pad by conserving the edge gradients\n",
      "    ndim = np.ndim(phi)\n",
      "    pw = [1, 1]\n",
      "    pp = np.pad(phi, [tuple(pw)]*ndim, mode=\"constant\")\n",
      "    for dim in range(ndim):\n",
      "        # get the indices first\n",
      "        idx_pad0_l = _get_idx(ndim, dim, slice(pw[0], pw[0]+1, None))\n",
      "        idx_pad0_r = _get_idx(ndim, dim, slice(pw[0]+1, pw[0]+2, None))\n",
      "        idx_pad0 = _get_idx(ndim, dim, slice(pw[0], pw[0]+1, None))\n",
      "        idx_pad0_fill = _get_idx(ndim, dim, slice(None, pw[0], None))\n",
      "        idx_pad1_l = _get_idx(ndim, dim, slice(-pw[1]-2, -pw[1]-1, None))\n",
      "        idx_pad1_r = _get_idx(ndim, dim, slice(-pw[1]-1, -pw[1], None))\n",
      "        idx_pad1 = _get_idx(ndim, dim, slice(-pw[1]-1, -pw[1], None))\n",
      "        idx_pad1_fill = _get_idx(ndim, dim, slice(-pw[1], None, None))\n",
      "\n",
      "        # now get the padded values\n",
      "        grad0 = pp[idx_pad0_r] - pp[idx_pad0_l] # (n0, ..., ndim=1, ..., nd1)\n",
      "        grad1 = pp[idx_pad1_r] - pp[idx_pad1_l]\n",
      "        pad_arange0 = np.arange(-pw[0],0) # (1, ..., ndim=pw[i], ..., 1)\n",
      "        pad_arange1 = np.arange(1,pw[0]+1)\n",
      "        pad0 = pad_arange0 * grad0 + pp[idx_pad0] # (n0,...,ndim=pw[i],...,nd1)\n",
      "        pad1 = pad_arange1 * grad1 + pp[idx_pad1]\n",
      "        pp[idx_pad0_fill] = pad0\n",
      "        pp[idx_pad1_fill] = pad1\n",
      "\n",
      "    return pp\n",
      "\n",
      "_pad_conserve_grads(phi=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n",
      "idx_pad0_r = _get_idx(ndim, dim, slice(pw[0]+1, pw[0]+2, None))\n",
      "{'ndim': '1', 'dim': '0'}\n",
      "idx_pad0_r = (slice(2, 3, None),)\n",
      "def _test_forward_nd_slanted_pot(n, ndim, abs=None):\n",
      "    x = np.arange(n) - n / 2.0\n",
      "    xs = np.meshgrid(*([x]*ndim), indexing=\"ij\")\n",
      "    xs_sq = reduce(lambda x,y:x+y*y, xs, 0.0)\n",
      "\n",
      "    # phi is slanted on the first dimension, so the target will be shifted\n",
      "    # source on the first dimension by A\n",
      "    A = n / 6.0\n",
      "    sigma = (n/30.)\n",
      "    source = np.exp(-xs_sq / (2*sigma**2))\n",
      "    source_copy = np.copy(source)\n",
      "    phi = xs[0] * A\n",
      "    target = sb.forward(source, phi)\n",
      "\n",
      "    xs2 = np.copy(xs)\n",
      "    xs2[0] -= A\n",
      "    xs2_sq = reduce(lambda x,y:x+y*y, xs2, 0.0)\n",
      "    target_calc = np.exp(-xs2_sq / (2*sigma**2))\n",
      "\n",
      "    # accurate within 2.5*(1/n)*100%\n",
      "    abs = 2.5/n if abs is None else abs\n",
      "    assert target == pytest.approx(target_calc, abs=abs)\n",
      "\n",
      "    # check the dimension of target\n",
      "    assert np.ndim(target) == ndim\n",
      "\n",
      "    # make sure the source is not changed\n",
      "    assert np.all(source == source_copy)\n",
      "\n",
      "_test_forward_nd_slanted_pot(n=100, ndim=1, abs=None)\n",
      "xs2_sq = reduce(lambda x,y:x+y*y, xs2, 0.0)\n",
      "{'xs2': 'array([[-66.66666667, -65.66666667, -64.66666667, -63.66666667,        -62.66666667, -61.66666667, -60.66666667, -59.66666667,        -58.66666667, -57.66666667, -56.66666667, -55.66666667,        -54.66666667, -53.66666667, -52.66666667, -51.66666667,        -50.66666667, -49.66666667, -48.66666667, -47.66666667,        -46.66666667, -45.66666667, -44.66666667, -43.66666667,        -42.66666667, -41.66666667, -40.66666667, -39.66666667,        -38.66666667, -37.66666667, -36.66666667, -35.66666667,        -34.66666667, -33.66666667, -32.66666667, -31.66666667,        -30.66666667, -29.66666667, -28.66666667, -27.66666667,        -26.66666667, -25.66666667, -24.66666667, -23.66666667,        -22.66666667, -21.66666667, -20.66666667, -19.66666667,        -18.66666667, -17.66666667, -16.66666667, -15.66666667,        -14.66666667, -13.66666667, -12.66666667, -11.66666667,        -10.66666667,  -9.66666667,  -8.66666667,  -7.66666667,         -6.66666667,  -5.66666667,  -4.66666667,  -3.66666667,         -2.66666667,  -1.66666667,  -0.66666667,   0.33333333,          1.33333333,   2.33333333,   3.33333333,   4.33333333,          5.33333333,   6.33333333,   7.33333333,   8.33333333,          9.33333333,  10.33333333,  11.33333333,  12.33333333,         13.33333333,  14.33333333,  15.33333333,  16.33333333,         17.33333333,  18.33333333,  19.33333333,  20.33333333,         21.33333333,  22.33333333,  23.33333333,  24.33333333,         25.33333333,  26.33333333,  27.33333333,  28.33333333,         29.33333333,  30.33333333,  31.33333333,  32.33333333]])'}\n",
      "xs2_sq = array([4.44444444e+03, 4.31211111e+03, 4.18177778e+03, 4.05344444e+03,       3.92711111e+03, 3.80277778e+03, 3.68044444e+03, 3.56011111e+03,       3.44177778e+03, 3.32544444e+03, 3.21111111e+03, 3.09877778e+03,       2.98844444e+03, 2.88011111e+03, 2.77377778e+03, 2.66944444e+03,       2.56711111e+03, 2.46677778e+03, 2.36844444e+03, 2.27211111e+03,       2.17777778e+03, 2.08544444e+03, 1.99511111e+03, 1.90677778e+03,       1.82044444e+03, 1.73611111e+03, 1.65377778e+03, 1.57344444e+03,       1.49511111e+03, 1.41877778e+03, 1.34444444e+03, 1.27211111e+03,       1.20177778e+03, 1.13344444e+03, 1.06711111e+03, 1.00277778e+03,       9.40444444e+02, 8.80111111e+02, 8.21777778e+02, 7.65444444e+02,       7.11111111e+02, 6.58777778e+02, 6.08444444e+02, 5.60111111e+02,       5.13777778e+02, 4.69444444e+02, 4.27111111e+02, 3.86777778e+02,       3.48444444e+02, 3.12111111e+02, 2.77777778e+02, 2.45444444e+02,       2.15111111e+02, 1.86777778e+02, 1.60444444e+02, 1.36111111e+02,       1.13777778e+02, 9.34444444e+01, 7.51111111e+01, 5.87777778e+01,       4.44444444e+01, 3.21111111e+01, 2.17777778e+01, 1.34444444e+01,       7.11111111e+00, 2.77777778e+00, 4.44444444e-01, 1.11111111e-01,       1.77777778e+00, 5.44444444e+00, 1.11111111e+01, 1.87777778e+01,       2.84444444e+01, 4.01111111e+01, 5.37777778e+01, 6.94444444e+01,       8.71111111e+01, 1.06777778e+02, 1.28444444e+02, 1.52111111e+02,       1.77777778e+02, 2.05444444e+02, 2.35111111e+02, 2.66777778e+02,       3.00444444e+02, 3.36111111e+02, 3.73777778e+02, 4.13444444e+02,       4.55111111e+02, 4.98777778e+02, 5.44444444e+02, 5.92111111e+02,       6.41777778e+02, 6.93444444e+02, 7.47111111e+02, 8.02777778e+02,       8.60444444e+02, 9.20111111e+02, 9.81777778e+02, 1.04544444e+03])\n",
      "def _test_forward_nd_quad_pot(n, ndim, abs=None):\n",
      "    x = np.arange(n) - n / 2.0\n",
      "    xs = np.meshgrid(*([x]*ndim), indexing=\"ij\")\n",
      "    xs_sq = reduce(lambda x,y:x+y*y, xs, 0.0)\n",
      "\n",
      "    # phi is quadratic on all dimension, so the target will be scaled\n",
      "    # source on all dimension by (1+B)\n",
      "    B = 1.0\n",
      "    scale = 1 + B\n",
      "    sigma = (n/30.)\n",
      "    source = np.exp(-xs_sq / (2*sigma**2))\n",
      "    source_copy = np.copy(source)\n",
      "    phi = 0.5*B*xs_sq\n",
      "    target = sb.forward(source, phi)\n",
      "\n",
      "    target_calc = (scale**-ndim)*np.exp(-xs_sq / (2*(sigma*scale)**2))\n",
      "\n",
      "    # accurate within 2.5*(1/n)*100%\n",
      "    abs = 2.5/n if abs is None else abs\n",
      "    assert target == pytest.approx(target_calc, abs=abs)\n",
      "\n",
      "    # check the dimension of target\n",
      "    assert np.ndim(target) == ndim\n",
      "\n",
      "    # make sure the source is not changed\n",
      "    assert np.all(source == source_copy)\n",
      "\n",
      "_test_forward_nd_quad_pot(n=100, ndim=1, abs=None)\n",
      "assert target == pytest.approx(target_calc, abs=abs)\n",
      "{'target_calc': 'array([3.05096834e-13, 9.29251306e-13, 2.76730504e-12, 8.05766599e-12,       2.29398124e-11, 6.38555777e-11, 1.73794564e-10, 4.62489538e-10,       1.20336122e-09, 3.06138876e-09, 7.61498987e-09, 1.85203228e-08,       4.40408960e-08, 1.02398151e-07, 2.32785786e-07, 5.17427106e-07,       1.12452798e-06, 2.38956987e-06, 4.96475215e-06, 1.00856475e-05,       2.00326487e-05, 3.89046343e-05, 7.38741801e-05, 1.37155234e-04,       2.48977711e-04, 4.41913153e-04, 7.66905340e-04, 1.30129263e-03,       2.15892000e-03, 3.50208357e-03, 5.55449827e-03, 8.61373566e-03,       1.30607049e-02, 1.93628852e-02, 2.80673814e-02, 3.97797544e-02,       5.51252627e-02, 7.46908876e-02, 9.89493495e-02, 1.28170076e-01,       1.62326234e-01, 2.01010692e-01, 2.43376128e-01, 2.88114537e-01,       3.33488405e-01, 3.77419801e-01, 4.17635106e-01, 4.51853539e-01,       4.77998741e-01, 4.94406522e-01, 5.00000000e-01, 4.94406522e-01,       4.77998741e-01, 4.51853539e-01, 4.17635106e-01, 3.77419801e-01,       3.33488405e-01, 2.88114537e-01, 2.43376128e-01, 2.01010692e-01,       1.62326234e-01, 1.28170076e-01, 9.89493495e-02, 7.46908876e-02,       5.51252627e-02, 3.97797544e-02, 2.80673814e-02, 1.93628852e-02,       1.30607049e-02, 8.61373566e-03, 5.55449827e-03, 3.50208357e-03,       2.15892000e-03, 1.30129263e-03, 7.66905340e-04, 4.41913153e-04,       2.48977711e-04, 1.37155234e-04, 7.38741801e-05, 3.89046343e-05,       2.00326487e-05, 1.00856475e-05, 4.96475215e-06, 2.38956987e-06,       1.12452798e-06, 5.17427106e-07, 2.32785786e-07, 1.02398151e-07,       4.40408960e-08, 1.85203228e-08, 7.61498987e-09, 3.06138876e-09,       1.20336122e-09, 4.62489538e-10, 1.73794564e-10, 6.38555777e-11,       2.29398124e-11, 8.05766599e-12, 2.76730504e-12, 9.29251306e-13])'}\n",
      "@py_assert3 = None\n",
      "def test_audioclip_stereo_max_volume(nchannels, channel_muted):\n",
      "    def make_frame(t):\n",
      "        frame = []\n",
      "        # build channels (one of each pair muted)\n",
      "        for i in range(int(nchannels / 2)):\n",
      "            if channel_muted == \"left\":\n",
      "                # if muted channel is left, [0, sound, 0, sound...]\n",
      "                frame.append(np.sin(t * 0))\n",
      "                frame.append(np.sin(440 * 2 * np.pi * t))\n",
      "            else:\n",
      "                # if muted channel is right, [sound, 0, sound, 0...]\n",
      "                frame.append(np.sin(440 * 2 * np.pi * t))\n",
      "                frame.append(np.sin(t * 0))\n",
      "        return np.array(frame).T\n",
      "\n",
      "    clip = AudioClip(make_frame, fps=44100, duration=1)\n",
      "    max_volume = clip.max_volume(stereo=True)\n",
      "    # if `stereo == True`, `AudioClip.max_volume` returns a Numpy array`\n",
      "    assert isinstance(max_volume, np.ndarray)\n",
      "    assert len(max_volume) == nchannels\n",
      "\n",
      "    # check channels muted and with sound\n",
      "    for i, channel_max_volume in enumerate(max_volume):\n",
      "        if i % 2 == 0:\n",
      "            if channel_muted == \"left\":\n",
      "                assert channel_max_volume == 0\n",
      "            else:\n",
      "                assert channel_max_volume > 0\n",
      "        else:\n",
      "            if channel_muted == \"right\":\n",
      "                assert channel_max_volume == 0\n",
      "            else:\n",
      "                assert channel_max_volume > 0\n",
      "\n",
      "test_audioclip_stereo_max_volume(nchannels=2, channel_muted='left')\n",
      "assert len(max_volume) == nchannels\n",
      "{'max_volume': 'array([0.        , 0.99999975])'}\n",
      "@py_assert2 = None\n",
      "def test_clip_with_end(duration, start, end, expected_start, expected_duration):\n",
      "    clip = ColorClip(color=(255, 0, 0), size=(2, 2), duration=duration).with_fps(1)\n",
      "    if start is not None:\n",
      "        clip = clip.with_start(start)\n",
      "    else:\n",
      "        clip.start = None\n",
      "    clip = clip.with_end(end)\n",
      "\n",
      "    assert clip.start == expected_start\n",
      "    assert clip.duration == expected_duration\n",
      "\n",
      "test_clip_with_end(duration=3, start=1, end=2, expected_start=1, expected_duration=1)\n",
      "clip = clip.with_start(start)\n",
      "{'start': '1'}\n",
      "clip = {start=1, end=4, duration=3, memoize=False, memoized_t=None, memoized_frame=None, mask=None, audio=None, relative_pos=False, layer=0, is_mask=False, has_constant_size=True, size=(2, 2), img=array([[[255,   0,   0],        [255,   0,   0]],       [[255,   0,   0],        [255,   0,   0]]]), fps=1}\n",
      "def subprocess_call(cmd, logger=\"bar\"):\n",
      "    \"\"\"Executes the given subprocess command.\n",
      "\n",
      "    Set logger to None or a custom Proglog logger to avoid printings.\n",
      "    \"\"\"\n",
      "    logger = proglog.default_bar_logger(logger)\n",
      "    logger(message=\"MoviePy - Running:\\n>>> \" + \" \".join(cmd))\n",
      "\n",
      "    popen_params = cross_platform_popen_params(\n",
      "        {\"stdout\": sp.DEVNULL, \"stderr\": sp.PIPE, \"stdin\": sp.DEVNULL}\n",
      "    )\n",
      "\n",
      "    proc = sp.Popen(cmd, **popen_params)\n",
      "\n",
      "    out, err = proc.communicate()  # proc.wait()\n",
      "    proc.stderr.close()\n",
      "\n",
      "    if proc.returncode:\n",
      "        logger(message=\"MoviePy - Command returned an error\")\n",
      "        raise IOError(err.decode(\"utf8\"))\n",
      "    else:\n",
      "        logger(message=\"MoviePy - Command successful\")\n",
      "\n",
      "    del proc\n",
      "\n",
      "subprocess_call(cmd=['unset', '-background', 'transparent', '-fill', 'white', '-font', 'Liberation-Mono', '-pointsize', '25', '-size', '640x480', '-gravity', 'center', 'caption:@/tmp/tmp6pdbi5v3.txt', '-type', 'truecolormatte', 'PNG32:/tmp/tmpkd0efulh.png'], logger=None)\n",
      "logger = proglog.default_bar_logger(logger)\n",
      "{'logger': 'None'}\n",
      "logger = {state={'bars': OrderedDict()}, stored={}, logs=[], log_indent=0, ignored_bars=None, logged_bars='all', min_time_interval=0, ignore_bars_under=0}\n",
      "def ffmpeg_write_image(filename, image, logfile=False, pixel_format=None):\n",
      "    \"\"\"Writes an image (HxWx3 or HxWx4 numpy array) to a file, using ffmpeg.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "\n",
      "    filename : str\n",
      "        Path to the output file.\n",
      "\n",
      "    image : np.ndarray\n",
      "        Numpy array with the image data.\n",
      "\n",
      "    logfile : bool, optional\n",
      "        Writes the ffmpeg output inside a logging file (``True``) or not\n",
      "        (``False``).\n",
      "\n",
      "    pixel_format : str, optional\n",
      "        Pixel format for ffmpeg. If not defined, it will be discovered checking\n",
      "        if the image data contains an alpha channel (``\"rgba\"``) or not\n",
      "        (``\"rgb24\"``).\n",
      "    \"\"\"\n",
      "    if image.dtype != \"uint8\":\n",
      "        image = image.astype(\"uint8\")\n",
      "    if not pixel_format:\n",
      "        pixel_format = \"rgba\" if (image.shape[2] == 4) else \"rgb24\"\n",
      "\n",
      "    cmd = [\n",
      "        FFMPEG_BINARY,\n",
      "        \"-y\",\n",
      "        \"-s\",\n",
      "        \"%dx%d\" % (image.shape[:2][::-1]),\n",
      "        \"-f\",\n",
      "        \"rawvideo\",\n",
      "        \"-pix_fmt\",\n",
      "        pixel_format,\n",
      "        \"-i\",\n",
      "        \"-\",\n",
      "        filename,\n",
      "    ]\n",
      "\n",
      "    if logfile:\n",
      "        log_file = open(filename + \".log\", \"w+\")\n",
      "    else:\n",
      "        log_file = sp.PIPE\n",
      "\n",
      "    popen_params = cross_platform_popen_params(\n",
      "        {\"stdout\": sp.DEVNULL, \"stderr\": log_file, \"stdin\": sp.PIPE}\n",
      "    )\n",
      "\n",
      "    proc = sp.Popen(cmd, **popen_params)\n",
      "    out, err = proc.communicate(image.tobytes())\n",
      "\n",
      "    if proc.returncode:\n",
      "        error = (\n",
      "            f\"{err}\\n\\nMoviePy error: FFMPEG encountered the following error while \"\n",
      "            f\"writing file {filename} with command {cmd}:\\n\\n {err.decode()}\"\n",
      "        )\n",
      "\n",
      "        raise IOError(error)\n",
      "\n",
      "    del proc\n",
      "\n",
      "ffmpeg_write_image(filename='/tmp/moviepy_ffmpeg_write_image.png', image=array([[[  0., 255.,   0.],        [ 51., 204.,   0.],        [102., 153.,   0.],        [153., 102.,   0.],        [204.,  51.,   0.]]]), logfile=False, pixel_format=None)\n",
      "proc = sp.Popen(cmd, **popen_params)\n",
      "{'cmd': \"['/local/rcs/XXX/miniforge3/envs/Zulko+moviepy/lib/python3.9/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux64-v4.2.2', '-y', '-s', '5x1', '-f', 'rawvideo', '-pix_fmt', 'rgb24', '-i', '-', '/tmp/moviepy_ffmpeg_write_image.png']\"}\n",
      "proc = <Popen: returncode: None args: ['/local/rcs/XXX/miniforge3/envs/Zulko+mov...>\n",
      "def test_freeze(t, freeze_duration, total_duration, padding_end, output_frames):\n",
      "    input_frames = [\"R\", \"G\", \"B\"]\n",
      "    clip_duration = len(input_frames)\n",
      "\n",
      "    # create BitmapClip with predefined set of colors, during 1 second each one\n",
      "    clip = BitmapClip([list(color) for color in input_frames], fps=1).with_duration(\n",
      "        clip_duration\n",
      "    )\n",
      "\n",
      "    # build kwargs passed to `freeze`\n",
      "    possible_kwargs = {\n",
      "        \"t\": t,\n",
      "        \"freeze_duration\": freeze_duration,\n",
      "        \"total_duration\": total_duration,\n",
      "        \"padding_end\": padding_end,\n",
      "    }\n",
      "    kwargs = {\n",
      "        kw_name: kw_value\n",
      "        for kw_name, kw_value in possible_kwargs.items()\n",
      "        if kw_value is not None\n",
      "    }\n",
      "\n",
      "    # freeze clip\n",
      "    if hasattr(output_frames, \"__traceback__\"):\n",
      "        with pytest.raises(output_frames):\n",
      "            freeze(clip, **kwargs)\n",
      "        return\n",
      "    else:\n",
      "        freezed_clip = freeze(clip, **kwargs)\n",
      "\n",
      "    # assert new duration\n",
      "    expected_freeze_duration = (\n",
      "        freeze_duration\n",
      "        if freeze_duration is not None\n",
      "        else total_duration - clip_duration\n",
      "    )\n",
      "    assert freezed_clip.duration == clip_duration + expected_freeze_duration\n",
      "\n",
      "    # assert colors are the expected\n",
      "    for i, color in enumerate(freezed_clip.iter_frames()):\n",
      "        expected_color = list(BitmapClip.DEFAULT_COLOR_DICT[output_frames[i]])\n",
      "        assert list(color[0][0]) == expected_color\n",
      "\n",
      "test_freeze(t=None, freeze_duration=1, total_duration=None, padding_end=None, output_frames=['R', 'R', 'G', 'B'])\n",
      "freezed_clip = freeze(clip, **kwargs)\n",
      "{'clip': \"{color_dict={'R': (255, 0, 0), 'G': (0, 255, 0), 'B': (0, 0, 255), 'O': (0, 0, 0), 'W': (255, 255, 255), 'A': (89, 225, 62), 'C': (113, 157, 108), 'D': (215, 182, 143), 'E': (57, 26, 252), 'F': (225, 135, 33)}, total_frames=3, start=0, end=3, duration=3, memoize=False, memoized_t=None, memoized_frame=None, mask=None, audio=None, relative_pos=False, layer=0, size=(1, 1), is_mask=False, has_constant_size=True, fps=1}\"}\n",
      "freezed_clip = {start=0, end=4, duration=4, memoize=False, memoized_t=None, memoized_frame=None, mask=None, audio=None, relative_pos=False, layer=0, size=(1, 1), is_mask=False, has_constant_size=True, timings=array([0, 1, 4]), start_times=array([0, 1]), fps=1}\n",
      "def test_mask_and(image_from, duration, color, mask_color, expected_color):\n",
      "    \"\"\"Checks ``mask_and`` FX behaviour.\"\"\"\n",
      "    clip_size = tuple(random.randint(3, 10) for i in range(2))\n",
      "\n",
      "    if duration == \"random\":\n",
      "        duration = round(random.uniform(0, 0.5), 2)\n",
      "\n",
      "    # test ImageClip and np.ndarray types as mask argument\n",
      "    clip = ColorClip(color=color, size=clip_size).with_duration(duration)\n",
      "    mask_clip = ColorClip(color=mask_color, size=clip.size)\n",
      "    masked_clip = mask_and(\n",
      "        clip, mask_clip if image_from == \"ImageClip\" else mask_clip.get_frame(0)\n",
      "    )\n",
      "\n",
      "    assert masked_clip.duration == clip.duration\n",
      "    assert np.array_equal(masked_clip.get_frame(0)[0][0], np.array(expected_color))\n",
      "\n",
      "    # test VideoClip as mask argument\n",
      "    color_frame, mask_color_frame = (np.array([[color]]), np.array([[mask_color]]))\n",
      "    clip = VideoClip(lambda t: color_frame).with_duration(duration)\n",
      "    mask_clip = VideoClip(lambda t: mask_color_frame).with_duration(duration)\n",
      "    masked_clip = mask_and(clip, mask_clip)\n",
      "\n",
      "    assert np.array_equal(masked_clip.get_frame(0)[0][0], np.array(expected_color))\n",
      "\n",
      "test_mask_and(image_from='np.ndarray', duration=None, color=(0, 0, 0), mask_color=(255, 255, 255), expected_color=(0, 0, 0))\n",
      "mask_clip = VideoClip(lambda t: mask_color_frame).with_duration(duration)\n",
      "{'duration': 'None'}\n",
      "mask_clip = {start=0, end=None, duration=None, memoize=False, memoized_t=None, memoized_frame=None, mask=None, audio=None, relative_pos=False, layer=0, size=(1, 1), is_mask=False, has_constant_size=True}\n",
      "def _validate_htype_overwrites(htype: str, htype_overwrite: dict):\n",
      "    \"\"\"Raises errors if ``htype_overwrite`` has invalid keys or was missing required values.\"\"\"\n",
      "\n",
      "    defaults = HTYPE_CONFIGURATIONS[htype]\n",
      "\n",
      "    for key, value in htype_overwrite.items():\n",
      "        if key not in defaults:\n",
      "            raise TensorMetaInvalidHtypeOverwriteKey(htype, key, list(defaults.keys()))\n",
      "\n",
      "        if isinstance(value, str) and value == UNSPECIFIED:\n",
      "            if defaults[key] == REQUIRE_USER_SPECIFICATION:\n",
      "                raise TensorMetaMissingRequiredValue(htype, key)\n",
      "\n",
      "    sc = htype_overwrite[\"sample_compression\"]\n",
      "    cc = htype_overwrite[\"chunk_compression\"]\n",
      "    compr = sc if cc in (None, UNSPECIFIED) else cc\n",
      "    actual_htype = f\"link[{htype}]\" if htype_overwrite[\"is_link\"] else htype\n",
      "    if htype.startswith(\"image\") and sc == UNSPECIFIED and cc == UNSPECIFIED:\n",
      "        raise TensorMetaMissingRequiredValue(\n",
      "            actual_htype, [\"chunk_compression\", \"sample_compression\"]  # type: ignore\n",
      "        )\n",
      "    if htype in (\"audio\", \"video\", \"point_cloud\", \"mesh\", \"nifti\"):\n",
      "        if cc not in (UNSPECIFIED, None):\n",
      "            raise UnsupportedCompressionError(\"Chunk compression\", htype=htype)\n",
      "        elif sc == UNSPECIFIED:\n",
      "            raise TensorMetaMissingRequiredValue(\n",
      "                actual_htype, \"sample_compression\"  # type: ignore\n",
      "            )\n",
      "    supported_compressions = HTYPE_SUPPORTED_COMPRESSIONS.get(htype)\n",
      "    if (\n",
      "        compr\n",
      "        and compr != UNSPECIFIED\n",
      "        and supported_compressions\n",
      "        and compr not in supported_compressions\n",
      "    ):\n",
      "        raise UnsupportedCompressionError(compr, htype=htype)\n",
      "\n",
      "_validate_htype_overwrites(htype='generic', htype_overwrite={'sample_compression': 'unspecified', 'chunk_compression': 'unspecified', 'dtype': 'unspecified', 'hidden': False, 'tiling_threshold': None, 'max_chunk_size': 2000000, 'is_sequence': False, 'is_link': False, 'verify': True})\n",
      "supported_compressions = HTYPE_SUPPORTED_COMPRESSIONS.get(htype)\n",
      "{'htype': \"'generic'\"}\n",
      "supported_compressions = None\n",
      "def serialize_chunkids(version: str, arr: np.ndarray) -> memoryview:\n",
      "    \"\"\"Serializes chunk ID encoders into a single byte stream. This is how the encoders will be written to the storage provider.\n",
      "\n",
      "    Args:\n",
      "        version: (str) Version of deeplake library.\n",
      "        arr: (np.ndarray) Encoded chunk ids from a `ChunkIdEncoder` instance.\n",
      "\n",
      "    Returns:\n",
      "        Serialized chunk ids as memoryview.\n",
      "    \"\"\"\n",
      "    len_version = len(version)\n",
      "    write_dtype = version_compare(version, \"2.7.6\") >= 0\n",
      "    flatbuff = bytearray(1 + int(write_dtype) + len_version + arr.nbytes)\n",
      "\n",
      "    # Write version\n",
      "    len_version = len(version)\n",
      "    flatbuff[0] = len_version\n",
      "    flatbuff[1 : 1 + len_version] = version.encode(\"ascii\")\n",
      "    offset = 1 + len_version\n",
      "\n",
      "    # write encoder dtype\n",
      "    if write_dtype:\n",
      "        dtype = arr.dtype\n",
      "        num_bytes = int(dtype.itemsize)\n",
      "        flatbuff[offset] = num_bytes\n",
      "        offset += 1\n",
      "\n",
      "    # Write ids\n",
      "    flatbuff[offset : offset + arr.nbytes] = arr.tobytes()\n",
      "    offset += arr.nbytes\n",
      "    return memoryview(flatbuff)\n",
      "\n",
      "serialize_chunkids(version='3.8.18', arr=array([], shape=(0, 2), dtype=uint64))\n",
      "write_dtype = version_compare(version, \"2.7.6\") >= 0\n",
      "{'version': \"'3.8.18'\"}\n",
      "write_dtype = True\n",
      "def _draw_random_tile(self):\n",
      "        random_index = random.randrange(0, len(self.tile_bag))\n",
      "        selected_tile = self.tile_bag.pop(random_index)\n",
      "\n",
      "        return selected_tile\n",
      "\n",
      "_draw_random_tile(self=REPR FAILED, self.tile_bag=[*, *, A, A, A, A, A, A, A, A, A, B, B, C, C, D, D, D, D, E, E, E, E, E, E, E, E, E, E, E, E, F, F, G, G, G, H, H, I, I, I, I, I, I, I, I, I, J, K, L, L, L, L, M, M, N, N, N, N, N, N, O, O, O, O, O, O, O, O, P, P, Q, R, R, R, R, R, R, S, S, S, S, T, T, T, T, T, T, U, U, U, U, V, V, W, W, X, Y, Y, Z])\n",
      "selected_tile = self.tile_bag.pop(random_index)\n",
      "{'random_index': '89'}\n",
      "selected_tile = U\n",
      "def move_is_sublist(letter_list_1, letter_list_2):\n",
      "    letter_counter_1 = collections.Counter(letter_list_1)\n",
      "    letter_counter_2 = collections.Counter(letter_list_2)\n",
      "    for letter, cardinality in letter_counter_1.items():\n",
      "        if cardinality > letter_counter_2[letter]:\n",
      "            # print('Not enough {} tiles in rack.'.format(letter))\n",
      "            return False\n",
      "\n",
      "    return True\n",
      "\n",
      "move_is_sublist(letter_list_1=[1, 2, 3], letter_list_2=[1, 2, 3, 4])\n",
      "letter_counter_2 = collections.Counter(letter_list_2)\n",
      "{'letter_list_2': '[1, 2, 3, 4]'}\n",
      "letter_counter_2 = Counter({1: 1, 2: 1, 3: 1, 4: 1})\n",
      "def place_word(self, word, start_location, is_vertical_move):\n",
      "        letter_location_set = get_word_letter_location_set(word,\n",
      "                                                           start_location,\n",
      "                                                           is_vertical_move)\n",
      "\n",
      "        return self.next_player_move(letter_location_set)\n",
      "\n",
      "place_word(self=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 ______________9 _______________10_______________11_______________12_______________13_______________14_______________15_______________[[Q, E, O, S, G, H, E, B, A, K, E, R], [R, Z, L, A, U, L, A], [W, C, E, T, N, E, E], [T, R, O, B, T, N, N]]Moves played: 0Player 1's move72 tiles remain in bagPlayer 1: 0Player 2: 0Player 3: 0Player 4: 0, word='BAKER', start_location=('h', 8), is_vertical_move=False, self.board=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 ______________9 _______________10_______________11_______________12_______________13_______________14_______________15_______________, self.move_number=0, self.player_rack_list=[[Q, E, O, S, G, H, E, B, A, K, E, R], [R, Z, L, A, U, L, A], [W, C, E, T, N, E, E], [T, R, O, B, T, N, N]], self.player_score_list_list=[[], [], [], []], self.tile_bag=[*, *, A, A, A, A, A, A, A, B, C, D, D, D, D, E, E, E, E, E, E, E, F, F, G, G, H, I, I, I, I, I, I, I, I, I, J, K, L, L, M, M, N, N, N, O, O, O, O, O, O, P, P, R, R, R, R, S, S, S, T, T, T, U, U, U, V, V, W, X, Y, Y])\n",
      "letter_location_set = get_word_letter_location_set(word,\n",
      "{'word': \"'BAKER'\", 'start_location': \"('h', 8)\", 'is_vertical_move': 'False'}\n",
      "letter_location_set = {('K', ('j', 8)), ('B', ('h', 8)), ('A', ('i', 8)), ('E', ('k', 8)), ('R', ('l', 8))}\n",
      "def get_word_letter_location_set(word, start_location, is_vertical_move):\n",
      "    letter_location_set = set()\n",
      "    next_location_func = get_next_location_function(\n",
      "        use_positive_seek=True,\n",
      "        use_vertical_words=is_vertical_move\n",
      "    )\n",
      "\n",
      "    current_location = start_location\n",
      "    word_iterator = iter(word)\n",
      "    for character in word_iterator:\n",
      "        if character == '(':  # characters in parenthesis are existing tiles\n",
      "            character = next(word_iterator, None)\n",
      "            while character != ')':\n",
      "                current_location = next_location_func(current_location)\n",
      "                character = next(word_iterator, None)\n",
      "\n",
      "            character = next(word_iterator, None)\n",
      "\n",
      "        if character:\n",
      "            letter_location_set.add((character, current_location))\n",
      "            current_location = next_location_func(current_location)\n",
      "\n",
      "    return letter_location_set\n",
      "\n",
      "get_word_letter_location_set(word='BAKER', start_location=('h', 8), is_vertical_move=False)\n",
      "word_iterator = iter(word)\n",
      "{'word': \"'BAKER'\"}\n",
      "word_iterator = REPR FAILED\n",
      "def next_player_move(self, letter_location_set):\n",
      "        player_to_move_id, player_rack = get_current_player_data(\n",
      "            self.move_number,\n",
      "            self.player_rack_list\n",
      "        )\n",
      "\n",
      "        is_legal_move = move_is_legal(self.board,\n",
      "                                      self.move_number,\n",
      "                                      letter_location_set,\n",
      "                                      player_rack)\n",
      "\n",
      "        if is_legal_move:\n",
      "            if move_successfully_challenged():\n",
      "                letter_location_set = set()\n",
      "\n",
      "            for move_letter, board_location in letter_location_set:\n",
      "                tile_index = get_rack_tile_index(player_rack, move_letter)\n",
      "                tile_obj = player_rack.pop(tile_index)\n",
      "                self.board[board_location] = tile_obj\n",
      "\n",
      "            move_score = score_move(letter_location_set, self.board)\n",
      "            self.player_score_list_list[player_to_move_id].append(move_score)\n",
      "            self._refill_player_rack(player_rack)\n",
      "            self._cancel_bonus_squares(letter_location_set)\n",
      "\n",
      "            if len(player_rack) == 0 and len(self.tile_bag) == 0:  # Final move\n",
      "                last_move_score_list = score_end_of_game(self.player_rack_list,\n",
      "                                                         player_to_move_id)\n",
      "\n",
      "                for i, last_move_score in enumerate(last_move_score_list):\n",
      "                    self.player_score_list_list[i].append(last_move_score)\n",
      "\n",
      "                conclude_game(self.player_score_list_list)\n",
      "\n",
      "            self.move_number += 1\n",
      "            return True\n",
      "        else:\n",
      "            return False\n",
      "\n",
      "next_player_move(self=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 ______________9 _______________10_______________11_______________12_______________13_______________14_______________15_______________[[Q, E, O, S, G, H, E, B, A, K, E, R], [R, Z, L, A, U, L, A], [W, C, E, T, N, E, E], [T, R, O, B, T, N, N]]Moves played: 0Player 1's move72 tiles remain in bagPlayer 1: 0Player 2: 0Player 3: 0Player 4: 0, letter_location_set={('K', ('j', 8)), ('B', ('h', 8)), ('A', ('i', 8)), ('E', ('k', 8)), ('R', ('l', 8))}, self.board=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 ______________9 _______________10_______________11_______________12_______________13_______________14_______________15_______________, self.move_number=0, self.player_rack_list=[[Q, E, O, S, G, H, E, B, A, K, E, R], [R, Z, L, A, U, L, A], [W, C, E, T, N, E, E], [T, R, O, B, T, N, N]], self.player_score_list_list=[[], [], [], []], self.tile_bag=[*, *, A, A, A, A, A, A, A, B, C, D, D, D, D, E, E, E, E, E, E, E, F, F, G, G, H, I, I, I, I, I, I, I, I, I, J, K, L, L, M, M, N, N, N, O, O, O, O, O, O, P, P, R, R, R, R, S, S, S, T, T, T, U, U, U, V, V, W, X, Y, Y])\n",
      "move_score = score_move(letter_location_set, self.board)\n",
      "{'letter_location_set': \"{('K', ('j', 8)), ('B', ('h', 8)), ('A', ('i', 8)), ('E', ('k', 8)), ('R', ('l', 8))}\"}\n",
      "move_score = 24\n",
      "def all_move_tiles_connected(board, location_set):\n",
      "    column_list = [column for column, _ in location_set]\n",
      "    row_list = [row for _, row in location_set]\n",
      "    move_is_vertical = (len(set(column_list)) == 1)\n",
      "\n",
      "    if move_is_vertical:\n",
      "        this_column = column_list[0]\n",
      "        for this_row in range(min(row_list), max(row_list) + 1):\n",
      "            this_tile = board[(this_column, this_row)]\n",
      "            if not (this_tile or (this_column, this_row) in location_set):\n",
      "                # print('Not all tiles in vertical move are connected: '\n",
      "                #       'location {} is empty'.format((this_column, this_row)))\n",
      "\n",
      "                return False\n",
      "    else:\n",
      "        column_range = range(\n",
      "            config.LETTER_CODE_DICT[min(column_list)],\n",
      "            config.LETTER_CODE_DICT[max(column_list)] + 1\n",
      "        )\n",
      "\n",
      "        this_row = row_list[0]\n",
      "        for this_column_num in column_range:\n",
      "            this_column = chr(this_column_num)\n",
      "            this_tile = board[(this_column, this_row)]\n",
      "            if not (this_tile or (this_column, this_row) in location_set):\n",
      "                # print('Not all tiles in horizontal move are connected: '\n",
      "                #       'location {} is empty'.format((this_column, this_row)))\n",
      "\n",
      "                return False\n",
      "\n",
      "    return True\n",
      "\n",
      "all_move_tiles_connected(board=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 ______________9 _______________10_______________11_______________12_______________13_______________14_______________15_______________, location_set={('j', 8), ('l', 8), ('h', 8), ('i', 8), ('k', 8)})\n",
      "move_is_vertical = (len(set(column_list)) == 1)\n",
      "{'column_list': \"['j', 'l', 'h', 'i', 'k']\"}\n",
      "move_is_vertical = False\n",
      "def location_touches_tile(board, location):\n",
      "    adjacent_location_set = get_adjacent_location_set(location)\n",
      "    for adjacent_location in adjacent_location_set:\n",
      "        if board[adjacent_location]:\n",
      "            return True\n",
      "\n",
      "    return False\n",
      "\n",
      "location_touches_tile(board=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 _______SCRAB___9 _______________10_______________11_______________12_______________13_______________14_______________15_______________, location=('i', 13))\n",
      "adjacent_location_set = get_adjacent_location_set(location)\n",
      "{'location': \"('i', 13)\"}\n",
      "adjacent_location_set = {('j', 13), ('h', 13), ('i', 14), ('i', 12)}\n",
      "def rolling_mean_by_h(x, h, w, name):\n",
      "    \"\"\"Compute a rolling mean of x, after first aggregating by h.\n",
      "\n",
      "    Right-aligned. Computes a single mean for each unique value of h. Each\n",
      "    mean is over at least w samples.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    x: Array.\n",
      "    h: Array of horizon for each value in x.\n",
      "    w: Integer window size (number of elements).\n",
      "    name: Name for metric in result dataframe\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    Dataframe with columns horizon and name, the rolling mean of x.\n",
      "    \"\"\"\n",
      "    # Aggregate over h\n",
      "    df = pd.DataFrame({'x': x, 'h': h})\n",
      "    df2 = (\n",
      "        df.groupby('h').agg(['sum', 'count']).reset_index().sort_values('h')\n",
      "    )\n",
      "    xs = df2['x']['sum'].values\n",
      "    ns = df2['x']['count'].values\n",
      "    hs = df2.h.values\n",
      "\n",
      "    trailing_i = len(df2) - 1\n",
      "    x_sum = 0\n",
      "    n_sum = 0\n",
      "    # We don't know output size but it is bounded by len(df2)\n",
      "    res_x = np.empty(len(df2))\n",
      "\n",
      "    # Start from the right and work backwards\n",
      "    for i in range(len(df2) - 1, -1, -1):\n",
      "        x_sum += xs[i]\n",
      "        n_sum += ns[i]\n",
      "        while n_sum >= w:\n",
      "            # Include points from the previous horizon. All of them if still\n",
      "            # less than w, otherwise weight the mean by the difference\n",
      "            excess_n = n_sum - w\n",
      "            excess_x = excess_n * xs[i] / ns[i]\n",
      "            res_x[trailing_i] = (x_sum - excess_x)/ w\n",
      "            x_sum -= xs[trailing_i]\n",
      "            n_sum -= ns[trailing_i]\n",
      "            trailing_i -= 1\n",
      "\n",
      "    res_h = hs[(trailing_i + 1):]\n",
      "    res_x = res_x[(trailing_i + 1):]\n",
      "\n",
      "    return pd.DataFrame({'horizon': res_h, name: res_x})\n",
      "\n",
      "rolling_mean_by_h(x=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), h=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), w=1, name='x')\n",
      "trailing_i = len(df2) - 1\n",
      "{'df2': '   h   x           sum count0  0   0     11  1   1     12  2   2     13  3   3     14  4   4     15  5   5     16  6   6     17  7   7     18  8   8     19  9   9     1'}\n",
      "trailing_i = 9\n",
      "def rolling_median_by_h(x, h, w, name):\n",
      "    \"\"\"Compute a rolling median of x, after first aggregating by h.\n",
      "\n",
      "    Right-aligned. Computes a single median for each unique value of h. Each\n",
      "    median is over at least w samples.\n",
      "\n",
      "    For each h where there are fewer than w samples, we take samples from the previous h,\n",
      "    moving backwards. (In other words, we ~ assume that the x's are shuffled within each h.)\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    x: Array.\n",
      "    h: Array of horizon for each value in x.\n",
      "    w: Integer window size (number of elements).\n",
      "    name: Name for metric in result dataframe\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    Dataframe with columns horizon and name, the rolling median of x.\n",
      "    \"\"\"\n",
      "    # Aggregate over h\n",
      "    df = pd.DataFrame({'x': x, 'h': h})\n",
      "    grouped = df.groupby('h')\n",
      "    df2 = grouped.size().reset_index().sort_values('h')\n",
      "    hs = df2['h']\n",
      "\n",
      "    res_h = []\n",
      "    res_x = []\n",
      "    # Start from the right and work backwards\n",
      "    i = len(hs) - 1\n",
      "    while i >= 0:\n",
      "        h_i = hs[i]\n",
      "        xs = grouped.get_group(h_i).x.tolist()\n",
      "\n",
      "        # wrap in array so this works if h is pandas Series with custom index or numpy array\n",
      "        next_idx_to_add = np.array(h == h_i).argmax() - 1\n",
      "        while (len(xs) < w) and (next_idx_to_add >= 0):\n",
      "            # Include points from the previous horizon. All of them if still\n",
      "            # less than w, otherwise just enough to get to w.\n",
      "            xs.append(x[next_idx_to_add])\n",
      "            next_idx_to_add -= 1\n",
      "        if len(xs) < w:\n",
      "            # Ran out of points before getting enough.\n",
      "            break\n",
      "        res_h.append(hs[i])\n",
      "        res_x.append(np.median(xs))\n",
      "        i -= 1\n",
      "    res_h.reverse()\n",
      "    res_x.reverse()\n",
      "    return pd.DataFrame({'horizon': res_h, name: res_x})\n",
      "\n",
      "rolling_median_by_h(x=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), h=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), w=1, name='x')\n",
      "i = len(hs) - 1\n",
      "{'hs': '0    01    12    23    34    45    56    67    78    89    9Name: h, dtype: int64'}\n",
      "i = 9\n",
      "def create_request_parameters(parent, request_model, params=None, index=None):\n",
      "    \"\"\"\n",
      "    Handle request parameters that can be filled in from identifiers,\n",
      "    resource data members or constants.\n",
      "\n",
      "    By passing ``params``, you can invoke this method multiple times and\n",
      "    build up a parameter dict over time, which is particularly useful\n",
      "    for reverse JMESPath expressions that append to lists.\n",
      "\n",
      "    :type parent: ServiceResource\n",
      "    :param parent: The resource instance to which this action is attached.\n",
      "    :type request_model: :py:class:`~boto3.resources.model.Request`\n",
      "    :param request_model: The action request model.\n",
      "    :type params: dict\n",
      "    :param params: If set, then add to this existing dict. It is both\n",
      "                   edited in-place and returned.\n",
      "    :type index: int\n",
      "    :param index: The position of an item within a list\n",
      "    :rtype: dict\n",
      "    :return: Pre-filled parameters to be sent to the request operation.\n",
      "    \"\"\"\n",
      "    if params is None:\n",
      "        params = {}\n",
      "\n",
      "    for param in request_model.params:\n",
      "        source = param.source\n",
      "        target = param.target\n",
      "\n",
      "        if source == 'identifier':\n",
      "            # Resource identifier, e.g. queue.url\n",
      "            value = getattr(parent, xform_name(param.name))\n",
      "        elif source == 'data':\n",
      "            # If this is a data member then it may incur a load\n",
      "            # action before returning the value.\n",
      "            value = get_data_member(parent, param.path)\n",
      "        elif source in ['string', 'integer', 'boolean']:\n",
      "            # These are hard-coded values in the definition\n",
      "            value = param.value\n",
      "        elif source == 'input':\n",
      "            # This is provided by the user, so ignore it here\n",
      "            continue\n",
      "        else:\n",
      "            raise NotImplementedError(f'Unsupported source type: {source}')\n",
      "\n",
      "        build_param_structure(params, target, value, index)\n",
      "\n",
      "    return params\n",
      "\n",
      "create_request_parameters(parent=dynamodb.Table(name='MyTable'), request_model={_definition=OrderedDict([('operation', 'Scan'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])]), operation='Scan'}, params=None, index=None)\n",
      "build_param_structure(params, target, value, index)\n",
      "{'params': '{}', 'target': \"'TableName'\", 'value': \"'MyTable'\", 'index': 'None'}\n",
      "params = {'TableName': 'MyTable'}\n",
      "def test_lexer_single_token(expression, result):\n",
      "    lexer = MOALexer()\n",
      "    tokens = tuple(token.type for token in lexer.tokenize(expression))\n",
      "    assert tokens == result\n",
      "\n",
      "test_lexer_single_token(expression='1234', result=('INTEGER',))\n",
      "tokens = tuple(token.type for token in lexer.tokenize(expression))\n",
      "{'expression': \"'1234'\"}\n",
      "tokens = ('INTEGER',)\n",
      "def suggest_type(full_text, text_before_cursor):\n",
      "    \"\"\"Takes the full_text that is typed so far and also the text before the\n",
      "    cursor to suggest completion type and scope.\n",
      "\n",
      "    Returns a tuple with a type of entity ('table', 'column' etc) and a scope.\n",
      "    A scope for a column category will be a list of tables.\n",
      "    \"\"\"\n",
      "\n",
      "    word_before_cursor = last_word(text_before_cursor,\n",
      "            include='many_punctuations')\n",
      "\n",
      "    identifier = None\n",
      "\n",
      "    # here should be removed once sqlparse has been fixed\n",
      "    try:\n",
      "        # If we've partially typed a word then word_before_cursor won't be an empty\n",
      "        # string. In that case we want to remove the partially typed string before\n",
      "        # sending it to the sqlparser. Otherwise the last token will always be the\n",
      "        # partially typed string which renders the smart completion useless because\n",
      "        # it will always return the list of keywords as completion.\n",
      "        if word_before_cursor:\n",
      "            if word_before_cursor.endswith(\n",
      "                    '(') or word_before_cursor.startswith('\\\\'):\n",
      "                parsed = sqlparse.parse(text_before_cursor)\n",
      "            else:\n",
      "                parsed = sqlparse.parse(\n",
      "                    text_before_cursor[:-len(word_before_cursor)])\n",
      "\n",
      "                # word_before_cursor may include a schema qualification, like\n",
      "                # \"schema_name.partial_name\" or \"schema_name.\", so parse it\n",
      "                # separately\n",
      "                p = sqlparse.parse(word_before_cursor)[0]\n",
      "\n",
      "                if p.tokens and isinstance(p.tokens[0], Identifier):\n",
      "                    identifier = p.tokens[0]\n",
      "        else:\n",
      "            parsed = sqlparse.parse(text_before_cursor)\n",
      "    except (TypeError, AttributeError):\n",
      "        return [{'type': 'keyword'}]\n",
      "\n",
      "    if len(parsed) > 1:\n",
      "        # Multiple statements being edited -- isolate the current one by\n",
      "        # cumulatively summing statement lengths to find the one that bounds the\n",
      "        # current position\n",
      "        current_pos = len(text_before_cursor)\n",
      "        stmt_start, stmt_end = 0, 0\n",
      "\n",
      "        for statement in parsed:\n",
      "            stmt_len = len(str(statement))\n",
      "            stmt_start, stmt_end = stmt_end, stmt_end + stmt_len\n",
      "\n",
      "            if stmt_end >= current_pos:\n",
      "                text_before_cursor = full_text[stmt_start:current_pos]\n",
      "                full_text = full_text[stmt_start:]\n",
      "                break\n",
      "\n",
      "    elif parsed:\n",
      "        # A single statement\n",
      "        statement = parsed[0]\n",
      "    else:\n",
      "        # The empty string\n",
      "        statement = None\n",
      "\n",
      "    # Check for special commands and handle those separately\n",
      "    if statement:\n",
      "        # Be careful here because trivial whitespace is parsed as a statement,\n",
      "        # but the statement won't have a first token\n",
      "        tok1 = statement.token_first()\n",
      "        if tok1 and (tok1.value == 'source' or tok1.value.startswith('\\\\')):\n",
      "            return suggest_special(text_before_cursor)\n",
      "\n",
      "    last_token = statement and statement.token_prev(len(statement.tokens))[1] or ''\n",
      "\n",
      "    return suggest_based_on_last_token(last_token, text_before_cursor,\n",
      "                                       full_text, identifier)\n",
      "\n",
      "suggest_type(full_text='SELECT  FROM tabl', text_before_cursor='SELECT ')\n",
      "word_before_cursor = last_word(text_before_cursor,\n",
      "{'text_before_cursor': \"'SELECT '\"}\n",
      "word_before_cursor = ''\n",
      "def get_mylogin_cnf_path():\n",
      "    \"\"\"Return the path to the login path file or None if it doesn't exist.\"\"\"\n",
      "    mylogin_cnf_path = os.getenv('MYSQL_TEST_LOGIN_FILE')\n",
      "\n",
      "    if mylogin_cnf_path is None:\n",
      "        app_data = os.getenv('APPDATA')\n",
      "        default_dir = os.path.join(app_data, 'MySQL') if app_data else '~'\n",
      "        mylogin_cnf_path = os.path.join(default_dir, '.mylogin.cnf')\n",
      "\n",
      "    mylogin_cnf_path = os.path.expanduser(mylogin_cnf_path)\n",
      "\n",
      "    if exists(mylogin_cnf_path):\n",
      "        logger.debug(\"Found login path file at '{0}'\".format(mylogin_cnf_path))\n",
      "        return mylogin_cnf_path\n",
      "    return None\n",
      "\n",
      "get_mylogin_cnf_path()\n",
      "mylogin_cnf_path = os.path.expanduser(mylogin_cnf_path)\n",
      "{'mylogin_cnf_path': \"'~/.mylogin.cnf'\"}\n",
      "mylogin_cnf_path = '/home/XXX/.mylogin.cnf'\n",
      "def read_config_file(f, list_values=True):\n",
      "    \"\"\"Read a config file.\n",
      "\n",
      "    *list_values* set to `True` is the default behavior of ConfigObj.\n",
      "    Disabling it causes values to not be parsed for lists,\n",
      "    (e.g. 'a,b,c' -> ['a', 'b', 'c']. Additionally, the config values are\n",
      "    not unquoted. We are disabling list_values when reading MySQL config files\n",
      "    so we can correctly interpret commas in passwords.\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    if isinstance(f, basestring):\n",
      "        f = os.path.expanduser(f)\n",
      "\n",
      "    try:\n",
      "        config = ConfigObj(f, interpolation=False, encoding='utf8',\n",
      "                           list_values=list_values)\n",
      "    except ConfigObjError as e:\n",
      "        log(logger, logging.WARNING, \"Unable to parse line {0} of config file \"\n",
      "            \"'{1}'.\".format(e.line_number, f))\n",
      "        log(logger, logging.WARNING, \"Using successfully parsed config values.\")\n",
      "        return e.config\n",
      "    except (IOError, OSError) as e:\n",
      "        log(logger, logging.WARNING, \"You don't have permission to read \"\n",
      "            \"config file '{0}'.\".format(e.filename))\n",
      "        return None\n",
      "\n",
      "    return config\n",
      "\n",
      "read_config_file(f={}, list_values=True)\n",
      "config = ConfigObj(f, interpolation=False, encoding='utf8',\n",
      "{'f': '{}'}\n",
      "config = ConfigObj({'main': {'weather': 'cloudy with a chance of meatballs'}})\n",
      "def format_uptime(uptime_in_seconds):\n",
      "    \"\"\"Format number of seconds into human-readable string.\n",
      "\n",
      "    :param uptime_in_seconds: The server uptime in seconds.\n",
      "    :returns: A human-readable string representing the uptime.\n",
      "\n",
      "    >>> uptime = format_uptime('56892')\n",
      "    >>> print(uptime)\n",
      "    15 hours 48 min 12 sec\n",
      "    \"\"\"\n",
      "\n",
      "    m, s = divmod(int(uptime_in_seconds), 60)\n",
      "    h, m = divmod(m, 60)\n",
      "    d, h = divmod(h, 24)\n",
      "\n",
      "    uptime_values = []\n",
      "\n",
      "    for value, unit in ((d, 'days'), (h, 'hours'), (m, 'min'), (s, 'sec')):\n",
      "        if value == 0 and not uptime_values:\n",
      "            # Don't include a value/unit if the unit isn't applicable to\n",
      "            # the uptime. E.g. don't do 0 days 0 hours 1 min 30 sec.\n",
      "            continue\n",
      "        elif value == 1 and unit.endswith('s'):\n",
      "            # Remove the \"s\" if the unit is singular.\n",
      "            unit = unit[:-1]\n",
      "        uptime_values.append('{0} {1}'.format(value, unit))\n",
      "\n",
      "    uptime = ' '.join(uptime_values)\n",
      "    return uptime\n",
      "\n",
      "format_uptime(uptime_in_seconds=59)\n",
      "m, s = divmod(int(uptime_in_seconds), 60)\n",
      "{'uptime_in_seconds': '59'}\n",
      "m = 0\n",
      "def read_config_files(files, list_values=True):\n",
      "    \"\"\"Read and merge a list of config files.\"\"\"\n",
      "\n",
      "    config = create_default_config(list_values=list_values)\n",
      "    _files = copy(files)\n",
      "    while _files:\n",
      "        _file = _files.pop(0)\n",
      "        _config = read_config_file(_file, list_values=list_values)\n",
      "\n",
      "        # expand includes only if we were able to parse config\n",
      "        # (otherwise we'll just encounter the same errors again)\n",
      "        if config is not None:\n",
      "            _files = get_included_configs(_file) + _files\n",
      "        if bool(_config) is True:\n",
      "            config.merge(_config)\n",
      "            config.filename = _config.filename\n",
      "\n",
      "    return config\n",
      "\n",
      "read_config_files(files=['/etc/myclirc', '/home/XXX/.config/mycli/myclirc', '~/.myclirc', '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/dbcli+mycli/dbcli+mycli/.myclirc'], list_values=True)\n",
      "_files = copy(files)\n",
      "{'files': \"['/etc/myclirc', '/home/XXX/.config/mycli/myclirc', '~/.myclirc', '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/dbcli+mycli/dbcli+mycli/.myclirc']\"}\n",
      "_files = ['/etc/myclirc', '/home/XXX/.config/mycli/myclirc', '~/.myclirc', '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/dbcli+mycli/dbcli+mycli/.myclirc']\n",
      "def style_factory_output(name, cli_style):\n",
      "    try:\n",
      "        style = pygments.styles.get_style_by_name(name).styles\n",
      "    except ClassNotFound:\n",
      "        style = pygments.styles.get_style_by_name('native').styles\n",
      "\n",
      "    for token in cli_style:\n",
      "        if token.startswith('Token.'):\n",
      "            token_type, style_value = parse_pygments_style(\n",
      "                token, style, cli_style)\n",
      "            style.update({token_type: style_value})\n",
      "        elif token in PROMPT_STYLE_TO_TOKEN:\n",
      "            token_type = PROMPT_STYLE_TO_TOKEN[token]\n",
      "            style.update({token_type: cli_style[token]})\n",
      "        elif token in OVERRIDE_STYLE_TO_TOKEN:\n",
      "            token_type = OVERRIDE_STYLE_TO_TOKEN[token]\n",
      "            style.update({token_type: cli_style[token]})\n",
      "        else:\n",
      "            # TODO: cli helpers will have to switch to ptk.Style\n",
      "            logger.error('Unhandled style / class name: %s', token)\n",
      "\n",
      "    class OutputStyle(PygmentsStyle):\n",
      "        default_style = \"\"\n",
      "        styles = style\n",
      "\n",
      "    return OutputStyle\n",
      "\n",
      "style_factory_output(name='default', cli_style={'completion-menu.completion.current': 'bg:#ffffff #000000', 'completion-menu.completion': 'bg:#008888 #ffffff', 'completion-menu.meta.completion.current': 'bg:#44aaaa #000000', 'completion-menu.meta.completion': 'bg:#448888 #ffffff', 'completion-menu.multi-column-meta': 'bg:#aaffff #000000', 'scrollbar.arrow': 'bg:#003333', 'scrollbar': 'bg:#00aaaa', 'selected': '#ffffff bg:#6666aa', 'search': '#ffffff bg:#4444aa', 'search.current': '#ffffff bg:#44aa44', 'bottom-toolbar': 'bg:#222222 #aaaaaa', 'bottom-toolbar.off': 'bg:#222222 #888888', 'bottom-toolbar.on': 'bg:#222222 #ffffff', 'search-toolbar': 'noinherit bold', 'search-toolbar.text': 'nobold', 'system-toolbar': 'noinherit bold', 'arg-toolbar': 'noinherit bold', 'arg-toolbar.text': 'nobold', 'bottom-toolbar.transaction.valid': 'bg:#222222 #00ff5f bold', 'bottom-toolbar.transaction.failed': 'bg:#222222 #ff005f bold', 'output.header': '#00ff5f bold', 'output.odd-row': '', 'output.even-row': '', 'output.null': '#808080'})\n",
      "style = pygments.styles.get_style_by_name(name).styles\n",
      "{'name': \"'default'\"}\n",
      "style = {Token.Text.Whitespace: '#bbbbbb', Token.Comment: 'italic #3D7B7B', Token.Comment.Preproc: 'noitalic #9C6500', Token.Keyword: 'bold #008000', Token.Keyword.Pseudo: 'nobold', Token.Keyword.Type: 'nobold #B00040', Token.Operator: '#666666', Token.Operator.Word: 'bold #AA22FF', Token.Name.Builtin: '#008000', Token.Name.Function: '#0000FF', Token.Name.Class: 'bold #0000FF', Token.Name.Namespace: 'bold #0000FF', Token.Name.Exception: 'bold #CB3F38', Token.Name.Variable: '#19177C', Token.Name.Constant: '#880000', Token.Name.Label: '#767600', Token.Name.Entity: 'bold #717171', Token.Name.Attribute: '#687822', Token.Name.Tag: 'bold #008000', Token.Name.Decorator: '#AA22FF', Token.Literal.String: '#BA2121', Token.Literal.String.Doc: 'italic', Token.Literal.String.Interpol: 'bold #A45A77', Token.Literal.String.Escape: 'bold #AA5D1F', Token.Literal.String.Regex: '#A45A77', Token.Literal.String.Symbol: '#19177C', Token.Literal.String.Other: '#008000', Token.Literal.Number: '#666666', Token.Generic.Heading: 'bold #000080', Token.Generic.Subheading: 'bold #800080', Token.Generic.Deleted: '#A00000', Token.Generic.Inserted: '#008400', Token.Generic.Error: '#E40000', Token.Generic.Emph: 'italic', Token.Generic.Strong: 'bold', Token.Generic.EmphStrong: 'bold italic', Token.Generic.Prompt: 'bold #000080', Token.Generic.Output: '#717171', Token.Generic.Traceback: '#04D', Token.Error: 'border:#FF0000', Token: '', Token.Text: '', Token.Escape: '', Token.Other: '', Token.Keyword.Constant: '', Token.Keyword.Declaration: '', Token.Keyword.Namespace: '', Token.Keyword.Reserved: '', Token.Name: '', Token.Name.Builtin.Pseudo: '', Token.Name.Function.Magic: '', Token.Name.Property: '', Token.Name.Other: '', Token.Name.Variable.Class: '', Token.Name.Variable.Global: '', Token.Name.Variable.Instance: '', Token.Name.Variable.Magic: '', Token.Literal: '', Token.Literal.Date: '', Token.Literal.String.Affix: '', Token.Literal.String.Backtick: '', Token.Literal.String.Char: '', Token.Literal.String.Delimiter: '', Token.Literal.String.Double: '', Token.Literal.String.Heredoc: '', Token.Literal.String.Single: '', Token.Literal.Number.Bin: '', Token.Literal.Number.Float: '', Token.Literal.Number.Hex: '', Token.Literal.Number.Integer: '', Token.Literal.Number.Integer.Long: '', Token.Literal.Number.Oct: '', Token.Punctuation: '', Token.Punctuation.Marker: '', Token.Comment.Hashbang: '', Token.Comment.Multiline: '', Token.Comment.PreprocFile: '', Token.Comment.Single: '', Token.Comment.Special: '', Token.Generic: ''}\n",
      "def contains_sublist(list_: List[Any], sublist: List[Any]) -> bool:\n",
      "    \"\"\"Determine if a `list` contains a `sublist`.\n",
      "\n",
      "    :param list_:\n",
      "        list to search for the `sublist` in.\n",
      "    :param sublist:\n",
      "        Sub list to search for.\n",
      "\n",
      "    :return:\n",
      "        True if `list` contains `sublist`.\n",
      "\n",
      "    \"\"\"\n",
      "    # Adapted from: https://stackoverflow.com/a/12576755\n",
      "    if not sublist:\n",
      "        return False\n",
      "    for i in range(len(list_)):\n",
      "        if list_[i] == sublist[0] and list_[i : i + len(sublist)] == sublist:\n",
      "            return True\n",
      "    return False\n",
      "\n",
      "contains_sublist(list_=[1, 2, 3, 4], sublist=[1, 2])\n",
      "for i in range(len(list_)):\n",
      "{'list_': '[1, 2, 3, 4]'}\n",
      "i = 0\n",
      "def delete_sublist(list_: List[Any], sublist: List[Any]) -> List[Any]:\n",
      "    \"\"\"Remove a `sublist` from the given `list_`.\n",
      "\n",
      "    :param list_:\n",
      "        List to remove the `sublist` from.\n",
      "    :param sublist:\n",
      "        Sublist to remove from `list_`.\n",
      "\n",
      "    :return:\n",
      "        A copy of `list_` with the `sublist` removed.\n",
      "    \"\"\"\n",
      "    if not sublist:\n",
      "        return list_[:]\n",
      "    for i in range(len(list_)):\n",
      "        if list_[i] == sublist[0] and list_[i : i + len(sublist)] == sublist:\n",
      "            return list_[:i] + list_[i + len(sublist) :]\n",
      "    return list_[:]\n",
      "\n",
      "delete_sublist(list_=[1, 2, 3, 4], sublist=[1, 2])\n",
      "for i in range(len(list_)):\n",
      "{'list_': '[1, 2, 3, 4]'}\n",
      "i = 0\n",
      "def bounded_product(sequence, n=None, seed=None):\n",
      "    \"\"\"\n",
      "    Returns a shuffled, bounded cartesian product of the input sequence.\n",
      "    Designed to cover as wide a range of permutations as possible with a limited number of iterations.\n",
      "    Will manifest the whole list in memory, so not suitable for super large sequences.\n",
      "\n",
      "    :param sequence: iterable\n",
      "    :param n: length of returned list\n",
      "    :param seed: random seed for reproducibility\n",
      "    :return: list\n",
      "    \"\"\"\n",
      "    p = list(itertools.product(*sequence))\n",
      "    if seed is not None:\n",
      "        random.seed(seed)\n",
      "    random.shuffle(p)\n",
      "    return p if n is None else p[:n]\n",
      "\n",
      "bounded_product(sequence=([[0, 1, 1], [1, 2, 2], [0, 2, 2]], [True, False], [[[['global'], 'all']], [[['local'], 'all']], [[['sparse_variable'], 'all']], [[['sparse_fixed'], 'all']]], [[True, False], [False, True]], [[{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]]), n=50, seed=None)\n",
      "random.shuffle(p)\n",
      "{'p': \"[([0, 1, 1], True, [[['global'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 1, 1], True, [[['global'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 1, 1], True, [[['global'], 'all']], [False, True], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 1, 1], True, [[['global'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 1, 1], True, [[['local'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 1, 1], True, [[['local'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 1, 1], True, [[['local'], 'all']], [False, True], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 1, 1], True, [[['local'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 1, 1], True, [[['sparse_variable'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 1, 1], True, [[['sparse_variable'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 1, 1], True, [[['sparse_variable'], 'all']], [False, True], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 1, 1], True, [[['sparse_variable'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 1, 1], True, [[['sparse_fixed'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 1, 1], True, [[['sparse_fixed'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 1, 1], True, [[['sparse_fixed'], 'all']], [False, True], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 1, 1], True, [[['sparse_fixed'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 1, 1], False, [[['global'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 1, 1], False, [[['global'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 1, 1], False, [[['global'], 'all']], [False, True], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 1, 1], False, [[['global'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 1, 1], False, [[['local'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 1, 1], False, [[['local'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 1, 1], False, [[['local'], 'all']], [False, True], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis... True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 2, 2], True, [[['sparse_variable'], 'all']], [False, True], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 2, 2], True, [[['sparse_variable'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 2, 2], True, [[['sparse_fixed'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 2, 2], True, [[['sparse_fixed'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 2, 2], True, [[['sparse_fixed'], 'all']], [False, True], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 2, 2], True, [[['sparse_fixed'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 2, 2], False, [[['global'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 2, 2], False, [[['global'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 2, 2], False, [[['global'], 'all']], [False, True], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 2, 2], False, [[['global'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 2, 2], False, [[['local'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 2, 2], False, [[['local'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 2, 2], False, [[['local'], 'all']], [False, True], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 2, 2], False, [[['local'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 2, 2], False, [[['sparse_variable'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 2, 2], False, [[['sparse_variable'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 2, 2], False, [[['sparse_variable'], 'all']], [False, True], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 2, 2], False, [[['sparse_variable'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 2, 2], False, [[['sparse_fixed'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 2, 2], False, [[['sparse_fixed'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 2, 2], False, [[['sparse_fixed'], 'all']], [False, True], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 2, 2], False, [[['sparse_fixed'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False])]\"}\n",
      "p = [([0, 1, 1], False, [[['sparse_variable'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 1, 1], False, [[['local'], 'all']], [False, True], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([1, 2, 2], True, [[['sparse_fixed'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([1, 2, 2], True, [[['sparse_variable'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([1, 2, 2], True, [[['local'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 2, 2], True, [[['sparse_fixed'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 2, 2], True, [[['sparse_variable'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 2, 2], True, [[['local'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([1, 2, 2], True, [[['global'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([1, 2, 2], False, [[['global'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 1, 1], True, [[['global'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 2, 2], True, [[['global'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 1, 1], True, [[['global'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 1, 1], False, [[['sparse_fixed'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 1, 1], True, [[['sparse_fixed'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 1, 1], False, [[['sparse_variable'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([1, 2, 2], False, [[['sparse_fixed'], 'all']], [False, True], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 2, 2], False, [[['sparse_fixed'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([1, 2, 2], True, [[['local'], 'all']], [False, True], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 1, 1], True, [[['local'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 1, 1], True, [[['sparse_variable'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 1, 1], True, [[['sparse_variable'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([1, 2, 2], False, [[['local'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, '...abled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([1, 2, 2], False, [[['local'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 2, 2], False, [[['local'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([1, 2, 2], True, [[['sparse_variable'], 'all']], [False, True], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([1, 2, 2], True, [[['sparse_fixed'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 1, 1], True, [[['local'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([1, 2, 2], False, [[['sparse_fixed'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([1, 2, 2], True, [[['local'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([1, 2, 2], True, [[['sparse_fixed'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 2, 2], False, [[['global'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 2, 2], False, [[['sparse_fixed'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 2, 2], True, [[['sparse_fixed'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 2, 2], False, [[['sparse_variable'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 2, 2], True, [[['sparse_variable'], 'all']], [False, True], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 2, 2], False, [[['global'], 'all']], [False, True], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 1, 1], False, [[['global'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([1, 2, 2], False, [[['sparse_fixed'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 2, 2], False, [[['sparse_variable'], 'all']], [False, True], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 2, 2], False, [[['sparse_fixed'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 2, 2], False, [[['sparse_fixed'], 'all']], [False, True], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True]), ([0, 1, 1], True, [[['sparse_variable'], 'all']], [True, False], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 1, 1], True, [[['sparse_fixed'], 'all']], [False, True], [{'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, False]), ([0, 2, 2], False, [[['local'], 'all']], [True, False], [{'enabled': True, 'type': 'bfloat16', 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, True])]\n",
      "def set_up_autotuning(encoded_config, overwrite_values):\n",
      "        config = json.loads(base64.urlsafe_b64decode(encoded_config).decode(\"utf-8\"))\n",
      "        overwrite_values = overwrite_values if overwrite_values else {}\n",
      "        for tuning_param in AUTOTUNING_ARGS:\n",
      "            # TODO: This is for autotuning specifically, may cause surprises for someone with a weird setup\n",
      "            if tuning_param in config:\n",
      "                overwrite_values[tuning_param] = config[tuning_param]\n",
      "        return overwrite_values\n",
      "\n",
      "set_up_autotuning(encoded_config='eyJ0cmFpbl9iYXRjaF9zaXplIjogNCwgInRyYWluX21pY3JvX2JhdGNoX3NpemVfcGVyX2dwdSI6IDQsICJvcHRpbWl6ZXIiOiB7InR5cGUiOiAic20zIiwgInBhcmFtcyI6IHt9fSwgImZwMTYiOiB7InR5cGUiOiAiZnAxNiIsICJlbmFibGVkIjogdHJ1ZX0sICJ6ZXJvX29wdGltaXphdGlvbiI6IHsic3RhZ2UiOiAwLCAiYWxsZ2F0aGVyX3BhcnRpdGlvbnMiOiB0cnVlLCAicmVkdWNlX3NjYXR0ZXIiOiB0cnVlLCAiYWxsZ2F0aGVyX2J1Y2tldF9zaXplIjogNTAwMDAwMDAwLCAib3ZlcmxhcF9jb21tIjogZmFsc2UsICJyZWR1Y2VfYnVja2V0X3NpemUiOiA1MDAwMDAwMDAsICJjb250aWd1b3VzX2dyYWRpZW50cyI6IGZhbHNlfSwgIndhbGxfY2xvY2tfYnJlYWtkb3duIjogdHJ1ZSwgImNvbW1zX2xvZ2dlciI6IHsiZW5hYmxlZCI6IHRydWUsICJ2ZXJib3NlIjogdHJ1ZSwgInByb2ZfYWxsIjogdHJ1ZSwgImRlYnVnIjogZmFsc2V9fQ==', overwrite_values={'train_iters': 32})\n",
      "config = json.loads(base64.urlsafe_b64decode(encoded_config).decode(\"utf-8\"))\n",
      "{'encoded_config': \"'eyJ0cmFpbl9iYXRjaF9zaXplIjogNCwgInRyYWluX21pY3JvX2JhdGNoX3NpemVfcGVyX2dwdSI6IDQsICJvcHRpbWl6ZXIiOiB7InR5cGUiOiAic20zIiwgInBhcmFtcyI6IHt9fSwgImZwMTYiOiB7InR5cGUiOiAiZnAxNiIsICJlbmFibGVkIjogdHJ1ZX0sICJ6ZXJvX29wdGltaXphdGlvbiI6IHsic3RhZ2UiOiAwLCAiYWxsZ2F0aGVyX3BhcnRpdGlvbnMiOiB0cnVlLCAicmVkdWNlX3NjYXR0ZXIiOiB0cnVlLCAiYWxsZ2F0aGVyX2J1Y2tldF9zaXplIjogNTAwMDAwMDAwLCAib3ZlcmxhcF9jb21tIjogZmFsc2UsICJyZWR1Y2VfYnVja2V0X3NpemUiOiA1MDAwMDAwMDAsICJjb250aWd1b3VzX2dyYWRpZW50cyI6IGZhbHNlfSwgIndhbGxfY2xvY2tfYnJlYWtkb3duIjogdHJ1ZSwgImNvbW1zX2xvZ2dlciI6IHsiZW5hYmxlZCI6IHRydWUsICJ2ZXJib3NlIjogdHJ1ZSwgInByb2ZfYWxsIjogdHJ1ZSwgImRlYnVnIjogZmFsc2V9fQ=='\"}\n",
      "config = {'train_batch_size': 4, 'train_micro_batch_size_per_gpu': 4, 'optimizer': {'type': 'sm3', 'params': {}}, 'fp16': {'type': 'fp16', 'enabled': True}, 'zero_optimization': {'stage': 0, 'allgather_partitions': True, 'reduce_scatter': True, 'allgather_bucket_size': 500000000, 'overlap_comm': False, 'reduce_bucket_size': 500000000, 'contiguous_gradients': False}, 'wall_clock_breakdown': True, 'comms_logger': {'enabled': True, 'verbose': True, 'prof_all': True, 'debug': False}}\n",
      "def run_train_test(monkeypatch, overwrite_values: dict):\n",
      "    max_train_iters = 32\n",
      "    checkpoint_args = {\"train_iters\": max_train_iters}\n",
      "    overwrite_values = checkpoint_args\n",
      "    input_args = [\"train.py\", \"tests/config/test_setup.yml\"]\n",
      "    deepspeed_main_args = simulate_deepy_env(monkeypatch, input_args)\n",
      "\n",
      "    # Train model, whilst patching collect_loss_for_unit_test to track model loss at each step\n",
      "    loss_per_iteration = []\n",
      "    with patch(\n",
      "        \"megatron.training.collect_loss_for_unit_test\",\n",
      "        side_effect=lambda x: loss_per_iteration.append(x),\n",
      "    ):\n",
      "        train.main(input_args=deepspeed_main_args, overwrite_values=overwrite_values)\n",
      "        assert (\n",
      "            len(loss_per_iteration) == max_train_iters\n",
      "        ), \"patching should have collected loss values from each train step\"\n",
      "\n",
      "        # loss should have decreased by now (otherwise increasing the max_steps parameter could have the testcase pass)\n",
      "        assert min(loss_per_iteration) < loss_per_iteration[0], (\n",
      "            \"training loss should improve within \" + str(max_train_iters) + \" steps\"\n",
      "        )\n",
      "\n",
      "run_train_test(monkeypatch={_setattr=[], _setitem=[], _cwd=None, _savesyspath=None}, overwrite_values={'pos_emb': 'rpe'})\n",
      "deepspeed_main_args = simulate_deepy_env(monkeypatch, input_args)\n",
      "{'monkeypatch': '{_setattr=[], _setitem=[], _cwd=None, _savesyspath=None}', 'input_args': \"['train.py', 'tests/config/test_setup.yml']\"}\n",
      "deepspeed_main_args = ['--hostfile', 'None', '--include', 'localhost:1', 'train.py', '--deepspeed_config', 'eyJ0cmFpbl9iYXRjaF9zaXplIjogNCwgInRyYWluX21pY3JvX2JhdGNoX3NpemVfcGVyX2dwdSI6IDQsICJvcHRpbWl6ZXIiOiB7InR5cGUiOiAic20zIiwgInBhcmFtcyI6IHt9fSwgImZwMTYiOiB7InR5cGUiOiAiZnAxNiIsICJlbmFibGVkIjogdHJ1ZX0sICJ6ZXJvX29wdGltaXphdGlvbiI6IHsic3RhZ2UiOiAwLCAiYWxsZ2F0aGVyX3BhcnRpdGlvbnMiOiB0cnVlLCAicmVkdWNlX3NjYXR0ZXIiOiB0cnVlLCAiYWxsZ2F0aGVyX2J1Y2tldF9zaXplIjogNTAwMDAwMDAwLCAib3ZlcmxhcF9jb21tIjogZmFsc2UsICJyZWR1Y2VfYnVja2V0X3NpemUiOiA1MDAwMDAwMDAsICJjb250aWd1b3VzX2dyYWRpZW50cyI6IGZhbHNlfSwgIndhbGxfY2xvY2tfYnJlYWtkb3duIjogdHJ1ZSwgImNvbW1zX2xvZ2dlciI6IHsiZW5hYmxlZCI6IHRydWUsICJ2ZXJib3NlIjogdHJ1ZSwgInByb2ZfYWxsIjogdHJ1ZSwgImRlYnVnIjogZmFsc2V9fQ==', '--megatron_config', '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']\n",
      "def run_neox_args_load_test(yaml_files):\n",
      "    from megatron.neox_arguments import NeoXArgs\n",
      "\n",
      "    yaml_list = get_configs_with_path(yaml_files)\n",
      "    args_loaded = NeoXArgs.from_ymls(yaml_list)\n",
      "    assert isinstance(args_loaded, NeoXArgs)\n",
      "\n",
      "    # initialize an empty config dictionary to be filled by yamls\n",
      "    config = dict()\n",
      "\n",
      "    # iterate of all to be loaded yaml files\n",
      "    for conf_file_name in yaml_list:\n",
      "\n",
      "        # load file\n",
      "        with open(conf_file_name) as conf_file:\n",
      "            conf = yaml.load(conf_file, Loader=yaml.FullLoader)\n",
      "\n",
      "        # check for key duplicates and load values\n",
      "        for conf_key, conf_value in conf.items():\n",
      "            if conf_key in config:\n",
      "                raise ValueError(\n",
      "                    f\"Conf file {conf_file_name} has the following duplicate keys with previously loaded file: {conf_key}\"\n",
      "                )\n",
      "\n",
      "            conf_key_converted = conf_key.replace(\n",
      "                \"-\", \"_\"\n",
      "            )  # TODO remove replace and update configuration files?\n",
      "            config[conf_key_converted] = conf_value\n",
      "\n",
      "    # validate that neox args has the same value as specified in the config (if specified in the config)\n",
      "    for k, v in config.items():\n",
      "        neox_args_value = getattr(args_loaded, k)\n",
      "        assert v == neox_args_value, (\n",
      "            \"loaded neox args value \"\n",
      "            + str(k)\n",
      "            + \" == \"\n",
      "            + str(neox_args_value)\n",
      "            + \" different from config file \"\n",
      "            + str(v)\n",
      "        )\n",
      "\n",
      "run_neox_args_load_test(yaml_files=['125M.yml', 'local_setup.yml', 'cpu_mock_config.yml'])\n",
      "args_loaded = NeoXArgs.from_ymls(yaml_list)\n",
      "{'yaml_list': \"['/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/EleutherAI+gpt-neox/EleutherAI+gpt-neox/configs/125M.yml', '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/EleutherAI+gpt-neox/EleutherAI+gpt-neox/configs/local_setup.yml', '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/EleutherAI+gpt-neox/EleutherAI+gpt-neox/configs/cpu_mock_config.yml']\"}\n",
      "args_loaded = NeoXArgs(distributed_backend='nccl', local_rank=None, rank=None, lazy_mpu_init=False, short_seq_prob=0.1, eod_mask_loss=False, adlr_autoresume=False, adlr_autoresume_interval=1000, seed=1234, onnx_safe=False, deepscale=False, deepscale_config=None, deepspeed_mpi=False, deepspeed_slurm=False, user_script=None, iteration=None, do_train=None, do_valid=None, do_test=None, save_iters=[10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000, 110000, 120000, 130000, 140000, 150000, 160000, 170000, 180000, 190000, 200000, 210000, 220000, 230000, 240000, 250000, 260000, 270000, 280000, 290000, 300000, 310000], global_num_gpus=1, text_gen_type='unconditional', temperature=0.0, top_p=0.0, top_k=0, return_logits=False, maximum_tokens=64, prompt_end='\\n', sample_input_file=None, sample_output_file='samples.txt', num_samples=1, recompute=False, eval_results_prefix='', eval_tasks=None, use_wandb=True, wandb_group=None, wandb_team=None, wandb_project='neox', wandb_host='https://api.wandb.ai', wandb_init_all_ranks=False, git_hash='7a8fa2f0', log_dir='logs', tensorboard_dir='tensorboard', log_interval=100, log_grad_pct_zeros=False, log_param_norm=False, log_grad_norm=False, log_optimizer_states=False, log_gradient_noise_scale=False, gradient_noise_scale_n_batches=5, gradient_noise_scale_cpu_offload=False, pipe_parallel_size=1, model_parallel_size=1, pipe_partition_method='type:transformer|mlp', world_size=None, is_pipe_parallel=True, data_path='data/enwik8/enwik8_text_document', use_shared_fs=True, train_data_paths=None, label_data_paths=None, test_data_paths=None, valid_data_paths=None, train_data_weights=None, valid_data_weights=None, test_data_weights=None, weight_by_num_documents=False, weighted_sampler_alpha=1.0, data_impl='mmap', mmap_warmup=False, save='checkpoints', s3_path=None, s3_chunk_size=104857600, config_files={'125M.yml': '# GPT-2 pretraining setup\\n{\\n   # parallelism settings ( you will want to change these based on your cluster setup, ideally scheduling pipeline stages\\n   # across the node boundaries )\\n   \"pipe_parallel_size\": 1,\\n   \"model_parallel_size\": 1,\\n\\n   # model settings\\n   \"num_layers\": 12,\\n   \"hidden_size\": 768,\\n   \"num_attention_heads\": 12,\\n   \"seq_length\": 2048,\\n   \"max_position_embeddings\": 2048,\\n   \"norm\": \"layernorm\",\\n   \"pos_emb\": \"rotary\",\\n   \"no_weight_tying\": true,\\n   \"gpt_j_residual\": false,\\n   \"output_layer_parallelism\": \"column\",\\n\\n   # these should provide some speedup but takes a while to build, set to true if desired\\n   \"scaled_upper_triang_masked_softmax_fusion\": false,\\n   \"bias_gelu_fusion\": false,\\n   \"rope_fusion\": false,\\n\\n   # init methods\\n   \"init_method\": \"small_init\",\\n   \"output_layer_init_method\": \"wang_init\",\\n\\n\\n   # optimizer settings\\n   \"optimizer\": {\\n     \"type\": \"Adam\",\\n     \"params\": {\\n       \"lr\": 0.0006,\\n       \"betas\": [0.9, 0.95],\\n       \"eps\": 1.0e-8,\\n     }\\n   },\\n   \"min_lr\": 0.00006,\\n\\n   # for all zero_optimization options, see https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training\\n   \"zero_optimization\": {\\n    \"stage\": 1,\\n    \"allgather_partitions\": True,\\n    \"allgather_bucket_size\": 500000000,\\n    \"overlap_comm\": True,\\n    \"reduce_scatter\": True,\\n    \"reduce_bucket_size\": 500000000,\\n    \"contiguous_gradients\": True,\\n  },\\n\\n   # batch / data settings\\n   \"train_micro_batch_size_per_gpu\": 4,\\n   \"data_impl\": \"mmap\",\\n\\n   # activation checkpointing\\n   \"checkpoint_activations\": true,\\n   \"checkpoint_num_layers\": 1,\\n   \"partition_activations\": true,\\n   \"synchronize_each_layer\": true,\\n\\n   # regularization\\n   \"gradient_clipping\": 1.0,\\n   \"weight_decay\": 0.1,\\n   \"hidden_dropout\": 0.0,\\n   \"attention_dropout\": 0.0,\\n\\n   # precision settings\\n   \"fp16\": {\\n     \"enabled\": true,\\n     \"loss_scale\": 0,\\n     \"loss_scale_window\": 1000,\\n     \"hysteresis\": 2,\\n     \"min_loss_scale\": 1\\n   },\\n\\n   # misc. training settings\\n   \"train_iters\": 320000,\\n   \"lr_decay_iters\": 320000,\\n   \"distributed_backend\": \"nccl\",\\n   \"lr_decay_style\": \"cosine\",\\n   \"warmup\": 0.01,\\n   \"checkpoint_factor\": 10...\\n{\\n  \"global_num_gpus\": 1\\n}\\n'}, load='checkpoints', checkpoint_validation_with_forward_pass=False, checkpoint_scale='linear', checkpoint_factor=10000, extra_save_iters=None, no_save_optim=False, no_save_rng=False, no_load_optim=False, no_load_rng=False, finetune=False, batch_size=4, train_iters=320000, eval_iters=10, keep_last_n_checkpoints=4, eval_interval=1000, split='969, 30, 1', vocab_file='data/gpt2-vocab.json', merge_file='data/gpt2-merges.txt', num_workers=2, exit_interval=None, attention_dropout=0.0, hidden_dropout=0.0, weight_decay=0.1, checkpoint_activations=True, checkpoint_num_layers=1, deepspeed_activation_checkpointing=True, contiguous_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=True, profile_backward=False, partition_activations=True, gas=1, clip_grad=1.0, hysteresis=2, dynamic_loss_scale=True, loss_scale=None, loss_scale_window=1000.0, min_scale=1.0, char_level_ppl=False, use_mup=False, coord_check=False, save_base_shapes=False, base_shapes_file=None, mup_init_scale=1.0, mup_attn_temp=1.0, mup_output_temp=1.0, mup_embedding_mult=1.0, mup_rp_embedding_mult=1.0, mup_width_scale=2, tokenizer_type='GPT2BPETokenizer', padded_vocab_size=None, optimizer_type='Adam', use_bnb_optimizer=False, zero_stage=1, zero_reduce_scatter=True, zero_contiguous_gradients=True, zero_reduce_bucket_size=500000000, zero_allgather_bucket_size=500000000, lr=0.0006, lr_decay_style='cosine', lr_decay_iters=320000, min_lr=6e-05, warmup=0.01, override_lr_scheduler=False, use_checkpoint_lr_scheduler=False, precision='fp16', num_layers=12, hidden_size=768, num_attention_heads=12, seq_length=2048, max_position_embeddings=2048, norm='layernorm', use_qk_layernorm=False, layernorm_epsilon=1e-05, rms_norm_epsilon=1e-08, scalenorm_epsilon=1e-08, pos_emb='rotary', rpe_num_buckets=32, rpe_max_distance=128, opt_pos_emb_offset=0, no_weight_tying=True, attention_config=['global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global'], sparsity_config={}, num_unique_layers=None, param_sharing_style='grouped', make_vocab_size_divisible_by=128, activation='gelu', scaled_upper_triang_masked_softmax_fusion=False, scaled_masked_softmax_fusion=False, bias_gelu_fusion=False, bias_dropout_fusion=False, rope_fusion=False, fp16_lm_cross_entropy=False, init_method_std=0.02, apply_query_key_layer_scaling=False, use_cpu_initialization=False, attention_softmax_in_fp32=False, rotary_pct=1.0, rotary_emb_base=10000, init_method='small_init', output_layer_init_method='wang_init', gmlp_attn_dim=64, gpt_j_residual=False, gpt_j_tied=False, use_bias_in_norms=True, use_bias_in_attn_linear=True, mlp_type='regular', soft_prompt_tuning=None, output_layer_parallelism='column', deepspeed=True, train_batch_size=4, train_micro_batch_size_per_gpu=4, gradient_accumulation_steps=1, optimizer={'type': 'Adam', 'params': {'lr': 0.0006, 'betas': [0.9, 0.95], 'eps': 1e-08}}, scheduler=None, fp32_allreduce=False, prescale_gradients=False, gradient_predivide_factor=1.0, sparse_gradients=False, fp16={'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, bf16=None, amp=None, gradient_clipping=1.0, zero_optimization={'stage': 1, 'allgather_partitions': True, 'allgather_bucket_size': 500000000, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 500000000, 'contiguous_gradients': True}, curriculum_learning=None, curriculum_seqlen=0, steps_per_print=10, wall_clock_breakdown=True, dump_state=False, flops_profiler=None, communication_data_type=None, autotuning=None, activation_checkpointing=None, sparse_attention=None, data_efficiency=None, tensorboard=None, wandb=None, csv_monitor=None, elasticity=None, comms_logger=None, compression_training=None, checkpoint=None, data_types=None, deepspeed_extra_args=None, hostfile='/mock_path', include=None, exclude=None, num_nodes=-1, num_gpus=None, master_port=29500, master_addr=None, launcher='pdsh', force_multi=False, detect_nvlink_pairs=False, autotuning_run=None, no_ssh_check=False, comment=None, account=None)\n",
      "def get_args(input_args=None):\n",
      "    parser = argparse.ArgumentParser()\n",
      "    group = parser.add_argument_group(title=\"input data\")\n",
      "    group.add_argument(\n",
      "        \"--input\",\n",
      "        type=str,\n",
      "        required=True,\n",
      "        help=\"Path to input jsonl files or lmd archive(s) - if using multiple archives, put them in a comma separated \"\n",
      "        \"list\",\n",
      "    )\n",
      "    group.add_argument(\n",
      "        \"--jsonl-keys\",\n",
      "        nargs=\"+\",\n",
      "        default=[\"text\"],\n",
      "        help=\"space separate listed of keys to extract from jsonl. Defa\",\n",
      "    )\n",
      "    group.add_argument(\n",
      "        \"--num-docs\",\n",
      "        default=None,\n",
      "        help=\"Optional: Number of documents in the input data (if known) for an accurate progress bar.\",\n",
      "        type=int,\n",
      "    )\n",
      "    group = parser.add_argument_group(title=\"tokenizer\")\n",
      "    group.add_argument(\n",
      "        \"--tokenizer-type\",\n",
      "        type=str,\n",
      "        required=True,\n",
      "        choices=[\n",
      "            \"HFGPT2Tokenizer\",\n",
      "            \"HFTokenizer\",\n",
      "            \"GPT2BPETokenizer\",\n",
      "            \"CharLevelTokenizer\",\n",
      "            \"TiktokenTokenizer\",\n",
      "            \"SPMTokenizer\",\n",
      "        ],\n",
      "        help=\"What type of tokenizer to use.\",\n",
      "    )\n",
      "    group.add_argument(\n",
      "        \"--vocab-file\", type=str, default=None, help=\"Path to the vocab file\"\n",
      "    )\n",
      "    group.add_argument(\n",
      "        \"--merge-file\",\n",
      "        type=str,\n",
      "        default=None,\n",
      "        help=\"Path to the BPE merge file (if necessary).\",\n",
      "    )\n",
      "    group.add_argument(\n",
      "        \"--append-eod\",\n",
      "        action=\"store_true\",\n",
      "        help=\"Append an <eod> token to the end of a document.\",\n",
      "    )\n",
      "    group.add_argument(\"--ftfy\", action=\"store_true\", help=\"Use ftfy to clean text\")\n",
      "    group = parser.add_argument_group(title=\"output data\")\n",
      "    group.add_argument(\n",
      "        \"--output-prefix\",\n",
      "        type=str,\n",
      "        required=True,\n",
      "        help=\"Path to binary output file without suffix\",\n",
      "    )\n",
      "    group.add_argument(\n",
      "        \"--dataset-impl\",\n",
      "        type=str,\n",
      "        default=\"mmap\",\n",
      "        choices=[\"lazy\", \"cached\", \"mmap\"],\n",
      "        help=\"Dataset implementation to use. Default: mmap\",\n",
      "    )\n",
      "\n",
      "    group = parser.add_argument_group(title=\"runtime\")\n",
      "    group.add_argument(\n",
      "        \"--workers\", type=int, default=1, help=\"Number of worker processes to launch\"\n",
      "    )\n",
      "    group.add_argument(\n",
      "        \"--log-interval\",\n",
      "        type=int,\n",
      "        default=100,\n",
      "        help=\"Interval between progress updates\",\n",
      "    )\n",
      "    args = parser.parse_args(input_args)\n",
      "    args.keep_empty = False\n",
      "\n",
      "    # some default/dummy values for the tokenizer\n",
      "    args.rank = 0\n",
      "    args.make_vocab_size_divisible_by = 128\n",
      "    args.model_parallel_size = 1\n",
      "\n",
      "    return args\n",
      "\n",
      "get_args(input_args=['--input', './tests/data/enwik8_first100.txt', '--output-prefix', './tests/data/enwik8_first100', '--vocab', 'gpt2', '--tokenizer-type', 'HFGPT2Tokenizer', '--merge-file', './data/gpt2-merges.txt', '--append-eod'])\n",
      "args = parser.parse_args(input_args)\n",
      "{'input_args': \"['--input', './tests/data/enwik8_first100.txt', '--output-prefix', './tests/data/enwik8_first100', '--vocab', 'gpt2', '--tokenizer-type', 'HFGPT2Tokenizer', '--merge-file', './data/gpt2-merges.txt', '--append-eod']\"}\n",
      "args = Namespace(input='./tests/data/enwik8_first100.txt', jsonl_keys=['text'], num_docs=None, tokenizer_type='HFGPT2Tokenizer', vocab_file='gpt2', merge_file='./data/gpt2-merges.txt', append_eod=True, ftfy=False, output_prefix='./tests/data/enwik8_first100', dataset_impl='mmap', workers=1, log_interval=100)\n",
      "def __handle_content_url(content_url):\n",
      "        content_url = replace_html(content_url)\n",
      "        return ('http://mp.weixin.qq.com{}'.format(\n",
      "            content_url) if 'http://mp.weixin.qq.com' not in content_url else content_url) if content_url else ''\n",
      "\n",
      "__handle_content_url(content_url='/s?timestamp=1500903767&amp;src=3&amp;ver=1&amp;signature=X4l0IQ091w0DY2ERU7fD*h0VUwBxeHPOJH-Uk-vAfaPamMl6ij7fqAIHomnXQ2X2*2J94H0pixVjsjEkL0TbILtKInZ4hqPp3-lC1nQZcN9Fd*BGbTQp7WlZyzLvCXy0Z8yFVF*lIDlo75pemv7kW8wov4Hz5-uiVzBT5q*Nwaw=')\n",
      "content_url = replace_html(content_url)\n",
      "{'content_url': \"'/s?timestamp=1500903767&amp;src=3&amp;ver=1&amp;signature=X4l0IQ091w0DY2ERU7fD*h0VUwBxeHPOJH-Uk-vAfaPamMl6ij7fqAIHomnXQ2X2*2J94H0pixVjsjEkL0TbILtKInZ4hqPp3-lC1nQZcN9Fd*BGbTQp7WlZyzLvCXy0Z8yFVF*lIDlo75pemv7kW8wov4Hz5-uiVzBT5q*Nwaw='\"}\n",
      "content_url = '/s?timestamp=1500903767&src=3&ver=1&signature=X4l0IQ091w0DY2ERU7fD*h0VUwBxeHPOJH-Uk-vAfaPamMl6ij7fqAIHomnXQ2X2*2J94H0pixVjsjEkL0TbILtKInZ4hqPp3-lC1nQZcN9Fd*BGbTQp7WlZyzLvCXy0Z8yFVF*lIDlo75pemv7kW8wov4Hz5-uiVzBT5q*Nwaw='\n",
      "def glob_absolute_paths(file: Union[str, Path]) -> List[Path]:\n",
      "            path = Path(file)\n",
      "            if not path.is_absolute():\n",
      "                path = base / path\n",
      "            return sorted(path.parent.glob(path.name), key=lambda p: p.stem)\n",
      "\n",
      "glob_absolute_paths(file='aggrid.js', base=PosixPath('/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/zauberzeug+nicegui/zauberzeug+nicegui/nicegui/elements'))\n",
      "path = Path(file)\n",
      "{'file': \"'aggrid.js'\"}\n",
      "path = PosixPath('aggrid.js')\n",
      "def compute_key(path: Path) -> str:\n",
      "    \"\"\"Compute a key for a given path using a hash function.\n",
      "\n",
      "    If the path is relative to the NiceGUI base directory, the key is computed from the relative path.\n",
      "    \"\"\"\n",
      "    nicegui_base = Path(__file__).parent\n",
      "    is_file = path.is_file()\n",
      "    try:\n",
      "        path = path.relative_to(nicegui_base)\n",
      "    except ValueError:\n",
      "        pass\n",
      "    if is_file:\n",
      "        return f'{hash_file_path(path.parent)}/{path.name}'\n",
      "    return f'{hash_file_path(path)}'\n",
      "\n",
      "compute_key(path=PosixPath('/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/zauberzeug+nicegui/zauberzeug+nicegui/nicegui/elements/aggrid.js'))\n",
      "path = path.relative_to(nicegui_base)\n",
      "{'nicegui_base': \"PosixPath('/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/zauberzeug+nicegui/zauberzeug+nicegui/nicegui')\"}\n",
      "path = PosixPath('elements/aggrid.js')\n",
      "def show_android_class_methods(args: list = None) -> None:\n",
      "    \"\"\"\n",
      "        Shows the methods available on an Android class.\n",
      "\n",
      "        :param args:\n",
      "        :return:\n",
      "    \"\"\"\n",
      "\n",
      "    if len(clean_argument_flags(args)) <= 0:\n",
      "        click.secho('Usage: android hooking list class_methods <class name>', bold=True)\n",
      "        return\n",
      "\n",
      "    class_name = args[0]\n",
      "\n",
      "    api = state_connection.get_api()\n",
      "    methods = api.android_hooking_get_class_methods(class_name)\n",
      "\n",
      "    # print the enumerated classes\n",
      "    for class_name in sorted(methods):\n",
      "        click.secho(class_name)\n",
      "\n",
      "    click.secho('\\nFound {0} method(s)'.format(len(methods)), bold=True)\n",
      "\n",
      "show_android_class_methods(args=['com.foo.bar'])\n",
      "methods = api.android_hooking_get_class_methods(class_name)\n",
      "{'class_name': \"'com.foo.bar'\"}\n",
      "methods = ['foo', 'bar', 'baz']\n",
      "def show_ios_class_methods(args: list) -> None:\n",
      "    \"\"\"\n",
      "        Displays the methods available in a class.\n",
      "\n",
      "        :param args:\n",
      "        :return:\n",
      "    \"\"\"\n",
      "\n",
      "    if len(clean_argument_flags(args)) <= 0:\n",
      "        click.secho('Usage: ios hooking list class_methods <class name> (--include-parents)', bold=True)\n",
      "        return\n",
      "\n",
      "    classname = args[0]\n",
      "\n",
      "    api = state_connection.get_api()\n",
      "    methods = api.ios_hooking_get_class_methods(classname, _should_include_parent_methods(args))\n",
      "\n",
      "    if len(methods) > 0:\n",
      "\n",
      "        # dump the methods to screen\n",
      "        for method in methods:\n",
      "            click.secho(method)\n",
      "\n",
      "        click.secho('\\nFound {0} methods'.format(len(methods)), bold=True)\n",
      "\n",
      "    else:\n",
      "        click.secho('No class / methods found')\n",
      "\n",
      "show_ios_class_methods(args=['TEKeychainManager'])\n",
      "methods = api.ios_hooking_get_class_methods(classname, _should_include_parent_methods(args))\n",
      "{'classname': \"'TEKeychainManager'\"}\n",
      "methods = ['foo', 'bar']\n",
      "def cat(args: list = None) -> None:\n",
      "    \"\"\"\n",
      "        Parses a plist on an iOS device and echoes it in a more human\n",
      "        readable way.\n",
      "\n",
      "        :param args:\n",
      "        :return:\n",
      "    \"\"\"\n",
      "\n",
      "    if len(args) <= 0:\n",
      "        click.secho('Usage: ios plist cat <remote_plist>', bold=True)\n",
      "        return\n",
      "\n",
      "    plist = args[0]\n",
      "\n",
      "    if not os.path.isabs(plist):\n",
      "        pwd = filemanager.pwd()\n",
      "        plist = device_state.platform.path_separator.join([pwd, plist])\n",
      "\n",
      "    api = state_connection.get_api()\n",
      "    plist_data = api.ios_plist_read(plist)\n",
      "\n",
      "    click.secho(plist_data, bold=True)\n",
      "\n",
      "cat(args=['/foo'])\n",
      "plist_data = api.ios_plist_read(plist)\n",
      "{'plist': \"'/foo'\"}\n",
      "plist_data = 'foo'\n",
      "def save(args: list) -> None:\n",
      "    \"\"\"\n",
      "        Save the current sessions command history to a file.\n",
      "\n",
      "        :param args:\n",
      "        :return:\n",
      "    \"\"\"\n",
      "\n",
      "    if len(args) <= 0:\n",
      "        click.secho('Usage: commands save <local destination>', bold=True)\n",
      "        return\n",
      "\n",
      "    destination = os.path.expanduser(args[0]) if args[0].startswith('~') else args[0]\n",
      "\n",
      "    with open(destination, 'w') as f:\n",
      "        for command in app_state.successful_commands:\n",
      "            f.write('{0}\\n'.format(command))\n",
      "\n",
      "    click.secho('Saved commands to: {0}'.format(destination), fg='green')\n",
      "\n",
      "save(args=['foo.rc'])\n",
      "with open(destination, 'w') as f:\n",
      "{'destination': \"'foo.rc'\"}\n",
      "f = <MagicMock name='open().__enter__()' id='140041834114064'>\n",
      "def cd(args: list) -> None:\n",
      "    \"\"\"\n",
      "        Change the current working directory of the device.\n",
      "\n",
      "        While this method does not actually change any directories,\n",
      "        it simply updates the value in the file_manager_state property\n",
      "        that keeps record of the current directory.\n",
      "\n",
      "        Before changing directories though, some checks are performed\n",
      "        on the device to at least ensure that the destination directory\n",
      "        exists.\n",
      "\n",
      "        :param args:\n",
      "        :return:\n",
      "    \"\"\"\n",
      "\n",
      "    if len(args) <= 0:\n",
      "        click.secho('Usage: cd <destination directory>', bold=True)\n",
      "        return\n",
      "\n",
      "    path = args[0]\n",
      "    current_dir = pwd()\n",
      "\n",
      "    # nothing to do\n",
      "    if path == '.':\n",
      "        return\n",
      "\n",
      "    # moving one directory back\n",
      "    if path == '..':\n",
      "\n",
      "        split_path = os.path.split(current_dir)\n",
      "\n",
      "        # nothing to do if we are already at root\n",
      "        if len(split_path) == 1:\n",
      "            return\n",
      "\n",
      "        new_path = ''.join(split_path[:-1])\n",
      "        click.secho(new_path, fg='green', bold=True)\n",
      "\n",
      "        file_manager_state.cwd = new_path\n",
      "\n",
      "        return\n",
      "\n",
      "    # if we got an absolute path, check if the path\n",
      "    # actually exists, and then cd to it if we can\n",
      "    if os.path.isabs(path):\n",
      "\n",
      "        # assume the path does not exist by default\n",
      "        does_exist = False\n",
      "\n",
      "        # check for existence based on the runtime\n",
      "        if device_state.platform == Ios:\n",
      "            does_exist = _path_exists_ios(path)\n",
      "\n",
      "        if device_state.platform == Android:\n",
      "            does_exist = _path_exists_android(path)\n",
      "\n",
      "        # if we checked with the device that the path exists\n",
      "        # and it did, update the state manager, otherwise\n",
      "        # show an error that the path may be invalid\n",
      "        if does_exist:\n",
      "            click.secho(path, fg='green', bold=True)\n",
      "\n",
      "            file_manager_state.cwd = path\n",
      "            return\n",
      "\n",
      "        else:\n",
      "            click.secho('Invalid path: `{0}`'.format(path), fg='red')\n",
      "\n",
      "    # directory is not absolute, tack it on at the end and\n",
      "    # see if its legit.\n",
      "    else:\n",
      "\n",
      "        proposed_path = device_state.platform.path_separator.join([current_dir, path])\n",
      "\n",
      "        # assume the proposed_path does not exist by default\n",
      "        does_exist = False\n",
      "\n",
      "        # check for existence based on the runtime\n",
      "        if device_state.platform == Ios:\n",
      "            does_exist = _path_exists_ios(proposed_path)\n",
      "\n",
      "        if device_state.platform == Android:\n",
      "            does_exist = _path_exists_android(proposed_path)\n",
      "\n",
      "        # if we checked with the device that the path exists\n",
      "        # and it did, update the state manager, otherwise\n",
      "        # show an error that the path may be invalid\n",
      "        if does_exist:\n",
      "            click.secho(proposed_path, fg='green', bold=True)\n",
      "\n",
      "            file_manager_state.cwd = proposed_path\n",
      "            return\n",
      "\n",
      "        else:\n",
      "            click.secho('Invalid path: `{0}`'.format(proposed_path), fg='red')\n",
      "\n",
      "cd(args=['/foo/bar/baz'])\n",
      "does_exist = _path_exists_android(path)\n",
      "{'path': \"'/foo/bar/baz'\"}\n",
      "does_exist = True\n",
      "def dump_all(args: list) -> None:\n",
      "    \"\"\"\n",
      "        Dump memory from the currently injected process.\n",
      "        Loosely based on:\n",
      "            https://github.com/Nightbringer21/fridump\n",
      "\n",
      "        :param args:\n",
      "        :return:\n",
      "    \"\"\"\n",
      "\n",
      "    if len(clean_argument_flags(args)) <= 0:\n",
      "        click.secho('Usage: memory dump all <local destination>', bold=True)\n",
      "        return\n",
      "\n",
      "    # the destination file to write the dump to\n",
      "    destination = args[0]\n",
      "\n",
      "    # Check for file override\n",
      "    if os.path.exists(destination):\n",
      "        click.secho('Destination file {dest} already exists'.format(dest=destination), fg='yellow', bold=True)\n",
      "        if not click.confirm('Continue, appending to the file?'):\n",
      "            return\n",
      "\n",
      "    # access type used when enumerating ranges\n",
      "    access = 'rw-'\n",
      "\n",
      "    api = state_connection.get_api()\n",
      "    ranges = api.memory_list_ranges(access)\n",
      "\n",
      "    total_size = sum([x['size'] for x in ranges])\n",
      "    click.secho('Will dump {0} {1} images, totalling {2}'.format(\n",
      "        len(ranges), access, sizeof_fmt(total_size)), fg='green', dim=True)\n",
      "\n",
      "    with click.progressbar(ranges) as bar:\n",
      "        for image in bar:\n",
      "            dump = bytearray()\n",
      "            bar.label = 'Dumping {0} from base: {1}'.format(sizeof_fmt(image['size']), hex(int(image['base'], 16)))\n",
      "\n",
      "            # catch and exception thrown while dumping.\n",
      "            # this could for a few reasons like if the protection\n",
      "            # changes or the range is reallocated\n",
      "            try:\n",
      "                # grab the (size) bytes starting at the (base_address) in chunks of BLOCK_SIZE\n",
      "                chunks = _get_chunks(int(image['base'], 16), int(image['size']), BLOCK_SIZE)\n",
      "                for chunk in chunks:\n",
      "                    dump.extend(bytearray(api.memory_dump(chunk[0], chunk[1])))\n",
      "\n",
      "            except Exception as e:\n",
      "                continue\n",
      "\n",
      "            # append the results to the destination file\n",
      "            with open(destination, 'ab') as f:\n",
      "                f.write(dump)\n",
      "\n",
      "    click.secho('Memory dumped to file: {0}'.format(destination), fg='green')\n",
      "\n",
      "dump_all(args=['/foo'])\n",
      "ranges = api.memory_list_ranges(access)\n",
      "{'access': \"'rw-'\"}\n",
      "ranges = [{'size': 100, 'base': '0x7fff90800000'}]\n",
      "def dump_from_base(args: list) -> None:\n",
      "    \"\"\"\n",
      "        Dump memory from a base address for a specific size to file\n",
      "\n",
      "        :param args:\n",
      "        :return:\n",
      "    \"\"\"\n",
      "\n",
      "    if len(clean_argument_flags(args)) < 3:\n",
      "        click.secho('Usage: memory dump from_base <base_address> <size_to_dump> <local_destination>', bold=True)\n",
      "        return\n",
      "\n",
      "    # the destination file to write the dump to\n",
      "    base_address = args[0]\n",
      "    memory_size = args[1]\n",
      "    destination = args[2]\n",
      "\n",
      "    # Check for file override\n",
      "    if os.path.exists(destination):\n",
      "        click.secho('Destination file {dest} already exists'.format(dest=destination), fg='yellow', bold=True)\n",
      "        if not click.confirm('Override?'):\n",
      "            return\n",
      "\n",
      "    click.secho('Dumping {0} from {1} to {2}'.format(sizeof_fmt(int(memory_size)), base_address, destination),\n",
      "                fg='green', dim=True)\n",
      "\n",
      "    api = state_connection.get_api()\n",
      "\n",
      "    # iirc, if you don't cast the return type to a bytearray it uses the sizeof(int) per cell, which is massive\n",
      "    dump = bytearray()\n",
      "    chunks = _get_chunks(int(base_address, 16), int(memory_size), BLOCK_SIZE)\n",
      "    for chunk in chunks:\n",
      "        dump.extend(bytearray(api.memory_dump(chunk[0], chunk[1])))\n",
      "\n",
      "    # append the results to the destination file\n",
      "    with open(destination, 'wb') as f:\n",
      "        f.write(dump)\n",
      "\n",
      "    click.secho('Memory dumped to file: {0}'.format(destination), fg='green')\n",
      "\n",
      "dump_from_base(args=['0x00008000', '200', '/foo'])\n",
      "chunks = _get_chunks(int(base_address, 16), int(memory_size), BLOCK_SIZE)\n",
      "{'base_address': \"'0x00008000'\"}\n",
      "chunks = [(32768, 200)]\n",
      "def ios_screenshot(args: list = None) -> None:\n",
      "    \"\"\"\n",
      "        Take an iOS screenshot.\n",
      "\n",
      "        :param args:\n",
      "        :return:\n",
      "    \"\"\"\n",
      "\n",
      "    if len(args) <= 0:\n",
      "        click.secho('Usage: ios ui screenshot <local png destination>', bold=True)\n",
      "        return\n",
      "\n",
      "    destination = args[0]\n",
      "\n",
      "    if not destination.endswith('.png'):\n",
      "        destination = destination + '.png'\n",
      "\n",
      "    api = state_connection.get_api()\n",
      "    png = api.ios_ui_screenshot()\n",
      "\n",
      "    with open(destination, 'wb') as f:\n",
      "        f.write(png)\n",
      "\n",
      "    click.secho('Screenshot saved to: {0}'.format(destination), fg='green')\n",
      "\n",
      "ios_screenshot(args=['foo'])\n",
      "with open(destination, 'wb') as f:\n",
      "{'destination': \"'foo.png'\"}\n",
      "f = <MagicMock name='open().__enter__()' id='140041828431664'>\n",
      "def aes(text, key):\n",
      "    pad = 16 - len(text) % 16\n",
      "    text = text + bytearray([pad] * pad)\n",
      "    encryptor = AES.new(key, 2, b\"0102030405060708\")\n",
      "    ciphertext = encryptor.encrypt(text)\n",
      "    return base64.b64encode(ciphertext)\n",
      "\n",
      "aes(text=b'{\"ids\": [347230, 496619464, 405998841, 28012031], \"br\": 320000, \"csrf_token\": \"\"}', key=b'0CoJUm6Qyw8W8jud')\n",
      "pad = 16 - len(text) % 16\n",
      "{'text': 'b\\'{\"ids\": [347230, 496619464, 405998841, 28012031], \"br\": 320000, \"csrf_token\": \"\"}\\''}\n",
      "pad = 15\n",
      "def rsa(text, pubkey, modulus):\n",
      "    text = text[::-1]\n",
      "    rs = pow(int(binascii.hexlify(text), 16), int(pubkey, 16), int(modulus, 16))\n",
      "    return format(rs, \"x\").zfill(256)\n",
      "\n",
      "rsa(text=b'8f0f5370d2e586d8', pubkey='010001', modulus='00e0b509f6259df8642dbc35662901477df22677ec152b5ff68ace615bb7b725152b3ab17a876aea8a5aa76d2e417629ec4ee341f56135fccf695280104e0312ecbda92557c93870114af6c9d05c4f7f0c3685b7a46bee255932575cce10b424d813cfe4875d3e82047b97ddef52741d546b8e289dc6935b3ece0462db0a22b8e7')\n",
      "rs = pow(int(binascii.hexlify(text), 16), int(pubkey, 16), int(modulus, 16))\n",
      "{'pubkey': \"'010001'\"}\n",
      "rs = 41289025236364532763659893484694654271957746525301846225496173810914971221673940219469794803075175772641409601769538690249948382654879184221263545282891541468281379611086448019401240456345998524768441427254251530526498242614378789439179129682525095499524622736788988351523341804681052326293260583730641212123\n",
      "def __setitem__(self, key, val):\n",
      "        if key.startswith('path.') and not val.startswith('/'):\n",
      "            val = self._absolute_path(val)\n",
      "        dict.__setitem__(self, key, val)\n",
      "\n",
      "__setitem__(self={}, key='path.workdir', val='/home/XXX/.cheat.sh')\n",
      "dict.__setitem__(self, key, val)\n",
      "{'self': '{}', 'key': \"'path.workdir'\", 'val': \"'/home/XXX/.cheat.sh'\"}\n",
      "self['path.workdir'] = '/home/XXX/.cheat.sh'\n",
      "def _load_config_from_file(default_config, filename):\n",
      "    import yaml\n",
      "\n",
      "    update = {}\n",
      "    if not os.path.exists(filename):\n",
      "        return update\n",
      "\n",
      "    with open(filename) as f:\n",
      "        newconfig = yaml.load(f.read(), Loader=yaml.SafeLoader)\n",
      "    for key, val in default_config.items():\n",
      "        newval = _get_nested(newconfig, key)\n",
      "        if newval is None:\n",
      "            continue\n",
      "\n",
      "        if isinstance(val, int):\n",
      "            try:\n",
      "                newval = int(newval)\n",
      "            except (ValueError, TypeError):\n",
      "                continue\n",
      "\n",
      "        update[key] = newval\n",
      "\n",
      "    return update\n",
      "\n",
      "_load_config_from_file(default_config={'adapters.active': ['tldr', 'cheat', 'fosdem', 'translation', 'rosetta', 'late.nz', 'question', 'cheat.sheets', 'cheat.sheets dir', 'learnxiny', 'rfc', 'oeis', 'chmod'], 'adapters.mandatory': ['search'], 'cache.redis.db': 0, 'cache.redis.host': 'localhost', 'cache.redis.port': 6379, 'cache.redis.prefix': '', 'cache.type': 'redis', 'frontend.styles': ['abap', 'algol', 'algol_nu', 'arduino', 'autumn', 'borland', 'bw', 'colorful', 'default', 'dracula', 'emacs', 'friendly', 'friendly_grayscale', 'fruity', 'github-dark', 'gruvbox-dark', 'gruvbox-light', 'igor', 'inkpot', 'lightbulb', 'lilypond', 'lovelace', 'manni', 'material', 'monokai', 'murphy', 'native', 'nord', 'nord-darker', 'one-dark', 'paraiso-dark', 'paraiso-light', 'pastie', 'perldoc', 'rainbow_dash', 'rrt', 'sas', 'solarized-dark', 'solarized-light', 'staroffice', 'stata-dark', 'stata-light', 'tango', 'trac', 'vim', 'vs', 'xcode', 'zenburn'], 'log.level': 4, 'path.internal.ansi2html': '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/chubin+cheat.sh/chubin+cheat.sh/share/ansi2html.sh', 'path.internal.bin': '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/chubin+cheat.sh/chubin+cheat.sh/bin', 'path.internal.bin.upstream': '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/chubin+cheat.sh/chubin+cheat.sh/bin/upstream', 'path.internal.malformed': '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/chubin+cheat.sh/chubin+cheat.sh/share/static/malformed-response.html', 'path.internal.pages': '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/chubin+cheat.sh/chubin+cheat.sh/share', 'path.internal.static': '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/chubin+cheat.sh/chubin+cheat.sh/share/static', 'path.internal.templates': '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/chubin+cheat.sh/chubin+cheat.sh/share/templates', 'path.internal.vim': '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/chubin+cheat.sh/chubin+cheat.sh/share/vim', 'path.log.main': 'log/main.log', 'path.log.queries': 'log/queries.log', 'path.log.fetch': 'log/fetch.log', 'path.repositories': 'upstream', 'path.spool': 'spool', 'path.workdir': '/home/XXX/.cheat.sh', 'routing.pre': [('^$', 'search'), ('^[^/]*/rosetta(/|$)', 'rosetta'), ('^rfc/', 'rfc'), ('^oeis/', 'oeis'), ('^chmod/', 'chmod'), ('^:', 'internal'), ('/:list$', 'internal'), ('/$', 'cheat.sheets dir')], 'routing.main': [('', 'cheat.sheets'), ('', 'cheat'), ('', 'tldr'), ('', 'late.nz'), ('', 'fosdem'), ('', 'learnxiny')], 'routing.post': [('^[^/ +]*$', 'unknown'), ('^[a-z][a-z]-[a-z][a-z]$', 'translation')], 'routing.default': 'question', 'upstream.url': 'https://cheat.sh', 'upstream.timeout': 5, 'search.limit': 20, 'server.bind': '0.0.0.0', 'server.port': 8002}, filename='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/chubin+cheat.sh/chubin+cheat.sh/etc/config.yaml')\n",
      "with open(filename) as f:\n",
      "{'filename': \"'/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/chubin+cheat.sh/chubin+cheat.sh/etc/config.yaml'\"}\n",
      "f = <_io.TextIOWrapper name='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/chubin+cheat.sh/chubin+cheat.sh/etc/config.yaml' mode='r' encoding='UTF-8'>\n",
      "def test_base_model_can_be_adapter_v2_loaded(name):\n",
      "    from lit_gpt.adapter_v2 import GPT as AdapterV2GPT\n",
      "    from lit_gpt.adapter_v2 import adapter_filter\n",
      "    from lit_gpt.model import GPT as BaseGPT\n",
      "\n",
      "    kwargs = {\"n_layer\": 2, \"n_head\": 8, \"n_embd\": 16, \"padded_vocab_size\": 32}\n",
      "    base_model = BaseGPT.from_name(name, **kwargs)\n",
      "    base_model_state_dict = base_model.state_dict()\n",
      "    lora_model = AdapterV2GPT.from_name(name, **kwargs, adapter_start_layer=0)\n",
      "    keys = lora_model.load_state_dict(base_model_state_dict, strict=False)\n",
      "    assert not keys.unexpected_keys\n",
      "    for k in keys.missing_keys:\n",
      "        assert adapter_filter(k, None)\n",
      "\n",
      "test_base_model_can_be_adapter_v2_loaded(name='stablelm-base-alpha-3b')\n",
      "base_model = BaseGPT.from_name(name, **kwargs)\n",
      "{'name': \"'stablelm-base-alpha-3b'\"}\n",
      "base_model = GPT(  (lm_head): Linear(in_features=16, out_features=32, bias=False)  (transformer): ModuleDict(    (wte): Embedding(32, 16)    (h): ModuleList(      (0-1): 2 x Block(        (norm_1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)        (attn): CausalSelfAttention(          (attn): Linear(in_features=16, out_features=48, bias=True)          (proj): Linear(in_features=16, out_features=16, bias=True)        )        (norm_2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)        (mlp): GptNeoxMLP(          (fc): Linear(in_features=16, out_features=64, bias=True)          (proj): Linear(in_features=64, out_features=16, bias=True)        )      )    )    (ln_f): LayerNorm((16,), eps=1e-05, elementwise_affine=True)  ))\n",
      "def layer_template(layer_name: str, idx: int) -> Tuple[str, int]:\n",
      "    split = layer_name.split(\".\")\n",
      "    number = int(split[idx])\n",
      "    split[idx] = \"{}\"\n",
      "    from_name = \".\".join(split)\n",
      "    return from_name, number\n",
      "\n",
      "layer_template(layer_name='model.layers.0.self_attn.q_proj.weight', idx=2)\n",
      "from_name = \".\".join(split)\n",
      "{'split': \"['model', 'layers', '{}', 'self_attn', 'q_proj', 'weight']\"}\n",
      "from_name = 'model.layers.{}.self_attn.q_proj.weight'\n",
      "def test_generate(monkeypatch, generated, stop_tokens, expected):\n",
      "    import chat.base as chat\n",
      "    import generate.base as generate\n",
      "\n",
      "    input_idx = torch.tensor([5, 3])\n",
      "    max_returned_tokens = len(input_idx) + 8\n",
      "    model = MagicMock()\n",
      "    model.config.block_size = 100\n",
      "    model.max_seq_length = 100\n",
      "    it = iter(generated)\n",
      "\n",
      "    def multinomial(*_, **__):\n",
      "        out = next(it)\n",
      "        return torch.tensor([out])\n",
      "\n",
      "    monkeypatch.setattr(generate, \"multinomial_num_samples_1\", multinomial)\n",
      "    actual = chat.generate(model, input_idx, max_returned_tokens, stop_tokens=stop_tokens)\n",
      "    actual = list(actual)\n",
      "\n",
      "    assert len(actual) == len(expected)\n",
      "    if not actual:\n",
      "        assert actual == expected\n",
      "    else:\n",
      "        for t in actual:\n",
      "            assert t.dtype == torch.long\n",
      "        assert torch.cat(actual).tolist() == expected\n",
      "\n",
      "test_generate(monkeypatch={_setattr=[], _setitem=[], _cwd=None, _savesyspath=None}, generated=repeat(1), stop_tokens=(), expected=[1, 1, 1, 1, 1, 1, 1, 1])\n",
      "assert torch.cat(actual).tolist() == expected\n",
      "{'actual': '[tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1]), tensor([1])]'}\n",
      "@py_assert6 = None\n",
      "def rowcol_to_a1(row, col):\n",
      "    \"\"\"Translates a row and column cell address to A1 notation.\n",
      "\n",
      "    :param row: The row of the cell to be converted.\n",
      "        Rows start at index 1.\n",
      "    :type row: int, str\n",
      "\n",
      "    :param col: The column of the cell to be converted.\n",
      "        Columns start at index 1.\n",
      "    :type row: int, str\n",
      "\n",
      "    :returns: a string containing the cell's coordinates in A1 notation.\n",
      "\n",
      "    Example:\n",
      "\n",
      "    >>> rowcol_to_a1(1, 1)\n",
      "    A1\n",
      "\n",
      "    \"\"\"\n",
      "    row = int(row)\n",
      "    col = int(col)\n",
      "\n",
      "    if row < 1 or col < 1:\n",
      "        raise IncorrectCellLabel(\"({}, {})\".format(row, col))\n",
      "\n",
      "    div = col\n",
      "    column_label = \"\"\n",
      "\n",
      "    while div:\n",
      "        (div, mod) = divmod(div, 26)\n",
      "        if mod == 0:\n",
      "            mod = 26\n",
      "            div -= 1\n",
      "        column_label = chr(mod + MAGIC_NUMBER) + column_label\n",
      "\n",
      "    label = \"{}{}\".format(column_label, row)\n",
      "\n",
      "    return label\n",
      "\n",
      "rowcol_to_a1(row=4, col=4)\n",
      "(div, mod) = divmod(div, 26)\n",
      "{'div': '4'}\n",
      "mod = 4\n",
      "def a1_to_rowcol(label):\n",
      "    \"\"\"Translates a cell's address in A1 notation to a tuple of integers.\n",
      "\n",
      "    :param str label: A cell label in A1 notation, e.g. 'B1'.\n",
      "        Letter case is ignored.\n",
      "    :returns: a tuple containing `row` and `column` numbers. Both indexed\n",
      "              from 1 (one).\n",
      "    :rtype: tuple\n",
      "\n",
      "    Example:\n",
      "\n",
      "    >>> a1_to_rowcol('A1')\n",
      "    (1, 1)\n",
      "\n",
      "    \"\"\"\n",
      "    m = CELL_ADDR_RE.match(label)\n",
      "    if m:\n",
      "        column_label = m.group(1).upper()\n",
      "        row = int(m.group(2))\n",
      "\n",
      "        col = 0\n",
      "        for i, c in enumerate(reversed(column_label)):\n",
      "            col += (ord(c) - MAGIC_NUMBER) * (26**i)\n",
      "    else:\n",
      "        raise IncorrectCellLabel(label)\n",
      "\n",
      "    return (row, col)\n",
      "\n",
      "a1_to_rowcol(label='B1')\n",
      "m = CELL_ADDR_RE.match(label)\n",
      "{'label': \"'B1'\"}\n",
      "m = <re.Match object; span=(0, 2), match='B1'>\n",
      "def _a1_to_rowcol_unbounded(label):\n",
      "    \"\"\"Translates a cell's address in A1 notation to a tuple of integers.\n",
      "\n",
      "    Same as `a1_to_rowcol()` but allows for missing row or column part\n",
      "    (e.g. \"A\" for the first column)\n",
      "\n",
      "    :returns: a tuple containing `row` and `column` numbers. Both indexed\n",
      "        from 1 (one).\n",
      "    :rtype: tuple\n",
      "\n",
      "    Example:\n",
      "\n",
      "    >>> _a1_to_rowcol_unbounded('A1')\n",
      "    (1, 1)\n",
      "\n",
      "    >>> _a1_to_rowcol_unbounded('A')\n",
      "    (inf, 1)\n",
      "\n",
      "    >>> _a1_to_rowcol_unbounded('1')\n",
      "    (1, inf)\n",
      "\n",
      "    >>> _a1_to_rowcol_unbounded('ABC123')\n",
      "    (123, 731)\n",
      "\n",
      "    >>> _a1_to_rowcol_unbounded('ABC')\n",
      "    (inf, 731)\n",
      "\n",
      "    >>> _a1_to_rowcol_unbounded('123')\n",
      "    (123, inf)\n",
      "\n",
      "    >>> _a1_to_rowcol_unbounded('1A')\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    gspread.exceptions.IncorrectCellLabel: 1A\n",
      "\n",
      "    >>> _a1_to_rowcol_unbounded('')\n",
      "    (inf, inf)\n",
      "\n",
      "    \"\"\"\n",
      "    m = A1_ADDR_ROW_COL_RE.match(label)\n",
      "    if m:\n",
      "        column_label, row = m.groups()\n",
      "\n",
      "        if column_label:\n",
      "            col = 0\n",
      "            for i, c in enumerate(reversed(column_label.upper())):\n",
      "                col += (ord(c) - MAGIC_NUMBER) * (26**i)\n",
      "        else:\n",
      "            col = inf\n",
      "\n",
      "        if row:\n",
      "            row = int(row)\n",
      "        else:\n",
      "            row = inf\n",
      "    else:\n",
      "        raise IncorrectCellLabel(label)\n",
      "\n",
      "    return (row, col)\n",
      "\n",
      "_a1_to_rowcol_unbounded(label='A1')\n",
      "col += (ord(c) - MAGIC_NUMBER) * (26**i)\n",
      "{'c': \"'A'\"}\n",
      "col = 1\n",
      "def render_pep440_feature(pieces):\n",
      "    \"\"\"Build up version string, used within \"feature\" branch of repository.\n",
      "\n",
      "    Our goal: MERGE-POINT.post.devN+gHEX.BRANCH-NAME.M[.dirty]\n",
      "        +) MERGE-POINT = Most recent common ancestor for `develop` and `master`\n",
      "          *) Does not yet handle branch from `release-*`\n",
      "        +) N = DISTANCE from the MERGE-POINT of `develop` and `master`\n",
      "        +) M = DISTANCE from the MERGE-POINT of \"feature\" and `develop`\n",
      "\n",
      "    Exceptions:\n",
      "    1: no tags. 0.post.devDISTANCE+gHEX[.dirty]\n",
      "    \"\"\"\n",
      "    if pieces[\"closest-tag\"] and pieces[\"develop\"]:\n",
      "        rendered = pieces[\"closest-tag\"]\n",
      "        distance_to_develop = pieces[\"distance-to-develop\"]\n",
      "        distance_to_merge = pieces[\"distance-to-master\"]\n",
      "        distance_merge_to_tag = (pieces[\"distance\"] - distance_to_merge)\n",
      "        distance_dev_to_merge = (distance_to_merge - distance_to_develop)\n",
      "        if (distance_merge_to_tag > 0):\n",
      "            rendered += \".%d\" % distance_merge_to_tag\n",
      "        rendered += \".post.dev%d\" % distance_dev_to_merge\n",
      "        rendered += plus_or_dot(pieces)\n",
      "        rendered += \"g%s\" % pieces[\"short\"]\n",
      "        rendered += \".%s\" % pieces[\"branch\"]\n",
      "        rendered += \".%d\" % distance_to_develop\n",
      "    else:\n",
      "        # exception #1\n",
      "        rendered = \"0.post.dev%d\" % (pieces[\"distance\"] - 1)\n",
      "        rendered += plus_or_dot(pieces)\n",
      "        rendered += \"g%s\" % pieces[\"short\"]\n",
      "    if pieces[\"dirty\"]:\n",
      "        rendered += \".dirty\"\n",
      "    return rendered\n",
      "\n",
      "render_pep440_feature(pieces={'long': 'e60d005d389cb31b6e99f937e35adbe3fccb7aaf', 'short': 'e60d005', 'error': None, 'dirty': False, 'closest-tag': '0.8', 'distance': 3, 'branch': 'HEAD', 'distance-to-master': 0, 'develop': None, 'distance-to-develop': None, 'date': '2018-05-05T22:17:58-0700', 'authors': ['Padraic Shafer']})\n",
      "rendered += plus_or_dot(pieces)\n",
      "{'pieces': \"{'long': 'e60d005d389cb31b6e99f937e35adbe3fccb7aaf', 'short': 'e60d005', 'error': None, 'dirty': False, 'closest-tag': '0.8', 'distance': 3, 'branch': 'HEAD', 'distance-to-master': 0, 'develop': None, 'distance-to-develop': None, 'date': '2018-05-05T22:17:58-0700', 'authors': ['Padraic Shafer']}\"}\n",
      "rendered = '0.post.dev2+'\n",
      "def compute_loc(idx, shape):\n",
      "    loc = [0] * len(shape)\n",
      "    for i in range(len(shape)):\n",
      "        prod = int(np.prod(shape[i + 1:]))\n",
      "        loc[i] = idx // prod\n",
      "        idx = idx % prod\n",
      "    return tuple(loc)\n",
      "\n",
      "compute_loc(idx=0, shape=(2, 4))\n",
      "loc = [0] * len(shape)\n",
      "{'shape': '(2, 4)'}\n",
      "loc = [0, 0]\n",
      "def byteatoms(characterstring):\n",
      "            binary = characterstring.encode()\n",
      "            atoms.extend(binary[i:i + 1] for i in range(len(binary)))\n",
      "\n",
      "byteatoms(characterstring='xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx', atoms=[])\n",
      "atoms.extend(binary[i:i + 1] for i in range(len(binary)))\n",
      "{'binary': \"b'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\"}\n",
      "atoms = [b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x']\n",
      "def __build_func(verb, args, kwargs={}):\n",
      "        params = ['self']\n",
      "        params += ['%s' % stringcase.snakecase(k) for k in args]\n",
      "        params += ['%s=%s' % (stringcase.snakecase(k), v) for k, v in kwargs.items()]\n",
      "        largs = list(args) + list(kwargs.keys())\n",
      "        return eval(\n",
      "            'lambda %s: self._%s(%s)' % (\n",
      "                ','.join(params), verb, ','.join(['%s=%s' % (k, stringcase.snakecase(k)) for k in largs])\n",
      "            )\n",
      "        )\n",
      "\n",
      "__build_func(verb='get', args=[], kwargs={})\n",
      "largs = list(args) + list(kwargs.keys())\n",
      "{'args': '[]'}\n",
      "largs = []\n",
      "def run_validator_for_test_file(filename: str) -> List:\n",
      "    test_file_path = os.path.join(\n",
      "        os.path.dirname(os.path.abspath(__file__)),\n",
      "        'test_files',\n",
      "        filename,\n",
      "    )\n",
      "    with open(test_file_path, 'r') as file_handler:\n",
      "        raw_content = file_handler.read()\n",
      "    tree = ast.parse(raw_content)\n",
      "    checker = SuperMarionChecker(tree=tree, filename=test_file_path)\n",
      "\n",
      "    return list(checker.run())\n",
      "\n",
      "run_validator_for_test_file(filename='ok_pipe.py')\n",
      "with open(test_file_path, 'r') as file_handler:\n",
      "{'test_file_path': \"'/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/Melevir+flake8-super-mario/Melevir+flake8-super-mario/tests/test_files/ok_pipe.py'\"}\n",
      "file_handler = <_io.TextIOWrapper name='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/Melevir+flake8-super-mario/Melevir+flake8-super-mario/tests/test_files/ok_pipe.py' mode='r' encoding='UTF-8'>\n",
      "def append(self, item):\n",
      "        self._deque.append(item)\n",
      "        if self._clear_all_updates:\n",
      "            self._clear_all_updates = False\n",
      "            self._clear_updates_by_symbol.clear()\n",
      "            self._all_new_updates = 0\n",
      "            self._new_updates_by_symbol.clear()\n",
      "        if self._clear_updates_by_symbol.get(item['symbol']):\n",
      "            self._clear_updates_by_symbol[item['symbol']] = False\n",
      "            self._new_updates_by_symbol[item['symbol']] = 0\n",
      "        self._new_updates_by_symbol[item['symbol']] = self._new_updates_by_symbol.get(item['symbol'], 0) + 1\n",
      "        self._all_new_updates = (self._all_new_updates or 0) + 1\n",
      "\n",
      "append(self=[], item={'symbol': 'BTC/USDT', 'data': 1})\n",
      "self._deque.append(item)\n",
      "{'item': \"{'symbol': 'BTC/USDT', 'data': 1}\"}\n",
      "self[0] = {'symbol': 'BTC/USDT', 'data': 1}\n",
      "def bit_length(num):\n",
      "    # http://docs.python.org/dev/library/stdtypes.html#int.bit_length\n",
      "    s = bin(num)  # binary representation:  bin(-37) --> '-0b100101'\n",
      "    s = s.lstrip('-0b')  # remove leading zeros and minus sign\n",
      "    return len(s)\n",
      "\n",
      "bit_length(num=115792089237316195423570985008687907852837564279074904382605163141518161494337)\n",
      "s = bin(num)  # binary representation:  bin(-37) --> '-0b100101'\n",
      "{'num': '115792089237316195423570985008687907852837564279074904382605163141518161494337'}\n",
      "s = '0b1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111010111010101011101101110011100110101011110100100010100000001110111011111111010010010111101000110011010000001101100100000101000001'\n",
      "def bits2octets(data, order):\n",
      "    z1 = bits2int(data, bit_length(order))\n",
      "    z2 = z1 - order\n",
      "\n",
      "    if z2 < 0:\n",
      "        z2 = z1\n",
      "\n",
      "    return number_to_string_crop(z2, order)\n",
      "\n",
      "bits2octets(data=b'\\xa7?\\xcf3\\x96@\\x92\\x92\\x07(\\x1f\\xb8\\xe08\\x88H\\x06\\xe2\\xeb\\x08@\\xf2$V\\x94\\xdb\\xba\\x1d\\\\\\xc8\\x9ee', order=115792089237316195423570985008687907852837564279074904382605163141518161494337)\n",
      "z1 = bits2int(data, bit_length(order))\n",
      "{'order': '115792089237316195423570985008687907852837564279074904382605163141518161494337'}\n",
      "z1 = 75648987130760998095283026105289635390775519882394219481699348731002280779365\n",
      "def __str__(self):\n",
      "        self.reduce()\n",
      "        sign = '-' if self.integer < 0 else ''\n",
      "        integer_array = list(str(abs(self.integer)).rjust(self.decimals, '0'))\n",
      "        index = len(integer_array) - self.decimals\n",
      "        if index == 0:\n",
      "            item = '0.'\n",
      "        elif self.decimals < 0:\n",
      "            item = '0' * (-self.decimals)\n",
      "        elif self.decimals == 0:\n",
      "            item = ''\n",
      "        else:\n",
      "            item = '.'\n",
      "        integer_array.insert(index, item)\n",
      "        return sign + ''.join(integer_array)\n",
      "\n",
      "__str__(self=Precise(1393.938), self.base=10, self.decimals=3, self.integer=1393938)\n",
      "index = len(integer_array) - self.decimals\n",
      "{'integer_array': \"['1', '3', '9', '3', '9', '3', '8']\"}\n",
      "index = 4\n",
      "def div(self, other, precision=18):\n",
      "        distance = precision - self.decimals + other.decimals\n",
      "        if distance == 0:\n",
      "            numerator = self.integer\n",
      "        elif distance < 0:\n",
      "            exponent = self.base ** -distance\n",
      "            numerator = self.integer // exponent\n",
      "        else:\n",
      "            exponent = self.base ** distance\n",
      "            numerator = self.integer * exponent\n",
      "        result, mod = divmod(numerator, other.integer)\n",
      "        # python floors negative numbers down instead of truncating\n",
      "        # if mod is zero it will be floored to itself so we do not add one\n",
      "        result = result + 1 if result < 0 and mod else result\n",
      "        return Precise(result, precision)\n",
      "\n",
      "div(self=Precise(0.00000002), other=Precise(69696900000), precision=1, self.base=10, self.decimals=8, self.integer=2)\n",
      "result, mod = divmod(numerator, other.integer)\n",
      "{'numerator': '0'}\n",
      "result = 0\n",
      "def _import_plugins(plugin_names):\n",
      "    plugins = []\n",
      "    for name, path in plugin_names.items():\n",
      "        log.debug(\"Importing plugin %s: %s\", name, path)\n",
      "        if '.' not in path:\n",
      "            plugins.append(__import__(path))\n",
      "        else:\n",
      "            package, module = path.rsplit('.', 1)\n",
      "            module = __import__(path, fromlist=[module])\n",
      "            plugins.append(module)\n",
      "    return plugins\n",
      "\n",
      "_import_plugins(plugin_names={'__builtin__': 'awscli.handlers'})\n",
      "plugins.append(module)\n",
      "{'module': \"<module 'awscli.handlers' from '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/aws+aws-cli/aws+aws-cli/awscli/handlers.py'>\"}\n",
      "plugins = [<module 'awscli.handlers' from '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/aws+aws-cli/aws+aws-cli/awscli/handlers.py'>]\n",
      "def test_soft_jaccard_score(y_true, y_pred, expected, eps):\n",
      "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "    actual = F.soft_jaccard_score(y_pred, y_true, eps=eps)\n",
      "    assert float(actual) == pytest.approx(expected, eps)\n",
      "\n",
      "test_soft_jaccard_score(y_true=[1, 1, 1, 1], y_pred=[1, 1, 1, 1], expected=1.0, eps=1e-05)\n",
      "y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "{'y_pred': '[1, 1, 1, 1]'}\n",
      "y_pred = tensor([1., 1., 1., 1.])\n",
      "def test_soft_jaccard_score_2(y_true, y_pred, expected, eps):\n",
      "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "    actual = F.soft_jaccard_score(y_pred, y_true, dims=[1], eps=eps)\n",
      "    actual = actual.mean()\n",
      "    assert float(actual) == pytest.approx(expected, eps)\n",
      "\n",
      "test_soft_jaccard_score_2(y_true=[[1, 1, 0, 0], [0, 0, 1, 1]], y_pred=[[1, 1, 0, 0], [0, 0, 1, 1]], expected=1.0, eps=1e-05)\n",
      "assert float(actual) == pytest.approx(expected, eps)\n",
      "{'expected': '1.0', 'eps': '1e-05'}\n",
      "@py_assert2 = None\n",
      "def test_soft_dice_score(y_true, y_pred, expected, eps):\n",
      "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "    actual = F.soft_dice_score(y_pred, y_true, eps=eps)\n",
      "    assert float(actual) == pytest.approx(expected, eps)\n",
      "\n",
      "test_soft_dice_score(y_true=[1, 1, 1, 1], y_pred=[1, 1, 1, 1], expected=1.0, eps=1e-05)\n",
      "assert float(actual) == pytest.approx(expected, eps)\n",
      "{'actual': 'tensor(1.)'}\n",
      "@py_assert2 = None\n",
      "def test_soft_tversky_score(y_true, y_pred, expected, eps, alpha, beta):\n",
      "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "    actual = F.soft_tversky_score(y_pred, y_true, eps=eps, alpha=alpha, beta=beta)\n",
      "    assert float(actual) == pytest.approx(expected, eps)\n",
      "\n",
      "test_soft_tversky_score(y_true=[1, 1, 1, 1], y_pred=[1, 1, 1, 1], expected=1.0, eps=1e-05, alpha=0.5, beta=0.5)\n",
      "assert float(actual) == pytest.approx(expected, eps)\n",
      "{'expected': '1.0', 'eps': '1e-05'}\n",
      "@py_assert2 = None\n",
      "def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
      "\n",
      "        assert y_true.size(0) == y_pred.size(0)\n",
      "\n",
      "        if self.from_logits:\n",
      "            # Apply activations to get [0..1] class probabilities\n",
      "            # Using Log-Exp as this gives more numerically stable result and does not cause vanishing gradient on\n",
      "            # extreme values 0 and 1\n",
      "            if self.mode == MULTICLASS_MODE:\n",
      "                y_pred = y_pred.log_softmax(dim=1).exp()\n",
      "            else:\n",
      "                y_pred = F.logsigmoid(y_pred).exp()\n",
      "\n",
      "        bs = y_true.size(0)\n",
      "        num_classes = y_pred.size(1)\n",
      "        dims = (0, 2)\n",
      "\n",
      "        if self.mode == BINARY_MODE:\n",
      "            y_true = y_true.view(bs, 1, -1)\n",
      "            y_pred = y_pred.view(bs, 1, -1)\n",
      "\n",
      "            if self.ignore_index is not None:\n",
      "                mask = y_true != self.ignore_index\n",
      "                y_pred = y_pred * mask\n",
      "                y_true = y_true * mask\n",
      "\n",
      "        if self.mode == MULTICLASS_MODE:\n",
      "            y_true = y_true.view(bs, -1)\n",
      "            y_pred = y_pred.view(bs, num_classes, -1)\n",
      "\n",
      "            if self.ignore_index is not None:\n",
      "                mask = y_true != self.ignore_index\n",
      "                y_pred = y_pred * mask.unsqueeze(1)\n",
      "\n",
      "                y_true = F.one_hot((y_true * mask).to(torch.long), num_classes)  # N,H*W -> N,H*W, C\n",
      "                y_true = y_true.permute(0, 2, 1) * mask.unsqueeze(1)  # N, C, H*W\n",
      "            else:\n",
      "                y_true = F.one_hot(y_true, num_classes)  # N,H*W -> N,H*W, C\n",
      "                y_true = y_true.permute(0, 2, 1)  # N, C, H*W\n",
      "\n",
      "        if self.mode == MULTILABEL_MODE:\n",
      "            y_true = y_true.view(bs, num_classes, -1)\n",
      "            y_pred = y_pred.view(bs, num_classes, -1)\n",
      "\n",
      "            if self.ignore_index is not None:\n",
      "                mask = y_true != self.ignore_index\n",
      "                y_pred = y_pred * mask\n",
      "                y_true = y_true * mask\n",
      "\n",
      "        scores = self.compute_score(y_pred, y_true.type_as(y_pred), smooth=self.smooth, eps=self.eps, dims=dims)\n",
      "\n",
      "        if self.log_loss:\n",
      "            loss = -torch.log(scores.clamp_min(self.eps))\n",
      "        else:\n",
      "            loss = 1.0 - scores\n",
      "\n",
      "        # Dice loss is undefined for non-empty classes\n",
      "        # So we zero contribution of channel that does not have true pixels\n",
      "        # NOTE: A better workaround would be to use loss term `mean(y_pred)`\n",
      "        # for this case, however it will be a modified jaccard loss\n",
      "\n",
      "        mask = y_true.sum(dims) > 0\n",
      "        loss *= mask.to(loss.dtype)\n",
      "\n",
      "        if self.classes is not None:\n",
      "            loss = loss[self.classes]\n",
      "\n",
      "        return self.aggregate_loss(loss)\n",
      "\n",
      "forward(self=DiceLoss(), y_pred=tensor([[[[1., 1., 1.]]]]), y_true=tensor([[[[1, 1, 1]]]]), self._backward_hooks=OrderedDict(), self._backward_pre_hooks=OrderedDict(), self._buffers=OrderedDict(), self._forward_hooks=OrderedDict(), self._forward_hooks_always_called=OrderedDict(), self._forward_hooks_with_kwargs=OrderedDict(), self._forward_pre_hooks=OrderedDict(), self._forward_pre_hooks_with_kwargs=OrderedDict(), self._is_full_backward_hook=None, self._load_state_dict_post_hooks=OrderedDict(), self._load_state_dict_pre_hooks=OrderedDict(), self._modules=OrderedDict(), self._non_persistent_buffers_set=set(), self._parameters=OrderedDict(), self._state_dict_hooks=OrderedDict(), self._state_dict_pre_hooks=OrderedDict(), self.classes=None, self.eps=1e-07, self.from_logits=False, self.ignore_index=None, self.log_loss=False, self.mode='binary', self.reduction='mean', self.smooth=0.0, self.training=True)\n",
      "mask = y_true.sum(dims) > 0\n",
      "{'dims': '(0, 2)'}\n",
      "mask = tensor([True])\n",
      "def load_translation(self) -> \"I18N\":\n",
      "        \"\"\"Load translations from a JSON file based on the specified language.\"\"\"\n",
      "        try:\n",
      "            dir_path = os.path.dirname(os.path.realpath(__file__))\n",
      "            prompts_path = os.path.join(\n",
      "                dir_path, f\"../translations/{self.language}.json\"\n",
      "            )\n",
      "\n",
      "            with open(prompts_path, \"r\") as f:\n",
      "                self._translations = json.load(f)\n",
      "        except FileNotFoundError:\n",
      "            raise ValidationError(\n",
      "                f\"Trasnlation file for language '{self.language}' not found.\"\n",
      "            )\n",
      "        except json.JSONDecodeError:\n",
      "            raise ValidationError(f\"Error decoding JSON from the prompts file.\")\n",
      "        return self\n",
      "\n",
      "load_translation(self=I18N(language='en'), self.__dict__={'language': 'en'}, self.__pydantic_extra__=None, self.__pydantic_fields_set__=set(), self.__pydantic_private__={}, self.language='en')\n",
      "prompts_path = os.path.join(\n",
      "{'dir_path': \"'/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/joaomdmoura+crewAI/joaomdmoura+crewAI/src/crewai/utilities'\"}\n",
      "prompts_path = '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/joaomdmoura+crewAI/joaomdmoura+crewAI/src/crewai/utilities/../translations/en.json'\n",
      "def _to_str(size, suffixes, base):\n",
      "    # type: (SupportsInt, Iterable[Text], int) -> Text\n",
      "    try:\n",
      "        size = int(size)\n",
      "    except ValueError:\n",
      "        raise TypeError(\"filesize requires a numeric value, not {!r}\".format(size))\n",
      "    if size == 1:\n",
      "        return \"1 byte\"\n",
      "    elif size < base:\n",
      "        return \"{:,} bytes\".format(size)\n",
      "\n",
      "    for i, suffix in enumerate(suffixes, 2):\n",
      "        unit = base ** i\n",
      "        if size < unit:\n",
      "            break\n",
      "    return \"{:,.1f} {}\".format((base * size / unit), suffix)\n",
      "\n",
      "_to_str(size=1024, suffixes=('KiB', 'MiB', 'GiB', 'TiB', 'PiB', 'EiB', 'ZiB', 'YiB'), base=1024)\n",
      "for i, suffix in enumerate(suffixes, 2):\n",
      "{'suffixes': \"('KiB', 'MiB', 'GiB', 'TiB', 'PiB', 'EiB', 'ZiB', 'YiB')\"}\n",
      "i = 2\n",
      "def _parse_time(t):\n",
      "    t = \" \".join(token.strip() for token in t.lower().split(\" \"))\n",
      "    try:\n",
      "        try:\n",
      "            _t = time.strptime(t, \"%b %d %Y\")\n",
      "        except ValueError:\n",
      "            _t = time.strptime(t, \"%b %d %H:%M\")\n",
      "    except ValueError:\n",
      "        # Unknown time format\n",
      "        return None\n",
      "\n",
      "    year = _t.tm_year if _t.tm_year != 1900 else time.localtime().tm_year\n",
      "    month = _t.tm_mon\n",
      "    day = _t.tm_mday\n",
      "    hour = _t.tm_hour\n",
      "    minutes = _t.tm_min\n",
      "    dt = datetime.datetime(year, month, day, hour, minutes, tzinfo=UTC)\n",
      "\n",
      "    epoch_time = (dt - epoch_dt).total_seconds()\n",
      "    return epoch_time\n",
      "\n",
      "_parse_time(t='Jan 18  2006')\n",
      "dt = datetime.datetime(year, month, day, hour, minutes, tzinfo=UTC)\n",
      "{'year': '2006', 'month': '1', 'day': '18', 'hour': '0', 'minutes': '0'}\n",
      "dt = datetime.datetime(2006, 1, 18, 0, 0, tzinfo=<UTC>)\n",
      "def __setitem__(self, key, value):\n",
      "        # type: (_K, _V) -> None\n",
      "        \"\"\"Store a new views, potentially discarding an old value.\n",
      "        \"\"\"\n",
      "        if key not in self:\n",
      "            if len(self) >= self.cache_size:\n",
      "                self.popitem(last=False)\n",
      "        OrderedDict.__setitem__(self, key, value)\n",
      "\n",
      "__setitem__(self=LRUCache(), key='foo', value=1)\n",
      "OrderedDict.__setitem__(self, key, value)\n",
      "{'self': 'LRUCache()', 'key': \"'foo'\", 'value': '1'}\n",
      "self['foo'] = 1\n",
      "def _insert(self, key, value):\n",
      "        flat_key = self._serialize_key(key)\n",
      "        i = self._index.get(flat_key, -1)\n",
      "        if i >= 0:\n",
      "            self._items[i] = (key, value)\n",
      "        else:\n",
      "            self._items.append((key, value))\n",
      "            self._index[flat_key] = len(self._items) - 1\n",
      "\n",
      "_insert(self=OrderedMapSerializedKey([]), key='bob', value=199)\n",
      "flat_key = self._serialize_key(key)\n",
      "{'key': \"'bob'\"}\n",
      "flat_key = b'\\xe3\\x81\\xbfbob'\n",
      "def _prep_ordered_arg(desired_length, arguments=None):\n",
      "    \"\"\"Ensure list of arguments passed to add_ordered_transitions has the proper length.\n",
      "    Expands the given arguments and apply same condition, callback\n",
      "    to all transitions if only one has been given.\n",
      "\n",
      "    Args:\n",
      "        desired_length (int): The size of the resulting list\n",
      "        arguments (optional[str, reference or list]): Parameters to be expanded.\n",
      "    Returns:\n",
      "        list: Parameter sets with the desired length.\n",
      "    \"\"\"\n",
      "    arguments = listify(arguments) if arguments is not None else [None]\n",
      "    if len(arguments) != desired_length and len(arguments) != 1:\n",
      "        raise ValueError(\"Argument length must be either 1 or the same length as \"\n",
      "                         \"the number of transitions.\")\n",
      "    if len(arguments) == 1:\n",
      "        return arguments * desired_length\n",
      "    return arguments\n",
      "\n",
      "_prep_ordered_arg(desired_length=3, arguments=None)\n",
      "arguments = listify(arguments) if arguments is not None else [None]\n",
      "{'arguments': 'None'}\n",
      "arguments = [None]\n",
      "def config(_config=None, **kwargs):\n",
      "    \"\"\"\n",
      "    A decorator for setting the default kwargs of `BaseHandler.crawl`.\n",
      "    Any self.crawl with this callback will use this config.\n",
      "    \"\"\"\n",
      "    if _config is None:\n",
      "        _config = {}\n",
      "    _config.update(kwargs)\n",
      "\n",
      "    def wrapper(func):\n",
      "        func._config = _config\n",
      "        return func\n",
      "    return wrapper\n",
      "\n",
      "config(_config=None, kwargs={'age': 864000})\n",
      "_config.update(kwargs)\n",
      "{'kwargs': \"{'age': 864000}\"}\n",
      "_config = {'age': 864000}\n",
      "def quat_from_axis_angle(axis, angle):\n",
      "        axis_ = np.array(axis, dtype=np.float64)\n",
      "        half_angle = angle * 0.5\n",
      "        ret = np.empty(4)\n",
      "        ret[0] = math.cos(half_angle)\n",
      "        ret[1:4] = math.sin(half_angle) * axis_\n",
      "        return ret\n",
      "\n",
      "quat_from_axis_angle(axis=[1.0, 0.0, 0.0], angle=6.1086523819801535)\n",
      "ret[1:4] = math.sin(half_angle) * axis_\n",
      "{'half_angle': '3.0543261909900767'}\n",
      "ret = array([-0.9961947 ,  0.08715574,  0.        ,  0.        ])\n",
      "def base64url_decode(input: Union[bytes, str]) -> bytes:\n",
      "    input_bytes = force_bytes(input)\n",
      "\n",
      "    rem = len(input_bytes) % 4\n",
      "\n",
      "    if rem > 0:\n",
      "        input_bytes += b\"=\" * (4 - rem)\n",
      "\n",
      "    return base64.urlsafe_b64decode(input_bytes)\n",
      "\n",
      "base64url_decode(input='hJtXIZ2uSN5kbQfbtTNWbpdmhkV8FJG-Onbc6mxCcYg')\n",
      "rem = len(input_bytes) % 4\n",
      "{'input_bytes': \"b'hJtXIZ2uSN5kbQfbtTNWbpdmhkV8FJG-Onbc6mxCcYg'\"}\n",
      "rem = 3\n",
      "def to_base64url_uint(val: int) -> bytes:\n",
      "    if val < 0:\n",
      "        raise ValueError(\"Must be a positive integer\")\n",
      "\n",
      "    int_bytes = bytes_from_int(val)\n",
      "\n",
      "    if len(int_bytes) == 0:\n",
      "        int_bytes = b\"\\x00\"\n",
      "\n",
      "    return base64url_encode(int_bytes)\n",
      "\n",
      "to_base64url_uint(val=0)\n",
      "int_bytes = bytes_from_int(val)\n",
      "{'val': '0'}\n",
      "int_bytes = b''\n",
      "def get_targets_from_csv(csv_filename):\n",
      "        '''Returns list of Target objects parsed from CSV file.'''\n",
      "        targets = []\n",
      "        import csv\n",
      "        with open(csv_filename, 'r') as csvopen:\n",
      "            lines = []\n",
      "            for line in csvopen:\n",
      "                line = line.replace('\\0', '')\n",
      "                lines.append(line)\n",
      "            csv_reader = csv.reader(lines,\n",
      "                    delimiter=',',\n",
      "                    quoting=csv.QUOTE_ALL,\n",
      "                    skipinitialspace=True,\n",
      "                    escapechar='\\\\')\n",
      "\n",
      "            hit_clients = False\n",
      "            for row in csv_reader:\n",
      "                # Each 'row' is a list of fields for a target/client\n",
      "\n",
      "                if len(row) == 0: continue\n",
      "\n",
      "                if row[0].strip() == 'BSSID':\n",
      "                    # This is the 'header' for the list of Targets\n",
      "                    hit_clients = False\n",
      "                    continue\n",
      "\n",
      "                elif row[0].strip() == 'Station MAC':\n",
      "                    # This is the 'header' for the list of Clients\n",
      "                    hit_clients = True\n",
      "                    continue\n",
      "\n",
      "                if hit_clients:\n",
      "                    # The current row corresponds to a 'Client' (computer)\n",
      "                    try:\n",
      "                        client = Client(row)\n",
      "                    except (IndexError, ValueError) as e:\n",
      "                        # Skip if we can't parse the client row\n",
      "                        continue\n",
      "\n",
      "                    if 'not associated' in client.bssid:\n",
      "                        # Ignore unassociated clients\n",
      "                        continue\n",
      "\n",
      "                    # Add this client to the appropriate Target\n",
      "                    for t in targets:\n",
      "                        if t.bssid == client.bssid:\n",
      "                            t.clients.append(client)\n",
      "                            break\n",
      "\n",
      "                else:\n",
      "                    # The current row corresponds to a 'Target' (router)\n",
      "                    try:\n",
      "                        target = Target(row)\n",
      "                        targets.append(target)\n",
      "                    except Exception:\n",
      "                        continue\n",
      "\n",
      "        return targets\n",
      "\n",
      "get_targets_from_csv(csv_filename='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/derv82+wifite2/derv82+wifite2/tests/files/airodump-weird-ssids.csv')\n",
      "with open(csv_filename, 'r') as csvopen:\n",
      "{'csv_filename': \"'/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/derv82+wifite2/derv82+wifite2/tests/files/airodump-weird-ssids.csv'\"}\n",
      "csvopen = <_io.TextIOWrapper name='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/derv82+wifite2/derv82+wifite2/tests/files/airodump-weird-ssids.csv' mode='r' encoding='UTF-8'>\n",
      "def f(e):\n",
      "            result.append(e)\n",
      "\n",
      "f(e=1, result=[])\n",
      "result.append(e)\n",
      "{'e': '1'}\n",
      "result = [1]\n",
      "def _create_key_val_str(input_dict: Union[Mapping[Any, Any], Any]) -> str:\n",
      "    \"\"\"\n",
      "    Returns string of format {'key': val, 'key2': val2}\n",
      "    Function is called recursively for nested dictionaries\n",
      "\n",
      "    :param input_dict: dictionary to transform\n",
      "    :return: (str) reformatted string\n",
      "    \"\"\"\n",
      "\n",
      "    def list_to_str(input_list: List[str]) -> str:\n",
      "        \"\"\"\n",
      "        Convert all list items to string.\n",
      "        Function is called recursively for nested lists\n",
      "        \"\"\"\n",
      "        converted_list = []\n",
      "        for item in sorted(input_list, key=lambda x: str(x)):\n",
      "            if isinstance(item, dict):\n",
      "                item = _create_key_val_str(item)\n",
      "            elif isinstance(item, list):\n",
      "                item = list_to_str(item)\n",
      "\n",
      "            converted_list.append(str(item))\n",
      "        list_str = \", \".join(converted_list)\n",
      "        return \"[\" + list_str + \"]\"\n",
      "\n",
      "    items_list = []\n",
      "    for key in sorted(input_dict.keys(), key=lambda x: str(x)):\n",
      "        val = input_dict[key]\n",
      "        if isinstance(val, dict):\n",
      "            val = _create_key_val_str(val)\n",
      "        elif isinstance(val, list):\n",
      "            val = list_to_str(input_list=val)\n",
      "\n",
      "        items_list.append(f\"{key}: {val}\")\n",
      "\n",
      "    key_val_str = \"{{{}}}\".format(\", \".join(items_list))\n",
      "    return key_val_str\n",
      "\n",
      "_create_key_val_str(input_dict={})\n",
      "key_val_str = \"{{{}}}\".format(\", \".join(items_list))\n",
      "{'items_list': '[]'}\n",
      "key_val_str = '{}'\n",
      "def _clean_unicode(url: str) -> str:\n",
      "    \"\"\"Clean up URLs, which use punycode to handle unicode chars.\n",
      "\n",
      "    Applies percent encoding to URL path and query if required.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    url : str\n",
      "        URL that should be cleaned from unicode\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    str\n",
      "        Cleaned URL\n",
      "\n",
      "    \"\"\"\n",
      "    urllist = list(urlsplit(url))\n",
      "    netloc = urllist[1]\n",
      "    if _has_unicode(netloc):\n",
      "        domains = netloc.split(\".\")\n",
      "        for i, d in enumerate(domains):\n",
      "            if _has_unicode(d):\n",
      "                d = \"xn--\" + d.encode(\"punycode\").decode(\"ascii\")\n",
      "                domains[i] = d\n",
      "        urllist[1] = \".\".join(domains)\n",
      "        url = urlunsplit(urllist)\n",
      "\n",
      "    # Clean up path/query/params, which use url-encoding to handle unicode chars\n",
      "    chars = list(url)\n",
      "    for i, x in enumerate(chars):\n",
      "        if ord(x) > 128:\n",
      "            chars[i] = quote(x)\n",
      "\n",
      "    return \"\".join(chars)\n",
      "\n",
      "_clean_unicode(url='http://example.com/test?type=2&ie=utf8&query=')\n",
      "chars = list(url)\n",
      "{'url': \"'http://example.com/test?type=2&ie=utf8&query='\"}\n",
      "chars = ['h', 't', 't', 'p', ':', '/', '/', 'e', 'x', 'a', 'm', 'p', 'l', 'e', '.', 'c', 'o', 'm', '/', 't', 'e', 's', 't', '?', 't', 'y', 'p', 'e', '=', '2', '&', 'i', 'e', '=', 'u', 't', 'f', '8', '&', 'q', 'u', 'e', 'r', 'y', '=', '', '']\n",
      "def class_to_tg(sub_class: str):\n",
      "    trans = {\"Online\": \"_online\", \"Offline\": \"_offline\"}\n",
      "\n",
      "    for upper, lower in trans.items():\n",
      "        sub_class = sub_class.replace(upper, lower)\n",
      "\n",
      "    return sub_class.lower()\n",
      "\n",
      "class_to_tg(sub_class='YYeTsOffline')\n",
      "sub_class = sub_class.replace(upper, lower)\n",
      "{'upper': \"'Offline'\", 'lower': \"'_offline'\"}\n",
      "sub_class = 'YYeTs_offline'\n",
      "def patch_terminal_size(monkeypatch):\n",
      "    term_width = '250'\n",
      "    term_height = '60'\n",
      "    monkeypatch.setitem(os.environ, 'COLUMNS', term_width)\n",
      "    monkeypatch.setitem(os.environ, 'LINES', term_height)\n",
      "\n",
      "patch_terminal_size(monkeypatch={_setattr=[], _setitem=[], _cwd=None, _savesyspath=None})\n",
      "monkeypatch.setitem(os.environ, 'LINES', term_height)\n",
      "{'term_height': \"'60'\"}\n",
      "monkeypatch = {_setattr=[], _setitem=[(environ({'SHELL': '/bin/bash', 'LSCOLORS': 'Gxfxcxdxbxegedabagacad', 'USER_ZDOTDIR': '/home/XXX', 'COLORTERM': 'truecolor', 'LESS': '-R', 'TERM_PROGRAM_VERSION': '3.2a', 'GVM_VERSION': '1.0.22', 'CONDA_EXE': '/local/rcs/XXX/miniforge3/bin/conda', '_CE_M': '', 'TMUX': '/tmp/tmux-19200/default,59951,3', 'PKG_CONFIG_PATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib/pkgconfig:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib/pkgconfig:', '_P9K_TTY': '/dev/pts/20', 'GVM_PATH_BACKUP': '/home/XXX/.gvm/bin:/local/rcs/XXX/miniforge3/envs/mal/bin:/local/rcs/XXX/miniforge3/condabin:/home/XXX/.gdrive-downloader:/local/arise/XXX/miniforge3/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/bin:/home/XXX/.gvm/gos/go1.19.1/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/bin:/home/XXX/.gvm/bin:/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/XXX/.local/bin:/home/XXX/.local/bin:/home/XXX/.local/bin', 'P9K_TTY': 'old', 'LC_FIG_SET_PARENT': '4c022497-5122-4b80-b325-c89bab32302a', 'PWD': '/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/AmmsA+Githeat/AmmsA+Githeat', 'LOGNAME': 'XXX', 'XDG_SESSION_TYPE': 'tty', 'CONDA_PREFIX': '/local/rcs/XXX/miniforge3/envs/AmmsA+Githeat', 'VSCODE_GIT_ASKPASS_NODE': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/node', 'MOTD_SHOWN': 'pam', 'VSCODE_INJECTION': '1', 'GVM_OVERLAY_PREFIX': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay', 'HOME': '/home/XXX', 'LANG': 'en_US.UTF-8', 'DYLD_LIBRARY_PATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'gvm_pkgset_name': 'global', 'SSL_CERT_DIR': '/usr/lib/ssl/certs', 'CONDA_PROMPT_MODIFIER': '(AmmsA+Githeat) ', 'GIT_ASKPASS': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/extensions/git/dist/askpass.sh', 'GVM_ROOT': '/home/XXX/.gvm', 'SSH_CONNECTION': '127.0.0.1 39996 127.0.0.1 22', 'GOROOT': '/home/XXX/.gvm/gos/go1.19.1', 'NVM_DIR': '/local/rcs/XXX/.nvm', 'VSCODE_GIT_ASKPASS_EXTRA_ARGS': '', 'XDG_SESSION_CLASS': 'user', 'PYTHONPATH': ':/local/rcs/XXX/code/pytrace-collector:/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/AmmsA+Githeat/AmmsA+Githeat', 'TERM': 'screen', 'ZSH': '/home/XXX/.oh-my-zsh', '_CE_CONDA': '', 'V...deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'gvm_pkgset_name': 'global', 'SSL_CERT_DIR': '/usr/lib/ssl/certs', 'CONDA_PROMPT_MODIFIER': '(AmmsA+Githeat) ', 'GIT_ASKPASS': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/extensions/git/dist/askpass.sh', 'GVM_ROOT': '/home/XXX/.gvm', 'SSH_CONNECTION': '127.0.0.1 39996 127.0.0.1 22', 'GOROOT': '/home/XXX/.gvm/gos/go1.19.1', 'NVM_DIR': '/local/rcs/XXX/.nvm', 'VSCODE_GIT_ASKPASS_EXTRA_ARGS': '', 'XDG_SESSION_CLASS': 'user', 'PYTHONPATH': ':/local/rcs/XXX/code/pytrace-collector:/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/AmmsA+Githeat/AmmsA+Githeat', 'TERM': 'screen', 'ZSH': '/home/XXX/.oh-my-zsh', '_CE_CONDA': '', 'VSCODE_NONCE': 'd0bc7031-48a3-4719-8bb5-ef236ddd0016', 'ZDOTDIR': '/home/XXX', 'USER': 'XXX', 'TMUX_PANE': '%3', 'VSCODE_GIT_IPC_HANDLE': '/run/user/19200/vscode-git-13d67c6199.sock', 'CONDA_SHLVL': '3', 'SHLVL': '3', 'PAGER': 'less', '_P9K_SSH_TTY': '/dev/pts/20', 'XDG_SESSION_ID': '43', 'CONDA_PYTHON_EXE': '/local/rcs/XXX/miniforge3/bin/python', 'LD_LIBRARY_PATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib', 'XDG_RUNTIME_DIR': '/run/user/19200', 'SSL_CERT_FILE': '/usr/lib/ssl/certs/ca-certificates.crt', 'SSH_CLIENT': '127.0.0.1 46946 22', 'CONDA_DEFAULT_ENV': 'AmmsA+Githeat', 'P9K_SSH': '1', 'LC_ALL': 'en_US.UTF-8', 'VSCODE_GIT_ASKPASS_MAIN': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/extensions/git/dist/askpass-main.js', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'BROWSER': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/helpers/browser.sh', 'PATH': '/home/XXX/.gdrive-downloader:/local/arise/XXX/miniforge3/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/bin:/home/XXX/.gvm/gos/go1.19.1/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/bin:/home/XXX/.gvm/bin:/local/rcs/XXX/miniforge3/envs/AmmsA+Githeat/bin:/local/rcs/XXX/miniforge3/condabin:/home/XXX/.gdrive-downloader:/local/arise/XXX/miniforge3/bin:/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/XXX/.local/bin:/home/XXX/.local/bin', 'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/19200/bus', 'gvm_go_name': 'go1.19.1', 'CONDA_PREFIX_1': '/local/rcs/XXX/miniforge3', 'CONDA_PREFIX_2': '/local/rcs/XXX/miniforge3/envs/mal', 'OLDPWD': '/local/rcs/XXX/code/pytrace-collector', 'GOPATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global', 'TERM_PROGRAM': 'tmux', 'VSCODE_IPC_HOOK_CLI': '/run/user/19200/vscode-ipc-518d6355-acaf-4714-a359-be3fe9f21e09.sock', '_': '/local/rcs/XXX/miniforge3/envs/AmmsA+Githeat/bin/python', 'PYTEST_CURRENT_TEST': 'test/test_interactive.py::test_print_left_header (setup)', 'COLUMNS': '250', 'LINES': '60'}), 'LINES', <notset>)], _cwd=None, _savesyspath=None}\n",
      "def generate_samples_loguniform(low, high, step, base, size=1):\n",
      "    \"\"\"Generate sample for (discrete)uniform density.\"\"\"\n",
      "\n",
      "    samples = base ** (random.uniform(low=logb(low, base), high=logb(high, base), size=size))\n",
      "    if step:\n",
      "        samples = step * np.floor(samples / step)\n",
      "    return samples\n",
      "\n",
      "generate_samples_loguniform(low=5.8884365535558836e-08, high=2.6977394324449206e-07, step=None, base=10, size=100000)\n",
      "samples = base ** (random.uniform(low=logb(low, base), high=logb(high, base), size=size))\n",
      "{'high': '2.6977394324449206e-07', 'base': '10'}\n",
      "samples = array([8.36400364e-08, 2.64090744e-07, 2.64351674e-07, ...,       6.02217873e-08, 1.13953435e-07, 8.52185827e-08])\n",
      "def parzen_estimator_build_posterior_parameter(parameter, observations):\n",
      "    \"\"\"TPE algorith transform a prior parameter into a posterior parameters using observations\n",
      "    to build posterior.\n",
      "    \"\"\"\n",
      "    posterior_parameter = None\n",
      "    parameter_values = [observation.sample[parameter.name] for observation in observations]\n",
      "    search_space = parameter.search_space\n",
      "    if parameter.category == \"categorical\":\n",
      "        \"\"\" TODO Compare mean (current implem) vs hyperopt approach.\"\"\"\n",
      "        prior_probabilities = np.array(search_space[\"probabilities\"])\n",
      "        posterior_probabilities = prior_probabilities\n",
      "        if len(parameter_values) != 0:\n",
      "            observed_probabilities = np.array([parameter_values.count(value)\n",
      "                                               for value in search_space[\"values\"]])\n",
      "            observed_probabilities = observed_probabilities / np.sum(observed_probabilities)\n",
      "            posterior_probabilities += observed_probabilities\n",
      "        posterior_probabilities /= sum(posterior_probabilities)\n",
      "\n",
      "        # Build param\n",
      "        posterior_parameter = Parameter.from_dict(\n",
      "            {\n",
      "                \"name\": parameter.name,\n",
      "                \"category\": \"categorical\",\n",
      "                \"search_space\": {\n",
      "                    \"values\": search_space[\"values\"],\n",
      "                    \"probabilities\": list(posterior_probabilities),\n",
      "                }\n",
      "            }\n",
      "        )\n",
      "\n",
      "    if parameter.category in (\"uniform\", \"normal\", \"loguniform\", \"lognormal\"):\n",
      "        if parameter.category in (\"uniform\", \"loguniform\"):\n",
      "            prior_mu = 0.5 * (search_space[\"high\"] + search_space[\"low\"])\n",
      "            prior_sigma = (search_space[\"high\"] - search_space[\"low\"])\n",
      "        elif parameter.category in (\"normal\", \"lognormal\"):\n",
      "            prior_mu = search_space[\"mu\"]\n",
      "            prior_sigma = search_space[\"sigma\"]\n",
      "\n",
      "        # Mus\n",
      "        mus = np.sort(parameter_values + [prior_mu])\n",
      "\n",
      "        # Sigmas\n",
      "        # Trick to get for each mu the greater distance from left and right neighbor\n",
      "        # when low and high are not defined we use inf to get the only available distance\n",
      "        # (right neighbor for sigmas[0] and left for sigmas[-1])\n",
      "        tmp = np.concatenate(\n",
      "            (\n",
      "                [search_space.get(\"low\", np.inf)],\n",
      "                mus,\n",
      "                [search_space.get(\"high\", -np.inf)],\n",
      "            )\n",
      "        )\n",
      "        sigmas = np.maximum(tmp[1:-1] - tmp[0:-2], tmp[2:] - tmp[1:-1])\n",
      "\n",
      "        # Use formulas from hyperopt to clip sigmas\n",
      "        sigma_max_value = prior_sigma\n",
      "        sigma_min_value = prior_sigma / min(100.0, (1.0 + len(mus)))\n",
      "        sigmas = np.clip(sigmas, sigma_min_value, sigma_max_value)\n",
      "\n",
      "        # Fix prior sigma with correct value\n",
      "        sigmas[np.where(mus == prior_mu)[0]] = prior_sigma\n",
      "\n",
      "        posterior_parameter = Parameter.from_dict(\n",
      "            {\n",
      "                \"name\": parameter.name,\n",
      "                \"category\": \"mixture\",\n",
      "                \"search_space\": {\n",
      "                    \"parameters\": [\n",
      "                        {\n",
      "                            \"category\": \"normal\",\n",
      "                            \"search_space\": {\n",
      "                                \"mu\": mu.tolist(),\n",
      "                                \"sigma\": sigma.tolist(),\n",
      "                                \"low\": search_space[\"low\"],\n",
      "                                \"high\": search_space[\"high\"],\n",
      "                                \"step\": search_space.get(\"step\", None)\n",
      "                            }\n",
      "                        } if parameter.category[:3] != \"log\" else\n",
      "                        {\n",
      "                            \"category\": \"lognormal\",\n",
      "                            \"search_space\": {\n",
      "                                \"mu\": mu.tolist(),\n",
      "                                \"sigma\": sigma.tolist(),\n",
      "                                \"low\": search_space[\"low\"],\n",
      "                                \"high\": search_space[\"high\"],\n",
      "                                \"step\": search_space[\"step\"],\n",
      "                                \"base\": search_space[\"base\"],\n",
      "                            }\n",
      "                        } for mu, sigma in zip(mus, sigmas)\n",
      "                    ],\n",
      "                    \"weights\": [1 / len(mus) for _ in range(len(mus))]\n",
      "                }\n",
      "            }\n",
      "        )\n",
      "\n",
      "    return posterior_parameter\n",
      "\n",
      "parzen_estimator_build_posterior_parameter(parameter=x, observations=[])\n",
      "sigma_min_value = prior_sigma / min(100.0, (1.0 + len(mus)))\n",
      "{'mus': 'array([1.57079633])'}\n",
      "sigma_min_value = 1.5707963267948966\n",
      "def validate_categorical(search_space):\n",
      "    # error = \"Expected a dict with mandatory key 'values' (list) and optional key 'probabilities' (list)\"\n",
      "    search_space = search_space.copy()\n",
      "\n",
      "    if type(search_space) != dict:\n",
      "        raise ValueError\n",
      "    if \"values\" not in search_space.keys() or type(search_space['values']) != list:\n",
      "        raise ValueError\n",
      "    if \"probabilities\" in search_space.keys() and (\n",
      "            type(search_space['probabilities']) != list or\n",
      "            len(search_space['probabilities']) != len(search_space['values'])):\n",
      "        raise ValueError\n",
      "\n",
      "    # Test that proba sum to 1 but we are lazy and we try directly\n",
      "    if \"probabilities\" in search_space.keys():\n",
      "        np.random.choice(range(len(search_space[\"probabilities\"])),\n",
      "                         p=search_space[\"probabilities\"])\n",
      "\n",
      "    if \"probabilities\" not in search_space.keys():\n",
      "        number_of_values = len(search_space[\"values\"])\n",
      "        search_space[\"probabilities\"] = list(np.ones(number_of_values) / number_of_values)\n",
      "\n",
      "    return search_space\n",
      "\n",
      "validate_categorical(search_space={'values': [0, 0.6283185307179586, 0.7853981633974483, 1.0471975511965976, 1.5707963267948966, 3.141592653589793]})\n",
      "search_space[\"probabilities\"] = list(np.ones(number_of_values) / number_of_values)\n",
      "{'number_of_values': '6'}\n",
      "search_space = {'values': [0, 0.6283185307179586, 0.7853981633974483, 1.0471975511965976, 1.5707963267948966, 3.141592653589793], 'probabilities': [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]}\n",
      "def __missing__(self, b):\n",
      "        # Handle a cache miss, store encoded string in cache and return.\n",
      "        if b in _TILDE_ENCODING_SAFE:\n",
      "            res = chr(b)\n",
      "        elif b == _space:\n",
      "            res = \"+\"\n",
      "        else:\n",
      "            res = \"~{:02X}\".format(b)\n",
      "        self[b] = res\n",
      "        return res\n",
      "\n",
      "__missing__(self={}, b=102)\n",
      "res = chr(b)\n",
      "{'b': '102'}\n",
      "res = 'f'\n",
      "def to_css_class(s):\n",
      "    \"\"\"\n",
      "    Given a string (e.g. a table name) returns a valid unique CSS class.\n",
      "    For simple cases, just returns the string again. If the string is not a\n",
      "    valid CSS class (we disallow - and _ prefixes even though they are valid\n",
      "    as they may be confused with browser prefixes) we strip invalid characters\n",
      "    and add a 6 char md5 sum suffix, to make sure two tables with identical\n",
      "    names after stripping characters don't end up with the same CSS class.\n",
      "    \"\"\"\n",
      "    if css_class_re.match(s):\n",
      "        return s\n",
      "    md5_suffix = hashlib.md5(s.encode(\"utf8\")).hexdigest()[:6]\n",
      "    # Strip leading _, -\n",
      "    s = s.lstrip(\"_\").lstrip(\"-\")\n",
      "    # Replace any whitespace with hyphens\n",
      "    s = \"-\".join(s.split())\n",
      "    # Remove any remaining invalid characters\n",
      "    s = css_invalid_chars_re.sub(\"\", s)\n",
      "    # Attach the md5 suffix\n",
      "    bits = [b for b in (s, md5_suffix) if b]\n",
      "    return \"-\".join(bits)\n",
      "\n",
      "to_css_class(s='table/with/slashes.csv')\n",
      "s = css_invalid_chars_re.sub(\"\", s)\n",
      "{'s': \"'table/with/slashes.csv'\"}\n",
      "s = 'tablewithslashescsv'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import tree_sitter\n",
    "from tree_sitter_languages import get_parser\n",
    "import re\n",
    "\n",
    "parser = get_parser(\"python\")\n",
    "\n",
    "def function_filtering(function_node):\n",
    "    function_name = function_node.child_by_field_name(\"function\")\n",
    "    arguments_node = function_node.child_by_field_name(\"arguments\") \n",
    "\n",
    "    function_name_text = function_name.text.decode() if function_name else \"Unknown\"\n",
    "\n",
    "\n",
    "    identifiers = [\n",
    "    child.text.decode()\n",
    "    for child in arguments_node.children\n",
    "    if child.type == \"identifier\"]\n",
    "\n",
    "    if len(identifiers) == 0:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def filtering_function_calls(function_calls, annotated_lines):\n",
    "    filtered_function_calls = []\n",
    "    for function in function_calls:\n",
    "        line_no = function.start_point[0]\n",
    "        if \"# [STATE]\" in annotated_lines[line_no]:\n",
    "            does_match_requirements = function_filtering(function)\n",
    "            if does_match_requirements:\n",
    "                filtered_function_calls.append(function)\n",
    "    return filtered_function_calls\n",
    "\n",
    "\n",
    "def get_last_value(var_name, variable_values, current_line):\n",
    "    \"\"\"Fetch the last recorded value of a variable before a given line.\"\"\"\n",
    "    if var_name in variable_values:\n",
    "        values = variable_values[var_name]\n",
    "        for line, value in reversed(values):\n",
    "            if line < current_line:\n",
    "                return value\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_function_calls(node):\n",
    "    \"\"\"Recursively find all function call expressions in the AST, ignoring function definitions.\"\"\"\n",
    "    calls = []\n",
    "\n",
    "    if node.type == \"call\": \n",
    "        calls.append(node)\n",
    "\n",
    "    # if node.type == \"function_definition\":\n",
    "    #     return calls\n",
    "\n",
    "    for child in node.children:\n",
    "        calls.extend(find_function_calls(child))\n",
    "\n",
    "    return calls\n",
    "\n",
    "\n",
    "def process_calls(function_node, variable_values):\n",
    "    \"\"\"Process API calls expression and generate comments/statements.\"\"\"\n",
    "\n",
    "    current_line = function_node.start_point[0] + 1 \n",
    "    arguments_node = function_node.child_by_field_name(\"arguments\") \n",
    "\n",
    "    identifiers = [\n",
    "    child.text.decode()\n",
    "    for child in arguments_node.children\n",
    "    if child.type == \"identifier\"]\n",
    "    \n",
    "    values = {var: get_last_value(var, variable_values, current_line) for var in identifiers}\n",
    "\n",
    "\n",
    "    if all(v is not None for v in values.values()):\n",
    "        comment = f\"# {' '.join(f'{k} = {v}' for k, v in values.items())}\"\n",
    "        statement = f\"The value of the parameter(s) '{', '.join(values.keys())}' is '{', '.join(map(str, values.values()))}' before the execution of function \\\"{function_node.text.decode()}\\\". Can you predict the output of the function \\\"{function_node.text.decode()}\\\" with the given parameter(s) value(s)?\"\n",
    "        return comment, statement, values\n",
    "    \n",
    "    return None, None, None\n",
    "\n",
    "\n",
    "def annotate_code(example):\n",
    "    \"\"\"Parse and curate the dataset\"\"\"\n",
    "    code = example[\"Source Code\"]\n",
    "    source_code = example[\"scratchpad_format\"]\n",
    "    variable_values = example[\"variable_values\"]\n",
    "    tree = parser.parse(code.encode())\n",
    "\n",
    "    code_lines = code.split(\"\\n\")\n",
    "    annotated_lines = source_code.split(\"\\n\")\n",
    "\n",
    "\n",
    "    function_calls = find_function_calls(tree.root_node)\n",
    "    filtered_function_calls = filtering_function_calls(function_calls, annotated_lines)\n",
    "\n",
    "    if len(filtered_function_calls) == 0:\n",
    "        return None, None, None, None\n",
    "\n",
    "    chosen_function = random.choice(filtered_function_calls)\n",
    "    comment, statement, values = process_calls(chosen_function, variable_values)\n",
    "\n",
    "    if not comment or not statement:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    line_number = chosen_function.start_point[0]\n",
    "\n",
    "\n",
    "    match_item = re.search(r'\\# \\[STATE\\](.*?)\\[/STATE\\]', annotated_lines[line_number], re.DOTALL)\n",
    "\n",
    "    ground_truth = None\n",
    "    if match_item:\n",
    "        ground_truth = match_item.group(1).strip()\n",
    "    else:\n",
    "        ground_truth = None   \n",
    "\n",
    "    return \"\\n\".join(code_lines), code_lines[line_number], ground_truth, values\n",
    "\n",
    "\n",
    "with open(\"dataset_all.jsonl\", \"r\") as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "\n",
    "filtered_results = []\n",
    "for example in dataset:\n",
    "    if any(contains_complex_type(val) for val in example[\"input\"].values()):\n",
    "        continue\n",
    "    code, statements, ground_truth, values = annotate_code(example)\n",
    "    if code and statements and ground_truth and values:\n",
    "        if contains_complex_type(ground_truth) or not token_filtering(code):\n",
    "            continue\n",
    "        if \"=\" in ground_truth:\n",
    "            print(code)\n",
    "            print(statements.strip())\n",
    "            print(values)\n",
    "            print(ground_truth)\n",
    "\n",
    "            filtered_results.append(\n",
    "                {\"Programming Language\":example[\"Language\"],\n",
    "                \"Statement Type\": \"API\",\n",
    "                \"Source Code\": code, \n",
    "                \"Selected Statement\": statements.strip(),\n",
    "                \"Function Input\": example[\"input\"],\n",
    "                \"Variable Values Before Statement\": values, \n",
    "                \"Value After Statement Execution\": ground_truth.split(\"=\")[1].strip(),\n",
    "                \"Variable States During Runtime\": example[\"variable_values\"],\n",
    "                \"Program Information\": f\"Project Name: {example['Project_Name']}\"\n",
    "                })\n",
    "\n",
    "with open(\"Unified_dataset_api.jsonl\", \"w\") as f:\n",
    "    for result in filtered_results:\n",
    "        f.write(json.dumps(result) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 545 entries into statement_prediction_dataset_1.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Input JSONL file paths\n",
    "input_files = [\"Unified_dataset_assignment.jsonl\", \"Unified_dataset_branch.jsonl\", \"Unified_dataset_api.jsonl\", \"Unified_dataset_arithmetic_assignment.jsonl\", \"Unified_dataset_constant_assignment.jsonl\"]\n",
    "output_file = \"statement_prediction_dataset_1.jsonl\"\n",
    "\n",
    "merged_data = []\n",
    "\n",
    "# Read each file and append its content\n",
    "for file in input_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)  # Parse JSONL entry\n",
    "            merged_data.append(data)\n",
    "\n",
    "# Add idx field starting from 1\n",
    "for idx, entry in enumerate(merged_data, start=1):\n",
    "    entry[\"idx\"] = idx\n",
    "\n",
    "# Write the merged data into a new JSONL file\n",
    "with open(output_file, \"w\") as f:\n",
    "    for entry in merged_data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"Merged {len(merged_data)} entries into {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
