{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Code:\n",
      "def secrets_dir(env=os.getenv('D2_ENVIRONMENT', None),\n",
      "                basedir=os.getenv('D2_SECRETS_BASEDIR', None)):\n",
      "    if env is not None:\n",
      "        env_str = str(env)\n",
      "    else:\n",
      "        cwd = os.getcwd()\n",
      "        default_file = os.path.join(cwd, '.python_secrets_environment')\n",
      "        if os.path.exists(default_file):\n",
      "            with open(default_file, 'r') as f:\n",
      "                env_str = f.read().strip()\n",
      "        else:\n",
      "            env_str = os.path.basename(cwd)\n",
      "    if basedir is None:\n",
      "        basedir = os.path.join(\n",
      "                HOME,\n",
      "                'secrets' if sys.platform.startswith('win') else '.secrets')\n",
      "    return os.path.join(basedir, env_str)\n",
      "secrets_dir(env=None, basedir=None)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "cwd = os.getcwd()\n",
      "State:\n",
      "'/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/davedittrich+python_secrets/davedittrich+python_secrets'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def setUp(self):\n",
      "        self.host = HOST\n",
      "        with patch.dict('os.environ'):\n",
      "            for v in ['D2_ENVIRONMENT', 'D2_SECRETS_BASEDIR']:\n",
      "                try:\n",
      "                    del os.environ[v]\n",
      "                except KeyError as e:\n",
      "                    pass\n",
      "            self.secrets_env = psec.secrets.SecretsEnvironment()\n",
      "setUp(self=<tests.test_secrets.Test_SecretsEnvironment_general testMethod=test_environment_patchtest>, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fefdec6c6a0>, self._subtest=None, self._testMethodDoc='Sample test patching environment', self._testMethodName='test_environment_patchtest', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_environment_patchtest=<bound method Test_SecretsEnvironment_general.test_environment_patchtest of <tests.test_secrets.Test_SecretsEnvironment_general testMethod=test_environment_patchtest>>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.host = HOST\n",
      "State:\n",
      "'example.com'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def secrets_basedir_exists(self):\n",
      "        _secrets_basedir = self.secrets_basedir()\n",
      "        return os.path.exists(_secrets_basedir)\n",
      "secrets_basedir_exists(self=<psec.secrets.SecretsEnvironment object at 0x7fefdec6ca30>, self._cwd='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/davedittrich+python_secrets/davedittrich+python_secrets', self._environment='davedittrich+python_secrets', self._secrets_basedir=None, self._secrets_file='secrets.yml', self.verbose_level=1)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "_secrets_basedir = self.secrets_basedir()\n",
      "State:\n",
      "'/home/XXX/.secrets'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def build_simple_snippet(notes):\n",
      "    return Snippet('test', 'test', notes, 0)\n",
      "build_simple_snippet(notes=<music21.stream.iterator.StreamIterator for Part:0x7f1b97b695e0 @:0>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "return Snippet('test', 'test', notes, 0)\n",
      "State:\n",
      "<music21.stream.iterator.StreamIterator for Part:0x7f1b97b695e0 @:8>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse_route_template(template):\n",
      "    rbuilder = [\"^\"]\n",
      "    fbuilder = []\n",
      "    position = 0\n",
      "    schema = {}\n",
      "    for match in template_var_re_finditer(template):\n",
      "        param_name = match.group(\"name\")\n",
      "        param_type = match.group(\"type\") or \"id\"\n",
      "        param_formatchar, param_re, param_schema = _schema_map[param_type]\n",
      "        schema[param_name] = param_schema\n",
      "        rbuilder.append(re.escape(template[position:match.start()]))\n",
      "        rbuilder.append(param_re.format(param_name))\n",
      "        fbuilder.append(template[position:match.start()])\n",
      "        fbuilder.append(\"{\")\n",
      "        fbuilder.append(param_name)\n",
      "        fbuilder.append(\":\")\n",
      "        fbuilder.append(param_formatchar)\n",
      "        fbuilder.append(\"}\")\n",
      "        position = match.end()\n",
      "    rbuilder.append(re.escape(template[position:]))\n",
      "    rbuilder.append(\"$\")\n",
      "    fbuilder.append(template[position:])\n",
      "    return (valid.Schema(schema),\n",
      "            re.compile(\"\".join(rbuilder)),\n",
      "            u\"\".join(fbuilder).format)\n",
      "parse_route_template(template='get/{variable}')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "rbuilder = [\"^\"]\n",
      "State:\n",
      "['^']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse_route_template(template):\n",
      "    rbuilder = [\"^\"]\n",
      "    fbuilder = []\n",
      "    position = 0\n",
      "    schema = {}\n",
      "    for match in template_var_re_finditer(template):\n",
      "        param_name = match.group(\"name\")\n",
      "        param_type = match.group(\"type\") or \"id\"\n",
      "        param_formatchar, param_re, param_schema = _schema_map[param_type]\n",
      "        schema[param_name] = param_schema\n",
      "        rbuilder.append(re.escape(template[position:match.start()]))\n",
      "        rbuilder.append(param_re.format(param_name))\n",
      "        fbuilder.append(template[position:match.start()])\n",
      "        fbuilder.append(\"{\")\n",
      "        fbuilder.append(param_name)\n",
      "        fbuilder.append(\":\")\n",
      "        fbuilder.append(param_formatchar)\n",
      "        fbuilder.append(\"}\")\n",
      "        position = match.end()\n",
      "    rbuilder.append(re.escape(template[position:]))\n",
      "    rbuilder.append(\"$\")\n",
      "    fbuilder.append(template[position:])\n",
      "    return (valid.Schema(schema),\n",
      "            re.compile(\"\".join(rbuilder)),\n",
      "            u\"\".join(fbuilder).format)\n",
      "parse_route_template(template='get/{variable}')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "fbuilder = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse_route_template(template):\n",
      "    rbuilder = [\"^\"]\n",
      "    fbuilder = []\n",
      "    position = 0\n",
      "    schema = {}\n",
      "    for match in template_var_re_finditer(template):\n",
      "        param_name = match.group(\"name\")\n",
      "        param_type = match.group(\"type\") or \"id\"\n",
      "        param_formatchar, param_re, param_schema = _schema_map[param_type]\n",
      "        schema[param_name] = param_schema\n",
      "        rbuilder.append(re.escape(template[position:match.start()]))\n",
      "        rbuilder.append(param_re.format(param_name))\n",
      "        fbuilder.append(template[position:match.start()])\n",
      "        fbuilder.append(\"{\")\n",
      "        fbuilder.append(param_name)\n",
      "        fbuilder.append(\":\")\n",
      "        fbuilder.append(param_formatchar)\n",
      "        fbuilder.append(\"}\")\n",
      "        position = match.end()\n",
      "    rbuilder.append(re.escape(template[position:]))\n",
      "    rbuilder.append(\"$\")\n",
      "    fbuilder.append(template[position:])\n",
      "    return (valid.Schema(schema),\n",
      "            re.compile(\"\".join(rbuilder)),\n",
      "            u\"\".join(fbuilder).format)\n",
      "parse_route_template(template='get/{variable}')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "position = 0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse_route_template(template):\n",
      "    rbuilder = [\"^\"]\n",
      "    fbuilder = []\n",
      "    position = 0\n",
      "    schema = {}\n",
      "    for match in template_var_re_finditer(template):\n",
      "        param_name = match.group(\"name\")\n",
      "        param_type = match.group(\"type\") or \"id\"\n",
      "        param_formatchar, param_re, param_schema = _schema_map[param_type]\n",
      "        schema[param_name] = param_schema\n",
      "        rbuilder.append(re.escape(template[position:match.start()]))\n",
      "        rbuilder.append(param_re.format(param_name))\n",
      "        fbuilder.append(template[position:match.start()])\n",
      "        fbuilder.append(\"{\")\n",
      "        fbuilder.append(param_name)\n",
      "        fbuilder.append(\":\")\n",
      "        fbuilder.append(param_formatchar)\n",
      "        fbuilder.append(\"}\")\n",
      "        position = match.end()\n",
      "    rbuilder.append(re.escape(template[position:]))\n",
      "    rbuilder.append(\"$\")\n",
      "    fbuilder.append(template[position:])\n",
      "    return (valid.Schema(schema),\n",
      "            re.compile(\"\".join(rbuilder)),\n",
      "            u\"\".join(fbuilder).format)\n",
      "parse_route_template(template='get/{variable}')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "schema = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse_route_template(template):\n",
      "    rbuilder = [\"^\"]\n",
      "    fbuilder = []\n",
      "    position = 0\n",
      "    schema = {}\n",
      "    for match in template_var_re_finditer(template):\n",
      "        param_name = match.group(\"name\")\n",
      "        param_type = match.group(\"type\") or \"id\"\n",
      "        param_formatchar, param_re, param_schema = _schema_map[param_type]\n",
      "        schema[param_name] = param_schema\n",
      "        rbuilder.append(re.escape(template[position:match.start()]))\n",
      "        rbuilder.append(param_re.format(param_name))\n",
      "        fbuilder.append(template[position:match.start()])\n",
      "        fbuilder.append(\"{\")\n",
      "        fbuilder.append(param_name)\n",
      "        fbuilder.append(\":\")\n",
      "        fbuilder.append(param_formatchar)\n",
      "        fbuilder.append(\"}\")\n",
      "        position = match.end()\n",
      "    rbuilder.append(re.escape(template[position:]))\n",
      "    rbuilder.append(\"$\")\n",
      "    fbuilder.append(template[position:])\n",
      "    return (valid.Schema(schema),\n",
      "            re.compile(\"\".join(rbuilder)),\n",
      "            u\"\".join(fbuilder).format)\n",
      "parse_route_template(template='get/{variable}')\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "param_formatchar, param_re, param_schema = _schema_map[param_type]\n",
      "State:\n",
      "'s'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse_route_template(template):\n",
      "    rbuilder = [\"^\"]\n",
      "    fbuilder = []\n",
      "    position = 0\n",
      "    schema = {}\n",
      "    for match in template_var_re_finditer(template):\n",
      "        param_name = match.group(\"name\")\n",
      "        param_type = match.group(\"type\") or \"id\"\n",
      "        param_formatchar, param_re, param_schema = _schema_map[param_type]\n",
      "        schema[param_name] = param_schema\n",
      "        rbuilder.append(re.escape(template[position:match.start()]))\n",
      "        rbuilder.append(param_re.format(param_name))\n",
      "        fbuilder.append(template[position:match.start()])\n",
      "        fbuilder.append(\"{\")\n",
      "        fbuilder.append(param_name)\n",
      "        fbuilder.append(\":\")\n",
      "        fbuilder.append(param_formatchar)\n",
      "        fbuilder.append(\"}\")\n",
      "        position = match.end()\n",
      "    rbuilder.append(re.escape(template[position:]))\n",
      "    rbuilder.append(\"$\")\n",
      "    fbuilder.append(template[position:])\n",
      "    return (valid.Schema(schema),\n",
      "            re.compile(\"\".join(rbuilder)),\n",
      "            u\"\".join(fbuilder).format)\n",
      "parse_route_template(template='get/{variable}')\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "fbuilder.append(param_formatchar)\n",
      "State:\n",
      "['get/', '{', 'variable', ':', 's']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse_route_template(template):\n",
      "    rbuilder = [\"^\"]\n",
      "    fbuilder = []\n",
      "    position = 0\n",
      "    schema = {}\n",
      "    for match in template_var_re_finditer(template):\n",
      "        param_name = match.group(\"name\")\n",
      "        param_type = match.group(\"type\") or \"id\"\n",
      "        param_formatchar, param_re, param_schema = _schema_map[param_type]\n",
      "        schema[param_name] = param_schema\n",
      "        rbuilder.append(re.escape(template[position:match.start()]))\n",
      "        rbuilder.append(param_re.format(param_name))\n",
      "        fbuilder.append(template[position:match.start()])\n",
      "        fbuilder.append(\"{\")\n",
      "        fbuilder.append(param_name)\n",
      "        fbuilder.append(\":\")\n",
      "        fbuilder.append(param_formatchar)\n",
      "        fbuilder.append(\"}\")\n",
      "        position = match.end()\n",
      "    rbuilder.append(re.escape(template[position:]))\n",
      "    rbuilder.append(\"$\")\n",
      "    fbuilder.append(template[position:])\n",
      "    return (valid.Schema(schema),\n",
      "            re.compile(\"\".join(rbuilder)),\n",
      "            u\"\".join(fbuilder).format)\n",
      "parse_route_template(template='get/{variable}')\n",
      "\n",
      "n:\n",
      "25\n",
      "Statement:\n",
      "rbuilder.append(re.escape(template[position:]))\n",
      "State:\n",
      "['^', 'get/', '(?P<variable>[_a-zA-Z][_\\\\w]*)', '']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def tearDown(self):\n",
      "        self.last_io = None\n",
      "tearDown(self=<omni.test.test_plain_store.TestPlainStoreFileFormat testMethod=test_extrainfo_roundtrip>, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7f0c58e5f640>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_extrainfo_roundtrip', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.last_io=<omni.test.test_plain_store.PersistentStringIO object at 0x7f0c58805f10>, self.test_extrainfo_roundtrip=<bound method TestPlainStoreFileFormat.test_extrainfo_roundtrip of <omni.test.test_plain_store.TestPlainStoreFileFormat testMethod=test_extrainfo_roundtrip>>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.last_io = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add(self, username, password, extrainfo=None):\n",
      "        if username in self._users:\n",
      "            raise KeyError(\"User already exists: {}\".format(username))\n",
      "        self._users[username] = self.crypt_password(username, password)\n",
      "        if extrainfo:\n",
      "            assert username not in self._infos\n",
      "            self._infos[username] = extrainfo\n",
      "        return self\n",
      "add(self=<omni.stores.plain.PlainFileFormat object at 0x7f0c58dd6f40>, username='XXX', password='andr3w', extrainfo=None, self._infos={}, self._open=<omni.test.test_plain_store.PersistentStringIO object at 0x7f0c58dd6be0>, self._users=OrderedDict([('bob', 'b0b'), ('alice', '4l1c3')]))\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self._users[username] = self.crypt_password(username, password)\n",
      "State:\n",
      "OrderedDict([('bob', 'b0b'), ('alice', '4l1c3'), ('XXX', 'andr3w')])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def delete(self, username):\n",
      "        if username in self._users:\n",
      "            del self._users[username]\n",
      "            if username in self._infos:\n",
      "                del self._infos[username]\n",
      "        else:\n",
      "            raise KeyError(\"User not found: {}\".format(username))\n",
      "delete(self=<omni.stores.plain.PlainFileFormat object at 0x7f0c58800400>, username='bob', self._infos={}, self._open=<omni.test.test_plain_store.PersistentStringIO object at 0x7f0c588002e0>, self._users=OrderedDict([('bob', 'b0b'), ('alice', '4l1c3')]))\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "del self._users[username]\n",
      "State:\n",
      "OrderedDict([('alice', '4l1c3')])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_password(self, username, password):\n",
      "        with self._format(self._open_file, *self._fargs) as db:\n",
      "            if username not in db:\n",
      "                raise KeyError(\"invalid username {}\".format(username))\n",
      "            db[username] = db.crypt_password(username, password)\n",
      "set_password(self=<omni.test.test_plain_store.TestablePlainStore object at 0x7f0c58cbcc40>, username='alice', password='shiny', self._fargs=(<crypt.METHOD_CRYPT>,), self._format=<class 'omni.stores.plain.HtpasswdFileFormat'>, self._path='path/to/file', self.last_io=<omni.test.test_plain_store.PersistentStringIO object at 0x7f0c58cbce50>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "db[username] = db.crypt_password(username, password)\n",
      "State:\n",
      "{_users=OrderedDict([('alice', 'NPjjyakhSJwgQ')]), _infos={}, _method=<crypt.METHOD_CRYPT>}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __setitem__(self, username, password):\n",
      "        self._users[username] = password\n",
      "__setitem__(self=<omni.stores.plain.HtpasswdFileFormat object at 0x7f0c58cbcdf0>, username='alice', password='NPjjyakhSJwgQ', self._infos={}, self._method=<crypt.METHOD_CRYPT>, self._open=<bound method TestablePlainStore._open_file of <omni.test.test_plain_store.TestablePlainStore object at 0x7f0c58cbcc40>>, self._users=OrderedDict([('alice', 'su7aWQyEG4lo.')]))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self._users[username] = password\n",
      "State:\n",
      "OrderedDict([('alice', 'NPjjyakhSJwgQ')])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def mknode(id=None, ip=None, port=None, intid=None):\n",
      "    if intid is not None:\n",
      "        id = pack('>l', intid)\n",
      "    id = id or hashlib.sha1(str(random.getrandbits(255))).digest()\n",
      "    return Node(id, ip, port)\n",
      "mknode(id=None, ip=None, port=None, intid=0)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "id = pack('>l', intid)\n",
      "State:\n",
      "b'\\x00\\x00\\x00\\x00'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def slice_times(self, df, options=None):\n",
      "        if options is None:\n",
      "            options = self.options\n",
      "        if len(df) == 0:\n",
      "            return df\n",
      "        if options.get('latest', None):\n",
      "            start_at = df.iloc[-1].name\n",
      "            end_at = start_at\n",
      "        else:\n",
      "            try:\n",
      "                start_at = options['start_at']\n",
      "                end_at = options['end_at']\n",
      "            except KeyError:\n",
      "                raise ValueError('Slicing by time requires start_at and end_at')\n",
      "        sorteddf = df.sort_index()\n",
      "        sliced = sorteddf.truncate(before=start_at, after=end_at)\n",
      "        return sliced\n",
      "slice_times(self=<bpa.BPAClient object at 0x7fcc387c9850>, df=                             Load    Wind    Hydro  ThermalDate/Time                                                  2014-04-15 17:10:00+00:00  6553.0  3732.0  11225.0   1599.02014-04-15 17:15:00+00:00  6580.0  3686.0  11230.0   1603.02014-04-15 17:20:00+00:00  6560.0  3700.0  11254.0   1602.02014-04-15 17:25:00+00:00  6537.0  3684.0  11281.0   1601.02014-04-15 17:30:00+00:00  6562.0  3680.0  11260.0   1607.02014-04-15 17:35:00+00:00  6525.0  3675.0  11212.0   1608.02014-04-15 17:40:00+00:00  6496.0  3706.0  11240.0   1605.02014-04-15 17:45:00+00:00  6514.0  3700.0  11261.0   1607.02014-04-15 17:50:00+00:00  6501.0  3727.0  11172.0   1607.02014-04-15 17:55:00+00:00  6451.0  3700.0  11066.0   1596.02014-04-15 18:00:00+00:00  6449.0  3696.0  10816.0   1601.02014-04-15 18:05:00+00:00  6464.0  3688.0  10662.0   1601.0, options={'latest': True}, self.NAME='BPA', self.options={})\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "end_at = start_at\n",
      "State:\n",
      "Timestamp('2014-04-15 18:05:00+0000', tz='UTC')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def serialize(self, df, header, extras={}):\n",
      "        data = []\n",
      "        for row in df.itertuples():\n",
      "            dp = dict(zip(header, list(row)))\n",
      "            dp.update(extras)\n",
      "            data.append(dp)\n",
      "        return data\n",
      "serialize(self=<bpa.BPAClient object at 0x7fcc3813c7f0>, df=                           level_1        0Date/Time                                  2014-04-15 17:10:00+00:00     wind   3732.02014-04-15 17:10:00+00:00    hydro  11225.02014-04-15 17:10:00+00:00  thermal   1599.02014-04-15 17:15:00+00:00     wind   3686.02014-04-15 17:15:00+00:00    hydro  11230.02014-04-15 17:15:00+00:00  thermal   1603.02014-04-15 17:20:00+00:00     wind   3700.02014-04-15 17:20:00+00:00    hydro  11254.02014-04-15 17:20:00+00:00  thermal   1602.02014-04-15 17:25:00+00:00     wind   3684.02014-04-15 17:25:00+00:00    hydro  11281.02014-04-15 17:25:00+00:00  thermal   1601.02014-04-15 17:30:00+00:00     wind   3680.02014-04-15 17:30:00+00:00    hydro  11260.02014-04-15 17:30:00+00:00  thermal   1607.02014-04-15 17:35:00+00:00     wind   3675.02014-04-15 17:35:00+00:00    hydro  11212.02014-04-15 17:35:00+00:00  thermal   1608.02014-04-15 17:40:00+00:00     wind   3706.02014-04-15 17:40:00+00:00    hydro  11240.02014-04-15 17:40:00+00:00  thermal   1605.02014-04-15 17:45:00+00:00     wind   3700.02014-04-15 17:45:00+00:00    hydro  11261.02014-04-15 17:45:00+00:00  thermal   1607.02014-04-15 17:50:00+00:00     wind   3727.02014-04-15 17:50:00+00:00    hydro  11172.02014-04-15 17:50:00+00:00  thermal   1607.02014-04-15 17:55:00+00:00     wind   3700.02014-04-15 17:55:00+00:00    hydro  11066.02014-04-15 17:55:00+00:00  thermal   1596.02014-04-15 18:00:00+00:00     wind   3696.02014-04-15 18:00:00+00:00    hydro  10816.02014-04-15 18:00:00+00:00  thermal   1601.02014-04-15 18:05:00+00:00     wind   3688.02014-04-15 18:05:00+00:00    hydro  10662.02014-04-15 18:05:00+00:00  thermal   1601.0, header=['timestamp', 'fuel_name', 'gen_MW'], extras={}, self.NAME='BPA', self.options={})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "data = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def dates(self):\n",
      "        dates = []\n",
      "        if self.options['latest']:\n",
      "            local_now = self.local_now()\n",
      "            if local_now.date() != (local_now - timedelta(minutes=30)).date():\n",
      "                dates.append((local_now - timedelta(minutes=30)).date())\n",
      "            dates.append(local_now.date())\n",
      "        elif self.options['start_at'] and self.options['end_at']:\n",
      "            local_start = self.options['start_at'].astimezone(pytz.timezone(self.TZ_NAME))\n",
      "            local_end = self.options['end_at'].astimezone(pytz.timezone(self.TZ_NAME))\n",
      "            this_date = local_start.date()\n",
      "            while this_date <= local_end.date():\n",
      "                dates.append(this_date)\n",
      "                this_date += timedelta(days=1)\n",
      "        else:\n",
      "            raise ValueError(\n",
      "                'Either latest must be True, or start_at and end_at must both be provided.')\n",
      "        return dates\n",
      "dates(self=<eu.EUClient object at 0x7fcc3813cc70>, self.NAME='EU', self.options={'data': 'load', 'start_at': datetime.datetime(2024, 4, 2, 22, 40, 50, 430661, tzinfo=<UTC>), 'end_at': datetime.datetime(2024, 4, 3, 22, 40, 50, 430939, tzinfo=<UTC>), 'forecast': False, 'latest': True, 'control_area': 'not-a-cta', 'sliceable': False})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "dates = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def dates(self):\n",
      "        dates = []\n",
      "        if self.options['latest']:\n",
      "            local_now = self.local_now()\n",
      "            if local_now.date() != (local_now - timedelta(minutes=30)).date():\n",
      "                dates.append((local_now - timedelta(minutes=30)).date())\n",
      "            dates.append(local_now.date())\n",
      "        elif self.options['start_at'] and self.options['end_at']:\n",
      "            local_start = self.options['start_at'].astimezone(pytz.timezone(self.TZ_NAME))\n",
      "            local_end = self.options['end_at'].astimezone(pytz.timezone(self.TZ_NAME))\n",
      "            this_date = local_start.date()\n",
      "            while this_date <= local_end.date():\n",
      "                dates.append(this_date)\n",
      "                this_date += timedelta(days=1)\n",
      "        else:\n",
      "            raise ValueError(\n",
      "                'Either latest must be True, or start_at and end_at must both be provided.')\n",
      "        return dates\n",
      "dates(self=<eu.EUClient object at 0x7fcc3813cc70>, self.NAME='EU', self.options={'data': 'load', 'start_at': datetime.datetime(2024, 4, 2, 22, 40, 50, 430661, tzinfo=<UTC>), 'end_at': datetime.datetime(2024, 4, 3, 22, 40, 50, 430939, tzinfo=<UTC>), 'forecast': False, 'latest': True, 'control_area': 'not-a-cta', 'sliceable': False})\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "dates.append(local_now.date())\n",
      "State:\n",
      "[datetime.date(2024, 4, 3)]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def client_factory(client_name, **kwargs):\n",
      "    dir_name = os.path.dirname(os.path.abspath(__file__))\n",
      "    error_msg = 'No client found for name %s' % client_name\n",
      "    client_key = client_name.upper()\n",
      "    try:\n",
      "        client_vals = BALANCING_AUTHORITIES[client_key]\n",
      "        module_name = client_vals['module']\n",
      "        class_name = client_vals['class']\n",
      "    except KeyError:\n",
      "        raise ValueError(error_msg)\n",
      "    try:\n",
      "        fp, pathname, description = imp.find_module(module_name, [dir_name])\n",
      "    except ImportError:\n",
      "        raise ValueError(error_msg)\n",
      "    try:\n",
      "        mod = imp.load_module(module_name, fp, pathname, description)\n",
      "    finally:\n",
      "        if fp:\n",
      "            fp.close()\n",
      "    try:\n",
      "        client_inst = getattr(mod, class_name)(**kwargs)\n",
      "    except AttributeError:\n",
      "        raise ValueError(error_msg)\n",
      "    client_inst.NAME = client_name\n",
      "    return client_inst\n",
      "client_factory(client_name='MISO', kwargs={})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "dir_name = os.path.dirname(os.path.abspath(__file__))\n",
      "State:\n",
      "'/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/WattTime+pyiso/WattTime+pyiso/pyiso'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def client_factory(client_name, **kwargs):\n",
      "    dir_name = os.path.dirname(os.path.abspath(__file__))\n",
      "    error_msg = 'No client found for name %s' % client_name\n",
      "    client_key = client_name.upper()\n",
      "    try:\n",
      "        client_vals = BALANCING_AUTHORITIES[client_key]\n",
      "        module_name = client_vals['module']\n",
      "        class_name = client_vals['class']\n",
      "    except KeyError:\n",
      "        raise ValueError(error_msg)\n",
      "    try:\n",
      "        fp, pathname, description = imp.find_module(module_name, [dir_name])\n",
      "    except ImportError:\n",
      "        raise ValueError(error_msg)\n",
      "    try:\n",
      "        mod = imp.load_module(module_name, fp, pathname, description)\n",
      "    finally:\n",
      "        if fp:\n",
      "            fp.close()\n",
      "    try:\n",
      "        client_inst = getattr(mod, class_name)(**kwargs)\n",
      "    except AttributeError:\n",
      "        raise ValueError(error_msg)\n",
      "    client_inst.NAME = client_name\n",
      "    return client_inst\n",
      "client_factory(client_name='MISO', kwargs={})\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "client_key = client_name.upper()\n",
      "State:\n",
      "'MISO'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _run_test(self, ba_name, expect_data=True, tol_min=8, **kwargs):\n",
      "        c = client_factory(ba_name)\n",
      "        data = c.get_lmp(**kwargs)\n",
      "        if expect_data:\n",
      "            self.assertGreaterEqual(len(data), 1)\n",
      "        else:\n",
      "            self.assertEqual(data, [])\n",
      "            return data\n",
      "        for dp in data:\n",
      "            self.assertEqual(set(['lmp', 'ba_name',\n",
      "                                  'timestamp', 'market', 'node_id', 'lmp_type', 'freq']),\n",
      "                             set(dp.keys()))\n",
      "            self.assertEqual(dp['timestamp'].tzinfo, pytz.utc)\n",
      "            self.assertIn(dp['ba_name'], self.BA_CHOICES)\n",
      "            self.assertIn(dp['lmp_type'],\n",
      "                          ['LMP', 'prc', 'energy', 'TotalLMP'])\n",
      "            self.assertGreaterEqual(dp['lmp']+1, dp['lmp'])\n",
      "        timestamps = [t['timestamp'] for t in data]\n",
      "        now = pytz.utc.localize(datetime.utcnow())\n",
      "        if c.options['forecast']:\n",
      "            self.assertGreaterEqual(max(timestamps), now - timedelta(hours=1))\n",
      "        elif c.options['latest']:\n",
      "            tset = list(set(timestamps))\n",
      "            self.assertEqual(len(tset), 1)\n",
      "            delta = now - tset[0]\n",
      "            self.assertLess(abs(delta.total_seconds()), tol_min*60)\n",
      "        else:\n",
      "            if 'end_at' in kwargs and kwargs['end_at']:\n",
      "                self.assertLess(max(timestamps), kwargs['end_at'] + timedelta(hours=1))\n",
      "            else:\n",
      "                self.assertLess(max(timestamps), now)\n",
      "        return data\n",
      "_run_test(self=<tests.test_lmp.TestCAISOLMP testMethod=test_bad_node>, ba_name='CAISO', expect_data=False, tol_min=8, kwargs={'node_id': 'badnode', 'latest': True}, self.BA_CHOICES=dict_keys(['AZPS', 'BPA', 'CAISO', 'DEAA', 'ELE', 'ERCOT', 'HGMA', 'IID', 'ISONE', 'GRIF', 'MISO', 'NEVP', 'NYISO', 'PJM', 'PNM', 'SPPC', 'SPP', 'SRP', 'TEPC', 'WALC', 'EU']), self.FREQUENCY_CHOICES=IntervalChoices(hourly='1hr', fivemin='5m', tenmin='10m', fifteenmin='15m', na='n/a', dam='1hr'), self.MARKET_CHOICES=IntervalChoices(hourly='RTHR', fivemin='RT5M', tenmin='RT5M', fifteenmin='RTPD', na='RT5M', dam='DAHR'), self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fcc391a29d0>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_bad_node', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_bad_node=<bound method TestCAISOLMP.test_bad_node of <tests.test_lmp.TestCAISOLMP testMethod=test_bad_node>>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "c = client_factory(ba_name)\n",
      "State:\n",
      "{options={}, NAME='CAISO'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_generation(ba_name, **kwargs):\n",
      "    c = client_factory(ba_name)\n",
      "    data = c.get_generation(**kwargs)\n",
      "    if len(data) == 0:\n",
      "        msg = '%s: No generation data at %s with args %s' % (ba_name, datetime.utcnow().isoformat(),\n",
      "                                                    kwargs)\n",
      "        logger.warn(msg)\n",
      "    return data\n",
      "get_generation(ba_name='CAISO', kwargs={'latest': True})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "c = client_factory(ba_name)\n",
      "State:\n",
      "{options={}, NAME='CAISO'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_load(ba_name, **kwargs):\n",
      "    c = client_factory(ba_name)\n",
      "    data = c.get_load(**kwargs)\n",
      "    if len(data) == 0:\n",
      "        msg = '%s: No load data at %s with args %s' % (ba_name, datetime.utcnow().isoformat(),\n",
      "                                                    kwargs)\n",
      "        logger.warn(msg)\n",
      "    return data\n",
      "get_load(ba_name='PJM', kwargs={'latest': True})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "c = client_factory(ba_name)\n",
      "State:\n",
      "{options={}, NAME='PJM'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _run_net_test(self, ba_name, **kwargs):\n",
      "        c = client_factory(ba_name)\n",
      "        data = c.get_trade(**kwargs)\n",
      "        self.assertGreaterEqual(len(data), 1)\n",
      "        for dp in data:\n",
      "            for key in ['ba_name', 'timestamp', 'freq', 'market']:\n",
      "                self.assertIn(key, dp.keys())\n",
      "            self.assertEqual(len(dp.keys()), 5)\n",
      "            self.assertEqual(dp['timestamp'].tzinfo, pytz.utc)\n",
      "            self.assertIn(dp['ba_name'], self.BA_CHOICES)\n",
      "            self.assertGreaterEqual(dp['net_exp_MW']+1, dp['net_exp_MW'])\n",
      "            if c.options['forecast']:\n",
      "                self.assertGreaterEqual(dp['timestamp'], pytz.utc.localize(datetime.utcnow()))\n",
      "            else:\n",
      "                self.assertLess(dp['timestamp'], pytz.utc.localize(datetime.utcnow()))\n",
      "        return data\n",
      "_run_net_test(self=<tests.test_trade.TestNYISOTrade testMethod=test_date_range>, ba_name='NYISO', kwargs={'start_at': datetime.datetime(2016, 5, 11, 23, 44, 19, 55552, tzinfo=<UTC>), 'end_at': datetime.datetime(2016, 5, 12, 23, 44, 19, 55552, tzinfo=<UTC>)}, self.BA_CHOICES=['ISONE', 'MISO', 'SPP', 'BPA', 'CAISO', 'ERCOT', 'PJM', 'NYISO', 'NEVP', 'SPPC'], self.FREQUENCY_CHOICES=IntervalChoices(hourly='1hr', fivemin='5m', tenmin='10m', fifteenmin='15m', na='n/a', dam='1hr'), self.MARKET_CHOICES=IntervalChoices(hourly='RTHR', fivemin='RT5M', tenmin='RT5M', fifteenmin='RTPD', na='RT5M', dam='DAHR'), self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fcc38b5e640>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_date_range', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_date_range=<bound method TestNYISOTrade.test_date_range of <tests.test_trade.TestNYISOTrade testMethod=test_date_range>>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "c = client_factory(ba_name)\n",
      "State:\n",
      "{options={}, NAME='NYISO'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def enqueue(self, item):\n",
      "        if self.N + 1 >= len(self.s):\n",
      "            self.resize(len(self.s) * 2)\n",
      "        self.N += 1\n",
      "        self.s[self.N] = item\n",
      "        self.swim(self.N)\n",
      "enqueue(self=<pycompressor.priority_queue.MinPQ object at 0x7fadac34fb50>, item=100, self.N=0, self.compare=<function MinPQUnitTest.test_max_pq.<locals>.<lambda> at 0x7fadac4131f0>, self.s=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "self.N += 1\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def enqueue(self, item):\n",
      "        if self.N + 1 >= len(self.s):\n",
      "            self.resize(len(self.s) * 2)\n",
      "        self.N += 1\n",
      "        self.s[self.N] = item\n",
      "        self.swim(self.N)\n",
      "enqueue(self=<pycompressor.priority_queue.MinPQ object at 0x7fadac34fb50>, item=100, self.N=0, self.compare=<function MinPQUnitTest.test_max_pq.<locals>.<lambda> at 0x7fadac4131f0>, self.s=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "self.s[self.N] = item\n",
      "State:\n",
      "[0, 100, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def swim(self, k):\n",
      "        while k > 1:\n",
      "            parent = k // 2\n",
      "            if self.less(self.s[k], self.s[parent]):\n",
      "                exchange(self.s, k, parent)\n",
      "                k = parent\n",
      "            else:\n",
      "                break\n",
      "swim(self=<pycompressor.priority_queue.MinPQ object at 0x7fadac34fb50>, k=2, self.N=2, self.compare=<function MinPQUnitTest.test_max_pq.<locals>.<lambda> at 0x7fadac4131f0>, self.s=[0, 100, 200, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "parent = k // 2\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def swim(self, k):\n",
      "        while k > 1:\n",
      "            parent = k // 2\n",
      "            if self.less(self.s[k], self.s[parent]):\n",
      "                exchange(self.s, k, parent)\n",
      "                k = parent\n",
      "            else:\n",
      "                break\n",
      "swim(self=<pycompressor.priority_queue.MinPQ object at 0x7fadac34fb50>, k=2, self.N=2, self.compare=<function MinPQUnitTest.test_max_pq.<locals>.<lambda> at 0x7fadac4131f0>, self.s=[0, 100, 200, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "exchange(self.s, k, parent)\n",
      "State:\n",
      "[0, 200, 100, 0, 0, 0, 0, 0, 0, 0]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def swim(self, k):\n",
      "        while k > 1:\n",
      "            parent = k // 2\n",
      "            if self.less(self.s[k], self.s[parent]):\n",
      "                exchange(self.s, k, parent)\n",
      "                k = parent\n",
      "            else:\n",
      "                break\n",
      "swim(self=<pycompressor.priority_queue.MinPQ object at 0x7fadac34fb50>, k=2, self.N=2, self.compare=<function MinPQUnitTest.test_max_pq.<locals>.<lambda> at 0x7fadac4131f0>, self.s=[0, 100, 200, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "k = parent\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def exchange(a, i, j):\n",
      "    temp = a[i]\n",
      "    a[i] = a[j]\n",
      "    a[j] = temp\n",
      "exchange(a=[0, 100, 200, 0, 0, 0, 0, 0, 0, 0], i=2, j=1)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "temp = a[i]\n",
      "State:\n",
      "200\n",
      "==================================================\n",
      "Clean Code:\n",
      "def exchange(a, i, j):\n",
      "    temp = a[i]\n",
      "    a[i] = a[j]\n",
      "    a[j] = temp\n",
      "exchange(a=[0, 100, 200, 0, 0, 0, 0, 0, 0, 0], i=2, j=1)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "a[i] = a[j]\n",
      "State:\n",
      "[0, 100, 100, 0, 0, 0, 0, 0, 0, 0]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def exchange(a, i, j):\n",
      "    temp = a[i]\n",
      "    a[i] = a[j]\n",
      "    a[j] = temp\n",
      "exchange(a=[0, 100, 200, 0, 0, 0, 0, 0, 0, 0], i=2, j=1)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "a[j] = temp\n",
      "State:\n",
      "[0, 200, 100, 0, 0, 0, 0, 0, 0, 0]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def resize(self, new_length):\n",
      "        temp = [0] * new_length\n",
      "        length = min(new_length, len(self.s))\n",
      "        for i in range(length):\n",
      "            temp[i] = self.s[i]\n",
      "        self.s = temp\n",
      "resize(self=<pycompressor.priority_queue.MinPQ object at 0x7fadac34fb50>, new_length=20, self.N=9, self.compare=<function MinPQUnitTest.test_max_pq.<locals>.<lambda> at 0x7fadac4131f0>, self.s=[0, 200, 100, 19, 18, 17, 16, 15, 14, 13])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "temp = [0] * new_length\n",
      "State:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def resize(self, new_length):\n",
      "        temp = [0] * new_length\n",
      "        length = min(new_length, len(self.s))\n",
      "        for i in range(length):\n",
      "            temp[i] = self.s[i]\n",
      "        self.s = temp\n",
      "resize(self=<pycompressor.priority_queue.MinPQ object at 0x7fadac34fb50>, new_length=20, self.N=9, self.compare=<function MinPQUnitTest.test_max_pq.<locals>.<lambda> at 0x7fadac4131f0>, self.s=[0, 200, 100, 19, 18, 17, 16, 15, 14, 13])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "length = min(new_length, len(self.s))\n",
      "State:\n",
      "10\n",
      "==================================================\n",
      "Clean Code:\n",
      "def resize(self, new_length):\n",
      "        temp = [0] * new_length\n",
      "        length = min(new_length, len(self.s))\n",
      "        for i in range(length):\n",
      "            temp[i] = self.s[i]\n",
      "        self.s = temp\n",
      "resize(self=<pycompressor.priority_queue.MinPQ object at 0x7fadac34fb50>, new_length=20, self.N=9, self.compare=<function MinPQUnitTest.test_max_pq.<locals>.<lambda> at 0x7fadac4131f0>, self.s=[0, 200, 100, 19, 18, 17, 16, 15, 14, 13])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "self.s = temp\n",
      "State:\n",
      "[0, 200, 100, 19, 18, 17, 16, 15, 14, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def del_min(self):\n",
      "        if self.N == 0:\n",
      "            return None\n",
      "        item = self.s[1]\n",
      "        exchange(self.s, 1, self.N)\n",
      "        self.N -= 1\n",
      "        self.sink(1)\n",
      "        return item\n",
      "del_min(self=<pycompressor.priority_queue.MinPQ object at 0x7fadac34fb50>, self.N=22, self.compare=<function MinPQUnitTest.test_max_pq.<locals>.<lambda> at 0x7fadac4131f0>, self.s=[0, 200, 100, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "item = self.s[1]\n",
      "State:\n",
      "200\n",
      "==================================================\n",
      "Clean Code:\n",
      "def del_min(self):\n",
      "        if self.N == 0:\n",
      "            return None\n",
      "        item = self.s[1]\n",
      "        exchange(self.s, 1, self.N)\n",
      "        self.N -= 1\n",
      "        self.sink(1)\n",
      "        return item\n",
      "del_min(self=<pycompressor.priority_queue.MinPQ object at 0x7fadac34fb50>, self.N=22, self.compare=<function MinPQUnitTest.test_max_pq.<locals>.<lambda> at 0x7fadac4131f0>, self.s=[0, 200, 100, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "exchange(self.s, 1, self.N)\n",
      "State:\n",
      "[0, 0, 100, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 200, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def del_min(self):\n",
      "        if self.N == 0:\n",
      "            return None\n",
      "        item = self.s[1]\n",
      "        exchange(self.s, 1, self.N)\n",
      "        self.N -= 1\n",
      "        self.sink(1)\n",
      "        return item\n",
      "del_min(self=<pycompressor.priority_queue.MinPQ object at 0x7fadac34fb50>, self.N=22, self.compare=<function MinPQUnitTest.test_max_pq.<locals>.<lambda> at 0x7fadac4131f0>, self.s=[0, 200, 100, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "self.N -= 1\n",
      "State:\n",
      "21\n",
      "==================================================\n",
      "Clean Code:\n",
      "def dequeue(self):\n",
      "        old_first = self.first\n",
      "        if old_first is None:\n",
      "            return None\n",
      "        self.N -= 1\n",
      "        item = old_first.value\n",
      "        self.first = old_first.next_node\n",
      "        if self.first is None:\n",
      "            self.last = None\n",
      "        return item\n",
      "dequeue(self=<pycompressor.queue.Queue object at 0x7fadac34f4f0>, self.N=22, self.first=<pycompressor.queue.QueueNode object at 0x7fadac34f7c0>, self.last=<pycompressor.queue.QueueNode object at 0x7fadac34f3d0>)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "self.N -= 1\n",
      "State:\n",
      "21\n",
      "==================================================\n",
      "Clean Code:\n",
      "def decorator(func):\n",
      "        msg = f\"Function {func.__name__}() has been deprecated.  {message}\"\n",
      "        msg = \"\\n\" + textwrap.fill(msg, initial_indent=\"  \", subsequent_indent=\"  \")\n",
      "        def newFunc(*args, **kwargs):\n",
      "            warnings.warn(msg, category=DeprecationWarning, stacklevel=2)\n",
      "            return func(*args, **kwargs)\n",
      "        newFunc.__dict__.update(func.__dict__)\n",
      "        newFunc.__name__ = func.__name__\n",
      "        newFunc.__doc__ = func.__doc__\n",
      "        newFunc.__deprecated__ = True\n",
      "        _add_epytext_field(newFunc, \"deprecated\", message)\n",
      "        return newFunc\n",
      "decorator(func=<function AnnotationTask.N at 0x7f5d5a284ca0>, message='Use Nk, Nik or Nck instead')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "msg = f\"Function {func.__name__}() has been deprecated.  {message}\"\n",
      "State:\n",
      "'Function N() has been deprecated.  Use Nk, Nik or Nck instead'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def decorator(func):\n",
      "        msg = f\"Function {func.__name__}() has been deprecated.  {message}\"\n",
      "        msg = \"\\n\" + textwrap.fill(msg, initial_indent=\"  \", subsequent_indent=\"  \")\n",
      "        def newFunc(*args, **kwargs):\n",
      "            warnings.warn(msg, category=DeprecationWarning, stacklevel=2)\n",
      "            return func(*args, **kwargs)\n",
      "        newFunc.__dict__.update(func.__dict__)\n",
      "        newFunc.__name__ = func.__name__\n",
      "        newFunc.__doc__ = func.__doc__\n",
      "        newFunc.__deprecated__ = True\n",
      "        _add_epytext_field(newFunc, \"deprecated\", message)\n",
      "        return newFunc\n",
      "decorator(func=<function AnnotationTask.N at 0x7f5d5a284ca0>, message='Use Nk, Nik or Nck instead')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "msg = \"\\n\" + textwrap.fill(msg, initial_indent=\"  \", subsequent_indent=\"  \")\n",
      "State:\n",
      "'\\n  Function N() has been deprecated.  Use Nk, Nik or Nck instead'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_proxy_url(self, with_scheme=True):\n",
      "        assert self.socket_created_future.result(self.spinup_timeout)\n",
      "        if self.auth:\n",
      "            auth = \"%s@\" % self.auth\n",
      "        else:\n",
      "            auth = \"\"\n",
      "        if with_scheme:\n",
      "            scheme = \"http://\"\n",
      "        else:\n",
      "            scheme = \"\"\n",
      "        return \"%s%s%s:%s\" % (scheme, auth, self.proxy_host, self.proxy_port)\n",
      "get_proxy_url(self=<ProxyServerThread(Thread-1, started daemon 140680169838144)>, with_scheme=True, self._args=(), self._daemonic=True, self._ident=140680169838144, self._initialized=True, self._invoke_excepthook=<function _make_invoke_excepthook.<locals>.invoke_excepthook at 0x7ff2a8747790>, self._is_stopped=False, self._kwargs={}, self._name='Thread-1', self._native_id=2466491, self._started=<threading.Event object at 0x7ff2a7e30be0>, self._stderr=<_io.TextIOWrapper name='<stderr>' mode='w' encoding='utf-8'>, self._target=None, self._tstate_lock=<locked _thread.lock object at 0x7ff2a7f0d9c0>, self.auth=None, self.proxy_host='localhost', self.proxy_port=None, self.proxy_server=None, self.requests=[], self.socket_created_future=<test.proxy_server.Future object at 0x7ff2a7d1bf40>, self.timeout=5)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "assert self.socket_created_future.result(self.spinup_timeout)\n",
      "State:\n",
      "36667\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_proxy_url(self, with_scheme=True):\n",
      "        assert self.socket_created_future.result(self.spinup_timeout)\n",
      "        if self.auth:\n",
      "            auth = \"%s@\" % self.auth\n",
      "        else:\n",
      "            auth = \"\"\n",
      "        if with_scheme:\n",
      "            scheme = \"http://\"\n",
      "        else:\n",
      "            scheme = \"\"\n",
      "        return \"%s%s%s:%s\" % (scheme, auth, self.proxy_host, self.proxy_port)\n",
      "get_proxy_url(self=<ProxyServerThread(Thread-1, started daemon 140680169838144)>, with_scheme=True, self._args=(), self._daemonic=True, self._ident=140680169838144, self._initialized=True, self._invoke_excepthook=<function _make_invoke_excepthook.<locals>.invoke_excepthook at 0x7ff2a8747790>, self._is_stopped=False, self._kwargs={}, self._name='Thread-1', self._native_id=2466491, self._started=<threading.Event object at 0x7ff2a7e30be0>, self._stderr=<_io.TextIOWrapper name='<stderr>' mode='w' encoding='utf-8'>, self._target=None, self._tstate_lock=<locked _thread.lock object at 0x7ff2a7f0d9c0>, self.auth=None, self.proxy_host='localhost', self.proxy_port=None, self.proxy_server=None, self.requests=[], self.socket_created_future=<test.proxy_server.Future object at 0x7ff2a7d1bf40>, self.timeout=5)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "auth = \"\"\n",
      "State:\n",
      "''\n",
      "==================================================\n",
      "Clean Code:\n",
      "def assert_rst(sig, doc, allowed_rtypes=(None,)):\n",
      "    doctree = docutils.core.publish_doctree(\n",
      "        doc,\n",
      "        settings_overrides={\n",
      "            \"report_level\": docutils.utils.Reporter.SEVERE_LEVEL + 1,\n",
      "        },\n",
      "    ).asdom()\n",
      "    def get_all_text(node):\n",
      "        if node.nodeType == node.TEXT_NODE:\n",
      "            return node.data\n",
      "        else:\n",
      "            text_string = \"\"\n",
      "            for child_node in node.childNodes:\n",
      "                if child_node.nodeName == \"system_message\":\n",
      "                    continue\n",
      "                if child_node.nodeName == \"literal\":\n",
      "                    tmpl = \"``%s``\"\n",
      "                else:\n",
      "                    tmpl = \"%s\"\n",
      "                text_string += tmpl % (get_all_text(child_node),)\n",
      "            return text_string\n",
      "    documented_rtype = None\n",
      "    documented_types = {}\n",
      "    documented_params = []\n",
      "    for field in doctree.getElementsByTagName(\"field\"):\n",
      "        field_name = get_all_text(field.getElementsByTagName(\"field_name\")[0])\n",
      "        if field_name == \"rtype\":\n",
      "            assert documented_rtype is None, \"There must be single :rtype: directive\"\n",
      "            field_body = get_all_text(field.getElementsByTagName(\"field_body\")[0])\n",
      "            assert field_body, \":rtype: directive must have a value\"\n",
      "            documented_rtype = field_body.replace(\"\\n\", \" \")\n",
      "        if field_name.startswith(\"type\"):\n",
      "            parts = field_name.split(\" \")\n",
      "            param_name = parts[-1]\n",
      "            assert param_name not in documented_types, \"Duplicate `type` definition\"\n",
      "            field_body = get_all_text(field.getElementsByTagName(\"field_body\")[0])\n",
      "            documented_types[param_name] = field_body\n",
      "        if field_name.startswith(\"param\"):\n",
      "            parts = field_name.split(\" \")\n",
      "            param_name = parts[-1]\n",
      "            documented_params.append(param_name)\n",
      "            if len(parts) == 3:\n",
      "                assert param_name not in documented_types, \"Duplicate `type` definition\"\n",
      "                documented_types[param_name] = parts[1]\n",
      "    method_params = list(sig.parameters.keys())[1:]\n",
      "    assert method_params == documented_params, (\n",
      "        \"Actual method params set or order doesn't match the documented \"\n",
      "        \":param ...: directives in the docstring.\"\n",
      "    )\n",
      "    missing_types = set(documented_params) - documented_types.keys()\n",
      "    assert not missing_types, \"Not all params have types\"\n",
      "    assert set(documented_types.keys()) == set(documented_params), (\n",
      "        \"There are extraneous :type: directives\"\n",
      "    )\n",
      "    assert documented_rtype in allowed_rtypes\n",
      "assert_rst(sig=<Signature (self, username=None, password=None, *, referer=None, token_lifetime=60, scheme=None, timeout=DEFAULT_SENTINEL, proxies=DEFAULT_SENTINEL, user_agent=None, ssl_context=DEFAULT_SENTINEL, adapter_factory=None, auth_domain='www.arcgis.com', domain='geocode.arcgis.com')>, doc=\"\\n\\n        :param str username: ArcGIS username. Required if authenticated\\n            mode is desired.\\n\\n        :param str password: ArcGIS password. Required if authenticated\\n            mode is desired.\\n\\n        :param str referer: Required if authenticated mode is desired.\\n            `Referer` HTTP header to send with each request,\\n            e.g., ``'http://www.example.com'``. This is tied to an issued token,\\n            so fielding queries for multiple referrers should be handled by\\n            having multiple ArcGIS geocoder instances.\\n\\n        :param int token_lifetime: Desired lifetime, in minutes, of an\\n            ArcGIS-issued token.\\n\\n        :param str scheme:\\n            See :attr:`geopy.geocoders.options.default_scheme`.\\n            If authenticated mode is in use, it must be ``'https'``.\\n\\n        :param int timeout:\\n            See :attr:`geopy.geocoders.options.default_timeout`.\\n\\n        :param dict proxies:\\n            See :attr:`geopy.geocoders.options.default_proxies`.\\n\\n        :param str user_agent:\\n            See :attr:`geopy.geocoders.options.default_user_agent`.\\n\\n        :type ssl_context: :class:`ssl.SSLContext`\\n        :param ssl_context:\\n            See :attr:`geopy.geocoders.options.default_ssl_context`.\\n\\n        :param callable adapter_factory:\\n            See :attr:`geopy.geocoders.options.default_adapter_factory`.\\n\\n            .. versionadded:: 2.0\\n\\n        :param str auth_domain: Domain where the target ArcGIS auth service\\n            is hosted. Used only in authenticated mode (i.e. username,\\n            password and referer are set).\\n\\n        :param str domain: Domain where the target ArcGIS service\\n            is hosted.\\n        \", allowed_rtypes=(None,))\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "doctree = docutils.core.publish_doctree(\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def assert_rst(sig, doc, allowed_rtypes=(None,)):\n",
      "    doctree = docutils.core.publish_doctree(\n",
      "        doc,\n",
      "        settings_overrides={\n",
      "            \"report_level\": docutils.utils.Reporter.SEVERE_LEVEL + 1,\n",
      "        },\n",
      "    ).asdom()\n",
      "    def get_all_text(node):\n",
      "        if node.nodeType == node.TEXT_NODE:\n",
      "            return node.data\n",
      "        else:\n",
      "            text_string = \"\"\n",
      "            for child_node in node.childNodes:\n",
      "                if child_node.nodeName == \"system_message\":\n",
      "                    continue\n",
      "                if child_node.nodeName == \"literal\":\n",
      "                    tmpl = \"``%s``\"\n",
      "                else:\n",
      "                    tmpl = \"%s\"\n",
      "                text_string += tmpl % (get_all_text(child_node),)\n",
      "            return text_string\n",
      "    documented_rtype = None\n",
      "    documented_types = {}\n",
      "    documented_params = []\n",
      "    for field in doctree.getElementsByTagName(\"field\"):\n",
      "        field_name = get_all_text(field.getElementsByTagName(\"field_name\")[0])\n",
      "        if field_name == \"rtype\":\n",
      "            assert documented_rtype is None, \"There must be single :rtype: directive\"\n",
      "            field_body = get_all_text(field.getElementsByTagName(\"field_body\")[0])\n",
      "            assert field_body, \":rtype: directive must have a value\"\n",
      "            documented_rtype = field_body.replace(\"\\n\", \" \")\n",
      "        if field_name.startswith(\"type\"):\n",
      "            parts = field_name.split(\" \")\n",
      "            param_name = parts[-1]\n",
      "            assert param_name not in documented_types, \"Duplicate `type` definition\"\n",
      "            field_body = get_all_text(field.getElementsByTagName(\"field_body\")[0])\n",
      "            documented_types[param_name] = field_body\n",
      "        if field_name.startswith(\"param\"):\n",
      "            parts = field_name.split(\" \")\n",
      "            param_name = parts[-1]\n",
      "            documented_params.append(param_name)\n",
      "            if len(parts) == 3:\n",
      "                assert param_name not in documented_types, \"Duplicate `type` definition\"\n",
      "                documented_types[param_name] = parts[1]\n",
      "    method_params = list(sig.parameters.keys())[1:]\n",
      "    assert method_params == documented_params, (\n",
      "        \"Actual method params set or order doesn't match the documented \"\n",
      "        \":param ...: directives in the docstring.\"\n",
      "    )\n",
      "    missing_types = set(documented_params) - documented_types.keys()\n",
      "    assert not missing_types, \"Not all params have types\"\n",
      "    assert set(documented_types.keys()) == set(documented_params), (\n",
      "        \"There are extraneous :type: directives\"\n",
      "    )\n",
      "    assert documented_rtype in allowed_rtypes\n",
      "assert_rst(sig=<Signature (self, username=None, password=None, *, referer=None, token_lifetime=60, scheme=None, timeout=DEFAULT_SENTINEL, proxies=DEFAULT_SENTINEL, user_agent=None, ssl_context=DEFAULT_SENTINEL, adapter_factory=None, auth_domain='www.arcgis.com', domain='geocode.arcgis.com')>, doc=\"\\n\\n        :param str username: ArcGIS username. Required if authenticated\\n            mode is desired.\\n\\n        :param str password: ArcGIS password. Required if authenticated\\n            mode is desired.\\n\\n        :param str referer: Required if authenticated mode is desired.\\n            `Referer` HTTP header to send with each request,\\n            e.g., ``'http://www.example.com'``. This is tied to an issued token,\\n            so fielding queries for multiple referrers should be handled by\\n            having multiple ArcGIS geocoder instances.\\n\\n        :param int token_lifetime: Desired lifetime, in minutes, of an\\n            ArcGIS-issued token.\\n\\n        :param str scheme:\\n            See :attr:`geopy.geocoders.options.default_scheme`.\\n            If authenticated mode is in use, it must be ``'https'``.\\n\\n        :param int timeout:\\n            See :attr:`geopy.geocoders.options.default_timeout`.\\n\\n        :param dict proxies:\\n            See :attr:`geopy.geocoders.options.default_proxies`.\\n\\n        :param str user_agent:\\n            See :attr:`geopy.geocoders.options.default_user_agent`.\\n\\n        :type ssl_context: :class:`ssl.SSLContext`\\n        :param ssl_context:\\n            See :attr:`geopy.geocoders.options.default_ssl_context`.\\n\\n        :param callable adapter_factory:\\n            See :attr:`geopy.geocoders.options.default_adapter_factory`.\\n\\n            .. versionadded:: 2.0\\n\\n        :param str auth_domain: Domain where the target ArcGIS auth service\\n            is hosted. Used only in authenticated mode (i.e. username,\\n            password and referer are set).\\n\\n        :param str domain: Domain where the target ArcGIS service\\n            is hosted.\\n        \", allowed_rtypes=(None,))\n",
      "\n",
      "n:\n",
      "25\n",
      "Statement:\n",
      "documented_rtype = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_all_text(node):\n",
      "        if node.nodeType == node.TEXT_NODE:\n",
      "            return node.data\n",
      "        else:\n",
      "            text_string = \"\"\n",
      "            for child_node in node.childNodes:\n",
      "                if child_node.nodeName == \"system_message\":\n",
      "                    continue\n",
      "                if child_node.nodeName == \"literal\":\n",
      "                    tmpl = \"``%s``\"\n",
      "                else:\n",
      "                    tmpl = \"%s\"\n",
      "                text_string += tmpl % (get_all_text(child_node),)\n",
      "            return text_string\n",
      "get_all_text(node=<DOM Element: field_name at 0x7ff2a6284790>, get_all_text=<function assert_rst.<locals>.get_all_text at 0x7ff2a61f5af0>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "text_string = \"\"\n",
      "State:\n",
      "''\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_ellipsoid(self, ellipsoid):\n",
      "        if isinstance(ellipsoid, str):\n",
      "            try:\n",
      "                self.ELLIPSOID = ELLIPSOIDS[ellipsoid]\n",
      "                self.ellipsoid_key = ellipsoid\n",
      "            except KeyError:\n",
      "                raise Exception(\n",
      "                    \"Invalid ellipsoid. See geopy.distance.ELLIPSOIDS\"\n",
      "                )\n",
      "        else:\n",
      "            self.ELLIPSOID = ellipsoid\n",
      "            self.ellipsoid_key = None\n",
      "set_ellipsoid(self=REPR FAILED, ellipsoid='WGS-84', self.ELLIPSOID=None, self.ellipsoid_key=None, self.geod=None)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.ELLIPSOID = ELLIPSOIDS[ellipsoid]\n",
      "State:\n",
      "(6378.137, 6356.7523142, 0.0033528106647474805)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_ellipsoid(self, ellipsoid):\n",
      "        if isinstance(ellipsoid, str):\n",
      "            try:\n",
      "                self.ELLIPSOID = ELLIPSOIDS[ellipsoid]\n",
      "                self.ellipsoid_key = ellipsoid\n",
      "            except KeyError:\n",
      "                raise Exception(\n",
      "                    \"Invalid ellipsoid. See geopy.distance.ELLIPSOIDS\"\n",
      "                )\n",
      "        else:\n",
      "            self.ELLIPSOID = ellipsoid\n",
      "            self.ellipsoid_key = None\n",
      "set_ellipsoid(self=REPR FAILED, ellipsoid='WGS-84', self.ELLIPSOID=None, self.ellipsoid_key=None, self.geod=None)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "self.ellipsoid_key = ellipsoid\n",
      "State:\n",
      "'WGS-84'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def feet(kilometers=0, meters=0, miles=0, nautical=0):\n",
      "    ret = 0.\n",
      "    if nautical:\n",
      "        kilometers += nautical / nm(1.)\n",
      "    if meters:\n",
      "        kilometers += meters / 1000.\n",
      "    if kilometers:\n",
      "        miles += mi(kilometers=kilometers)\n",
      "    ret += miles * 5280\n",
      "    return ret\n",
      "feet(kilometers=1.0, meters=0, miles=0, nautical=0)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "miles += mi(kilometers=kilometers)\n",
      "State:\n",
      "0.621371192237334\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_destination_bearing_east(self):\n",
      "        distance = self.cls(kilometers=100)\n",
      "        for EAST in (90, 90 + 360, -360+90):\n",
      "            p = distance.destination(Point(0, 160), bearing=EAST)\n",
      "            self.assertAlmostEqual(p.latitude, 0)\n",
      "            self.assertAlmostEqual(p.longitude, 160.8993, delta=1e-3)\n",
      "            p = distance.destination(Point(60, 160), bearing=EAST)\n",
      "            self.assertAlmostEqual(p.latitude, 59.9878, delta=1e-3)\n",
      "            self.assertAlmostEqual(p.longitude, 161.79, delta=1e-2)\n",
      "test_destination_bearing_east(self=<test.test_distance.TestWhenComputingGreatCircleDistance testMethod=test_destination_bearing_east>, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7ff29f342280>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_destination_bearing_east', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_destination_bearing_east=<bound method CommonDistanceComputationCases.test_destination_bearing_east of <test.test_distance.TestWhenComputingGreatCircleDistance testMethod=test_destination_bearing_east>>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "distance = self.cls(kilometers=100)\n",
      "State:\n",
      "Distance(100.0)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __setstate__(self, state):\n",
      "        self.latitude, self.longitude, self.altitude = state\n",
      "__setstate__(self=REPR FAILED, state=(40.752662, -73.9773, 0.0))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.latitude, self.longitude, self.altitude = state\n",
      "State:\n",
      "0.0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def run(solution, installer, builder=None, controller=None, conf_ignore=None):\n",
      "    (args, unknown) = parse_args()\n",
      "    if not builder:\n",
      "        builder = Builder()\n",
      "    if not controller:\n",
      "        controller = ControllerConsole()\n",
      "    manager = Manager(installer, solution, builder, controller.is_graphical(),\n",
      "                      conf_ignore=conf_ignore)\n",
      "    if args.iquail_validate:\n",
      "        v = Validate(os.path.realpath(\n",
      "            args.iquail_validate), installer, builder)\n",
      "        success, _ = v.run()\n",
      "        if not success:\n",
      "            print(\"VALIDATION FAILED\", file=sys.stderr)\n",
      "            exit(1)\n",
      "        print(\"VALIDATION PASSED\", file=sys.stderr)\n",
      "        exit(0)\n",
      "    controller.setup(manager)\n",
      "    if args.iquail_rm:\n",
      "        shutil.rmtree(args.iquail_rm)\n",
      "    elif args.iquail_replace:\n",
      "        dest, src = args.iquail_replace.split(Constants.PATH_SEP)\n",
      "        os.replace(src, dest)\n",
      "    elif args.iquail_build:\n",
      "        manager.build()\n",
      "    elif args.iquail_uninstall:\n",
      "        controller.start_uninstall()\n",
      "    else:\n",
      "        if misc.running_from_installed_binary():\n",
      "            controller.start_run_or_update()\n",
      "        else:\n",
      "            if manager.is_installed():\n",
      "                logger.info(misc.get_script_path())\n",
      "                controller.start_uninstall()\n",
      "            else:\n",
      "                controller.start_install()\n",
      "    if args.iquail_run:\n",
      "        controller.start_run_or_update()\n",
      "run(solution={_progress_hook=None, _SolutionBase__solutioner=None, _SolutionBase__manager=None, _path='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/QuailTeam+Quail/QuailTeam+Quail/Allum1', _tmp=None}, installer={_install_systemwide=False, _launch_with_quail=True, _binary_name='allum1', _binary_options='', _name='Allum1', _requires_root=False, _icon='icon.jpeg', _publisher='alies', _console=True, _install_path='/home/XXX/.iquail/Allum1-alies', _solution_path='/home/XXX/.iquail/Allum1-alies/solution', _launcher_name='iquail_launcher', _desktop_conf={'Name': 'Allum1', 'Icon': '/home/XXX/.iquail/Allum1-alies/solution/icon.jpeg', 'Terminal': 'true', 'Type': 'Application', 'Path': '/home/XXX/.iquail/Allum1-alies/solution', 'Exec': '/home/XXX/.iquail/Allum1-alies/iquail_launcher.py  %f', 'MimeTypes': 'text/plain', 'Comment': 'best game ever'}, _launch_shortcut='/home/XXX/.local/share/applications/Allum1-alies.desktop', _uninstall_shortcut='/home/XXX/.local/share/applications/Allum1-alies_uninstall.desktop', _add_to_path=True}, builder={_side_img='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/QuailTeam+Quail/QuailTeam+Quail/iquail/side_img.gif', _build_cmds=[<iquail.builder.cmd_integrity.CmdIntegrity object at 0x7f71f26eabb0>]}, controller=None, conf_ignore=None)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "controller = ControllerConsole()\n",
      "State:\n",
      "{_eula=None}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _global_import(name):\n",
      "    p = __import__(name, globals(), locals(), level=1)\n",
      "    lst = p.__all__ if '__all__' in dir(p) else dir(p)\n",
      "    if lst:\n",
      "        globals().pop(name, None)\n",
      "        for k in lst:\n",
      "            if not k.startswith('__'):\n",
      "                globals()[k] = p.__dict__[k]\n",
      "                __all__.append(k)\n",
      "_global_import(name='base')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "p = __import__(name, globals(), locals(), level=1)\n",
      "State:\n",
      "<module 'tensorpack.dataflow.base' from '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/tensorpack+tensorpack/tensorpack+tensorpack/tensorpack/dataflow/base.py'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _global_import(name):\n",
      "    p = __import__(name, globals(), locals(), level=1)\n",
      "    lst = p.__all__ if '__all__' in dir(p) else dir(p)\n",
      "    if lst:\n",
      "        globals().pop(name, None)\n",
      "        for k in lst:\n",
      "            if not k.startswith('__'):\n",
      "                globals()[k] = p.__dict__[k]\n",
      "                __all__.append(k)\n",
      "_global_import(name='base')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "lst = p.__all__ if '__all__' in dir(p) else dir(p)\n",
      "State:\n",
      "['DataFlow', 'ProxyDataFlow', 'RNGDataFlow', 'DataFlowTerminated']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def global_import(name):\n",
      "    p = __import__(name, globals(), locals(), level=1)\n",
      "    lst = p.__all__ if '__all__' in dir(p) else []\n",
      "    del globals()[name]\n",
      "    for k in lst:\n",
      "        if not k.startswith('__'):\n",
      "            globals()[k] = p.__dict__[k]\n",
      "            __all__.append(k)\n",
      "global_import(name='model_desc')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "p = __import__(name, globals(), locals(), level=1)\n",
      "State:\n",
      "<module 'tensorpack.graph_builder.model_desc' from '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/tensorpack+tensorpack/tensorpack+tensorpack/tensorpack/graph_builder/model_desc.py'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def global_import(name):\n",
      "    p = __import__(name, globals(), locals(), level=1)\n",
      "    lst = p.__all__ if '__all__' in dir(p) else []\n",
      "    del globals()[name]\n",
      "    for k in lst:\n",
      "        if not k.startswith('__'):\n",
      "            globals()[k] = p.__dict__[k]\n",
      "            __all__.append(k)\n",
      "global_import(name='model_desc')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "lst = p.__all__ if '__all__' in dir(p) else []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __post_init__(self):\n",
      "        self.js_dir = self.js_dir or self.template.lower()\n",
      "        self.python_file_name = self.python_file_name or f\"{self.template.lower()}.py\"\n",
      "__post_init__(self=ComponentFiles(template='AnnotatedImage', demo_code='\\nexample = {name}().example_inputs()\\n\\nwith gr.Blocks() as demo:\\n    with gr.Row():\\n        {name}(label=\"Blank\"),\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.js_dir = self.js_dir or self.template.lower()\n",
      "State:\n",
      "ComponentFiles(template='AnnotatedImage', demo_code='\\nexample = {name}().example_inputs()\\n\\nwith gr.Blocks() as demo:\\n    with gr.Row():\\n        {name}(label=\"Blank\"),  # blank component\\n        {name}(value=example, label=\"Populated\"),  # populated component\\n', python_file_name='annotated_image.py', js_dir='annotatedimage')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _kwargs_checks_gen(self, decorated_function, function_spec, arg_specs):\n",
      "\t\targs_names = []\n",
      "\t\targs_names.extend(function_spec.args)\n",
      "\t\tif function_spec.varargs is not None:\n",
      "\t\t\targs_names.append(function_spec.args)\n",
      "\t\targs_check = {}\n",
      "\t\tfor arg_name in arg_specs.keys():\n",
      "\t\t\tif arg_name not in args_names:\n",
      "\t\t\t\targs_check[arg_name] = self.check(\n",
      "\t\t\t\t\targ_specs[arg_name], arg_name, decorated_function\n",
      "\t\t\t\t)\n",
      "\t\treturn args_check\n",
      "_kwargs_checks_gen(self=<wasp_general.verify.ValueVerifier object at 0x7f592bd107f0>, decorated_function=<function WSignalWatcherProto.wait at 0x7f592a6a4940>, function_spec=FullArgSpec(args=['self', 'timeout'], varargs=None, varkw=None, defaults=(None,), kwonlyargs=[], kwonlydefaults=None, annotations={}), arg_specs={'timeout': <function WSignalWatcherProto.<lambda> at 0x7f592a6a4430>}, self._env_var='WASP_ENABLE_CHECKS', self._silent_checks=False, self._tags=[])\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "args_names.extend(function_spec.args)\n",
      "State:\n",
      "['self', 'timeout']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def init_conv_(self, conv):\n",
      "        o, i, h, w = conv.weight.shape\n",
      "        conv_weight = torch.empty(o // 4, i, h, w)\n",
      "        nn.init.kaiming_uniform_(conv_weight)\n",
      "        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n",
      "        conv.weight.data.copy_(conv_weight)\n",
      "        nn.init.zeros_(conv.bias.data)\n",
      "init_conv_(self=PixelShuffleUpsample(  (net): Sequential(    (0): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))    (1): SiLU()    (2): PixelShuffle(upscale_factor=2)  )), conv=Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1)), self._backward_hooks=OrderedDict(), self._backward_pre_hooks=OrderedDict(), self._buffers=OrderedDict(), self._forward_hooks=OrderedDict(), self._forward_hooks_always_called=OrderedDict(), self._forward_hooks_with_kwargs=OrderedDict(), self._forward_pre_hooks=OrderedDict(), self._forward_pre_hooks_with_kwargs=OrderedDict(), self._is_full_backward_hook=None, self._load_state_dict_post_hooks=OrderedDict(), self._load_state_dict_pre_hooks=OrderedDict(), self._modules=OrderedDict([('net', Sequential(  (0): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))  (1): SiLU()  (2): PixelShuffle(upscale_factor=2)))]), self._non_persistent_buffers_set=set(), self._parameters=OrderedDict(), self._state_dict_hooks=OrderedDict(), self._state_dict_pre_hooks=OrderedDict(), self.training=True)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "o, i, h, w = conv.weight.shape\n",
      "State:\n",
      "32\n",
      "==================================================\n",
      "Clean Code:\n",
      "def init_conv_(self, conv):\n",
      "        o, i, h, w = conv.weight.shape\n",
      "        conv_weight = torch.empty(o // 4, i, h, w)\n",
      "        nn.init.kaiming_uniform_(conv_weight)\n",
      "        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n",
      "        conv.weight.data.copy_(conv_weight)\n",
      "        nn.init.zeros_(conv.bias.data)\n",
      "init_conv_(self=PixelShuffleUpsample(  (net): Sequential(    (0): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))    (1): SiLU()    (2): PixelShuffle(upscale_factor=2)  )), conv=Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1)), self._backward_hooks=OrderedDict(), self._backward_pre_hooks=OrderedDict(), self._buffers=OrderedDict(), self._forward_hooks=OrderedDict(), self._forward_hooks_always_called=OrderedDict(), self._forward_hooks_with_kwargs=OrderedDict(), self._forward_pre_hooks=OrderedDict(), self._forward_pre_hooks_with_kwargs=OrderedDict(), self._is_full_backward_hook=None, self._load_state_dict_post_hooks=OrderedDict(), self._load_state_dict_pre_hooks=OrderedDict(), self._modules=OrderedDict([('net', Sequential(  (0): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))  (1): SiLU()  (2): PixelShuffle(upscale_factor=2)))]), self._non_persistent_buffers_set=set(), self._parameters=OrderedDict(), self._state_dict_hooks=OrderedDict(), self._state_dict_pre_hooks=OrderedDict(), self.training=True)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "conv_weight = torch.empty(o // 4, i, h, w)\n",
      "State:\n",
      "tensor([[[[ 1.4422e+00]],         [[ 1.0425e+00]],         [[-4.1854e-01]],         [[ 6.1111e-02]],         [[ 1.7011e-01]],         [[ 7.3859e-01]],         [[ 1.1234e-01]],         [[ 7.9434e-01]]],        [[[-5.1286e-02]],         [[ 6.4548e-01]],         [[-9.1563e-01]],         [[-1.3698e+00]],         [[-4.4227e-01]],         [[-3.2611e-01]],         [[-1.2114e+00]],         [[ 5.9611e-01]]],        [[[-1.5431e-01]],         [[-1.6676e-01]],         [[ 1.2692e+00]],         [[-2.8656e+00]],         [[ 2.2421e-43]],         [[ 0.0000e+00]],         [[ 3.6013e-43]],         [[ 0.0000e+00]]],        [[[ 2.1569e-07]],         [[ 3.0774e-41]],         [[ 2.1415e-07]],         [[ 3.0774e-41]],         [[ 1.4013e-45]],         [[ 0.0000e+00]],         [[ 1.4013e-45]],         [[ 0.0000e+00]]],        [[[ 1.4013e-45]],         [[ 0.0000e+00]],         [[ 1.4013e-45]],         [[ 0.0000e+00]],         [[ 1.4013e-45]],         [[ 0.0000e+00]],         [[ 1.4013e-45]],         [[ 0.0000e+00]]],        [[[ 1.4013e-45]],         [[ 0.0000e+00]],         [[ 1.4013e-45]],         [[ 0.0000e+00]],         [[ 1.4013e-45]],         [[ 0.0000e+00]],         [[ 1.4013e-45]],         [[ 0.0000e+00]]],        [[[ 1.4013e-45]],         [[ 0.0000e+00]],         [[ 2.0319e-43]],         [[ 0.0000e+00]],         [[ 2.1136e-07]],         [[ 3.0774e-41]],         [[ 2.0801e-07]],         [[ 3.0774e-41]]],        [[[ 4.4842e-44]],         [[ 0.0000e+00]],         [[ 1.5695e-43]],         [[ 0.0000e+00]],         [[ 7.7116e+24]],         [[ 3.0778e-41]],         [[ 0.0000e+00]],         [[ 0.0000e+00]]]])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def init_conv_(self, conv):\n",
      "        o, i, h, w = conv.weight.shape\n",
      "        conv_weight = torch.empty(o // 4, i, h, w)\n",
      "        nn.init.kaiming_uniform_(conv_weight)\n",
      "        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n",
      "        conv.weight.data.copy_(conv_weight)\n",
      "        nn.init.zeros_(conv.bias.data)\n",
      "init_conv_(self=PixelShuffleUpsample(  (net): Sequential(    (0): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))    (1): SiLU()    (2): PixelShuffle(upscale_factor=2)  )), conv=Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1)), self._backward_hooks=OrderedDict(), self._backward_pre_hooks=OrderedDict(), self._buffers=OrderedDict(), self._forward_hooks=OrderedDict(), self._forward_hooks_always_called=OrderedDict(), self._forward_hooks_with_kwargs=OrderedDict(), self._forward_pre_hooks=OrderedDict(), self._forward_pre_hooks_with_kwargs=OrderedDict(), self._is_full_backward_hook=None, self._load_state_dict_post_hooks=OrderedDict(), self._load_state_dict_pre_hooks=OrderedDict(), self._modules=OrderedDict([('net', Sequential(  (0): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))  (1): SiLU()  (2): PixelShuffle(upscale_factor=2)))]), self._non_persistent_buffers_set=set(), self._parameters=OrderedDict(), self._state_dict_hooks=OrderedDict(), self._state_dict_pre_hooks=OrderedDict(), self.training=True)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "nn.init.kaiming_uniform_(conv_weight)\n",
      "State:\n",
      "tensor([[[[ 0.2046]],         [[ 0.7383]],         [[-0.2354]],         [[ 0.7482]],         [[ 0.3718]],         [[-0.1113]],         [[ 0.5291]],         [[ 0.5602]]],        [[[-0.4435]],         [[-0.5901]],         [[ 0.3579]],         [[-0.2002]],         [[-0.1366]],         [[ 0.8172]],         [[-0.4366]],         [[ 0.8074]]],        [[[ 0.5757]],         [[ 0.6216]],         [[-0.0433]],         [[ 0.6816]],         [[-0.2335]],         [[-0.8362]],         [[-0.6477]],         [[-0.1428]]],        [[[ 0.6086]],         [[-0.8312]],         [[ 0.7359]],         [[ 0.4631]],         [[-0.2967]],         [[-0.5708]],         [[ 0.5318]],         [[-0.2615]]],        [[[ 0.2776]],         [[-0.6416]],         [[-0.1069]],         [[-0.3409]],         [[ 0.6007]],         [[ 0.7782]],         [[-0.4220]],         [[ 0.4654]]],        [[[-0.1207]],         [[-0.6514]],         [[-0.5793]],         [[-0.5897]],         [[-0.5506]],         [[-0.6959]],         [[ 0.8602]],         [[ 0.5812]]],        [[[-0.8445]],         [[ 0.3486]],         [[-0.7968]],         [[ 0.3068]],         [[-0.2670]],         [[ 0.4851]],         [[ 0.8124]],         [[ 0.8039]]],        [[[-0.2703]],         [[-0.2185]],         [[ 0.5634]],         [[ 0.8424]],         [[-0.8168]],         [[-0.2052]],         [[ 0.6885]],         [[ 0.8138]]]])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def init_conv_(self, conv):\n",
      "        o, i, h, w = conv.weight.shape\n",
      "        conv_weight = torch.empty(o // 4, i, h, w)\n",
      "        nn.init.kaiming_uniform_(conv_weight)\n",
      "        conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n",
      "        conv.weight.data.copy_(conv_weight)\n",
      "        nn.init.zeros_(conv.bias.data)\n",
      "init_conv_(self=PixelShuffleUpsample(  (net): Sequential(    (0): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))    (1): SiLU()    (2): PixelShuffle(upscale_factor=2)  )), conv=Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1)), self._backward_hooks=OrderedDict(), self._backward_pre_hooks=OrderedDict(), self._buffers=OrderedDict(), self._forward_hooks=OrderedDict(), self._forward_hooks_always_called=OrderedDict(), self._forward_hooks_with_kwargs=OrderedDict(), self._forward_pre_hooks=OrderedDict(), self._forward_pre_hooks_with_kwargs=OrderedDict(), self._is_full_backward_hook=None, self._load_state_dict_post_hooks=OrderedDict(), self._load_state_dict_pre_hooks=OrderedDict(), self._modules=OrderedDict([('net', Sequential(  (0): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))  (1): SiLU()  (2): PixelShuffle(upscale_factor=2)))]), self._non_persistent_buffers_set=set(), self._parameters=OrderedDict(), self._state_dict_hooks=OrderedDict(), self._state_dict_pre_hooks=OrderedDict(), self.training=True)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "conv_weight = repeat(conv_weight, 'o ... -> (o 4) ...')\n",
      "State:\n",
      "tensor([[[[ 0.2046]],         [[ 0.7383]],         [[-0.2354]],         [[ 0.7482]],         [[ 0.3718]],         [[-0.1113]],         [[ 0.5291]],         [[ 0.5602]]],        [[[ 0.2046]],         [[ 0.7383]],         [[-0.2354]],         [[ 0.7482]],         [[ 0.3718]],         [[-0.1113]],         [[ 0.5291]],         [[ 0.5602]]],        [[[ 0.2046]],         [[ 0.7383]],         [[-0.2354]],         [[ 0.7482]],         [[ 0.3718]],         [[-0.1113]],         [[ 0.5291]],         [[ 0.5602]]],        [[[ 0.2046]],         [[ 0.7383]],         [[-0.2354]],         [[ 0.7482]],         [[ 0.3718]],         [[-0.1113]],         [[ 0.5291]],         [[ 0.5602]]],        [[[-0.4435]],         [[-0.5901]],         [[ 0.3579]],         [[-0.2002]],         [[-0.1366]],         [[ 0.8172]],         [[-0.4366]],         [[ 0.8074]]],        [[[-0.4435]],         [[-0.5901]],         [[ 0.3579]],         [[-0.2002]],         [[-0.1366]],         [[ 0.8172]],         [[-0.4366]],         [[ 0.8074]]],        [[[-0.4435]],         [[-0.5901]],         [[ 0.3579]],         [[-0.2002]],         [[-0.1366]],         [[ 0.8172]],         [[-0.4366]],         [[ 0.8074]]],        [[[-0.4435]],         [[-0.5901]],         [[ 0.3579]],         [[-0.2002]],         [[-0.1366]],         [[ 0.8172]],         [[-0.4366]],         [[ 0.8074]]],        [[[ 0.5757]],         [[ 0.6216]],         [[-0.0433]],         [[ 0.6816]],         [[-0.2335]],         [[-0.8362]],         [[-0.6477]],         [[-0.1428]]],        [[[ 0.5757]],         [[ 0.6216]],         [[-0.0433]],         [[ 0.6816]],         [[-0.2335]],         [[-0.8362]],         [[-0.6477]],         [[-0.1428]]],        [[[ 0.5757]],         [[ 0.6216]],         [[-0.0433]],         [[ 0.6816]],         [[-0.2335]],         [[-0.8362]],         [[-0.6477]],         [[-0.1428]]],        [[[ 0.5757]],         [[ 0.6216]],         [[-0.0433]],         [[ 0.6816]],         [[-0.2335]],         [[-0.8362]],         [[-0.6477]],         [[-0.1428]]],        [[[ 0.6086]],         [[-0.8312]],         [[ 0.7359]],         [[ 0.4631]],         [[-0.2967]],         [[-0.5708]],         [[ 0.5318]],         [[-0.2615]]],        [[[ 0.6086]],         [[-0.8312]],         [[ 0.7359]],         [[ 0.4631]],         [[-0.2967]],         [[-0.5708]],         [[ 0.5318]],         [[-0.2615]]],        [[[ 0.6086]],         [[-0.8312]],         [[ 0.7359]],         [[ 0.4631]],         [[-0.2967]],         [[-0.5708]],         [[ 0.5318]],         [[-0.2615]]],        [[[ 0.6086]],         [[-0.8312]],         [[ 0.7359]],         [[ 0.4631]],         [[-0.2967]],         [[-0.5708]],         [[ 0.5318]],         [[-0.2615]]],        [[[ 0.2776]],         [[-0.6416]],         [[-0.1069]],         [[-0.3409]],         [[ 0.6007]],         [[ 0.7782]],         [[-0.4220]],         [[ 0.4654]]],        [[[ 0.2776]],         [[-0.6416]],         [[-0.1069]],         [[-0.3409]],         [[ 0.6007]],         [[ 0.7782]],         [[-0.4220]],         [[ 0.4654]]],        [[[ 0.2776]],         [[-0.6416]],         [[-0.1069]],         [[-0.3409]],         [[ 0.6007]],         [[ 0.7782]],         [[-0.4220]],         [[ 0.4654]]],        [[[ 0.2776]],         [[-0.6416]],         [[-0.1069]],         [[-0.3409]],         [[ 0.6007]],         [[ 0.7782]],         [[-0.4220]],         [[ 0.4654]]],        [[[-0.1207]],         [[-0.6514]],         [[-0.5793]],         [[-0.5897]],         [[-0.5506]],         [[-0.6959]],         [[ 0.8602]],         [[ 0.5812]]],        [[[-0.1207]],         [[-0.6514]],         [[-0.5793]],         [[-0.5897]],         [[-0.5506]],         [[-0.6959]],         [[ 0.8602]],         [[ 0.5812]]],        [[[-0.1207]],         [[-0.6514]],         [[-0.5793]],         [[-0.5897]],         [[-0.5506]],         [[-0.6959]],         [[ 0.8602]],         [[ 0.5812]]],        [[[-0.1207]],         [[-0.6514]],         [[-0.5793]],         [[-0.5897]],         [[-0.5506]],         [[-0.6959]],         [[ 0.8602]],         [[ 0.5812]]],        [[[-0.8445]],         [[ 0.3486]],         [[-0.7968]],         [[ 0.3068]],         [[-0.2670]],         [[ 0.4851]],         [[ 0.8124]],         [[ 0.8039]]],        [[[-0.8445]],         [[ 0.3486]],         [[-0.7968]],         [[ 0.3068]],         [[-0.2670]],         [[ 0.4851]],         [[ 0.8124]],         [[ 0.8039]]],        [[[-0.8445]],         [[ 0.3486]],         [[-0.7968]],         [[ 0.3068]],         [[-0.2670]],         [[ 0.4851]],         [[ 0.8124]],         [[ 0.8039]]],        [[[-0.8445]],         [[ 0.3486]],         [[-0.7968]],         [[ 0.3068]],         [[-0.2670]],         [[ 0.4851]],         [[ 0.8124]],         [[ 0.8039]]],        [[[-0.2703]],         [[-0.2185]],         [[ 0.5634]],         [[ 0.8424]],         [[-0.8168]],         [[-0.2052]],         [[ 0.6885]],         [[ 0.8138]]],        [[[-0.2703]],         [[-0.2185]],         [[ 0.5634]],         [[ 0.8424]],         [[-0.8168]],         [[-0.2052]],         [[ 0.6885]],         [[ 0.8138]]],        [[[-0.2703]],         [[-0.2185]],         [[ 0.5634]],         [[ 0.8424]],         [[-0.8168]],         [[-0.2052]],         [[ 0.6885]],         [[ 0.8138]]],        [[[-0.2703]],         [[-0.2185]],         [[ 0.5634]],         [[ 0.8424]],         [[-0.8168]],         [[-0.2052]],         [[ 0.6885]],         [[ 0.8138]]]])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def from_json(cls, command_tree_file, translations_file):\n",
      "        with open(command_tree_file, 'r') as fin:\n",
      "            tree_dict = json.load(fin)\n",
      "        with open(translations_file, 'r') as fin:\n",
      "            command_translations = json.load(fin)\n",
      "        return TokenTree(tree_dict, command_translations)\n",
      "from_json(cls=<class 'bted.token_tree.TokenTree'>, command_tree_file='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/XXXbihl+bted/XXXbihl+bted/bted/config/command_token_tree.json', translations_file='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/XXXbihl+bted/XXXbihl+bted/bted/config/command_perl_translations.json')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "tree_dict = json.load(fin)\n",
      "State:\n",
      "{'root': {'append': {'reusable_components': ['text-with-text', 'lines-relation-text-with-text']}, 'clear': {'reusable_components': ['line-types']}, 'delete': {'$USER_TEXT_INPUT': {}, 'reusable_components': ['line-types']}, 'insert': {'reusable_components': ['text-with-text', 'lines-relation-text-with-text']}, 'prepend': {'reusable_components': ['text-with-text', 'lines-relation-text-with-text']}, 'replace': {'reusable_components': ['text-with-text', 'lines-relation-text-with-text']}, 'wrap': {'reusable_components': ['text-with-text', 'lines-relation-text-with-text']}, 'select': {'reusable_components': ['line-types']}}, 'reusable_components': {'text-with-text': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}}, 'line-types': {'lines': {'starting': {'with': {'$USER_TEXT_INPUT': {}}}, 'ending': {'with': {'$USER_TEXT_INPUT': {}}}, 'containing': {'$USER_TEXT_INPUT': {}}}}, 'lines-relation-text-with-text': {'lines': {'starting': {'with': {'reusable_components': ['text-with-text']}}, 'ending': {'with': {'reusable_components': ['text-with-text']}}, 'containing': {'reusable_components': ['text-with-text']}}}}}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def from_json(cls, command_tree_file, translations_file):\n",
      "        with open(command_tree_file, 'r') as fin:\n",
      "            tree_dict = json.load(fin)\n",
      "        with open(translations_file, 'r') as fin:\n",
      "            command_translations = json.load(fin)\n",
      "        return TokenTree(tree_dict, command_translations)\n",
      "from_json(cls=<class 'bted.token_tree.TokenTree'>, command_tree_file='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/XXXbihl+bted/XXXbihl+bted/bted/config/command_token_tree.json', translations_file='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/XXXbihl+bted/XXXbihl+bted/bted/config/command_perl_translations.json')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "command_translations = json.load(fin)\n",
      "State:\n",
      "{'delete $USER_TEXT_INPUT': \"perl -wnl -e 's/{1}//g; print;' {0}\", 'replace $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -wnl -e 's/{1}/{2}/g; print;' {0}\", 'append $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -wnl -e 's/{1}/{1}{2}/g; print;' {0}\", 'prepend $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -wnl -e 's/{1}/{2}{1}/g; print;' {0}\", 'wrap $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -wnl -e 's/{1}/{2}{1}{2}/g; print;' {0}\", 'delete lines containing $USER_TEXT_INPUT': \"perl -ne '/{1}/ or print' {0}\", 'delete lines starting with $USER_TEXT_INPUT': \"perl -ne '/^{1}/ or print' {0}\", 'delete lines ending with $USER_TEXT_INPUT': \"perl -ne '/{1}$/ or print' {0}\", 'clear lines containing $USER_TEXT_INPUT': \"perl -wnl -e 's/.*{1}.*//g; print;' {0}\", 'clear lines starting with $USER_TEXT_INPUT': \"perl -wnl -e 's/^{1}.*//g; print;' {0}\", 'clear lines ending with $USER_TEXT_INPUT': \"perl -wnl -e 's/.*{1}$//g; print;' {0}\", 'replace lines containing $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -wnl -e 's/.*{1}.*/{2}/g; print;' {0}\", 'replace lines starting with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -wnl -e 's/^{1}.*/{2}/g; print;' {0}\", 'replace lines ending with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -wnl -e 's/.*{1}$/{2}/g; print;' {0}\", 'append lines containing $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/$/{2}/ if /{1}/' {0}\", 'append lines starting with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/$/{2}/ if /^{1}/' {0}\", 'append lines ending with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/$/{2}/ if /{1}$/' {0}\", 'prepend lines containing $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/.*{1}.*/{2}$&/' {0}\", 'prepend lines starting with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/^{1}.*/{2}$&/' {0}\", 'prepend lines ending with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/.*{1}$/{2}$&/' {0}\", 'wrap lines containing $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/.*{1}.*/{2}$&{2}/' {0}\", 'wrap lines starting with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/^{1}.*/{2}$&{2}/' {0}\", 'wrap lines ending with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/.*{1}$/{2}$&{2}/' {0}\", 'select lines containing $USER_TEXT_INPUT': \"perl -wln -e '/{1}/ and print;' {0}\", 'select lines starting with $USER_TEXT_INPUT': \"perl -wln -e '/^{1}/ and print;' {0}\", 'select lines ending with $USER_TEXT_INPUT': \"perl -wln -e '/{1}$/ and print;' {0}\"}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def from_json(cls, command_tree_file, translations_file):\n",
      "        with open(command_tree_file, 'r') as fin:\n",
      "            tree_dict = json.load(fin)\n",
      "        with open(translations_file, 'r') as fin:\n",
      "            command_translations = json.load(fin)\n",
      "        return TokenTree(tree_dict, command_translations)\n",
      "from_json(cls=<class 'bted.token_tree.TokenTree'>, command_tree_file='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/XXXbihl+bted/XXXbihl+bted/bted/config/command_token_tree.json', translations_file='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/XXXbihl+bted/XXXbihl+bted/bted/config/command_perl_translations.json')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "return TokenTree(tree_dict, command_translations)\n",
      "State:\n",
      "{'root': {'append': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}, 'lines': {'starting': {'with': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}}}, 'ending': {'with': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}}}, 'containing': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}}}}, 'clear': {'lines': {'starting': {'with': {'$USER_TEXT_INPUT': {}}}, 'ending': {'with': {'$USER_TEXT_INPUT': {}}}, 'containing': {'$USER_TEXT_INPUT': {}}}}, 'delete': {'$USER_TEXT_INPUT': {}, 'lines': {'starting': {'with': {'$USER_TEXT_INPUT': {}}}, 'ending': {'with': {'$USER_TEXT_INPUT': {}}}, 'containing': {'$USER_TEXT_INPUT': {}}}}, 'insert': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}, 'lines': {'starting': {'with': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}}}, 'ending': {'with': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}}}, 'containing': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}}}}, 'prepend': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}, 'lines': {'starting': {'with': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}}}, 'ending': {'with': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}}}, 'containing': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}}}}, 'replace': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}, 'lines': {'starting': {'with': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}}}, 'ending': {'with': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}}}, 'containing': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}}}}, 'wrap': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}, 'lines': {'starting': {'with': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}}}, 'ending': {'with': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}}}, 'containing': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}}}}, 'select': {'lines': {'starting': {'with': {'$USER_TEXT_INPUT': {}}}, 'ending': {'with': {'$USER_TEXT_INPUT': {}}}, 'containing': {'$USER_TEXT_INPUT': {}}}}}, 'reusable_components': {'text-with-text': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}}, 'line-types': {'lines': {'starting': {'with': {'$USER_TEXT_INPUT': {}}}, 'ending': {'with': {'$USER_TEXT_INPUT': {}}}, 'containing': {'$USER_TEXT_INPUT': {}}}}, 'lines-relation-text-with-text': {'lines': {'starting': {'with': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}}}, 'ending': {'with': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}}}, 'containing': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}}}}}}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def enumerate_node_dict(self, node: dict, node_text: str, start_depth=0):\n",
      "        children_nodes = {}\n",
      "        if Keyword.REUSABLE_COMPONENTS.value in node:\n",
      "            reusable_component_identifiers = node.pop(Keyword.REUSABLE_COMPONENTS.value)\n",
      "            for identifier in reusable_component_identifiers:\n",
      "                node.update(self.command_tree[Keyword.REUSABLE_COMPONENTS.value][identifier])\n",
      "        for child_text, child_dict in node.items():\n",
      "            child = self.enumerate_node_dict(child_dict, child_text, start_depth + 1)\n",
      "            children_nodes[child_text] = child\n",
      "        return TokenNode(node_text, children_nodes, start_depth)\n",
      "enumerate_node_dict(self=<bted.token_tree.TokenTree object at 0x7fd91d89bd30>, node={'reusable_components': ['text-with-text', 'lines-relation-text-with-text']}, node_text='append', start_depth=1, self.command_translations={'delete $USER_TEXT_INPUT': \"perl -wnl -e 's/{1}//g; print;' {0}\", 'replace $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -wnl -e 's/{1}/{2}/g; print;' {0}\", 'append $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -wnl -e 's/{1}/{1}{2}/g; print;' {0}\", 'prepend $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -wnl -e 's/{1}/{2}{1}/g; print;' {0}\", 'wrap $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -wnl -e 's/{1}/{2}{1}{2}/g; print;' {0}\", 'delete lines containing $USER_TEXT_INPUT': \"perl -ne '/{1}/ or print' {0}\", 'delete lines starting with $USER_TEXT_INPUT': \"perl -ne '/^{1}/ or print' {0}\", 'delete lines ending with $USER_TEXT_INPUT': \"perl -ne '/{1}$/ or print' {0}\", 'clear lines containing $USER_TEXT_INPUT': \"perl -wnl -e 's/.*{1}.*//g; print;' {0}\", 'clear lines starting with $USER_TEXT_INPUT': \"perl -wnl -e 's/^{1}.*//g; print;' {0}\", 'clear lines ending with $USER_TEXT_INPUT': \"perl -wnl -e 's/.*{1}$//g; print;' {0}\", 'replace lines containing $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -wnl -e 's/.*{1}.*/{2}/g; print;' {0}\", 'replace lines starting with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -wnl -e 's/^{1}.*/{2}/g; print;' {0}\", 'replace lines ending with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -wnl -e 's/.*{1}$/{2}/g; print;' {0}\", 'append lines containing $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/$/{2}/ if /{1}/' {0}\", 'append lines starting with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/$/{2}/ if /^{1}/' {0}\", 'append lines ending with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/$/{2}/ if /{1}$/' {0}\", 'prepend lines containing $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/.*{1}.*/{2}$&/' {0}\", 'prepend lines starting with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/^{1}.*/{2}$&/' {0}\", 'prepend lines ending with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/.*{1}$/{2}$&/' {0}\", 'wrap lines containing $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/.*{1}.*/{2}$&{2}/' {0}\", 'wrap lines starting with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/^{1}.*/{2}$&{2}/' {0}\", 'wrap lines ending with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/.*{1}$/{2}$&{2}/' {0}\", 'select lines containing $USER_TEXT_INPUT': \"perl -wln -e '/{1}/ and print;' {0}\", 'select lines starting with $USER_TEXT_INPUT': \"perl -wln -e '/^{1}/ and print;' {0}\", 'select lines ending with $USER_TEXT_INPUT': \"perl -wln -e '/{1}$/ and print;' {0}\"}, self.command_tree={'root': {'append': {'reusable_components': ['text-with-text', 'lines-relation-text-with-text']}, 'clear': {'reusable_components': ['line-types']}, 'delete': {'$USER_TEXT_INPUT': {}, 'reusable_components': ['line-types']}, 'insert': {'reusable_components': ['text-with-text', 'lines-relation-text-with-text']}, 'prepend': {'reusable_components': ['text-with-text', 'lines-relation-text-with-text']}, 'replace': {'reusable_components': ['text-with-text', 'lines-relation-text-with-text']}, 'wrap': {'reusable_components': ['text-with-text', 'lines-relation-text-with-text']}, 'select': {'reusable_components': ['line-types']}}, 'reusable_components': {'text-with-text': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}}, 'line-types': {'lines': {'starting': {'with': {'$USER_TEXT_INPUT': {}}}, 'ending': {'with': {'$USER_TEXT_INPUT': {}}}, 'containing': {'$USER_TEXT_INPUT': {}}}}, 'lines-relation-text-with-text': {'lines': {'starting': {'with': {'reusable_components': ['text-with-text']}}, 'ending': {'with': {'reusable_components': ['text-with-text']}}, 'containing': {'reusable_components': ['text-with-text']}}}}})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "children_nodes = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def enumerate_node_dict(self, node: dict, node_text: str, start_depth=0):\n",
      "        children_nodes = {}\n",
      "        if Keyword.REUSABLE_COMPONENTS.value in node:\n",
      "            reusable_component_identifiers = node.pop(Keyword.REUSABLE_COMPONENTS.value)\n",
      "            for identifier in reusable_component_identifiers:\n",
      "                node.update(self.command_tree[Keyword.REUSABLE_COMPONENTS.value][identifier])\n",
      "        for child_text, child_dict in node.items():\n",
      "            child = self.enumerate_node_dict(child_dict, child_text, start_depth + 1)\n",
      "            children_nodes[child_text] = child\n",
      "        return TokenNode(node_text, children_nodes, start_depth)\n",
      "enumerate_node_dict(self=<bted.token_tree.TokenTree object at 0x7fd91d89bd30>, node={'reusable_components': ['text-with-text', 'lines-relation-text-with-text']}, node_text='append', start_depth=1, self.command_translations={'delete $USER_TEXT_INPUT': \"perl -wnl -e 's/{1}//g; print;' {0}\", 'replace $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -wnl -e 's/{1}/{2}/g; print;' {0}\", 'append $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -wnl -e 's/{1}/{1}{2}/g; print;' {0}\", 'prepend $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -wnl -e 's/{1}/{2}{1}/g; print;' {0}\", 'wrap $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -wnl -e 's/{1}/{2}{1}{2}/g; print;' {0}\", 'delete lines containing $USER_TEXT_INPUT': \"perl -ne '/{1}/ or print' {0}\", 'delete lines starting with $USER_TEXT_INPUT': \"perl -ne '/^{1}/ or print' {0}\", 'delete lines ending with $USER_TEXT_INPUT': \"perl -ne '/{1}$/ or print' {0}\", 'clear lines containing $USER_TEXT_INPUT': \"perl -wnl -e 's/.*{1}.*//g; print;' {0}\", 'clear lines starting with $USER_TEXT_INPUT': \"perl -wnl -e 's/^{1}.*//g; print;' {0}\", 'clear lines ending with $USER_TEXT_INPUT': \"perl -wnl -e 's/.*{1}$//g; print;' {0}\", 'replace lines containing $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -wnl -e 's/.*{1}.*/{2}/g; print;' {0}\", 'replace lines starting with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -wnl -e 's/^{1}.*/{2}/g; print;' {0}\", 'replace lines ending with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -wnl -e 's/.*{1}$/{2}/g; print;' {0}\", 'append lines containing $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/$/{2}/ if /{1}/' {0}\", 'append lines starting with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/$/{2}/ if /^{1}/' {0}\", 'append lines ending with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/$/{2}/ if /{1}$/' {0}\", 'prepend lines containing $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/.*{1}.*/{2}$&/' {0}\", 'prepend lines starting with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/^{1}.*/{2}$&/' {0}\", 'prepend lines ending with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/.*{1}$/{2}$&/' {0}\", 'wrap lines containing $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/.*{1}.*/{2}$&{2}/' {0}\", 'wrap lines starting with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/^{1}.*/{2}$&{2}/' {0}\", 'wrap lines ending with $USER_TEXT_INPUT with $USER_TEXT_INPUT': \"perl -pe 's/.*{1}$/{2}$&{2}/' {0}\", 'select lines containing $USER_TEXT_INPUT': \"perl -wln -e '/{1}/ and print;' {0}\", 'select lines starting with $USER_TEXT_INPUT': \"perl -wln -e '/^{1}/ and print;' {0}\", 'select lines ending with $USER_TEXT_INPUT': \"perl -wln -e '/{1}$/ and print;' {0}\"}, self.command_tree={'root': {'append': {'reusable_components': ['text-with-text', 'lines-relation-text-with-text']}, 'clear': {'reusable_components': ['line-types']}, 'delete': {'$USER_TEXT_INPUT': {}, 'reusable_components': ['line-types']}, 'insert': {'reusable_components': ['text-with-text', 'lines-relation-text-with-text']}, 'prepend': {'reusable_components': ['text-with-text', 'lines-relation-text-with-text']}, 'replace': {'reusable_components': ['text-with-text', 'lines-relation-text-with-text']}, 'wrap': {'reusable_components': ['text-with-text', 'lines-relation-text-with-text']}, 'select': {'reusable_components': ['line-types']}}, 'reusable_components': {'text-with-text': {'$USER_TEXT_INPUT': {'with': {'$USER_TEXT_INPUT': {}}}}, 'line-types': {'lines': {'starting': {'with': {'$USER_TEXT_INPUT': {}}}, 'ending': {'with': {'$USER_TEXT_INPUT': {}}}, 'containing': {'$USER_TEXT_INPUT': {}}}}, 'lines-relation-text-with-text': {'lines': {'starting': {'with': {'reusable_components': ['text-with-text']}}, 'ending': {'with': {'reusable_components': ['text-with-text']}}, 'containing': {'reusable_components': ['text-with-text']}}}}})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "reusable_component_identifiers = node.pop(Keyword.REUSABLE_COMPONENTS.value)\n",
      "State:\n",
      "['text-with-text', 'lines-relation-text-with-text']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def notify(self, msg: str) -> None:\n",
      "        self.msg_queue.append(msg)\n",
      "notify(self=<patterns.behavioral.publish_subscribe.Provider object at 0x7f11068ce5e0>, msg='sub 1 msg 1', self.msg_queue=[], self.subscribers={'sub 1 msg 1': [<patterns.behavioral.publish_subscribe.Subscriber object at 0x7f11068ce610>], 'sub 1 msg 2': [<patterns.behavioral.publish_subscribe.Subscriber object at 0x7f11068ce610>], 'sub 2 msg 1': [<patterns.behavioral.publish_subscribe.Subscriber object at 0x7f11068ce6d0>], 'sub 2 msg 2': [<patterns.behavioral.publish_subscribe.Subscriber object at 0x7f11068ce6d0>]})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.msg_queue.append(msg)\n",
      "State:\n",
      "['sub 1 msg 1']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __set__(self, obj: Order, value: Callable = None) -> None:\n",
      "        if value and self.validate(obj, value):\n",
      "            setattr(obj, self.private_name, value)\n",
      "        else:\n",
      "            setattr(obj, self.private_name, None)\n",
      "__set__(self=<patterns.behavioral.strategy.DiscountStrategyValidator object at 0x7f11068ce310>, obj=REPR FAILED, value=None, self.private_name='_discount_strategy')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "setattr(obj, self.private_name, None)\n",
      "State:\n",
      "<Order price: 100 with discount strategy: None>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __get__(self, obj, type_):\n",
      "        if obj is None:\n",
      "            return self\n",
      "        val = self.function(obj)\n",
      "        obj.__dict__[self.function.__name__] = val\n",
      "        return val\n",
      "__get__(self=<patterns.creational.lazy_evaluation.lazy_property object at 0x7f11068acb50>, obj={name='John', occupation='Coder', call_count2=0}, type_=<class 'patterns.creational.lazy_evaluation.Person'>, self.__annotations__={}, self.__doc__=None, self.__module__='patterns.creational.lazy_evaluation', self.__name__='relatives', self.__qualname__='Person.relatives', self.__wrapped__=<function Person.relatives at 0x7f11067f01f0>, self.function=<function Person.relatives at 0x7f11067f01f0>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "val = self.function(obj)\n",
      "State:\n",
      "'Many relatives.'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __get__(self, obj, type_):\n",
      "        if obj is None:\n",
      "            return self\n",
      "        val = self.function(obj)\n",
      "        obj.__dict__[self.function.__name__] = val\n",
      "        return val\n",
      "__get__(self=<patterns.creational.lazy_evaluation.lazy_property object at 0x7f11068acb50>, obj={name='John', occupation='Coder', call_count2=0}, type_=<class 'patterns.creational.lazy_evaluation.Person'>, self.__annotations__={}, self.__doc__=None, self.__module__='patterns.creational.lazy_evaluation', self.__name__='relatives', self.__qualname__='Person.relatives', self.__wrapped__=<function Person.relatives at 0x7f11067f01f0>, self.function=<function Person.relatives at 0x7f11067f01f0>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "obj.__dict__[self.function.__name__] = val\n",
      "State:\n",
      "{name='John', occupation='Coder', call_count2=0, relatives='Many relatives.'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _lazy_property(self):\n",
      "        if not hasattr(self, attr):\n",
      "            setattr(self, attr, fn(self))\n",
      "        return getattr(self, attr)\n",
      "_lazy_property(self=<patterns.creational.lazy_evaluation.Person object at 0x7f11066e92e0>, attr='_lazy__parents', fn=<function Person.parents at 0x7f11067f04c0>, self.call_count2=0, self.name='John', self.occupation='Coder')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "setattr(self, attr, fn(self))\n",
      "State:\n",
      "'Father and mother'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def clone(self, **attrs: Any) -> Prototype:\n",
      "        obj = self.__class__(**self.__dict__)\n",
      "        obj.__dict__.update(attrs)\n",
      "        return obj\n",
      "clone(self=<patterns.creational.prototype.Prototype object at 0x7f11066e9880>, attrs={}, self.value='default')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "obj = self.__class__(**self.__dict__)\n",
      "State:\n",
      "{value='default'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def scale(self, pct):\n",
      "        self._radius *= pct\n",
      "scale(self=<patterns.structural.bridge.CircleShape object at 0x7f1106769490>, pct=2, self._drawing_api=<patterns.structural.bridge.DrawingAPI1 object at 0x7f11067aa910>, self._radius=3, self._x=1, self._y=2)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self._radius *= pct\n",
      "State:\n",
      "6\n",
      "==================================================\n",
      "Clean Code:\n",
      "def on_message(self, message_type):\n",
      "        if message_type in self.message_types.keys():\n",
      "            self.message_types[message_type]()\n",
      "        else:\n",
      "            raise UnsupportedMessageType\n",
      "on_message(self=<patterns.other.hsm.hsm.HierachicalStateMachine object at 0x7f11067cd760>, message_type='trigger', self._active_state=<patterns.other.hsm.hsm.Active object at 0x7f110652a550>, self._current_state=<patterns.other.hsm.hsm.Standby object at 0x7f110652a580>, self._failed_state=<patterns.other.hsm.hsm.Failed object at 0x7f1106505730>, self._standby_state=<patterns.other.hsm.hsm.Standby object at 0x7f1106505610>, self._suspect_state=<patterns.other.hsm.hsm.Suspect object at 0x7f1106505700>, self.message_types={'fault trigger': <bound method Inservice.on_fault_trigger of <patterns.other.hsm.hsm.Standby object at 0x7f1106505610>>, 'switchover': <bound method Standby.on_switchover of <patterns.other.hsm.hsm.Standby object at 0x7f1106505610>>, 'diagnostics passed': <bound method Unit.on_diagnostics_passed of <patterns.other.hsm.hsm.Standby object at 0x7f1106505610>>, 'diagnostics failed': <bound method Unit.on_diagnostics_failed of <patterns.other.hsm.hsm.Standby object at 0x7f1106505610>>, 'operator inservice': <bound method Unit.on_operator_inservice of <patterns.other.hsm.hsm.Standby object at 0x7f1106505610>>}, self.states={'active': <patterns.other.hsm.hsm.Active object at 0x7f110652a550>, 'standby': <patterns.other.hsm.hsm.Standby object at 0x7f1106505610>, 'suspect': <patterns.other.hsm.hsm.Suspect object at 0x7f1106505700>, 'failed': <patterns.other.hsm.hsm.Failed object at 0x7f1106505730>})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "raise UnsupportedMessageType\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def fit(self, refstring, substring, get_score: bool = False) -> \"FFTAligner\":\n",
      "        refstring, substring = [\n",
      "            list(map(int, s)) if isinstance(s, str) else s\n",
      "            for s in [refstring, substring]\n",
      "        ]\n",
      "        refstring, substring = map(\n",
      "            lambda s: 2 * np.array(s).astype(float) - 1, [refstring, substring]\n",
      "        )\n",
      "        total_bits = math.log(len(substring) + len(refstring), 2)\n",
      "        total_length = int(2 ** math.ceil(total_bits))\n",
      "        extra_zeros = total_length - len(substring) - len(refstring)\n",
      "        subft = np.fft.fft(np.append(np.zeros(extra_zeros + len(refstring)), substring))\n",
      "        refft = np.fft.fft(\n",
      "            np.flip(np.append(refstring, np.zeros(len(substring) + extra_zeros)), 0)\n",
      "        )\n",
      "        convolve = np.real(np.fft.ifft(subft * refft))\n",
      "        self._compute_argmax(\n",
      "            self._eliminate_extreme_offsets_from_solutions(convolve, substring),\n",
      "            substring,\n",
      "        )\n",
      "        self.get_score_ = get_score\n",
      "        return self\n",
      "fit(self=<ffsubsync.aligners.FFTAligner object at 0x7fb0b6c79490>, refstring='11001', substring='111001', get_score=False, self.best_offset_=None, self.best_score_=None, self.get_score_=False, self.max_offset_samples=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "refstring, substring = [\n",
      "State:\n",
      "[1, 1, 0, 0, 1]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def fit(self, refstring, substring, get_score: bool = False) -> \"FFTAligner\":\n",
      "        refstring, substring = [\n",
      "            list(map(int, s)) if isinstance(s, str) else s\n",
      "            for s in [refstring, substring]\n",
      "        ]\n",
      "        refstring, substring = map(\n",
      "            lambda s: 2 * np.array(s).astype(float) - 1, [refstring, substring]\n",
      "        )\n",
      "        total_bits = math.log(len(substring) + len(refstring), 2)\n",
      "        total_length = int(2 ** math.ceil(total_bits))\n",
      "        extra_zeros = total_length - len(substring) - len(refstring)\n",
      "        subft = np.fft.fft(np.append(np.zeros(extra_zeros + len(refstring)), substring))\n",
      "        refft = np.fft.fft(\n",
      "            np.flip(np.append(refstring, np.zeros(len(substring) + extra_zeros)), 0)\n",
      "        )\n",
      "        convolve = np.real(np.fft.ifft(subft * refft))\n",
      "        self._compute_argmax(\n",
      "            self._eliminate_extreme_offsets_from_solutions(convolve, substring),\n",
      "            substring,\n",
      "        )\n",
      "        self.get_score_ = get_score\n",
      "        return self\n",
      "fit(self=<ffsubsync.aligners.FFTAligner object at 0x7fb0b6c79490>, refstring='11001', substring='111001', get_score=False, self.best_offset_=None, self.best_score_=None, self.get_score_=False, self.max_offset_samples=None)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "refstring, substring = map(\n",
      "State:\n",
      "array([ 1.,  1., -1., -1.,  1.])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def fit(self, refstring, substring, get_score: bool = False) -> \"FFTAligner\":\n",
      "        refstring, substring = [\n",
      "            list(map(int, s)) if isinstance(s, str) else s\n",
      "            for s in [refstring, substring]\n",
      "        ]\n",
      "        refstring, substring = map(\n",
      "            lambda s: 2 * np.array(s).astype(float) - 1, [refstring, substring]\n",
      "        )\n",
      "        total_bits = math.log(len(substring) + len(refstring), 2)\n",
      "        total_length = int(2 ** math.ceil(total_bits))\n",
      "        extra_zeros = total_length - len(substring) - len(refstring)\n",
      "        subft = np.fft.fft(np.append(np.zeros(extra_zeros + len(refstring)), substring))\n",
      "        refft = np.fft.fft(\n",
      "            np.flip(np.append(refstring, np.zeros(len(substring) + extra_zeros)), 0)\n",
      "        )\n",
      "        convolve = np.real(np.fft.ifft(subft * refft))\n",
      "        self._compute_argmax(\n",
      "            self._eliminate_extreme_offsets_from_solutions(convolve, substring),\n",
      "            substring,\n",
      "        )\n",
      "        self.get_score_ = get_score\n",
      "        return self\n",
      "fit(self=<ffsubsync.aligners.FFTAligner object at 0x7fb0b6c79490>, refstring='11001', substring='111001', get_score=False, self.best_offset_=None, self.best_score_=None, self.get_score_=False, self.max_offset_samples=None)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "extra_zeros = total_length - len(substring) - len(refstring)\n",
      "State:\n",
      "5\n",
      "==================================================\n",
      "Clean Code:\n",
      "def fit(self, refstring, substring, get_score: bool = False) -> \"FFTAligner\":\n",
      "        refstring, substring = [\n",
      "            list(map(int, s)) if isinstance(s, str) else s\n",
      "            for s in [refstring, substring]\n",
      "        ]\n",
      "        refstring, substring = map(\n",
      "            lambda s: 2 * np.array(s).astype(float) - 1, [refstring, substring]\n",
      "        )\n",
      "        total_bits = math.log(len(substring) + len(refstring), 2)\n",
      "        total_length = int(2 ** math.ceil(total_bits))\n",
      "        extra_zeros = total_length - len(substring) - len(refstring)\n",
      "        subft = np.fft.fft(np.append(np.zeros(extra_zeros + len(refstring)), substring))\n",
      "        refft = np.fft.fft(\n",
      "            np.flip(np.append(refstring, np.zeros(len(substring) + extra_zeros)), 0)\n",
      "        )\n",
      "        convolve = np.real(np.fft.ifft(subft * refft))\n",
      "        self._compute_argmax(\n",
      "            self._eliminate_extreme_offsets_from_solutions(convolve, substring),\n",
      "            substring,\n",
      "        )\n",
      "        self.get_score_ = get_score\n",
      "        return self\n",
      "fit(self=<ffsubsync.aligners.FFTAligner object at 0x7fb0b6c79490>, refstring='11001', substring='111001', get_score=False, self.best_offset_=None, self.best_score_=None, self.get_score_=False, self.max_offset_samples=None)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "convolve = np.real(np.fft.ifft(subft * refft))\n",
      "State:\n",
      "array([ 2.22044605e-16,  1.11022302e-16, -4.44089210e-16, -1.94289029e-16,       -4.44089210e-16,  1.00000000e+00,  5.55111512e-16, -1.00000000e+00,       -2.00000000e+00,  1.00000000e+00,  5.00000000e+00,  1.94289029e-16,       -3.00000000e+00, -2.22044605e-16,  1.00000000e+00, -5.55111512e-17])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _compute_argmax(self, convolve: np.ndarray, substring: np.ndarray) -> None:\n",
      "        best_idx = int(np.argmax(convolve))\n",
      "        self.best_offset_ = len(convolve) - 1 - best_idx - len(substring)\n",
      "        self.best_score_ = convolve[best_idx]\n",
      "_compute_argmax(self=<ffsubsync.aligners.FFTAligner object at 0x7fb0b6c79490>, convolve=array([ 2.22044605e-16,  1.11022302e-16, -4.44089210e-16, -1.94289029e-16,       -4.44089210e-16,  1.00000000e+00,  5.55111512e-16, -1.00000000e+00,       -2.00000000e+00,  1.00000000e+00,  5.00000000e+00,  1.94289029e-16,       -3.00000000e+00, -2.22044605e-16,  1.00000000e+00, -5.55111512e-17]), substring=array([ 1.,  1.,  1., -1., -1.,  1.]), self.best_offset_=None, self.best_score_=None, self.get_score_=False, self.max_offset_samples=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "best_idx = int(np.argmax(convolve))\n",
      "State:\n",
      "10\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _compute_argmax(self, convolve: np.ndarray, substring: np.ndarray) -> None:\n",
      "        best_idx = int(np.argmax(convolve))\n",
      "        self.best_offset_ = len(convolve) - 1 - best_idx - len(substring)\n",
      "        self.best_score_ = convolve[best_idx]\n",
      "_compute_argmax(self=<ffsubsync.aligners.FFTAligner object at 0x7fb0b6c79490>, convolve=array([ 2.22044605e-16,  1.11022302e-16, -4.44089210e-16, -1.94289029e-16,       -4.44089210e-16,  1.00000000e+00,  5.55111512e-16, -1.00000000e+00,       -2.00000000e+00,  1.00000000e+00,  5.00000000e+00,  1.94289029e-16,       -3.00000000e+00, -2.22044605e-16,  1.00000000e+00, -5.55111512e-17]), substring=array([ 1.,  1.,  1., -1., -1.,  1.]), self.best_offset_=None, self.best_score_=None, self.get_score_=False, self.max_offset_samples=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.best_offset_ = len(convolve) - 1 - best_idx - len(substring)\n",
      "State:\n",
      "-1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _compute_argmax(self, convolve: np.ndarray, substring: np.ndarray) -> None:\n",
      "        best_idx = int(np.argmax(convolve))\n",
      "        self.best_offset_ = len(convolve) - 1 - best_idx - len(substring)\n",
      "        self.best_score_ = convolve[best_idx]\n",
      "_compute_argmax(self=<ffsubsync.aligners.FFTAligner object at 0x7fb0b6c79490>, convolve=array([ 2.22044605e-16,  1.11022302e-16, -4.44089210e-16, -1.94289029e-16,       -4.44089210e-16,  1.00000000e+00,  5.55111512e-16, -1.00000000e+00,       -2.00000000e+00,  1.00000000e+00,  5.00000000e+00,  1.94289029e-16,       -3.00000000e+00, -2.22044605e-16,  1.00000000e+00, -5.55111512e-17]), substring=array([ 1.,  1.,  1., -1., -1.,  1.]), self.best_offset_=None, self.best_score_=None, self.get_score_=False, self.max_offset_samples=None)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.best_score_ = convolve[best_idx]\n",
      "State:\n",
      "5.0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def make_version_tuple(vstr=None):\n",
      "    if vstr is None:\n",
      "        vstr = __version__\n",
      "    if vstr[0] == \"v\":\n",
      "        vstr = vstr[1:]\n",
      "    components = []\n",
      "    for component in vstr.split(\"+\")[0].split(\".\"):\n",
      "        try:\n",
      "            components.append(int(component))\n",
      "        except ValueError:\n",
      "            break\n",
      "    return tuple(components)\n",
      "make_version_tuple(vstr='v0.1.1')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "vstr = vstr[1:]\n",
      "State:\n",
      "'0.1.1'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def make_version_tuple(vstr=None):\n",
      "    if vstr is None:\n",
      "        vstr = __version__\n",
      "    if vstr[0] == \"v\":\n",
      "        vstr = vstr[1:]\n",
      "    components = []\n",
      "    for component in vstr.split(\"+\")[0].split(\".\"):\n",
      "        try:\n",
      "            components.append(int(component))\n",
      "        except ValueError:\n",
      "            break\n",
      "    return tuple(components)\n",
      "make_version_tuple(vstr='v0.1.1')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "components = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_start_seconds(start_seconds):\n",
      "    parser_zero = GenericSubtitleParser(start_seconds=0)\n",
      "    parser_zero.fit(BytesIO(fake_srt))\n",
      "    parser = GenericSubtitleParser(start_seconds=start_seconds)\n",
      "    parser.fit(BytesIO(fake_srt))\n",
      "    expected = [\n",
      "        sub\n",
      "        for sub in parser_zero.subs_\n",
      "        if sub.start >= timedelta(seconds=start_seconds)\n",
      "    ]\n",
      "    assert all(esub == psub for esub, psub in zip(expected, parser.subs_))\n",
      "test_start_seconds(start_seconds=0)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "parser_zero = GenericSubtitleParser(start_seconds=0)\n",
      "State:\n",
      "{subs_=None, sub_format='srt', encoding='infer', caching=False, fit_fname=None, detected_encoding_=None, max_subtitle_seconds=None, start_seconds=0, _skip_ssa_info=False, _strict=False}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_start_seconds(start_seconds):\n",
      "    parser_zero = GenericSubtitleParser(start_seconds=0)\n",
      "    parser_zero.fit(BytesIO(fake_srt))\n",
      "    parser = GenericSubtitleParser(start_seconds=start_seconds)\n",
      "    parser.fit(BytesIO(fake_srt))\n",
      "    expected = [\n",
      "        sub\n",
      "        for sub in parser_zero.subs_\n",
      "        if sub.start >= timedelta(seconds=start_seconds)\n",
      "    ]\n",
      "    assert all(esub == psub for esub, psub in zip(expected, parser.subs_))\n",
      "test_start_seconds(start_seconds=0)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "parser = GenericSubtitleParser(start_seconds=start_seconds)\n",
      "State:\n",
      "{subs_=None, sub_format='srt', encoding='infer', caching=False, fit_fname=None, detected_encoding_=None, max_subtitle_seconds=None, start_seconds=0, _skip_ssa_info=False, _strict=False}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_max_seconds(max_seconds):\n",
      "    parser = GenericSubtitleParser(max_subtitle_seconds=max_seconds)\n",
      "    parser.fit(BytesIO(fake_srt))\n",
      "    assert max(sub.end - sub.start for sub in parser.subs_) <= timedelta(\n",
      "        seconds=max_seconds\n",
      "    )\n",
      "test_max_seconds(max_seconds=1)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "parser = GenericSubtitleParser(max_subtitle_seconds=max_seconds)\n",
      "State:\n",
      "{subs_=None, sub_format='srt', encoding='infer', caching=False, fit_fname=None, detected_encoding_=None, max_subtitle_seconds=1, start_seconds=0, _skip_ssa_info=False, _strict=False}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_same_encoding(encoding):\n",
      "    parser = GenericSubtitleParser(encoding=encoding)\n",
      "    offseter = SubtitleShifter(1)\n",
      "    pipe = make_pipeline(parser, offseter)\n",
      "    pipe.fit(BytesIO(fake_srt))\n",
      "    assert parser.subs_._encoding == encoding\n",
      "    assert offseter.subs_._encoding == parser.subs_._encoding\n",
      "    assert offseter.subs_.set_encoding(\"same\")._encoding == encoding\n",
      "    assert offseter.subs_.set_encoding(\"utf-8\")._encoding == \"utf-8\"\n",
      "test_same_encoding(encoding='utf-8')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "parser = GenericSubtitleParser(encoding=encoding)\n",
      "State:\n",
      "{subs_=None, sub_format='srt', encoding='utf-8', caching=False, fit_fname=None, detected_encoding_=None, max_subtitle_seconds=None, start_seconds=0, _skip_ssa_info=False, _strict=False}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_same_encoding(encoding):\n",
      "    parser = GenericSubtitleParser(encoding=encoding)\n",
      "    offseter = SubtitleShifter(1)\n",
      "    pipe = make_pipeline(parser, offseter)\n",
      "    pipe.fit(BytesIO(fake_srt))\n",
      "    assert parser.subs_._encoding == encoding\n",
      "    assert offseter.subs_._encoding == parser.subs_._encoding\n",
      "    assert offseter.subs_.set_encoding(\"same\")._encoding == encoding\n",
      "    assert offseter.subs_.set_encoding(\"utf-8\")._encoding == \"utf-8\"\n",
      "test_same_encoding(encoding='utf-8')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "offseter = SubtitleShifter(1)\n",
      "State:\n",
      "{td_seconds=datetime.timedelta(seconds=1)}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_same_encoding(encoding):\n",
      "    parser = GenericSubtitleParser(encoding=encoding)\n",
      "    offseter = SubtitleShifter(1)\n",
      "    pipe = make_pipeline(parser, offseter)\n",
      "    pipe.fit(BytesIO(fake_srt))\n",
      "    assert parser.subs_._encoding == encoding\n",
      "    assert offseter.subs_._encoding == parser.subs_._encoding\n",
      "    assert offseter.subs_.set_encoding(\"same\")._encoding == encoding\n",
      "    assert offseter.subs_.set_encoding(\"utf-8\")._encoding == \"utf-8\"\n",
      "test_same_encoding(encoding='utf-8')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "assert parser.subs_._encoding == encoding\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def offset(self, td: timedelta) -> \"GenericSubtitlesFile\":\n",
      "        offset_subs = []\n",
      "        for sub in self.subs_:\n",
      "            offset_subs.append(GenericSubtitle(sub.start + td, sub.end + td, sub.inner))\n",
      "        return self.clone_props_for_subs(offset_subs)\n",
      "offset(self=<ffsubsync.generic_subtitles.GenericSubtitlesFile object at 0x7fb01543e700>, td=datetime.timedelta(seconds=1), self._encoding='utf-8', self._fonts_opaque=None, self._info=None, self._styles=None, self._sub_format='srt', self.subs_=[<ffsubsync.generic_subtitles.GenericSubtitle object at 0x7fb01543ef70>, <ffsubsync.generic_subtitles.GenericSubtitle object at 0x7fb01543eee0>, <ffsubsync.generic_subtitles.GenericSubtitle object at 0x7fb01543e490>])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "offset_subs = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_encoding(self, encoding: str) -> \"GenericSubtitlesFile\":\n",
      "        if encoding != \"same\":\n",
      "            self._encoding = encoding\n",
      "        return self\n",
      "set_encoding(self=<ffsubsync.generic_subtitles.GenericSubtitlesFile object at 0x7fb01543e430>, encoding='utf-8', self._encoding='ascii', self._fonts_opaque=None, self._info=None, self._styles=None, self._sub_format='srt', self.subs_=[<ffsubsync.generic_subtitles.GenericSubtitle object at 0x7fb01543e4c0>, <ffsubsync.generic_subtitles.GenericSubtitle object at 0x7fb01543ed90>, <ffsubsync.generic_subtitles.GenericSubtitle object at 0x7fb01543e3d0>])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self._encoding = encoding\n",
      "State:\n",
      "'utf-8'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_offset(offset):\n",
      "    parser = GenericSubtitleParser()\n",
      "    offseter = SubtitleShifter(offset)\n",
      "    pipe = make_pipeline(parser, offseter)\n",
      "    pipe.fit(BytesIO(fake_srt))\n",
      "    for sub_orig, sub_offset in zip(parser.subs_, offseter.subs_):\n",
      "        assert (\n",
      "            abs(\n",
      "                sub_offset.start.total_seconds()\n",
      "                - sub_orig.start.total_seconds()\n",
      "                - offset\n",
      "            )\n",
      "            < 1e-6\n",
      "        )\n",
      "        assert (\n",
      "            abs(sub_offset.end.total_seconds() - sub_orig.end.total_seconds() - offset)\n",
      "            < 1e-6\n",
      "        )\n",
      "test_offset(offset=1)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "parser = GenericSubtitleParser()\n",
      "State:\n",
      "{subs_=None, sub_format='srt', encoding='infer', caching=False, fit_fname=None, detected_encoding_=None, max_subtitle_seconds=None, start_seconds=0, _skip_ssa_info=False, _strict=False}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_offset(offset):\n",
      "    parser = GenericSubtitleParser()\n",
      "    offseter = SubtitleShifter(offset)\n",
      "    pipe = make_pipeline(parser, offseter)\n",
      "    pipe.fit(BytesIO(fake_srt))\n",
      "    for sub_orig, sub_offset in zip(parser.subs_, offseter.subs_):\n",
      "        assert (\n",
      "            abs(\n",
      "                sub_offset.start.total_seconds()\n",
      "                - sub_orig.start.total_seconds()\n",
      "                - offset\n",
      "            )\n",
      "            < 1e-6\n",
      "        )\n",
      "        assert (\n",
      "            abs(sub_offset.end.total_seconds() - sub_orig.end.total_seconds() - offset)\n",
      "            < 1e-6\n",
      "        )\n",
      "test_offset(offset=1)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "offseter = SubtitleShifter(offset)\n",
      "State:\n",
      "{td_seconds=datetime.timedelta(seconds=1)}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_speech_extraction(sample_rate, start_seconds):\n",
      "    parser = GenericSubtitleParser(start_seconds=start_seconds)\n",
      "    extractor = SubtitleSpeechTransformer(\n",
      "        sample_rate=sample_rate, start_seconds=start_seconds\n",
      "    )\n",
      "    pipe = make_pipeline(parser, extractor)\n",
      "    bitstring = pipe.fit_transform(BytesIO(fake_srt)).astype(bool)\n",
      "    bitstring_shifted_left = np.append(bitstring[1:], [False])\n",
      "    bitstring_shifted_right = np.append([False], bitstring[:-1])\n",
      "    bitstring_cumsum = np.cumsum(bitstring)\n",
      "    consec_ones_end_pos = np.nonzero(\n",
      "        bitstring_cumsum\n",
      "        * (bitstring ^ bitstring_shifted_left)\n",
      "        * (bitstring_cumsum != np.cumsum(bitstring_shifted_right))\n",
      "    )[0]\n",
      "    prev = 0\n",
      "    for pos, sub in zip(consec_ones_end_pos, parser.subs_):\n",
      "        start = int(round(sub.start.total_seconds() * sample_rate))\n",
      "        duration = sub.end.total_seconds() - sub.start.total_seconds()\n",
      "        stop = start + int(round(duration * sample_rate))\n",
      "        assert bitstring_cumsum[pos] - prev == stop - start\n",
      "        prev = bitstring_cumsum[pos]\n",
      "test_speech_extraction(sample_rate=10, start_seconds=0)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "parser = GenericSubtitleParser(start_seconds=start_seconds)\n",
      "State:\n",
      "{subs_=None, sub_format='srt', encoding='infer', caching=False, fit_fname=None, detected_encoding_=None, max_subtitle_seconds=None, start_seconds=0, _skip_ssa_info=False, _strict=False}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_speech_extraction(sample_rate, start_seconds):\n",
      "    parser = GenericSubtitleParser(start_seconds=start_seconds)\n",
      "    extractor = SubtitleSpeechTransformer(\n",
      "        sample_rate=sample_rate, start_seconds=start_seconds\n",
      "    )\n",
      "    pipe = make_pipeline(parser, extractor)\n",
      "    bitstring = pipe.fit_transform(BytesIO(fake_srt)).astype(bool)\n",
      "    bitstring_shifted_left = np.append(bitstring[1:], [False])\n",
      "    bitstring_shifted_right = np.append([False], bitstring[:-1])\n",
      "    bitstring_cumsum = np.cumsum(bitstring)\n",
      "    consec_ones_end_pos = np.nonzero(\n",
      "        bitstring_cumsum\n",
      "        * (bitstring ^ bitstring_shifted_left)\n",
      "        * (bitstring_cumsum != np.cumsum(bitstring_shifted_right))\n",
      "    )[0]\n",
      "    prev = 0\n",
      "    for pos, sub in zip(consec_ones_end_pos, parser.subs_):\n",
      "        start = int(round(sub.start.total_seconds() * sample_rate))\n",
      "        duration = sub.end.total_seconds() - sub.start.total_seconds()\n",
      "        stop = start + int(round(duration * sample_rate))\n",
      "        assert bitstring_cumsum[pos] - prev == stop - start\n",
      "        prev = bitstring_cumsum[pos]\n",
      "test_speech_extraction(sample_rate=10, start_seconds=0)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "extractor = SubtitleSpeechTransformer(\n",
      "State:\n",
      "{sample_rate=10, start_seconds=0, framerate_ratio=1.0, subtitle_speech_results_=None, max_time_=None}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_speech_extraction(sample_rate, start_seconds):\n",
      "    parser = GenericSubtitleParser(start_seconds=start_seconds)\n",
      "    extractor = SubtitleSpeechTransformer(\n",
      "        sample_rate=sample_rate, start_seconds=start_seconds\n",
      "    )\n",
      "    pipe = make_pipeline(parser, extractor)\n",
      "    bitstring = pipe.fit_transform(BytesIO(fake_srt)).astype(bool)\n",
      "    bitstring_shifted_left = np.append(bitstring[1:], [False])\n",
      "    bitstring_shifted_right = np.append([False], bitstring[:-1])\n",
      "    bitstring_cumsum = np.cumsum(bitstring)\n",
      "    consec_ones_end_pos = np.nonzero(\n",
      "        bitstring_cumsum\n",
      "        * (bitstring ^ bitstring_shifted_left)\n",
      "        * (bitstring_cumsum != np.cumsum(bitstring_shifted_right))\n",
      "    )[0]\n",
      "    prev = 0\n",
      "    for pos, sub in zip(consec_ones_end_pos, parser.subs_):\n",
      "        start = int(round(sub.start.total_seconds() * sample_rate))\n",
      "        duration = sub.end.total_seconds() - sub.start.total_seconds()\n",
      "        stop = start + int(round(duration * sample_rate))\n",
      "        assert bitstring_cumsum[pos] - prev == stop - start\n",
      "        prev = bitstring_cumsum[pos]\n",
      "test_speech_extraction(sample_rate=10, start_seconds=0)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "consec_ones_end_pos = np.nonzero(\n",
      "State:\n",
      "array([23, 44, 60])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_speech_extraction(sample_rate, start_seconds):\n",
      "    parser = GenericSubtitleParser(start_seconds=start_seconds)\n",
      "    extractor = SubtitleSpeechTransformer(\n",
      "        sample_rate=sample_rate, start_seconds=start_seconds\n",
      "    )\n",
      "    pipe = make_pipeline(parser, extractor)\n",
      "    bitstring = pipe.fit_transform(BytesIO(fake_srt)).astype(bool)\n",
      "    bitstring_shifted_left = np.append(bitstring[1:], [False])\n",
      "    bitstring_shifted_right = np.append([False], bitstring[:-1])\n",
      "    bitstring_cumsum = np.cumsum(bitstring)\n",
      "    consec_ones_end_pos = np.nonzero(\n",
      "        bitstring_cumsum\n",
      "        * (bitstring ^ bitstring_shifted_left)\n",
      "        * (bitstring_cumsum != np.cumsum(bitstring_shifted_right))\n",
      "    )[0]\n",
      "    prev = 0\n",
      "    for pos, sub in zip(consec_ones_end_pos, parser.subs_):\n",
      "        start = int(round(sub.start.total_seconds() * sample_rate))\n",
      "        duration = sub.end.total_seconds() - sub.start.total_seconds()\n",
      "        stop = start + int(round(duration * sample_rate))\n",
      "        assert bitstring_cumsum[pos] - prev == stop - start\n",
      "        prev = bitstring_cumsum[pos]\n",
      "test_speech_extraction(sample_rate=10, start_seconds=0)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "prev = 0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False):\n",
      "    assert isinstance(commands, list)\n",
      "    p = None\n",
      "    for c in commands:\n",
      "        try:\n",
      "            p = subprocess.Popen([c] + args, cwd=cwd, stdout=subprocess.PIPE,\n",
      "                                 stderr=(subprocess.PIPE if hide_stderr\n",
      "                                         else None))\n",
      "            break\n",
      "        except EnvironmentError:\n",
      "            e = sys.exc_info()[1]\n",
      "            if e.errno == errno.ENOENT:\n",
      "                continue\n",
      "            if verbose:\n",
      "                print(\"unable to run %s\" % args[0])\n",
      "                print(e)\n",
      "            return None\n",
      "    else:\n",
      "        if verbose:\n",
      "            print(\"unable to find command, tried %s\" % (commands,))\n",
      "        return None\n",
      "    stdout = p.communicate()[0].strip()\n",
      "    if sys.version >= '3':\n",
      "        stdout = stdout.decode()\n",
      "    if p.returncode != 0:\n",
      "        if verbose:\n",
      "            print(\"unable to run %s (error)\" % args[0])\n",
      "        return None\n",
      "    return stdout\n",
      "run_command(commands=['git'], args=['describe', '--tags', '--dirty', '--always'], cwd='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/andsor+pydevs/andsor+pydevs', verbose=False, hide_stderr=False)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "p = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _collect_valid_settings(meta, clsdict):\n",
      "        enum_members = clsdict['__members__']\n",
      "        valid_settings = []\n",
      "        for member in enum_members:\n",
      "            valid_settings.extend(member.valid_settings)\n",
      "        clsdict['_valid_settings'] = valid_settings\n",
      "_collect_valid_settings(meta=<class 'pptx.enum.base.MetaEnumeration'>, clsdict={'__module__': 'pptx.enum.base', '__doc__': \"\\n    Base class for all enumerations, used directly for enumerations requiring\\n    only basic behavior. It's __dict__ is used below in the Python 2+3\\n    compatible metaclass definition.\\n    \", '__members__': (), '__ms_name__': '', 'validate': <classmethod object at 0x7efd5b56c8b0>, '__dict__': <attribute '__dict__' of 'EnumerationBase' objects>, '__weakref__': <attribute '__weakref__' of 'EnumerationBase' objects>})\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "enum_members = clsdict['__members__']\n",
      "State:\n",
      "()\n",
      "==================================================\n",
      "Clean Code:\n",
      "def register_xml_mapping(self, clsdict):\n",
      "        member_to_xml = self._get_or_add_member_to_xml(clsdict)\n",
      "        member_to_xml[self.value] = self.xml_value\n",
      "        xml_to_member = self._get_or_add_xml_to_member(clsdict)\n",
      "        xml_to_member[self.xml_value] = self.value\n",
      "register_xml_mapping(self=<pptx.enum.base.XmlMappedEnumMember object at 0x7efd5b5f4610>, clsdict={'__module__': 'pptx.enum.chart', '__qualname__': 'XL_AXIS_CROSSES', '__doc__': '\\n    Specifies the point on the specified axis where the other axis crosses.\\n\\n    Example::\\n\\n        from pptx.enum.chart import XL_AXIS_CROSSES\\n\\n        value_axis.crosses = XL_AXIS_CROSSES.MAXIMUM\\n    ', '__ms_name__': 'XlAxisCrosses', '__url__': 'https://msdn.microsoft.com/en-us/library/office/ff745402.aspx', '__members__': (<pptx.enum.base.XmlMappedEnumMember object at 0x7efd5b5f4610>, <pptx.enum.base.ReturnValueOnlyEnumMember object at 0x7efd5b5f4d00>, <pptx.enum.base.XmlMappedEnumMember object at 0x7efd5b5f4ee0>, <pptx.enum.base.XmlMappedEnumMember object at 0x7efd5b5f4f40>), 'AUTOMATIC': -4105}, self._docstring='The axis crossing point is set automatically, often at zero.', self._name='AUTOMATIC', self._value=-4105, self._xml_value='autoZero')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "member_to_xml = self._get_or_add_member_to_xml(clsdict)\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _member_def(self, member):\n",
      "        member_docstring = textwrap.dedent(member.docstring).strip()\n",
      "        member_docstring = textwrap.fill(\n",
      "            member_docstring, width=78, initial_indent=' '*4,\n",
      "            subsequent_indent=' '*4\n",
      "        )\n",
      "        return '%s\\n%s\\n' % (member.name, member_docstring)\n",
      "_member_def(self=<pptx.enum.base._DocsPageFormatter object at 0x7efd5b56cc10>, member={_name='AUTOMATIC', _value=-4105, _docstring='The axis crossing point is set automatically, often at zero.', _xml_value='autoZero'}, self._clsdict={'__module__': 'pptx.enum.chart', '__qualname__': 'XL_AXIS_CROSSES', '__doc__': '\\n    Specifies the point on the specified axis where the other axis crosses.\\n\\n    Example::\\n\\n        from pptx.enum.chart import XL_AXIS_CROSSES\\n\\n        value_axis.crosses = XL_AXIS_CROSSES.MAXIMUM\\n    ', '__ms_name__': 'XlAxisCrosses', '__url__': 'https://msdn.microsoft.com/en-us/library/office/ff745402.aspx', '__members__': (<pptx.enum.base.XmlMappedEnumMember object at 0x7efd5b5f4610>, <pptx.enum.base.ReturnValueOnlyEnumMember object at 0x7efd5b5f4d00>, <pptx.enum.base.XmlMappedEnumMember object at 0x7efd5b5f4ee0>, <pptx.enum.base.XmlMappedEnumMember object at 0x7efd5b5f4f40>), 'AUTOMATIC': -4105, '_member_to_xml': {-4105: 'autoZero', 2: 'max', 4: 'min'}, '_xml_to_member': {'autoZero': -4105, 'max': 2, 'min': 4}, 'CUSTOM': -4114, 'MAXIMUM': 2, 'MINIMUM': 4, '_valid_settings': [-4105, 2, 4]}, self._clsname='XL_AXIS_CROSSES')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "member_docstring = textwrap.dedent(member.docstring).strip()\n",
      "State:\n",
      "'The axis crossing point is set automatically, often at zero.'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def chat(self, message=None, display=True, stream=False, blocking=True):\n",
      "        try:\n",
      "            self.responding = True\n",
      "            if self.anonymous_telemetry and not self.offline:\n",
      "                message_type = type(\n",
      "                    message\n",
      "                ).__name__\n",
      "                send_telemetry(\n",
      "                    \"started_chat\",\n",
      "                    properties={\n",
      "                        \"in_terminal_interface\": self.in_terminal_interface,\n",
      "                        \"message_type\": message_type,\n",
      "                        \"os_mode\": self.os,\n",
      "                    },\n",
      "                )\n",
      "            if not blocking:\n",
      "                chat_thread = threading.Thread(\n",
      "                    target=self.chat, args=(message, display, stream, True)\n",
      "                )\n",
      "                chat_thread.start()\n",
      "                return\n",
      "            if stream:\n",
      "                return self._streaming_chat(message=message, display=display)\n",
      "            for _ in self._streaming_chat(message=message, display=display):\n",
      "                pass\n",
      "            self.responding = False\n",
      "            return self.messages[self.last_messages_count :]\n",
      "        except Exception as e:\n",
      "            self.responding = False\n",
      "            if self.anonymous_telemetry and not self.offline:\n",
      "                message_type = type(message).__name__\n",
      "                send_telemetry(\n",
      "                    \"errored\",\n",
      "                    properties={\n",
      "                        \"error\": str(e),\n",
      "                        \"in_terminal_interface\": self.in_terminal_interface,\n",
      "                        \"message_type\": message_type,\n",
      "                        \"os_mode\": self.os,\n",
      "                    },\n",
      "                )\n",
      "            raise\n",
      "chat(self=<interpreter.core.core.OpenInterpreter object at 0x7f3ae468a0d0>, message='Hello!', display=True, stream=False, blocking=False, self.anonymous_telemetry=True, self.auto_run=True, self.computer=<interpreter.core.computer.computer.Computer object at 0x7f3a3c1a2310>, self.conversation_filename=None, self.conversation_history=True, self.conversation_history_path='/home/XXX/.config/Open Interpreter/conversations', self.custom_instructions='', self.force_task_completion=False, self.in_terminal_interface=False, self.last_messages_count=0, self.llm=<interpreter.core.llm.llm.Llm object at 0x7f3a3c075130>, self.max_output=2800, self.messages=[], self.offline=False, self.os=False, self.responding=False, self.safe_mode='off', self.shrink_images=False, self.speak_messages=False, self.system_message='You are Open Interpreter, a world-class programmer that can complete any goal by executing code.\\nFirst, write a plan. **Always recap the plan between each code block** (you have extreme short-term memory loss, so you need to recap the plan between each message block to retain it).\\nWhen you execute code, it will be executed **on the user\\'s machine**. The user has given you **full and complete permission** to execute any code necessary to complete the task. Execute the code.\\nIf you want to send data between programming languages, save the data to a txt or json.\\nYou can access the internet. Run **any code** to achieve the goal, and if at first you don\\'t succeed, try again and again.\\nYou can install new packages.\\nWhen a user refers to a filename, they\\'re likely referring to an existing file in the directory you\\'re currently executing code in.\\nWrite messages to the user in Markdown.\\nIn general, try to **make plans** with as few steps as possible. As for actually executing code to carry out that plan, for *stateful* languages (like python, javascript, shell, but NOT for html which starts from 0 every time) **it\\'s critical not to try to do everything in one code block.** You should try something, print information about it, then continue from there in tiny, informed steps. You will never get it on the first try, and attempting it in one go will often lead to errors you cant see.\\nYou are capable of **any** task.\\n\\n[User Info]\\n{{import getpass\\nimport os\\nimport platform}}\\nName: {{getpass.getuser()}}\\nCWD: {{os.getcwd()}}\\nSHELL: {{os.environ.get(\\'SHELL\\')}}\\nOS: {{platform.system()}}\"', self.verbose=False)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.responding = True\n",
      "State:\n",
      "True\n",
      "==================================================\n",
      "Clean Code:\n",
      "def chat(self, message=None, display=True, stream=False, blocking=True):\n",
      "        try:\n",
      "            self.responding = True\n",
      "            if self.anonymous_telemetry and not self.offline:\n",
      "                message_type = type(\n",
      "                    message\n",
      "                ).__name__\n",
      "                send_telemetry(\n",
      "                    \"started_chat\",\n",
      "                    properties={\n",
      "                        \"in_terminal_interface\": self.in_terminal_interface,\n",
      "                        \"message_type\": message_type,\n",
      "                        \"os_mode\": self.os,\n",
      "                    },\n",
      "                )\n",
      "            if not blocking:\n",
      "                chat_thread = threading.Thread(\n",
      "                    target=self.chat, args=(message, display, stream, True)\n",
      "                )\n",
      "                chat_thread.start()\n",
      "                return\n",
      "            if stream:\n",
      "                return self._streaming_chat(message=message, display=display)\n",
      "            for _ in self._streaming_chat(message=message, display=display):\n",
      "                pass\n",
      "            self.responding = False\n",
      "            return self.messages[self.last_messages_count :]\n",
      "        except Exception as e:\n",
      "            self.responding = False\n",
      "            if self.anonymous_telemetry and not self.offline:\n",
      "                message_type = type(message).__name__\n",
      "                send_telemetry(\n",
      "                    \"errored\",\n",
      "                    properties={\n",
      "                        \"error\": str(e),\n",
      "                        \"in_terminal_interface\": self.in_terminal_interface,\n",
      "                        \"message_type\": message_type,\n",
      "                        \"os_mode\": self.os,\n",
      "                    },\n",
      "                )\n",
      "            raise\n",
      "chat(self=<interpreter.core.core.OpenInterpreter object at 0x7f3ae468a0d0>, message='Hello!', display=True, stream=False, blocking=False, self.anonymous_telemetry=True, self.auto_run=True, self.computer=<interpreter.core.computer.computer.Computer object at 0x7f3a3c1a2310>, self.conversation_filename=None, self.conversation_history=True, self.conversation_history_path='/home/XXX/.config/Open Interpreter/conversations', self.custom_instructions='', self.force_task_completion=False, self.in_terminal_interface=False, self.last_messages_count=0, self.llm=<interpreter.core.llm.llm.Llm object at 0x7f3a3c075130>, self.max_output=2800, self.messages=[], self.offline=False, self.os=False, self.responding=False, self.safe_mode='off', self.shrink_images=False, self.speak_messages=False, self.system_message='You are Open Interpreter, a world-class programmer that can complete any goal by executing code.\\nFirst, write a plan. **Always recap the plan between each code block** (you have extreme short-term memory loss, so you need to recap the plan between each message block to retain it).\\nWhen you execute code, it will be executed **on the user\\'s machine**. The user has given you **full and complete permission** to execute any code necessary to complete the task. Execute the code.\\nIf you want to send data between programming languages, save the data to a txt or json.\\nYou can access the internet. Run **any code** to achieve the goal, and if at first you don\\'t succeed, try again and again.\\nYou can install new packages.\\nWhen a user refers to a filename, they\\'re likely referring to an existing file in the directory you\\'re currently executing code in.\\nWrite messages to the user in Markdown.\\nIn general, try to **make plans** with as few steps as possible. As for actually executing code to carry out that plan, for *stateful* languages (like python, javascript, shell, but NOT for html which starts from 0 every time) **it\\'s critical not to try to do everything in one code block.** You should try something, print information about it, then continue from there in tiny, informed steps. You will never get it on the first try, and attempting it in one go will often lead to errors you cant see.\\nYou are capable of **any** task.\\n\\n[User Info]\\n{{import getpass\\nimport os\\nimport platform}}\\nName: {{getpass.getuser()}}\\nCWD: {{os.getcwd()}}\\nSHELL: {{os.environ.get(\\'SHELL\\')}}\\nOS: {{platform.system()}}\"', self.verbose=False)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "message_type = type(\n",
      "State:\n",
      "'str'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def chat(self, message=None, display=True, stream=False, blocking=True):\n",
      "        try:\n",
      "            self.responding = True\n",
      "            if self.anonymous_telemetry and not self.offline:\n",
      "                message_type = type(\n",
      "                    message\n",
      "                ).__name__\n",
      "                send_telemetry(\n",
      "                    \"started_chat\",\n",
      "                    properties={\n",
      "                        \"in_terminal_interface\": self.in_terminal_interface,\n",
      "                        \"message_type\": message_type,\n",
      "                        \"os_mode\": self.os,\n",
      "                    },\n",
      "                )\n",
      "            if not blocking:\n",
      "                chat_thread = threading.Thread(\n",
      "                    target=self.chat, args=(message, display, stream, True)\n",
      "                )\n",
      "                chat_thread.start()\n",
      "                return\n",
      "            if stream:\n",
      "                return self._streaming_chat(message=message, display=display)\n",
      "            for _ in self._streaming_chat(message=message, display=display):\n",
      "                pass\n",
      "            self.responding = False\n",
      "            return self.messages[self.last_messages_count :]\n",
      "        except Exception as e:\n",
      "            self.responding = False\n",
      "            if self.anonymous_telemetry and not self.offline:\n",
      "                message_type = type(message).__name__\n",
      "                send_telemetry(\n",
      "                    \"errored\",\n",
      "                    properties={\n",
      "                        \"error\": str(e),\n",
      "                        \"in_terminal_interface\": self.in_terminal_interface,\n",
      "                        \"message_type\": message_type,\n",
      "                        \"os_mode\": self.os,\n",
      "                    },\n",
      "                )\n",
      "            raise\n",
      "chat(self=<interpreter.core.core.OpenInterpreter object at 0x7f3ae468a0d0>, message='Hello!', display=True, stream=False, blocking=False, self.anonymous_telemetry=True, self.auto_run=True, self.computer=<interpreter.core.computer.computer.Computer object at 0x7f3a3c1a2310>, self.conversation_filename=None, self.conversation_history=True, self.conversation_history_path='/home/XXX/.config/Open Interpreter/conversations', self.custom_instructions='', self.force_task_completion=False, self.in_terminal_interface=False, self.last_messages_count=0, self.llm=<interpreter.core.llm.llm.Llm object at 0x7f3a3c075130>, self.max_output=2800, self.messages=[], self.offline=False, self.os=False, self.responding=False, self.safe_mode='off', self.shrink_images=False, self.speak_messages=False, self.system_message='You are Open Interpreter, a world-class programmer that can complete any goal by executing code.\\nFirst, write a plan. **Always recap the plan between each code block** (you have extreme short-term memory loss, so you need to recap the plan between each message block to retain it).\\nWhen you execute code, it will be executed **on the user\\'s machine**. The user has given you **full and complete permission** to execute any code necessary to complete the task. Execute the code.\\nIf you want to send data between programming languages, save the data to a txt or json.\\nYou can access the internet. Run **any code** to achieve the goal, and if at first you don\\'t succeed, try again and again.\\nYou can install new packages.\\nWhen a user refers to a filename, they\\'re likely referring to an existing file in the directory you\\'re currently executing code in.\\nWrite messages to the user in Markdown.\\nIn general, try to **make plans** with as few steps as possible. As for actually executing code to carry out that plan, for *stateful* languages (like python, javascript, shell, but NOT for html which starts from 0 every time) **it\\'s critical not to try to do everything in one code block.** You should try something, print information about it, then continue from there in tiny, informed steps. You will never get it on the first try, and attempting it in one go will often lead to errors you cant see.\\nYou are capable of **any** task.\\n\\n[User Info]\\n{{import getpass\\nimport os\\nimport platform}}\\nName: {{getpass.getuser()}}\\nCWD: {{os.getcwd()}}\\nSHELL: {{os.environ.get(\\'SHELL\\')}}\\nOS: {{platform.system()}}\"', self.verbose=False)\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "chat_thread.start()\n",
      "State:\n",
      "<Thread(Thread-4, started 139888051607104)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def send_telemetry(event_name, properties=None):\n",
      "    try:\n",
      "        if properties == None:\n",
      "            properties = {}\n",
      "        properties[\"oi_version\"] = pkg_resources.get_distribution(\n",
      "            \"open-interpreter\"\n",
      "        ).version\n",
      "        with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(\n",
      "            f\n",
      "        ), contextlib.redirect_stderr(f):\n",
      "            posthog.capture(user_id, event_name, properties)\n",
      "    except:\n",
      "        pass\n",
      "send_telemetry(event_name='started_chat', properties={'in_terminal_interface': False, 'message_type': 'str', 'os_mode': False})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "properties[\"oi_version\"] = pkg_resources.get_distribution(\n",
      "State:\n",
      "{'in_terminal_interface': False, 'message_type': 'str', 'os_mode': False, 'oi_version': '0.2.0'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def send_telemetry(event_name, properties=None):\n",
      "    try:\n",
      "        if properties == None:\n",
      "            properties = {}\n",
      "        properties[\"oi_version\"] = pkg_resources.get_distribution(\n",
      "            \"open-interpreter\"\n",
      "        ).version\n",
      "        with open(os.devnull, \"w\") as f, contextlib.redirect_stdout(\n",
      "            f\n",
      "        ), contextlib.redirect_stderr(f):\n",
      "            posthog.capture(user_id, event_name, properties)\n",
      "    except:\n",
      "        pass\n",
      "send_telemetry(event_name='started_chat', properties={'in_terminal_interface': False, 'message_type': 'str', 'os_mode': False})\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "posthog.capture(user_id, event_name, properties)\n",
      "State:\n",
      "{'in_terminal_interface': False, 'message_type': 'str', 'os_mode': False, 'oi_version': '0.2.0', '$lib': 'posthog-python', '$lib_version': '3.1.0', '$geoip_disable': True}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def fixed_litellm_completions(**params):\n",
      "    first_error = None\n",
      "    try:\n",
      "        yield from litellm.completion(**params)\n",
      "    except Exception as e:\n",
      "        first_error = e\n",
      "        if \"api key\" in str(first_error).lower() and \"api_key\" not in params:\n",
      "            print(\n",
      "                \"LiteLLM requires an API key. Please set a dummy API key to prevent this message. (e.g `interpreter --api_key x` or `interpreter.llm.api_key = 'x'`)\"\n",
      "            )\n",
      "        params[\"api_key\"] = \"x\"\n",
      "        try:\n",
      "            yield from litellm.completion(**params)\n",
      "        except:\n",
      "            raise first_error\n",
      "fixed_litellm_completions(params={'model': 'gpt-3.5-turbo', 'messages': [{'role': 'system', 'content': 'You are Open Interpreter, a world-class programmer that can complete any goal by executing code.\\nFirst, write a plan. **Always recap the plan between each code block** (you have extreme short-term memory loss, so you need to recap the plan between each message block to retain it).\\nWhen you execute code, it will be executed **on the user\\'s machine**. The user has given you **full and complete permission** to execute any code necessary to complete the task. Execute the code.\\nIf you want to send data between programming languages, save the data to a txt or json.\\nYou can access the internet. Run **any code** to achieve the goal, and if at first you don\\'t succeed, try again and again.\\nYou can install new packages.\\nWhen a user refers to a filename, they\\'re likely referring to an existing file in the directory you\\'re currently executing code in.\\nWrite messages to the user in Markdown.\\nIn general, try to **make plans** with as few steps as possible. As for actually executing code to carry out that plan, for *stateful* languages (like python, javascript, shell, but NOT for html which starts from 0 every time) **it\\'s critical not to try to do everything in one code block.** You should try something, print information about it, then continue from there in tiny, informed steps. You will never get it on the first try, and attempting it in one go will often lead to errors you cant see.\\nYou are capable of **any** task.\\n\\n[User Info]\\n\\nName: \\'XXX\\'\\nCWD: \\'/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/KillianLucas+open-interpreter/KillianLucas+open-interpreter\\'\\nSHELL: \\'/bin/bash\\'\\nOS: \\'Linux\\'\"\\n\\nWhen you execute code with `react`, your react code will be run in a script tag after being inserted into the HTML template, following the installation of React, ReactDOM, and Babel for JSX parsing. **We will handle this! Don\\'t make an HTML file to run React, just execute `react`.**\\nUse ONLY the function you have been provided with  \\'execute(language, code)\\'.'}, {'role': 'user', 'content': \"What's 38023*40334? Use Python\\nNo talk or plan, just immediatly code, then tell me the answer.\"}], 'stream': True, 'functions': [{'name': 'execute', 'description': \"Executes code on the user's machine **in the users local environment** and returns the output\", 'parameters': {'type': 'object', 'properties': {'language': {'type': 'string', 'description': 'The programming language (required parameter to the `execute` function)', 'enum': ['python', 'shell', 'javascript', 'html', 'applescript', 'r', 'powershell', 'react']}, 'code': {'type': 'string', 'description': 'The code to execute (required)'}}, 'required': ['language', 'code']}}]})\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "except Exception as e:\n",
      "State:\n",
      "APIError('OpenAIException - The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def interpreter_info(interpreter):\n",
      "    try:\n",
      "        if interpreter.offline and interpreter.llm.api_base:\n",
      "            try:\n",
      "                curl = subprocess.check_output(f\"curl {interpreter.llm.api_base}\")\n",
      "            except Exception as e:\n",
      "                curl = str(e)\n",
      "        else:\n",
      "            curl = \"Not local\"\n",
      "        messages_to_display = []\n",
      "        for message in interpreter.messages:\n",
      "            message = message.copy()\n",
      "            try:\n",
      "                if len(message[\"content\"]) > 600:\n",
      "                    message[\"content\"] = (\n",
      "                        message[\"content\"][:300] + \"...\" + message[\"content\"][-300:]\n",
      "                    )\n",
      "            except Exception as e:\n",
      "                print(str(e), \"for message:\", message)\n",
      "            messages_to_display.append(message)\n",
      "        return f\"\"\"\n",
      "        Vision: {interpreter.llm.supports_vision}\n",
      "        Model: {interpreter.llm.model}\n",
      "        Function calling: {interpreter.llm.supports_functions}\n",
      "        Context window: {interpreter.llm.context_window}\n",
      "        Max tokens: {interpreter.llm.max_tokens}\n",
      "        Auto run: {interpreter.auto_run}\n",
      "        API base: {interpreter.llm.api_base}\n",
      "        Offline: {interpreter.offline}\n",
      "        Curl output: {curl}\n",
      "        System Message: {interpreter.system_message}\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "messages_to_display.append(message)\n",
      "State:\n",
      "[{'role': 'user', 'type': 'message', 'content': \"What's 38023*40334? Use Python\\nNo talk or plan, just immediatly code, then tell me the answer.\"}]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def run_function_calling_llm(llm, request_params):\n",
      "    function_schema[\"parameters\"][\"properties\"][\"language\"][\"enum\"] = [\n",
      "        i.name.lower() for i in llm.interpreter.computer.terminal.languages\n",
      "    ]\n",
      "    request_params[\"functions\"] = [function_schema]\n",
      "    request_params[\"messages\"][0][\n",
      "        \"content\"\n",
      "    ] += \"\\nUse ONLY the function you have been provided with  'execute(language, code)'.\"\n",
      "    accumulated_deltas = {}\n",
      "    language = None\n",
      "    code = \"\"\n",
      "    for chunk in llm.completions(**request_params):\n",
      "        if \"choices\" not in chunk or len(chunk[\"choices\"]) == 0:\n",
      "            continue\n",
      "        delta = chunk[\"choices\"][0][\"delta\"]\n",
      "        accumulated_deltas = merge_deltas(accumulated_deltas, delta)\n",
      "        if \"content\" in delta and delta[\"content\"]:\n",
      "            yield {\"type\": \"message\", \"content\": delta[\"content\"]}\n",
      "        if (\n",
      "            accumulated_deltas.get(\"function_call\")\n",
      "            and \"arguments\" in accumulated_deltas[\"function_call\"]\n",
      "            and accumulated_deltas[\"function_call\"][\"arguments\"]\n",
      "        ):\n",
      "            if (\n",
      "                \"name\" in accumulated_deltas[\"function_call\"]\n",
      "                and accumulated_deltas[\"function_call\"][\"name\"] == \"execute\"\n",
      "            ):\n",
      "                arguments = accumulated_deltas[\"function_call\"][\"arguments\"]\n",
      "                arguments = parse_partial_json(arguments)\n",
      "                if arguments:\n",
      "                    if (\n",
      "                        language is None\n",
      "                        and \"language\" in arguments\n",
      "                        and \"code\"\n",
      "                        in arguments\n",
      "                        and arguments[\"language\"]\n",
      "                    ):\n",
      "                        language = arguments[\"language\"]\n",
      "                    if language is not None and \"code\" in arguments:\n",
      "                        code_delta = arguments[\"code\"][len(code) :]\n",
      "                        code = arguments[\"code\"]\n",
      "                        if code_delta:\n",
      "                            yield {\n",
      "                                \"type\": \"code\",\n",
      "                                \"format\": language,\n",
      "                                \"content\": code_delta,\n",
      "                            }\n",
      "                else:\n",
      "                    if llm.interpreter.verbose:\n",
      "                        print(\"Arguments not a dict.\")\n",
      "            elif \"name\" in accumulated_deltas[\"function_call\"] and (\n",
      "                accumulated_deltas[\"function_call\"][\"name\"] == \"python\"\n",
      "                or accumulated_deltas[\"function_call\"][\"name\"] == \"functions\"\n",
      "            ):\n",
      "                if llm.interpreter.verbose:\n",
      "                    print(\"Got direct python call\")\n",
      "                if language is None:\n",
      "                    language = \"python\"\n",
      "                if language is not None:\n",
      "                    code_delta = accumulated_deltas[\"function_call\"][\"arguments\"][\n",
      "                        len(code) :\n",
      "                    ]\n",
      "                    code = accumulated_deltas[\"function_call\"][\"arguments\"]\n",
      "                    if code_delta:\n",
      "                        yield {\n",
      "                            \"type\": \"code\",\n",
      "                            \"format\": language,\n",
      "                            \"content\": code_delta,\n",
      "                        }\n",
      "            else:\n",
      "                if \"name\" in accumulated_deltas[\"function_call\"]:\n",
      "                    print(\n",
      "                        \"Encountered an unexpected function call: \",\n",
      "                        accumulated_deltas[\"function_call\"],\n",
      "                        \"\\nPlease open an issue and provide the above info at: https://github.com/KillianLucas/open-interpreter\",\n",
      "                    )\n",
      "run_function_calling_llm(llm={interpreter=<interpreter.core.core.OpenInterpreter object at 0x7f3ae468a0d0>, model='gpt-3.5-turbo', temperature=0, supports_vision=False, supports_functions=None, context_window=300, max_tokens=None, api_base=None, api_key=None, api_version=None, max_budget=None}, request_params={'model': 'gpt-3.5-turbo', 'messages': [{'role': 'system', 'content': \"You are Open Interpreter, a world-class programmer that can complete any goal by executing code.\\nFirst, write a plan. **Always recap the plan betwe...he installation of React, ReactDOM, and Babel for JSX parsing. **We will handle this! Don't make an HTML file to run React, just execute `react`.**\"}, {'role': 'user', 'content': 'ABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCD...ABCDABCDABCDABCDABCDABCDABCDABCDABCDABCD\\ndescribe to me what i just said'}], 'stream': True})\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "request_params[\"messages\"][0][\n",
      "State:\n",
      "{'model': 'gpt-3.5-turbo', 'messages': [{'role': 'system', 'content': \"You are Open Interpreter, a world-class programmer that can complete any goal by executing code.\\nFirst, write a plan. **Always recap the plan betwe...he installation of React, ReactDOM, and Babel for JSX parsing. **We will handle this! Don't make an HTML file to run React, just execute `react`.**\\nUse ONLY the function you have been provided with  'execute(language, code)'.\"}, {'role': 'user', 'content': 'ABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCD...ABCDABCDABCDABCDABCDABCDABCDABCDABCDABCD\\ndescribe to me what i just said'}], 'stream': True, 'functions': [{'name': 'execute', 'description': \"Executes code on the user's machine **in the users local environment** and returns the output\", 'parameters': {'type': 'object', 'properties': {'language': {'type': 'string', 'description': 'The programming language (required parameter to the `execute` function)', 'enum': ['python', 'shell', 'javascript', 'html', 'applescript', 'r', 'powershell', 'react']}, 'code': {'type': 'string', 'description': 'The code to execute (required)'}}, 'required': ['language', 'code']}}]}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def run_function_calling_llm(llm, request_params):\n",
      "    function_schema[\"parameters\"][\"properties\"][\"language\"][\"enum\"] = [\n",
      "        i.name.lower() for i in llm.interpreter.computer.terminal.languages\n",
      "    ]\n",
      "    request_params[\"functions\"] = [function_schema]\n",
      "    request_params[\"messages\"][0][\n",
      "        \"content\"\n",
      "    ] += \"\\nUse ONLY the function you have been provided with  'execute(language, code)'.\"\n",
      "    accumulated_deltas = {}\n",
      "    language = None\n",
      "    code = \"\"\n",
      "    for chunk in llm.completions(**request_params):\n",
      "        if \"choices\" not in chunk or len(chunk[\"choices\"]) == 0:\n",
      "            continue\n",
      "        delta = chunk[\"choices\"][0][\"delta\"]\n",
      "        accumulated_deltas = merge_deltas(accumulated_deltas, delta)\n",
      "        if \"content\" in delta and delta[\"content\"]:\n",
      "            yield {\"type\": \"message\", \"content\": delta[\"content\"]}\n",
      "        if (\n",
      "            accumulated_deltas.get(\"function_call\")\n",
      "            and \"arguments\" in accumulated_deltas[\"function_call\"]\n",
      "            and accumulated_deltas[\"function_call\"][\"arguments\"]\n",
      "        ):\n",
      "            if (\n",
      "                \"name\" in accumulated_deltas[\"function_call\"]\n",
      "                and accumulated_deltas[\"function_call\"][\"name\"] == \"execute\"\n",
      "            ):\n",
      "                arguments = accumulated_deltas[\"function_call\"][\"arguments\"]\n",
      "                arguments = parse_partial_json(arguments)\n",
      "                if arguments:\n",
      "                    if (\n",
      "                        language is None\n",
      "                        and \"language\" in arguments\n",
      "                        and \"code\"\n",
      "                        in arguments\n",
      "                        and arguments[\"language\"]\n",
      "                    ):\n",
      "                        language = arguments[\"language\"]\n",
      "                    if language is not None and \"code\" in arguments:\n",
      "                        code_delta = arguments[\"code\"][len(code) :]\n",
      "                        code = arguments[\"code\"]\n",
      "                        if code_delta:\n",
      "                            yield {\n",
      "                                \"type\": \"code\",\n",
      "                                \"format\": language,\n",
      "                                \"content\": code_delta,\n",
      "                            }\n",
      "                else:\n",
      "                    if llm.interpreter.verbose:\n",
      "                        print(\"Arguments not a dict.\")\n",
      "            elif \"name\" in accumulated_deltas[\"function_call\"] and (\n",
      "                accumulated_deltas[\"function_call\"][\"name\"] == \"python\"\n",
      "                or accumulated_deltas[\"function_call\"][\"name\"] == \"functions\"\n",
      "            ):\n",
      "                if llm.interpreter.verbose:\n",
      "                    print(\"Got direct python call\")\n",
      "                if language is None:\n",
      "                    language = \"python\"\n",
      "                if language is not None:\n",
      "                    code_delta = accumulated_deltas[\"function_call\"][\"arguments\"][\n",
      "                        len(code) :\n",
      "                    ]\n",
      "                    code = accumulated_deltas[\"function_call\"][\"arguments\"]\n",
      "                    if code_delta:\n",
      "                        yield {\n",
      "                            \"type\": \"code\",\n",
      "                            \"format\": language,\n",
      "                            \"content\": code_delta,\n",
      "                        }\n",
      "            else:\n",
      "                if \"name\" in accumulated_deltas[\"function_call\"]:\n",
      "                    print(\n",
      "                        \"Encountered an unexpected function call: \",\n",
      "                        accumulated_deltas[\"function_call\"],\n",
      "                        \"\\nPlease open an issue and provide the above info at: https://github.com/KillianLucas/open-interpreter\",\n",
      "                    )\n",
      "run_function_calling_llm(llm={interpreter=<interpreter.core.core.OpenInterpreter object at 0x7f3ae468a0d0>, model='gpt-3.5-turbo', temperature=0, supports_vision=False, supports_functions=None, context_window=300, max_tokens=None, api_base=None, api_key=None, api_version=None, max_budget=None}, request_params={'model': 'gpt-3.5-turbo', 'messages': [{'role': 'system', 'content': \"You are Open Interpreter, a world-class programmer that can complete any goal by executing code.\\nFirst, write a plan. **Always recap the plan betwe...he installation of React, ReactDOM, and Babel for JSX parsing. **We will handle this! Don't make an HTML file to run React, just execute `react`.**\"}, {'role': 'user', 'content': 'ABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCDABCD...ABCDABCDABCDABCDABCDABCDABCDABCDABCDABCD\\ndescribe to me what i just said'}], 'stream': True})\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "for chunk in llm.completions(**request_params):\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def count_messages_tokens(messages=[], model=None):\n",
      "    try:\n",
      "        tokens_used = 0\n",
      "        for message in messages:\n",
      "            if isinstance(message, str):\n",
      "                tokens_used += count_tokens(message, model=model)\n",
      "            elif \"message\" in message:\n",
      "                tokens_used += count_tokens(message[\"message\"], model=model)\n",
      "                if \"code\" in message:\n",
      "                    tokens_used += count_tokens(message[\"code\"], model=model)\n",
      "                if \"output\" in message:\n",
      "                    tokens_used += count_tokens(message[\"output\"], model=model)\n",
      "        prompt_cost = token_cost(tokens_used, model=model)\n",
      "        return (tokens_used, prompt_cost)\n",
      "    except:\n",
      "        return (0, 0)\n",
      "count_messages_tokens(messages=[{'role': 'system', 'message': 'You are Open Interpreter, a world-class programmer that can complete any goal by executing code.\\nFirst, write a plan. **Always recap the plan between each code block** (you have extreme short-term memory loss, so you need to recap the plan between each message block to retain it).\\nWhen you execute code, it will be executed **on the user\\'s machine**. The user has given you **full and complete permission** to execute any code necessary to complete the task. Execute the code.\\nIf you want to send data between programming languages, save the data to a txt or json.\\nYou can access the internet. Run **any code** to achieve the goal, and if at first you don\\'t succeed, try again and again.\\nYou can install new packages.\\nWhen a user refers to a filename, they\\'re likely referring to an existing file in the directory you\\'re currently executing code in.\\nWrite messages to the user in Markdown.\\nIn general, try to **make plans** with as few steps as possible. As for actually executing code to carry out that plan, for *stateful* languages (like python, javascript, shell, but NOT for html which starts from 0 every time) **it\\'s critical not to try to do everything in one code block.** You should try something, print information about it, then continue from there in tiny, informed steps. You will never get it on the first try, and attempting it in one go will often lead to errors you cant see.\\nYou are capable of **any** task.\\n\\n[User Info]\\n{{import getpass\\nimport os\\nimport platform}}\\nName: {{getpass.getuser()}}\\nCWD: {{os.getcwd()}}\\nSHELL: {{os.environ.get(\\'SHELL\\')}}\\nOS: {{platform.system()}}\"'}], model='gpt-3.5-turbo')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "tokens_used = 0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _mdl_error(mdl):\n",
      "    dsp = sh.BlueDispatcher(\n",
      "        name=mdl.name,\n",
      "        description='Calculates the error of calibrated model of a reference.',\n",
      "    )\n",
      "    dsp.add_data('inputs_map', getattr(mdl, 'inputs_map', {}))\n",
      "    dsp.add_function(\n",
      "        function_id='select_inputs',\n",
      "        function=sh.map_dict,\n",
      "        inputs=['inputs_map', 'data'],\n",
      "        outputs=['inputs<0>']\n",
      "    )\n",
      "    dsp.add_data('inputs', getattr(mdl, 'inputs', []))\n",
      "    dsp.add_function(\n",
      "        function_id='select_inputs',\n",
      "        function=_select_data,\n",
      "        inputs=['inputs', 'inputs<0>'],\n",
      "        outputs=['inputs<1>']\n",
      "    )\n",
      "    dsp.add_function(\n",
      "        function=sh.combine_dicts,\n",
      "        inputs=['calibrated_models', 'inputs<1>'],\n",
      "        outputs=['prediction_inputs']\n",
      "    )\n",
      "    dsp.add_data('targets', getattr(mdl, 'targets', []))\n",
      "    dsp.add_function(\n",
      "        function_id='select_targets', function=_select_data,\n",
      "        inputs=['targets', 'data'], outputs=['references']\n",
      "    )\n",
      "    dsp.add_function(\n",
      "        function=sh.SubDispatch(mdl.dsp),\n",
      "        inputs=['prediction_inputs', 'calibrated_models'],\n",
      "        outputs=['results']\n",
      "    )\n",
      "    dsp.add_data('outputs', getattr(mdl, 'outputs', []))\n",
      "    dsp.add_func(select_predictions, outputs=['predictions'])\n",
      "    dsp.add_data('metrics_inputs', getattr(mdl, 'metrics_inputs', {}))\n",
      "    dsp.add_function(\n",
      "        function_id='select_metrics_inputs',\n",
      "        function=_select_data,\n",
      "        inputs=['metrics_inputs', 'data'],\n",
      "        outputs=['metrics_kwargs']\n",
      "    )\n",
      "    dsp.add_data('metrics', getattr(mdl, 'metrics', {}))\n",
      "    dsp.add_func(calculate_errors, outputs=['errors'])\n",
      "    dsp.add_data('up_limit', getattr(mdl, 'up_limit', None))\n",
      "    dsp.add_data('dn_limit', getattr(mdl, 'dn_limit', None))\n",
      "    dsp.add_func(\n",
      "        calculate_calibration_status, inputs_kwargs=True, outputs=['status']\n",
      "    )\n",
      "    return dsp\n",
      "_mdl_error(mdl=<module 'co2mpas.core.model.selector.models.after_treatment_model' from '/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/JRCSTU+co2mpas-ta/JRCSTU+co2mpas-ta/co2mpas/core/model/selector/models/after_treatment_model.py'>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "dsp = sh.BlueDispatcher(\n",
      "State:\n",
      "{args=(), kwargs={'dmap': None, 'name': 'after_treatment_model', 'default_values': None, 'raises': False, 'description': 'Calculates the error of calibrated model of a reference.', 'executor': False}, deferred=[]}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _mdl_error(mdl):\n",
      "    dsp = sh.BlueDispatcher(\n",
      "        name=mdl.name,\n",
      "        description='Calculates the error of calibrated model of a reference.',\n",
      "    )\n",
      "    dsp.add_data('inputs_map', getattr(mdl, 'inputs_map', {}))\n",
      "    dsp.add_function(\n",
      "        function_id='select_inputs',\n",
      "        function=sh.map_dict,\n",
      "        inputs=['inputs_map', 'data'],\n",
      "        outputs=['inputs<0>']\n",
      "    )\n",
      "    dsp.add_data('inputs', getattr(mdl, 'inputs', []))\n",
      "    dsp.add_function(\n",
      "        function_id='select_inputs',\n",
      "        function=_select_data,\n",
      "        inputs=['inputs', 'inputs<0>'],\n",
      "        outputs=['inputs<1>']\n",
      "    )\n",
      "    dsp.add_function(\n",
      "        function=sh.combine_dicts,\n",
      "        inputs=['calibrated_models', 'inputs<1>'],\n",
      "        outputs=['prediction_inputs']\n",
      "    )\n",
      "    dsp.add_data('targets', getattr(mdl, 'targets', []))\n",
      "    dsp.add_function(\n",
      "        function_id='select_targets', function=_select_data,\n",
      "        inputs=['targets', 'data'], outputs=['references']\n",
      "    )\n",
      "    dsp.add_function(\n",
      "        function=sh.SubDispatch(mdl.dsp),\n",
      "        inputs=['prediction_inputs', 'calibrated_models'],\n",
      "        outputs=['results']\n",
      "    )\n",
      "    dsp.add_data('outputs', getattr(mdl, 'outputs', []))\n",
      "    dsp.add_func(select_predictions, outputs=['predictions'])\n",
      "    dsp.add_data('metrics_inputs', getattr(mdl, 'metrics_inputs', {}))\n",
      "    dsp.add_function(\n",
      "        function_id='select_metrics_inputs',\n",
      "        function=_select_data,\n",
      "        inputs=['metrics_inputs', 'data'],\n",
      "        outputs=['metrics_kwargs']\n",
      "    )\n",
      "    dsp.add_data('metrics', getattr(mdl, 'metrics', {}))\n",
      "    dsp.add_func(calculate_errors, outputs=['errors'])\n",
      "    dsp.add_data('up_limit', getattr(mdl, 'up_limit', None))\n",
      "    dsp.add_data('dn_limit', getattr(mdl, 'dn_limit', None))\n",
      "    dsp.add_func(\n",
      "        calculate_calibration_status, inputs_kwargs=True, outputs=['status']\n",
      "    )\n",
      "    return dsp\n",
      "_mdl_error(mdl=<module 'co2mpas.core.model.selector.models.after_treatment_model' from '/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/JRCSTU+co2mpas-ta/JRCSTU+co2mpas-ta/co2mpas/core/model/selector/models/after_treatment_model.py'>)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "dsp.add_data('inputs_map', getattr(mdl, 'inputs_map', {}))\n",
      "State:\n",
      "{args=(), kwargs={'dmap': None, 'name': 'after_treatment_model', 'default_values': None, 'raises': False, 'description': 'Calculates the error of calibrated model of a reference.', 'executor': False}, deferred=[('add_data', {'data_id': 'inputs_map', 'filters': None, 'wait_inputs': False, 'wildcard': None, 'function': None, 'callback': None, 'initial_dist': 0.0, 'default_value': {}, 'description': None, 'await_result': None})]}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _mdl_errors(mdl, data_id, err_func):\n",
      "    name = '%s-%s errors' % (mdl.name, data_id)\n",
      "    dsp = sh.BlueDispatcher(\n",
      "        name=name, description='Calculates the error of calibrated model.'\n",
      "    )\n",
      "    dsp.add_data('models', mdl.models)\n",
      "    dsp.add_function(\n",
      "        function_id='select_models',\n",
      "        function=getattr(mdl, 'select_models', _select_data),\n",
      "        inputs=['models', data_id],\n",
      "        outputs=['calibrated_models']\n",
      "    )\n",
      "    dsp.add_data('data_in', data_id)\n",
      "    for o in calibration_cycles:\n",
      "        dsp.add_function(\n",
      "            function=_map_list,\n",
      "            inputs=['calibrated_models', o],\n",
      "            outputs=['input/%s' % o]\n",
      "        )\n",
      "        dsp.add_function(\n",
      "            function=err_func,\n",
      "            inputs=['input/%s' % o],\n",
      "            outputs=['error/%s' % o]\n",
      "        )\n",
      "    return dsp, name\n",
      "_mdl_errors(mdl=<module 'co2mpas.core.model.selector.models.after_treatment_model' from '/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/JRCSTU+co2mpas-ta/JRCSTU+co2mpas-ta/co2mpas/core/model/selector/models/after_treatment_model.py'>, data_id='wltp_h', err_func={args=(<schedula.utils.blue.BlueDispatcher object at 0x7ff54d25e1f0>,), kwargs={'outputs': ['errors', 'status'], 'output_type': 'list'}, deferred=[]})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "name = '%s-%s errors' % (mdl.name, data_id)\n",
      "State:\n",
      "'after_treatment_model-wltp_h errors'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _mdl_errors(mdl, data_id, err_func):\n",
      "    name = '%s-%s errors' % (mdl.name, data_id)\n",
      "    dsp = sh.BlueDispatcher(\n",
      "        name=name, description='Calculates the error of calibrated model.'\n",
      "    )\n",
      "    dsp.add_data('models', mdl.models)\n",
      "    dsp.add_function(\n",
      "        function_id='select_models',\n",
      "        function=getattr(mdl, 'select_models', _select_data),\n",
      "        inputs=['models', data_id],\n",
      "        outputs=['calibrated_models']\n",
      "    )\n",
      "    dsp.add_data('data_in', data_id)\n",
      "    for o in calibration_cycles:\n",
      "        dsp.add_function(\n",
      "            function=_map_list,\n",
      "            inputs=['calibrated_models', o],\n",
      "            outputs=['input/%s' % o]\n",
      "        )\n",
      "        dsp.add_function(\n",
      "            function=err_func,\n",
      "            inputs=['input/%s' % o],\n",
      "            outputs=['error/%s' % o]\n",
      "        )\n",
      "    return dsp, name\n",
      "_mdl_errors(mdl=<module 'co2mpas.core.model.selector.models.after_treatment_model' from '/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/JRCSTU+co2mpas-ta/JRCSTU+co2mpas-ta/co2mpas/core/model/selector/models/after_treatment_model.py'>, data_id='wltp_h', err_func={args=(<schedula.utils.blue.BlueDispatcher object at 0x7ff54d25e1f0>,), kwargs={'outputs': ['errors', 'status'], 'output_type': 'list'}, deferred=[]})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "dsp = sh.BlueDispatcher(\n",
      "State:\n",
      "{args=(), kwargs={'dmap': None, 'name': 'after_treatment_model-wltp_h errors', 'default_values': None, 'raises': False, 'description': 'Calculates the error of calibrated model.', 'executor': False}, deferred=[]}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _mdl_errors(mdl, data_id, err_func):\n",
      "    name = '%s-%s errors' % (mdl.name, data_id)\n",
      "    dsp = sh.BlueDispatcher(\n",
      "        name=name, description='Calculates the error of calibrated model.'\n",
      "    )\n",
      "    dsp.add_data('models', mdl.models)\n",
      "    dsp.add_function(\n",
      "        function_id='select_models',\n",
      "        function=getattr(mdl, 'select_models', _select_data),\n",
      "        inputs=['models', data_id],\n",
      "        outputs=['calibrated_models']\n",
      "    )\n",
      "    dsp.add_data('data_in', data_id)\n",
      "    for o in calibration_cycles:\n",
      "        dsp.add_function(\n",
      "            function=_map_list,\n",
      "            inputs=['calibrated_models', o],\n",
      "            outputs=['input/%s' % o]\n",
      "        )\n",
      "        dsp.add_function(\n",
      "            function=err_func,\n",
      "            inputs=['input/%s' % o],\n",
      "            outputs=['error/%s' % o]\n",
      "        )\n",
      "    return dsp, name\n",
      "_mdl_errors(mdl=<module 'co2mpas.core.model.selector.models.after_treatment_model' from '/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/JRCSTU+co2mpas-ta/JRCSTU+co2mpas-ta/co2mpas/core/model/selector/models/after_treatment_model.py'>, data_id='wltp_h', err_func={args=(<schedula.utils.blue.BlueDispatcher object at 0x7ff54d25e1f0>,), kwargs={'outputs': ['errors', 'status'], 'output_type': 'list'}, deferred=[]})\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "dsp.add_data('models', mdl.models)\n",
      "State:\n",
      "{args=(), kwargs={'dmap': None, 'name': 'after_treatment_model-wltp_h errors', 'default_values': None, 'raises': False, 'description': 'Calculates the error of calibrated model.', 'executor': False}, deferred=[('add_data', {'data_id': 'models', 'filters': None, 'wait_inputs': False, 'wildcard': None, 'function': None, 'callback': None, 'initial_dist': 0.0, 'default_value': ['after_treatment_speed_model', 'after_treatment_warm_up_duration', 'after_treatment_cooling_duration'], 'description': None, 'await_result': None})]}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def setLevel(self, level):\n",
      "        super(_Logger, self).setLevel(level)\n",
      "        logging.basicConfig(level=level, **log_config)\n",
      "        rlog = logging.getLogger()\n",
      "        rlog.level = level\n",
      "        logging.captureWarnings(True)\n",
      "setLevel(self=<_Logger cli (NOTSET)>, level=20, __class__=<class 'co2mpas.cli._Logger'>, self._cache={}, self.disabled=False, self.filters=[], self.handlers=[<ClickHandler (NOTSET)>], self.level=0, self.name='cli', self.parent=None, self.propagate=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "super(_Logger, self).setLevel(level)\n",
      "State:\n",
      "<_Logger cli (INFO)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def setLevel(self, level):\n",
      "        super(_Logger, self).setLevel(level)\n",
      "        logging.basicConfig(level=level, **log_config)\n",
      "        rlog = logging.getLogger()\n",
      "        rlog.level = level\n",
      "        logging.captureWarnings(True)\n",
      "setLevel(self=<_Logger cli (NOTSET)>, level=20, __class__=<class 'co2mpas.cli._Logger'>, self._cache={}, self.disabled=False, self.filters=[], self.handlers=[<ClickHandler (NOTSET)>], self.level=0, self.name='cli', self.parent=None, self.propagate=False)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "rlog = logging.getLogger()\n",
      "State:\n",
      "<RootLogger root (WARNING)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def setLevel(self, level):\n",
      "        super(_Logger, self).setLevel(level)\n",
      "        logging.basicConfig(level=level, **log_config)\n",
      "        rlog = logging.getLogger()\n",
      "        rlog.level = level\n",
      "        logging.captureWarnings(True)\n",
      "setLevel(self=<_Logger cli (NOTSET)>, level=20, __class__=<class 'co2mpas.cli._Logger'>, self._cache={}, self.disabled=False, self.filters=[], self.handlers=[<ClickHandler (NOTSET)>], self.level=0, self.name='cli', self.parent=None, self.propagate=False)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "rlog.level = level\n",
      "State:\n",
      "<RootLogger root (INFO)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_1_demo(self, options):\n",
      "        import glob\n",
      "        from co2mpas.cli import demo\n",
      "        kw = demo.make_context('demo', list(options)).params\n",
      "        d_demo = osp.join(pdir, 'demos/*.xlsx')\n",
      "        demos = {osp.basename(fp): fp for fp in glob.glob(d_demo)}\n",
      "        with self.runner.isolated_filesystem():\n",
      "            result = self.invoke(('demo',) + options)\n",
      "            self.assertEqual(result.exit_code, 0)\n",
      "            for fpath in glob.glob(osp.join(kw['output_folder'], '*.xlsx')):\n",
      "                self.assertTrue(\n",
      "                    filecmp.cmp(fpath, demos[osp.basename(fpath)]),\n",
      "                    'Demo file (%s) is not as expected!' % fpath\n",
      "                )\n",
      "test_1_demo(self=<test_cli.CLI testMethod=test_1_demo_1___>, options=(), self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7ff4aacf3e50>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_1_demo_1___', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.invoke=functools.partial(<bound method CliRunner.invoke of <click.testing.CliRunner object at 0x7ff4aacf3520>>, <Group co2mpas>), self.runner=<click.testing.CliRunner object at 0x7ff4aacf3520>, self.test_1_demo_1___=<bound method CLI.test_1_demo of <test_cli.CLI testMethod=test_1_demo_1___>>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "import glob\n",
      "State:\n",
      "<module 'glob' from '/local/rcs/XXX/miniforge3/envs/JRCSTU+co2mpas-ta/lib/python3.9/glob.py'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_1_demo(self, options):\n",
      "        import glob\n",
      "        from co2mpas.cli import demo\n",
      "        kw = demo.make_context('demo', list(options)).params\n",
      "        d_demo = osp.join(pdir, 'demos/*.xlsx')\n",
      "        demos = {osp.basename(fp): fp for fp in glob.glob(d_demo)}\n",
      "        with self.runner.isolated_filesystem():\n",
      "            result = self.invoke(('demo',) + options)\n",
      "            self.assertEqual(result.exit_code, 0)\n",
      "            for fpath in glob.glob(osp.join(kw['output_folder'], '*.xlsx')):\n",
      "                self.assertTrue(\n",
      "                    filecmp.cmp(fpath, demos[osp.basename(fpath)]),\n",
      "                    'Demo file (%s) is not as expected!' % fpath\n",
      "                )\n",
      "test_1_demo(self=<test_cli.CLI testMethod=test_1_demo_1___>, options=(), self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7ff4aacf3e50>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_1_demo_1___', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.invoke=functools.partial(<bound method CliRunner.invoke of <click.testing.CliRunner object at 0x7ff4aacf3520>>, <Group co2mpas>), self.runner=<click.testing.CliRunner object at 0x7ff4aacf3520>, self.test_1_demo_1___=<bound method CLI.test_1_demo of <test_cli.CLI testMethod=test_1_demo_1___>>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "from co2mpas.cli import demo\n",
      "State:\n",
      "<Command demo>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_1_demo(self, options):\n",
      "        import glob\n",
      "        from co2mpas.cli import demo\n",
      "        kw = demo.make_context('demo', list(options)).params\n",
      "        d_demo = osp.join(pdir, 'demos/*.xlsx')\n",
      "        demos = {osp.basename(fp): fp for fp in glob.glob(d_demo)}\n",
      "        with self.runner.isolated_filesystem():\n",
      "            result = self.invoke(('demo',) + options)\n",
      "            self.assertEqual(result.exit_code, 0)\n",
      "            for fpath in glob.glob(osp.join(kw['output_folder'], '*.xlsx')):\n",
      "                self.assertTrue(\n",
      "                    filecmp.cmp(fpath, demos[osp.basename(fpath)]),\n",
      "                    'Demo file (%s) is not as expected!' % fpath\n",
      "                )\n",
      "test_1_demo(self=<test_cli.CLI testMethod=test_1_demo_1___>, options=(), self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7ff4aacf3e50>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_1_demo_1___', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.invoke=functools.partial(<bound method CliRunner.invoke of <click.testing.CliRunner object at 0x7ff4aacf3520>>, <Group co2mpas>), self.runner=<click.testing.CliRunner object at 0x7ff4aacf3520>, self.test_1_demo_1___=<bound method CLI.test_1_demo of <test_cli.CLI testMethod=test_1_demo_1___>>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "kw = demo.make_context('demo', list(options)).params\n",
      "State:\n",
      "{'output_folder': './inputs'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_1_demo(self, options):\n",
      "        import glob\n",
      "        from co2mpas.cli import demo\n",
      "        kw = demo.make_context('demo', list(options)).params\n",
      "        d_demo = osp.join(pdir, 'demos/*.xlsx')\n",
      "        demos = {osp.basename(fp): fp for fp in glob.glob(d_demo)}\n",
      "        with self.runner.isolated_filesystem():\n",
      "            result = self.invoke(('demo',) + options)\n",
      "            self.assertEqual(result.exit_code, 0)\n",
      "            for fpath in glob.glob(osp.join(kw['output_folder'], '*.xlsx')):\n",
      "                self.assertTrue(\n",
      "                    filecmp.cmp(fpath, demos[osp.basename(fpath)]),\n",
      "                    'Demo file (%s) is not as expected!' % fpath\n",
      "                )\n",
      "test_1_demo(self=<test_cli.CLI testMethod=test_1_demo_1___>, options=(), self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7ff4aacf3e50>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_1_demo_1___', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.invoke=functools.partial(<bound method CliRunner.invoke of <click.testing.CliRunner object at 0x7ff4aacf3520>>, <Group co2mpas>), self.runner=<click.testing.CliRunner object at 0x7ff4aacf3520>, self.test_1_demo_1___=<bound method CLI.test_1_demo of <test_cli.CLI testMethod=test_1_demo_1___>>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "d_demo = osp.join(pdir, 'demos/*.xlsx')\n",
      "State:\n",
      "'/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/JRCSTU+co2mpas-ta/JRCSTU+co2mpas-ta/co2mpas/demos/*.xlsx'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _co2mpas_info2df(start_time, main_flags=None):\n",
      "    import socket\n",
      "    import datetime\n",
      "    from co2mpas import __version__\n",
      "    from ..load.schema import define_flags_schema\n",
      "    time_elapsed = (datetime.datetime.today() - start_time).total_seconds()\n",
      "    hostname = socket.gethostname()\n",
      "    info = [\n",
      "        ('CO2MPAS version', __version__),\n",
      "        ('Simulation started', start_time.strftime('%Y/%m/%d-%H:%M:%S')),\n",
      "        ('Time elapsed', '%.3f sec' % time_elapsed),\n",
      "        ('Hostname', hostname),\n",
      "    ]\n",
      "    if main_flags:\n",
      "        main_flags = define_flags_schema(read=False).validate(main_flags)\n",
      "        info.extend(sorted(main_flags.items()))\n",
      "    import pandas as pd\n",
      "    df = pd.DataFrame(info, columns=['Parameter', 'Value'])\n",
      "    df.set_index(['Parameter'], inplace=True)\n",
      "    setattr(df, 'name', 'info')\n",
      "    return df\n",
      "_co2mpas_info2df(start_time=datetime.datetime(2024, 4, 3, 16, 28, 9, 62286), main_flags=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "import socket\n",
      "State:\n",
      "<module 'socket' from '/local/rcs/XXX/miniforge3/envs/JRCSTU+co2mpas-ta/lib/python3.9/socket.py'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _co2mpas_info2df(start_time, main_flags=None):\n",
      "    import socket\n",
      "    import datetime\n",
      "    from co2mpas import __version__\n",
      "    from ..load.schema import define_flags_schema\n",
      "    time_elapsed = (datetime.datetime.today() - start_time).total_seconds()\n",
      "    hostname = socket.gethostname()\n",
      "    info = [\n",
      "        ('CO2MPAS version', __version__),\n",
      "        ('Simulation started', start_time.strftime('%Y/%m/%d-%H:%M:%S')),\n",
      "        ('Time elapsed', '%.3f sec' % time_elapsed),\n",
      "        ('Hostname', hostname),\n",
      "    ]\n",
      "    if main_flags:\n",
      "        main_flags = define_flags_schema(read=False).validate(main_flags)\n",
      "        info.extend(sorted(main_flags.items()))\n",
      "    import pandas as pd\n",
      "    df = pd.DataFrame(info, columns=['Parameter', 'Value'])\n",
      "    df.set_index(['Parameter'], inplace=True)\n",
      "    setattr(df, 'name', 'info')\n",
      "    return df\n",
      "_co2mpas_info2df(start_time=datetime.datetime(2024, 4, 3, 16, 28, 9, 62286), main_flags=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "import datetime\n",
      "State:\n",
      "<module 'datetime' from '/local/rcs/XXX/miniforge3/envs/JRCSTU+co2mpas-ta/lib/python3.9/datetime.py'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _co2mpas_info2df(start_time, main_flags=None):\n",
      "    import socket\n",
      "    import datetime\n",
      "    from co2mpas import __version__\n",
      "    from ..load.schema import define_flags_schema\n",
      "    time_elapsed = (datetime.datetime.today() - start_time).total_seconds()\n",
      "    hostname = socket.gethostname()\n",
      "    info = [\n",
      "        ('CO2MPAS version', __version__),\n",
      "        ('Simulation started', start_time.strftime('%Y/%m/%d-%H:%M:%S')),\n",
      "        ('Time elapsed', '%.3f sec' % time_elapsed),\n",
      "        ('Hostname', hostname),\n",
      "    ]\n",
      "    if main_flags:\n",
      "        main_flags = define_flags_schema(read=False).validate(main_flags)\n",
      "        info.extend(sorted(main_flags.items()))\n",
      "    import pandas as pd\n",
      "    df = pd.DataFrame(info, columns=['Parameter', 'Value'])\n",
      "    df.set_index(['Parameter'], inplace=True)\n",
      "    setattr(df, 'name', 'info')\n",
      "    return df\n",
      "_co2mpas_info2df(start_time=datetime.datetime(2024, 4, 3, 16, 28, 9, 62286), main_flags=None)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "from co2mpas import __version__\n",
      "State:\n",
      "'4.1.6'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _co2mpas_info2df(start_time, main_flags=None):\n",
      "    import socket\n",
      "    import datetime\n",
      "    from co2mpas import __version__\n",
      "    from ..load.schema import define_flags_schema\n",
      "    time_elapsed = (datetime.datetime.today() - start_time).total_seconds()\n",
      "    hostname = socket.gethostname()\n",
      "    info = [\n",
      "        ('CO2MPAS version', __version__),\n",
      "        ('Simulation started', start_time.strftime('%Y/%m/%d-%H:%M:%S')),\n",
      "        ('Time elapsed', '%.3f sec' % time_elapsed),\n",
      "        ('Hostname', hostname),\n",
      "    ]\n",
      "    if main_flags:\n",
      "        main_flags = define_flags_schema(read=False).validate(main_flags)\n",
      "        info.extend(sorted(main_flags.items()))\n",
      "    import pandas as pd\n",
      "    df = pd.DataFrame(info, columns=['Parameter', 'Value'])\n",
      "    df.set_index(['Parameter'], inplace=True)\n",
      "    setattr(df, 'name', 'info')\n",
      "    return df\n",
      "_co2mpas_info2df(start_time=datetime.datetime(2024, 4, 3, 16, 28, 9, 62286), main_flags=None)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "from ..load.schema import define_flags_schema\n",
      "State:\n",
      "{__module__='co2mpas.core.load.schema', __name__='define_flags_schema', __qualname__='define_flags_schema', __doc__='\\n    Define flag schema.\\n\\n    :param read:\\n        Schema for reading?\\n    :type read: bool\\n\\n    :return:\\n        Flag schema.\\n    :rtype: schema.Schema\\n    ', __annotations__={}}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _co2mpas_info2df(start_time, main_flags=None):\n",
      "    import socket\n",
      "    import datetime\n",
      "    from co2mpas import __version__\n",
      "    from ..load.schema import define_flags_schema\n",
      "    time_elapsed = (datetime.datetime.today() - start_time).total_seconds()\n",
      "    hostname = socket.gethostname()\n",
      "    info = [\n",
      "        ('CO2MPAS version', __version__),\n",
      "        ('Simulation started', start_time.strftime('%Y/%m/%d-%H:%M:%S')),\n",
      "        ('Time elapsed', '%.3f sec' % time_elapsed),\n",
      "        ('Hostname', hostname),\n",
      "    ]\n",
      "    if main_flags:\n",
      "        main_flags = define_flags_schema(read=False).validate(main_flags)\n",
      "        info.extend(sorted(main_flags.items()))\n",
      "    import pandas as pd\n",
      "    df = pd.DataFrame(info, columns=['Parameter', 'Value'])\n",
      "    df.set_index(['Parameter'], inplace=True)\n",
      "    setattr(df, 'name', 'info')\n",
      "    return df\n",
      "_co2mpas_info2df(start_time=datetime.datetime(2024, 4, 3, 16, 28, 9, 62286), main_flags=None)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "time_elapsed = (datetime.datetime.today() - start_time).total_seconds()\n",
      "State:\n",
      "103.529885\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_3_conf(self, options):\n",
      "        import yaml\n",
      "        import schedula as sh\n",
      "        from co2mpas.defaults import dfl\n",
      "        from co2mpas.cli import conf\n",
      "        kw = conf.make_context('conf', list(options)).params\n",
      "        t = {k for k, _ in sh.stack_nested_keys(dfl.to_dict())}\n",
      "        with self.runner.isolated_filesystem():\n",
      "            result = self.invoke(('conf',) + options)\n",
      "            self.assertEqual(result.exit_code, 0)\n",
      "            with open(kw['output_file'], 'rb') as f:\n",
      "                r = dict(sh.stack_nested_keys(yaml.load(f)))\n",
      "                self.assertSetEqual(set(r), t)\n",
      "            if kw['model_conf']:\n",
      "                with open(kw['model_conf'], 'rb') as f:\n",
      "                    for k, v in sh.stack_nested_keys(yaml.load(f)):\n",
      "                        self.assertEqual(r[k], v)\n",
      "test_3_conf(self=<test_cli.CLI testMethod=test_3_conf_1___>, options=(), self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7ff4a5632910>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_3_conf_1___', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.invoke=functools.partial(<bound method CliRunner.invoke of <click.testing.CliRunner object at 0x7ff4a5582340>>, <Group co2mpas>), self.runner=<click.testing.CliRunner object at 0x7ff4a5582340>, self.test_3_conf_1___=<bound method CLI.test_3_conf of <test_cli.CLI testMethod=test_3_conf_1___>>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "import yaml\n",
      "State:\n",
      "<module 'yaml' from '/local/rcs/XXX/miniforge3/envs/JRCSTU+co2mpas-ta/lib/python3.9/site-packages/yaml/__init__.py'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_3_conf(self, options):\n",
      "        import yaml\n",
      "        import schedula as sh\n",
      "        from co2mpas.defaults import dfl\n",
      "        from co2mpas.cli import conf\n",
      "        kw = conf.make_context('conf', list(options)).params\n",
      "        t = {k for k, _ in sh.stack_nested_keys(dfl.to_dict())}\n",
      "        with self.runner.isolated_filesystem():\n",
      "            result = self.invoke(('conf',) + options)\n",
      "            self.assertEqual(result.exit_code, 0)\n",
      "            with open(kw['output_file'], 'rb') as f:\n",
      "                r = dict(sh.stack_nested_keys(yaml.load(f)))\n",
      "                self.assertSetEqual(set(r), t)\n",
      "            if kw['model_conf']:\n",
      "                with open(kw['model_conf'], 'rb') as f:\n",
      "                    for k, v in sh.stack_nested_keys(yaml.load(f)):\n",
      "                        self.assertEqual(r[k], v)\n",
      "test_3_conf(self=<test_cli.CLI testMethod=test_3_conf_1___>, options=(), self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7ff4a5632910>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_3_conf_1___', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.invoke=functools.partial(<bound method CliRunner.invoke of <click.testing.CliRunner object at 0x7ff4a5582340>>, <Group co2mpas>), self.runner=<click.testing.CliRunner object at 0x7ff4a5582340>, self.test_3_conf_1___=<bound method CLI.test_3_conf of <test_cli.CLI testMethod=test_3_conf_1___>>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "import schedula as sh\n",
      "State:\n",
      "<module 'schedula' from '/local/rcs/XXX/miniforge3/envs/JRCSTU+co2mpas-ta/lib/python3.9/site-packages/schedula/__init__.py'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_3_conf(self, options):\n",
      "        import yaml\n",
      "        import schedula as sh\n",
      "        from co2mpas.defaults import dfl\n",
      "        from co2mpas.cli import conf\n",
      "        kw = conf.make_context('conf', list(options)).params\n",
      "        t = {k for k, _ in sh.stack_nested_keys(dfl.to_dict())}\n",
      "        with self.runner.isolated_filesystem():\n",
      "            result = self.invoke(('conf',) + options)\n",
      "            self.assertEqual(result.exit_code, 0)\n",
      "            with open(kw['output_file'], 'rb') as f:\n",
      "                r = dict(sh.stack_nested_keys(yaml.load(f)))\n",
      "                self.assertSetEqual(set(r), t)\n",
      "            if kw['model_conf']:\n",
      "                with open(kw['model_conf'], 'rb') as f:\n",
      "                    for k, v in sh.stack_nested_keys(yaml.load(f)):\n",
      "                        self.assertEqual(r[k], v)\n",
      "test_3_conf(self=<test_cli.CLI testMethod=test_3_conf_1___>, options=(), self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7ff4a5632910>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_3_conf_1___', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.invoke=functools.partial(<bound method CliRunner.invoke of <click.testing.CliRunner object at 0x7ff4a5582340>>, <Group co2mpas>), self.runner=<click.testing.CliRunner object at 0x7ff4a5582340>, self.test_3_conf_1___=<bound method CLI.test_3_conf of <test_cli.CLI testMethod=test_3_conf_1___>>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "from co2mpas.defaults import dfl\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_3_conf(self, options):\n",
      "        import yaml\n",
      "        import schedula as sh\n",
      "        from co2mpas.defaults import dfl\n",
      "        from co2mpas.cli import conf\n",
      "        kw = conf.make_context('conf', list(options)).params\n",
      "        t = {k for k, _ in sh.stack_nested_keys(dfl.to_dict())}\n",
      "        with self.runner.isolated_filesystem():\n",
      "            result = self.invoke(('conf',) + options)\n",
      "            self.assertEqual(result.exit_code, 0)\n",
      "            with open(kw['output_file'], 'rb') as f:\n",
      "                r = dict(sh.stack_nested_keys(yaml.load(f)))\n",
      "                self.assertSetEqual(set(r), t)\n",
      "            if kw['model_conf']:\n",
      "                with open(kw['model_conf'], 'rb') as f:\n",
      "                    for k, v in sh.stack_nested_keys(yaml.load(f)):\n",
      "                        self.assertEqual(r[k], v)\n",
      "test_3_conf(self=<test_cli.CLI testMethod=test_3_conf_1___>, options=(), self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7ff4a5632910>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_3_conf_1___', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.invoke=functools.partial(<bound method CliRunner.invoke of <click.testing.CliRunner object at 0x7ff4a5582340>>, <Group co2mpas>), self.runner=<click.testing.CliRunner object at 0x7ff4a5582340>, self.test_3_conf_1___=<bound method CLI.test_3_conf of <test_cli.CLI testMethod=test_3_conf_1___>>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "from co2mpas.cli import conf\n",
      "State:\n",
      "<Command conf>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_3_conf(self, options):\n",
      "        import yaml\n",
      "        import schedula as sh\n",
      "        from co2mpas.defaults import dfl\n",
      "        from co2mpas.cli import conf\n",
      "        kw = conf.make_context('conf', list(options)).params\n",
      "        t = {k for k, _ in sh.stack_nested_keys(dfl.to_dict())}\n",
      "        with self.runner.isolated_filesystem():\n",
      "            result = self.invoke(('conf',) + options)\n",
      "            self.assertEqual(result.exit_code, 0)\n",
      "            with open(kw['output_file'], 'rb') as f:\n",
      "                r = dict(sh.stack_nested_keys(yaml.load(f)))\n",
      "                self.assertSetEqual(set(r), t)\n",
      "            if kw['model_conf']:\n",
      "                with open(kw['model_conf'], 'rb') as f:\n",
      "                    for k, v in sh.stack_nested_keys(yaml.load(f)):\n",
      "                        self.assertEqual(r[k], v)\n",
      "test_3_conf(self=<test_cli.CLI testMethod=test_3_conf_1___>, options=(), self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7ff4a5632910>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_3_conf_1___', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.invoke=functools.partial(<bound method CliRunner.invoke of <click.testing.CliRunner object at 0x7ff4a5582340>>, <Group co2mpas>), self.runner=<click.testing.CliRunner object at 0x7ff4a5582340>, self.test_3_conf_1___=<bound method CLI.test_3_conf of <test_cli.CLI testMethod=test_3_conf_1___>>)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "kw = conf.make_context('conf', list(options)).params\n",
      "State:\n",
      "{'output_file': './conf.yaml', 'model_conf': None}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def to_dict(self):\n",
      "        import inspect\n",
      "        s, pr = set(dir(self)) - set(dir(Constants)), {}\n",
      "        for n in s.union(self.__class__.__dict__.keys()):\n",
      "            if n.startswith('__'):\n",
      "                continue\n",
      "            v = getattr(self, n)\n",
      "            if inspect.ismethod(v) or inspect.isbuiltin(v):\n",
      "                continue\n",
      "            if isinstance(v, Constants):\n",
      "                pr[n] = {'__constants__': v.to_dict()}\n",
      "            elif inspect.isclass(v) and issubclass(v, Constants):\n",
      "                pr[n] = {'__constants__': v.to_dict(v)}\n",
      "            else:\n",
      "                pr[n] = v\n",
      "        return pr\n",
      "to_dict(self=<class 'co2mpas.defaults.Functions.calculate_frontal_area'>, self.__doc__=None, self.__module__='co2mpas.defaults', self.projection_factor=0.84)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "import inspect\n",
      "State:\n",
      "<module 'inspect' from '/local/rcs/XXX/miniforge3/envs/JRCSTU+co2mpas-ta/lib/python3.9/inspect.py'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def to_dict(self):\n",
      "        import inspect\n",
      "        s, pr = set(dir(self)) - set(dir(Constants)), {}\n",
      "        for n in s.union(self.__class__.__dict__.keys()):\n",
      "            if n.startswith('__'):\n",
      "                continue\n",
      "            v = getattr(self, n)\n",
      "            if inspect.ismethod(v) or inspect.isbuiltin(v):\n",
      "                continue\n",
      "            if isinstance(v, Constants):\n",
      "                pr[n] = {'__constants__': v.to_dict()}\n",
      "            elif inspect.isclass(v) and issubclass(v, Constants):\n",
      "                pr[n] = {'__constants__': v.to_dict(v)}\n",
      "            else:\n",
      "                pr[n] = v\n",
      "        return pr\n",
      "to_dict(self=<class 'co2mpas.defaults.Functions.calculate_frontal_area'>, self.__doc__=None, self.__module__='co2mpas.defaults', self.projection_factor=0.84)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "s, pr = set(dir(self)) - set(dir(Constants)), {}\n",
      "State:\n",
      "{'projection_factor'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def to_dict(self):\n",
      "        import inspect\n",
      "        s, pr = set(dir(self)) - set(dir(Constants)), {}\n",
      "        for n in s.union(self.__class__.__dict__.keys()):\n",
      "            if n.startswith('__'):\n",
      "                continue\n",
      "            v = getattr(self, n)\n",
      "            if inspect.ismethod(v) or inspect.isbuiltin(v):\n",
      "                continue\n",
      "            if isinstance(v, Constants):\n",
      "                pr[n] = {'__constants__': v.to_dict()}\n",
      "            elif inspect.isclass(v) and issubclass(v, Constants):\n",
      "                pr[n] = {'__constants__': v.to_dict(v)}\n",
      "            else:\n",
      "                pr[n] = v\n",
      "        return pr\n",
      "to_dict(self=<class 'co2mpas.defaults.Functions.calculate_frontal_area'>, self.__doc__=None, self.__module__='co2mpas.defaults', self.projection_factor=0.84)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "pr[n] = v\n",
      "State:\n",
      "{'projection_factor': 0.84}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def dump(self, file, default_flow_style=False, **kw):\n",
      "        import yaml\n",
      "        kw['Dumper'] = kw.get('Dumper', yaml.CDumper)\n",
      "        with open(file, 'w') as f:\n",
      "            yaml.dump(\n",
      "                self.to_dict(), f, default_flow_style=default_flow_style, **kw\n",
      "            )\n",
      "dump(self={}, file='./conf.yaml', default_flow_style=False, kw={})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "import yaml\n",
      "State:\n",
      "<module 'yaml' from '/local/rcs/XXX/miniforge3/envs/JRCSTU+co2mpas-ta/lib/python3.9/site-packages/yaml/__init__.py'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def try_to_compute_label(label_context: Context[str], wrapper: ArticleWrapper):\n",
      "    assert wrapper.article, \"Missing article for this step\"\n",
      "    with label_context.capture():\n",
      "        label_context.push(wrapper.article.label)\n",
      "try_to_compute_label(label_context=Context(history=None, error=None, data=None), wrapper=ArticleWrapper(article=<wostools.article.Article object at 0x7f1a971b1dc0>, label='L Robertson, 1999, Science'))\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "label_context.push(wrapper.article.label)\n",
      "State:\n",
      "Context(history=[], error=None, data='L Robertson, 1999, Science')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def push(self, data: Optional[T], error: Optional[Exception] = None):\n",
      "        if self.history is None:\n",
      "            self.history = []\n",
      "        if self.data:\n",
      "            self.history.append(self.data)\n",
      "        self.data = data\n",
      "        self.error = error\n",
      "push(self=Context(history=None, error=None, data=None), data='L Robertson, 1999, Science', error=None, self.data=None, self.error=None, self.history=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.history = []\n",
      "State:\n",
      "Context(history=[], error=None, data=None)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def push(self, data: Optional[T], error: Optional[Exception] = None):\n",
      "        if self.history is None:\n",
      "            self.history = []\n",
      "        if self.data:\n",
      "            self.history.append(self.data)\n",
      "        self.data = data\n",
      "        self.error = error\n",
      "push(self=Context(history=None, error=None, data=None), data='L Robertson, 1999, Science', error=None, self.data=None, self.error=None, self.history=None)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "self.data = data\n",
      "State:\n",
      "Context(history=[], error=None, data='L Robertson, 1999, Science')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def article_missing(field: str):\n",
      "    article = Article(\n",
      "        title=None, authors=[\"L, Robertson\"], year=1999, journal=\"Science\"\n",
      "    )\n",
      "    setattr(article, field, None)\n",
      "    return ArticleWrapper(article=article)\n",
      "article_missing(field='year')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "article = Article(\n",
      "State:\n",
      "{title=None, authors=['L, Robertson'], keywords=[], year=1999, journal='Science', volume=None, issue=None, page=None, doi=None, references=[], sources=set(), extra={}}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def article_missing(field: str):\n",
      "    article = Article(\n",
      "        title=None, authors=[\"L, Robertson\"], year=1999, journal=\"Science\"\n",
      "    )\n",
      "    setattr(article, field, None)\n",
      "    return ArticleWrapper(article=article)\n",
      "article_missing(field='year')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "setattr(article, field, None)\n",
      "State:\n",
      "{title=None, authors=['L, Robertson'], keywords=[], year=None, journal='Science', volume=None, issue=None, page=None, doi=None, references=[], sources=set(), extra={}}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def from_isi_text(cls, raw: str) -> \"Article\":\n",
      "        data = collections.defaultdict(list)\n",
      "        field = None\n",
      "        for line in raw.split(\"\\n\"):\n",
      "            match = ISI_LINE_PATTERN.match(line)\n",
      "            if not match:\n",
      "                raise InvalidIsiLine(line)\n",
      "            parsed = match.groupdict()\n",
      "            field = parsed.get(\"field\") or field\n",
      "            if not field or \"value\" not in parsed or parsed[\"value\"] is None:\n",
      "                continue\n",
      "            data[field].append(parsed[\"value\"])\n",
      "        processed = parse_all(dict(data))\n",
      "        return cls(\n",
      "            title=processed.get(\"title\"),\n",
      "            authors=processed.get(\"authors\", []),\n",
      "            year=processed.get(\"year\"),\n",
      "            journal=processed.get(\"source_abbreviation\"),\n",
      "            volume=processed.get(\"volume\"),\n",
      "            issue=processed.get(\"issue\"),\n",
      "            page=processed.get(\"beginning_page\"),\n",
      "            doi=processed.get(\"DOI\"),\n",
      "            references=processed.get(\"references\"),\n",
      "            keywords=processed.get(\"keywords\"),\n",
      "            extra=processed,\n",
      "            sources={raw},\n",
      "        )\n",
      "from_isi_text(cls=<class 'wostools.article.Article'>, raw='PT J\\nAU John Doe\\n   Jane Doe\\nAF John Doe\\n   Jane Doe\\nTI some title\\nSO JOURNAL OF MAGNETISM AND MAGNETIC MATERIALS\\nLA English\\nDT Article\\nDE Electrodeposition; Structural control; Nanodot array; Bit-patterned\\n   media; CoPt alloy\\nID BIT-PATTERNED MEDIA; ELECTRON-BEAM LITHOGRAPHY; RECORDING MEDIA;\\n   MAGNETIC MEDIA; DENSITY; FILMS; ANISOTROPY; STORAGE\\nAB CoPt nanodot arrays were fabricated by combining electrodeposition and electron beam lithography (EBL) for the use of bit-patterned media (BPM). To achieve precise control of deposition uniformity and coercivity of the CoPt nanodot arrays, their crystal structure and magnetic properties were controlled by controlling the diffusion state of metal ions from the initial deposition stage with the application of bath agitation. Following bath agitation, the composition gradient of the CoPt alloy with thickness was mitigated to have a near-ideal alloy composition of Co:Pt =80:20, which induces epitaxial-like growth from Ru substrate, thus resulting in the improvement of the crystal orientation of the hcp (002) structure from its initial deposition stages. Furthermore, the cross-sectional transmission electron microscope (TEM) analysis of the nanodots deposited with bath agitation showed CoPt growth along its c-axis oriented in the perpendicular direction, having uniform lattice fringes on the hcp (002) plane from the Ru underlayer interface, which is a significant factor to induce perpendicular magnetic anisotropy. Magnetic characterization of the CoPt nanodot arrays showed increase in the perpendicular coercivity and squareness of the hysteresis loops from 2.0 kOe and 0.64 (without agitation) to 4.0 kOe and 0.87 with bath agitation. Based on the detailed characterization of nanodot arrays, the precise crystal structure control of the nanodot arrays with ultra-high recording density by electrochemical process was successfully demonstrated.\\nC1 [Wodarz, Siggi; Homma, Takayuki] Waseda Univ, Dept Appl Chem, Shinjuku Ku, Tokyo 1698555, Japan.\\n   [Hasegawa, Takashi; Ishio, Shunji] Akita Univ, Dept Mat Sci, Akita 0108502, Japan.\\nRP Homma, T (reprint author), Waseda Univ, Dept Appl Chem, Shinjuku Ku, Tokyo 1698555, Japan.\\nEM t.homma@waseda.jp\\nOI Hasegawa, Takashi/0000-0002-8178-4980\\nFU JSPS KAKENHI Grant [25249104]\\nFX This work was supported in part by JSPS KAKENHI Grant Number 25249104.\\nCR Albrecht TR, 2013, IEEE T MAGN, V49, P773, DOI 10.1109/TMAG.2012.2227303\\n   BUSCHOW KHJ, 1983, J MAGN MAGN MATER, V38, P1, DOI 10.1016/0304-8853(83)90097-5\\n   Gapin AI, 2006, J APPL PHYS, V99, DOI 10.1063/1.2163289\\n   Homma Takayuki, 2015, ECS Transactions, V64, P1, DOI 10.1149/06431.0001ecst\\n   Kryder MH, 2008, P IEEE, V96, P1810, DOI 10.1109/JPROC.2008.2004315\\n   Kubo T, 2005, J APPL PHYS, V97, DOI 10.1063/1.1855572\\n   Lodder JC, 2004, J MAGN MAGN MATER, V272, P1692, DOI 10.1016/j.jmmm.2003.12.259\\n   Mitsuzuka K, 2007, IEEE T MAGN, V43, P2160, DOI 10.1109/TMAG.2007.893129\\n   Ouchi T, 2010, ELECTROCHIM ACTA, V55, P8081, DOI 10.1016/j.electacta.2010.02.073\\n   Pattanaik G, 2006, J APPL PHYS, V99, DOI 10.1063/1.2150805\\n   Pattanaik G, 2007, ELECTROCHIM ACTA, V52, P2755, DOI 10.1016/j.electacta.2006.07.062\\n   Piramanayagam SN, 2009, J MAGN MAGN MATER, V321, P485, DOI 10.1016/j.jmmm.2008.05.007\\n   Ross CA, 2008, MRS BULL, V33, P838, DOI 10.1557/mrs2008.179\\n   Shiroishi Y, 2009, IEEE T MAGN, V45, P3816, DOI 10.1109/TMAG.2009.2024879\\n   Sirtori V, 2011, ACS APPL MATER INTER, V3, P1800, DOI 10.1021/am200267u\\n   Sohn JS, 2009, NANOTECHNOLOGY, V20, DOI 10.1088/0957-4484/20/2/025302\\n   Sun SH, 2000, SCIENCE, V287, P1989, DOI 10.1126/science.287.5460.1989\\n   Terris BD, 2007, MICROSYST TECHNOL, V13, P189, DOI 10.1007/s00542-006-0144-9\\n   Wang JP, 2008, P IEEE, V96, P1847, DOI 10.1109/JPROC.2008.2004318\\n   Weller D, 1999, IEEE T MAGN, V35, P4423, DOI 10.1109/20.809134\\n   Weller D, 2000, IEEE T MAGN, V36, P10, DOI 10.1109/20.824418\\n   Wodarz S, 2016, ELECTROCHIM ACTA, V197, P330, DOI 10.1016/j.electacta.2015.11.136\\n   Xu X, 2012, J ELECTROCHEM SOC, V159, pD240, DOI 10.1149/2.090204jes\\n   Yang X, 2007, J VAC SCI TECHNOL B, V25, P2202, DOI 10.1116/1.2798711\\n   Yang XM, 2009, ACS NANO, V3, P1844, DOI 10.1021/nn900073r\\n   Yasui N, 2003, APPL PHYS LETT, V83, P3347, DOI 10.1063/1.1622787\\n   Yua H., 2009, J APPL PHYS, V105\\n   Zhu JG, 2008, IEEE T MAGN, V44, P125, DOI 10.1109/TMAG.2007.911031\\nNR 28\\nTC 0\\nZ9 0\\nU1 21\\nU2 21\\nPU ELSEVIER SCIENCE BV\\nPI AMSTERDAM\\nPA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS\\nSN 0304-8853\\nEI 1873-4766\\nJ9 J MAGN MAGN MATER\\nJI J. Magn. Magn. Mater.\\nPD MAY 15\\nPY 1994\\nVL 1000\\nIS 2\\nBP 1330-5\\nEP 58\\nDI 10.1016/j.jmmm.2017.01.061\\nPG 7\\nWC Materials Science, Multidisciplinary; Physics, Condensed Matter\\nSC Materials Science; Physics\\nGA EP2GP\\nUT WOS:000397201600008\\nER')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "field = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def from_isi_citation(cls, reference: str) -> \"Article\":\n",
      "        match = ISI_CITATION_PATTERN.match(reference)\n",
      "        if not match:\n",
      "            raise InvalidReference(reference)\n",
      "        data = {key: [value] for key, value in match.groupdict().items() if value}\n",
      "        processed = parse_all(data)\n",
      "        return cls(\n",
      "            title=processed.get(\"title\"),\n",
      "            authors=processed.get(\"authors\", []),\n",
      "            year=processed.get(\"year\"),\n",
      "            journal=processed.get(\"source_abbreviation\"),\n",
      "            volume=processed.get(\"volume\"),\n",
      "            page=processed.get(\"beginning_page\"),\n",
      "            doi=processed.get(\"DOI\"),\n",
      "            extra=processed,\n",
      "            sources={reference},\n",
      "        )\n",
      "from_isi_citation(cls=<class 'wostools.article.Article'>, reference='L Antuan, 2008, P IEEE, V69, P1810, DOI DOI 10.1109/JPROC.2008.2004315')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "match = ISI_CITATION_PATTERN.match(reference)\n",
      "State:\n",
      "<re.Match object; span=(0, 70), match='L Antuan, 2008, P IEEE, V69, P1810, DOI DOI 10.11>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def from_isi_citation(cls, reference: str) -> \"Article\":\n",
      "        match = ISI_CITATION_PATTERN.match(reference)\n",
      "        if not match:\n",
      "            raise InvalidReference(reference)\n",
      "        data = {key: [value] for key, value in match.groupdict().items() if value}\n",
      "        processed = parse_all(data)\n",
      "        return cls(\n",
      "            title=processed.get(\"title\"),\n",
      "            authors=processed.get(\"authors\", []),\n",
      "            year=processed.get(\"year\"),\n",
      "            journal=processed.get(\"source_abbreviation\"),\n",
      "            volume=processed.get(\"volume\"),\n",
      "            page=processed.get(\"beginning_page\"),\n",
      "            doi=processed.get(\"DOI\"),\n",
      "            extra=processed,\n",
      "            sources={reference},\n",
      "        )\n",
      "from_isi_citation(cls=<class 'wostools.article.Article'>, reference='L Antuan, 2008, P IEEE, V69, P1810, DOI DOI 10.1109/JPROC.2008.2004315')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "processed = parse_all(data)\n",
      "State:\n",
      "{'AU': ['L Antuan'], 'authors': ['L Antuan'], 'PY': 2008, 'year_published': 2008, 'year': 2008, 'publication_year': 2008, 'J9': 'P IEEE', 'source_abbreviation': 'P IEEE', 'VL': '69', 'volume': '69', 'BP': '1810', 'beginning_page': '1810', 'DI': 'DOI 10.1109/JPROC.2008.2004315', 'digital_object_identifier': 'DOI 10.1109/JPROC.2008.2004315', 'DOI': 'DOI 10.1109/JPROC.2008.2004315', 'CR': [], 'cited_references': [], 'references': [], 'citations': []}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _advance(self):\n",
      "        self._current += 1\n",
      "        return self.source[self._current - 1]\n",
      "_advance(self=<pycalc.core.scanner.Scanner object at 0x7fe109cc5b50>, self._current=0, self._start=0, self.source='102%12%7', self.tokens=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self._current += 1\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def warning(self, msg, *args):\n",
      "                _messages.append(\"W:\" + msg % args)\n",
      "warning(self=<test_api.TestUndefined.test_logging_undefined.<locals>.DebugLogger object at 0x7f3d58670fd0>, msg='Template variable warning: %s', args=(\"'missing' is undefined\",), _messages=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "_messages.append(\"W:\" + msg % args)\n",
      "State:\n",
      "[\"W:Template variable warning: 'missing' is undefined\"]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_includes(t, **ctx):\n",
      "            ctx[\"foo\"] = 42\n",
      "            assert t.render(ctx) == \"[42|23]\"\n",
      "test_includes(t=<Template memory:7f3d586442b0>, ctx={})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "ctx[\"foo\"] = 42\n",
      "State:\n",
      "{'foo': 42}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_includes(t, **ctx):\n",
      "            ctx[\"foo\"] = 42\n",
      "            assert t.render(ctx) == \"[42|23]\"\n",
      "test_includes(t=<Template memory:7f3d586442b0>, ctx={})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "assert t.render(ctx) == \"[42|23]\"\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_custom_async_iteratable_filter(env_async, items):\n",
      "    async def customfilter(iterable):\n",
      "        items = []\n",
      "        async for item in auto_aiter(iterable):\n",
      "            items.append(str(item))\n",
      "            if len(items) == 3:\n",
      "                break\n",
      "        return \",\".join(items)\n",
      "    env_async.filters[\"customfilter\"] = customfilter\n",
      "    tmpl = env_async.from_string(\n",
      "        \"{{ items()|customfilter }} .. {{ [3, 4, 5, 6]|customfilter }}\"\n",
      "    )\n",
      "    out = tmpl.render(items=items)\n",
      "    assert out == \"0,1,2 .. 3,4,5\"\n",
      "test_custom_async_iteratable_filter(env_async={block_start_string='{%', block_end_string='%}', variable_start_string='{{', variable_end_string='}}', comment_start_string='{\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "tmpl = env_async.from_string(\n",
      "State:\n",
      "<Template memory:7f3d586764f0>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_builtin_metadata(dataset_name):\n",
      "    if dataset_name == \"coco\":\n",
      "        return _get_coco_instances_meta()\n",
      "    if dataset_name == \"coco_panoptic_separated\":\n",
      "        return _get_coco_panoptic_separated_meta()\n",
      "    elif dataset_name == \"coco_panoptic_standard\":\n",
      "        meta = {}\n",
      "        thing_classes = [k[\"name\"] for k in COCO_CATEGORIES]\n",
      "        thing_colors = [k[\"color\"] for k in COCO_CATEGORIES]\n",
      "        stuff_classes = [k[\"name\"] for k in COCO_CATEGORIES]\n",
      "        stuff_colors = [k[\"color\"] for k in COCO_CATEGORIES]\n",
      "        meta[\"thing_classes\"] = thing_classes\n",
      "        meta[\"thing_colors\"] = thing_colors\n",
      "        meta[\"stuff_classes\"] = stuff_classes\n",
      "        meta[\"stuff_colors\"] = stuff_colors\n",
      "        thing_dataset_id_to_contiguous_id = {}\n",
      "        stuff_dataset_id_to_contiguous_id = {}\n",
      "        for i, cat in enumerate(COCO_CATEGORIES):\n",
      "            if cat[\"isthing\"]:\n",
      "                thing_dataset_id_to_contiguous_id[cat[\"id\"]] = i\n",
      "            else:\n",
      "                stuff_dataset_id_to_contiguous_id[cat[\"id\"]] = i\n",
      "        meta[\"thing_dataset_id_to_contiguous_id\"] = thing_dataset_id_to_contiguous_id\n",
      "        meta[\"stuff_dataset_id_to_contiguous_id\"] = stuff_dataset_id_to_contiguous_id\n",
      "        return meta\n",
      "    elif dataset_name == \"coco_person\":\n",
      "        return {\n",
      "            \"thing_classes\": [\"person\"],\n",
      "            \"keypoint_names\": COCO_PERSON_KEYPOINT_NAMES,\n",
      "            \"keypoint_flip_map\": COCO_PERSON_KEYPOINT_FLIP_MAP,\n",
      "            \"keypoint_connection_rules\": KEYPOINT_CONNECTION_RULES,\n",
      "        }\n",
      "    elif dataset_name == \"cityscapes\":\n",
      "        CITYSCAPES_THING_CLASSES = [\n",
      "            \"person\", \"rider\", \"car\", \"truck\",\n",
      "            \"bus\", \"train\", \"motorcycle\", \"bicycle\",\n",
      "        ]\n",
      "        CITYSCAPES_STUFF_CLASSES = [\n",
      "            \"road\", \"sidewalk\", \"building\", \"wall\", \"fence\", \"pole\", \"traffic light\",\n",
      "            \"traffic sign\", \"vegetation\", \"terrain\", \"sky\", \"person\", \"rider\", \"car\",\n",
      "            \"truck\", \"bus\", \"train\", \"motorcycle\", \"bicycle\",\n",
      "        ]\n",
      "        return {\n",
      "            \"thing_classes\": CITYSCAPES_THING_CLASSES,\n",
      "            \"stuff_classes\": CITYSCAPES_STUFF_CLASSES,\n",
      "        }\n",
      "    raise KeyError(\"No built-in metadata for dataset {}\".format(dataset_name))\n",
      "_get_builtin_metadata(dataset_name='coco_panoptic_standard')\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "stuff_classes = [k[\"name\"] for k in COCO_CATEGORIES]\n",
      "State:\n",
      "['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'banner', 'blanket', 'bridge', 'cardboard', 'counter', 'curtain', 'door-stuff', 'floor-wood', 'flower', 'fruit', 'gravel', 'house', 'light', 'mirror-stuff', 'net', 'pillow', 'platform', 'playingfield', 'railroad', 'river', 'road', 'roof', 'sand', 'sea', 'shelf', 'snow', 'stairs', 'tent', 'towel', 'wall-brick', 'wall-stone', 'wall-tile', 'wall-wood', 'water-other', 'window-blind', 'window-other', 'tree-merged', 'fence-merged', 'ceiling-merged', 'sky-other-merged', 'cabinet-merged', 'table-merged', 'floor-other-merged', 'pavement-merged', 'mountain-merged', 'grass-merged', 'dirt-merged', 'paper-merged', 'food-other-merged', 'building-other-merged', 'rock-merged', 'wall-other-merged', 'rug-merged']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def register_mesh(mesh_info: MeshInfo, base_path: Optional[str]) -> None:\n",
      "    geodists, symmetry, texcoords = mesh_info.geodists, mesh_info.symmetry, mesh_info.texcoords\n",
      "    if geodists:\n",
      "        geodists = maybe_prepend_base_path(base_path, geodists)\n",
      "    if symmetry:\n",
      "        symmetry = maybe_prepend_base_path(base_path, symmetry)\n",
      "    if texcoords:\n",
      "        texcoords = maybe_prepend_base_path(base_path, texcoords)\n",
      "    MeshCatalog[mesh_info.name] = MeshInfo(\n",
      "        name=mesh_info.name,\n",
      "        data=maybe_prepend_base_path(base_path, mesh_info.data),\n",
      "        geodists=geodists,\n",
      "        symmetry=symmetry,\n",
      "        texcoords=texcoords,\n",
      "    )\n",
      "register_mesh(mesh_info=MeshInfo(name='smpl_27554', data='smpl_27554.pkl', geodists='geodists/geodists_smpl_27554.pkl', symmetry='symmetry/symmetry_smpl_27554.pkl', texcoords='texcoords/texcoords_smpl_27554.pkl'), base_path='https://dl.fbaipublicfiles.com/densepose/meshes/')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "geodists, symmetry, texcoords = mesh_info.geodists, mesh_info.symmetry, mesh_info.texcoords\n",
      "State:\n",
      "'geodists/geodists_smpl_27554.pkl'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def register_mesh(mesh_info: MeshInfo, base_path: Optional[str]) -> None:\n",
      "    geodists, symmetry, texcoords = mesh_info.geodists, mesh_info.symmetry, mesh_info.texcoords\n",
      "    if geodists:\n",
      "        geodists = maybe_prepend_base_path(base_path, geodists)\n",
      "    if symmetry:\n",
      "        symmetry = maybe_prepend_base_path(base_path, symmetry)\n",
      "    if texcoords:\n",
      "        texcoords = maybe_prepend_base_path(base_path, texcoords)\n",
      "    MeshCatalog[mesh_info.name] = MeshInfo(\n",
      "        name=mesh_info.name,\n",
      "        data=maybe_prepend_base_path(base_path, mesh_info.data),\n",
      "        geodists=geodists,\n",
      "        symmetry=symmetry,\n",
      "        texcoords=texcoords,\n",
      "    )\n",
      "register_mesh(mesh_info=MeshInfo(name='smpl_27554', data='smpl_27554.pkl', geodists='geodists/geodists_smpl_27554.pkl', symmetry='symmetry/symmetry_smpl_27554.pkl', texcoords='texcoords/texcoords_smpl_27554.pkl'), base_path='https://dl.fbaipublicfiles.com/densepose/meshes/')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "geodists = maybe_prepend_base_path(base_path, geodists)\n",
      "State:\n",
      "'https://dl.fbaipublicfiles.com/densepose/meshes/geodists/geodists_smpl_27554.pkl'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def register_mesh(mesh_info: MeshInfo, base_path: Optional[str]) -> None:\n",
      "    geodists, symmetry, texcoords = mesh_info.geodists, mesh_info.symmetry, mesh_info.texcoords\n",
      "    if geodists:\n",
      "        geodists = maybe_prepend_base_path(base_path, geodists)\n",
      "    if symmetry:\n",
      "        symmetry = maybe_prepend_base_path(base_path, symmetry)\n",
      "    if texcoords:\n",
      "        texcoords = maybe_prepend_base_path(base_path, texcoords)\n",
      "    MeshCatalog[mesh_info.name] = MeshInfo(\n",
      "        name=mesh_info.name,\n",
      "        data=maybe_prepend_base_path(base_path, mesh_info.data),\n",
      "        geodists=geodists,\n",
      "        symmetry=symmetry,\n",
      "        texcoords=texcoords,\n",
      "    )\n",
      "register_mesh(mesh_info=MeshInfo(name='smpl_27554', data='smpl_27554.pkl', geodists='geodists/geodists_smpl_27554.pkl', symmetry='symmetry/symmetry_smpl_27554.pkl', texcoords='texcoords/texcoords_smpl_27554.pkl'), base_path='https://dl.fbaipublicfiles.com/densepose/meshes/')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "symmetry = maybe_prepend_base_path(base_path, symmetry)\n",
      "State:\n",
      "'https://dl.fbaipublicfiles.com/densepose/meshes/symmetry/symmetry_smpl_27554.pkl'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __call__(self, k, d):\n",
      "        getter = rebind(self.getter)\n",
      "        getter_name = \"get_\" + k\n",
      "        getter.__name__ = getter_name\n",
      "        getter.__doc__ = \"Get the %s field\" % k\n",
      "        d[getter_name] = getter\n",
      "        setter = rebind(self.setter)\n",
      "        setter_name = \"set_\" + k\n",
      "        setter.__name__ = setter_name\n",
      "        setter.__doc__ = \"Set the %s field\" % k\n",
      "        d[\"set_\" + k] = setter\n",
      "        d[k] = property(getter, setter, doc=\"%s property\" % k)\n",
      "__call__(self=<impacket.helper.Byte object at 0x7f1f4042d370>, k='op_code', d={'__module__': 'impacket.wps', '__qualname__': 'SimpleConfig', '__doc__': 'For now, it supports Simple configs with the bits more_fragments and length_field not set', 'header_size': 2, 'tail_size': 0, 'op_code': <impacket.helper.Byte object at 0x7f1f4042d370>, 'flags': <impacket.helper.Byte object at 0x7f1f4042d940>, 'more_fragments': <impacket.helper.Bit object at 0x7f1f4042d220>, 'length_field': <impacket.helper.Bit object at 0x7f1f4042d250>, 'BUILDERS': {4108: <impacket.wps.ByteBuilder object at 0x7f1f4042d8e0>, 4109: <impacket.wps.ByteBuilder object at 0x7f1f4042dc70>, 4170: <impacket.wps.ByteBuilder object at 0x7f1f4042de50>, 4130: <impacket.wps.ByteBuilder object at 0x7f1f4042d8b0>, 4134: <impacket.wps.ByteBuilder object at 0x7f1f4042d2e0>, 4136: <impacket.wps.ByteBuilder object at 0x7f1f4042d850>, 4143: <impacket.wps.ByteBuilder object at 0x7f1f4042dd30>, 4144: <impacket.wps.ByteBuilder object at 0x7f1f4042da00>, 4145: <impacket.wps.ByteBuilder object at 0x7f1f4042dd90>, 4149: <impacket.wps.ByteBuilder object at 0x7f1f4042d130>, 4152: <impacket.wps.ByteBuilder object at 0x7f1f4042ddf0>, 4154: <impacket.wps.ByteBuilder object at 0x7f1f4042dd60>, 4155: <impacket.wps.ByteBuilder object at 0x7f1f4042dca0>, 4156: <impacket.wps.ByteBuilder object at 0x7f1f4042ddc0>, 4164: <impacket.wps.ByteBuilder object at 0x7f1f4042d7c0>, 4166: <impacket.wps.ByteBuilder object at 0x7f1f4042de20>, 4196: <impacket.wps.ByteBuilder object at 0x7f1f4042de80>, 4106: <impacket.wps.StringBuilder object at 0x7f1f4042deb0>, 4107: <impacket.wps.StringBuilder object at 0x7f1f4042dee0>, 4113: <impacket.wps.StringBuilder object at 0x7f1f4042df10>, 4124: <impacket.wps.StringBuilder object at 0x7f1f4042df40>, 4129: <impacket.wps.StringBuilder object at 0x7f1f4042df70>, 4131: <impacket.wps.StringBuilder object at 0x7f1f4042dfa0>, 4132: <impacket.wps.StringBuilder object at 0x7f1f4042dfd0>, 4137: <impacket.wps.StringBuilder object at 0x7f1f4042d1f0>, 4138: <impacket.wps.StringBuilder object at 0x7f1f4042d1c0>, 4162: <impacket.wps.StringBuilder object at 0x7f1f4042d190>, 4173: <impacket.wps.StringBuilder object at 0x7f1f4042d760>, 4135: <impacket.wps.StringBuilder object at 0x7f1f4042d730>, 4097: <impacket.wps.NumBuilder object at 0x7f1f4042d700>, 4098: <impacket.wps.NumBuilder object at 0x7f1f4042db50>, 4099: <impacket.wps.NumBuilder object at 0x7f1f4042d3a0>, 4100: <impacket.wps.NumBuilder object at 0x7f1f4042db20>, 4104: <impacket.wps.NumBuilder object at 0x7f1f4042d6d0>, 4105: <impacket.wps.NumBuilder object at 0x7f1f4042dcd0>, 4114: <impacket.wps.NumBuilder object at 0x7f1f4042d6a0>, 4111: <impacket.wps.NumBuilder object at 0x7f1f4042d820>, 4112: <impacket.wps.NumBuilder object at 0x7f1f4042d4f0>, 4174: <impacket.wps.NumBuilder object at 0x7f1f4042d610>, 4177: <impacket.wps.NumBuilder object at 0x7f1f4042d400>, 4178: <impacket.wps.NumBuilder object at 0x7f1f4042dd00>, 4179: <impacket.wps.NumBuilder object at 0x7f1f4042d670>, 4146: <impacket.wps.NumBuilder object at 0x7f1f4042d550>}, 'build_tlv_container': <classmethod object at 0x7f1f4042d280>, '_fields': ['op_code']}, self.index=0)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "getter_name = \"get_\" + k\n",
      "State:\n",
      "'get_op_code'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_model(self):\n",
      "        from yolox.models import YOLOX, YOLOPAFPN, YOLOXHead\n",
      "        def init_yolo(M):\n",
      "            for m in M.modules():\n",
      "                if isinstance(m, nn.BatchNorm2d):\n",
      "                    m.eps = 1e-3\n",
      "                    m.momentum = 0.03\n",
      "        if getattr(self, \"model\", None) is None:\n",
      "            in_channels = [256, 512, 1024]\n",
      "            backbone = YOLOPAFPN(self.depth, self.width, in_channels=in_channels, act=self.act)\n",
      "            head = YOLOXHead(self.num_classes, self.width, in_channels=in_channels, act=self.act)\n",
      "            self.model = YOLOX(backbone, head)\n",
      "        self.model.apply(init_yolo)\n",
      "        self.model.head.initialize_biases(1e-2)\n",
      "        self.model.train()\n",
      "        return self.model\n",
      "get_model(self= keys               values                      seed               None                        output_dir         './YOLOX_outputs'           print_interval     10                          eval_interval      10                          dataset            None                        num_classes        80                          depth              0.33                        width              0.5                         act                'silu'                      data_num_workers   4                           input_size         (640, 640)                  multiscale_range   5                           data_dir           None                        train_ann          'instances_train2017.json'  val_ann            'instances_val2017.json'    test_ann           'instances_test2017.json'   mosaic_prob        1.0                         mixup_prob         1.0                         hsv_prob           1.0                         flip_prob          0.5                         degrees            10.0                        translate          0.1                         mosaic_scale       (0.1, 2)                    enable_mixup       True                        mixup_scale        (0.5, 1.5)                  shear              2.0                         warmup_epochs      5                           max_epoch          300                         warmup_lr          0                           min_lr_ratio       0.05                        basic_lr_per_img   0.00015625                  scheduler          'yoloxwarmcos'              no_aug_epochs      15                          ema                True                        weight_decay       0.0005                      momentum           0.9                         save_history_ckpt  True                        exp_name           'yolox_s'                   test_size          (640, 640)                  test_conf          0.01                        nmsthre            0.65                       , self.act='silu', self.basic_lr_per_img=0.00015625, self.data_dir=None, self.data_num_workers=4, self.dataset=None, self.degrees=10.0, self.depth=0.33, self.ema=True, self.enable_mixup=True, self.eval_interval=10, self.exp_name='yolox_s', self.flip_prob=0.5, self.hsv_prob=1.0, self.input_size=(640, 640), self.max_epoch=300, self.min_lr_ratio=0.05, self.mixup_prob=1.0, self.mixup_scale=(0.5, 1.5), self.momentum=0.9, self.mosaic_prob=1.0, self.mosaic_scale=(0.1, 2), self.multiscale_range=5, self.nmsthre=0.65, self.no_aug_epochs=15, self.num_classes=80, self.output_dir='./YOLOX_outputs', self.print_interval=10, self.save_history_ckpt=True, self.scheduler='yoloxwarmcos', self.seed=None, self.shear=2.0, self.test_ann='instances_test2017.json', self.test_conf=0.01, self.test_size=(640, 640), self.train_ann='instances_train2017.json', self.translate=0.1, self.val_ann='instances_val2017.json', self.warmup_epochs=5, self.warmup_lr=0, self.weight_decay=0.0005, self.width=0.5)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "in_channels = [256, 512, 1024]\n",
      "State:\n",
      "[256, 512, 1024]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_model(self):\n",
      "        from yolox.models import YOLOX, YOLOPAFPN, YOLOXHead\n",
      "        def init_yolo(M):\n",
      "            for m in M.modules():\n",
      "                if isinstance(m, nn.BatchNorm2d):\n",
      "                    m.eps = 1e-3\n",
      "                    m.momentum = 0.03\n",
      "        if getattr(self, \"model\", None) is None:\n",
      "            in_channels = [256, 512, 1024]\n",
      "            backbone = YOLOPAFPN(self.depth, self.width, in_channels=in_channels, act=self.act)\n",
      "            head = YOLOXHead(self.num_classes, self.width, in_channels=in_channels, act=self.act)\n",
      "            self.model = YOLOX(backbone, head)\n",
      "        self.model.apply(init_yolo)\n",
      "        self.model.head.initialize_biases(1e-2)\n",
      "        self.model.train()\n",
      "        return self.model\n",
      "get_model(self= keys               values                      seed               None                        output_dir         './YOLOX_outputs'           print_interval     10                          eval_interval      10                          dataset            None                        num_classes        80                          depth              0.33                        width              0.5                         act                'silu'                      data_num_workers   4                           input_size         (640, 640)                  multiscale_range   5                           data_dir           None                        train_ann          'instances_train2017.json'  val_ann            'instances_val2017.json'    test_ann           'instances_test2017.json'   mosaic_prob        1.0                         mixup_prob         1.0                         hsv_prob           1.0                         flip_prob          0.5                         degrees            10.0                        translate          0.1                         mosaic_scale       (0.1, 2)                    enable_mixup       True                        mixup_scale        (0.5, 1.5)                  shear              2.0                         warmup_epochs      5                           max_epoch          300                         warmup_lr          0                           min_lr_ratio       0.05                        basic_lr_per_img   0.00015625                  scheduler          'yoloxwarmcos'              no_aug_epochs      15                          ema                True                        weight_decay       0.0005                      momentum           0.9                         save_history_ckpt  True                        exp_name           'yolox_s'                   test_size          (640, 640)                  test_conf          0.01                        nmsthre            0.65                       , self.act='silu', self.basic_lr_per_img=0.00015625, self.data_dir=None, self.data_num_workers=4, self.dataset=None, self.degrees=10.0, self.depth=0.33, self.ema=True, self.enable_mixup=True, self.eval_interval=10, self.exp_name='yolox_s', self.flip_prob=0.5, self.hsv_prob=1.0, self.input_size=(640, 640), self.max_epoch=300, self.min_lr_ratio=0.05, self.mixup_prob=1.0, self.mixup_scale=(0.5, 1.5), self.momentum=0.9, self.mosaic_prob=1.0, self.mosaic_scale=(0.1, 2), self.multiscale_range=5, self.nmsthre=0.65, self.no_aug_epochs=15, self.num_classes=80, self.output_dir='./YOLOX_outputs', self.print_interval=10, self.save_history_ckpt=True, self.scheduler='yoloxwarmcos', self.seed=None, self.shear=2.0, self.test_ann='instances_test2017.json', self.test_conf=0.01, self.test_size=(640, 640), self.train_ann='instances_train2017.json', self.translate=0.1, self.val_ann='instances_val2017.json', self.warmup_epochs=5, self.warmup_lr=0, self.weight_decay=0.0005, self.width=0.5)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "self.model.apply(init_yolo)\n",
      "State:\n",
      " keys               values                                                                                                   seed               None                                                                                                     output_dir         './YOLOX_outputs'                                                                                        print_interval     10                                                                                                       eval_interval      10                                                                                                       dataset            None                                                                                                     num_classes        80                                                                                                       depth              0.33                                                                                                     width              0.5                                                                                                      act                'silu'                                                                                                   data_num_workers   4                                                                                                        input_size         (640, 640)                                                                                               multiscale_range   5                                                                                                        data_dir           None                                                                                                     train_ann          'instances_train2017.json'                                                                               val_ann            'instances_val2017.json'                                                                                 test_ann           'instances_test2017.json'                                                                                mosaic_prob        1.0                                                                                                      mixup_prob         1.0                                                                                                      hsv_prob           1.0                                                                                                      flip_prob          0.5                                                                                                      degrees            10.0                                                                                                     translate          0.1                                                                                                      mosaic_scale       (0.1, 2)                                                                                                 enable_mixup       True                                                                                                     mixup_scale        (0.5, 1.5)                                                                                               shear              2.0                                                                                                      warmup_epochs      5                                                                                                        max_epoch          300                                                                                                      warmup_lr          0                                                                                                        min_lr_ratio       0.05                                                                                                     basic_lr_per_img   0.00015625                                                                                               scheduler          'yoloxwarmcos'                                                                                           no_aug_epochs      15                                                                                                       ema                True                                                                                                     weight_decay       0.0005                                                                                                   momentum           0.9                                                                                                      save_history_ckpt  True                                                                                                     exp_name           'yolox_s'                                                                                                test_size          (640, 640)                                                                                               test_conf          0.01                                                                                                     nmsthre            0.65                                                                                                     model              YOLOX(                                                                                                                        (backbone): YOLOPAFPN(                                                                                                        (backbone): CSPDarknet(                                                                                                       (stem): Focus(                                                                                                                (conv): BaseConv(                                                                                                             (conv): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                                       (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                      (act): SiLU(inplace=True)                                                                                                 )                                                                                                                         )                                                                                                                           (dark2): Sequential(                                                                                                          (0): BaseConv(                                                                                                                (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)                                       (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                      (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (1): CSPLayer(                                                                                                                (conv1): BaseConv(                                                                                                            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                       (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                      (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv2): BaseConv(                                                                                                            (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                       (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                      (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv3): BaseConv(                                                                                                            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                       (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                      (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (m): Sequential(                                                                                                              (0): Bottleneck(                                                                                                              (conv1): BaseConv(                                                                                                            (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                       (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                      (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv2): BaseConv(                                                                                                            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                                       (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                      (act): SiLU(inplace=True)                                                                                                 )                                                                                                                         )                                                                                                                         )                                                                                                                         )                                                                                                                         )                                                                                                                           (dark3): Sequential(                                                                                                          (0): BaseConv(                                                                                                                (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)                                      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (1): CSPLayer(                                                                                                                (conv1): BaseConv(                                                                                                            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                      (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv2): BaseConv(                                                                                                            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                      (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv3): BaseConv(                                                                                                            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (m): Sequential(                                                                                                              (0): Bottleneck(                                                                                                              (conv1): BaseConv(                                                                                                            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                       (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                      (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv2): BaseConv(                                                                                                            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                                       (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                      (act): SiLU(inplace=True)                                                                                                 )                                                                                                                         )                                                                                                                           (1): Bottleneck(                                                                                                              (conv1): BaseConv(                                                                                                            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                       (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                      (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv2): BaseConv(                                                                                                            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                                       (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                      (act): SiLU(inplace=True)                                                                                                 )                                                                                                                         )                                                                                                                           (2): Bottleneck(                                                                                                              (conv1): BaseConv(                                                                                                            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                       (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                      (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv2): BaseConv(                                                                                                            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                                       (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                      (act): SiLU(inplace=True)                                                                                                 )                                                                                                                         )                                                                                                                         )                                                                                                                         )                                                                                                                         )                                                                                                                           (dark4): Sequential(                                                                                                          (0): BaseConv(                                                                                                                (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)                                     (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (1): CSPLayer(                                                                                                                (conv1): BaseConv(                                                                                                            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv2): BaseConv(                                                                                                            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv3): BaseConv(                                                                                                            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (m): Sequential(                                                                                                              (0): Bottleneck(                                                                                                              (conv1): BaseConv(                                                                                                            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv2): BaseConv(                                                                                                            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                         )                                                                                                                           (1): Bottleneck(                                                                                                              (conv1): BaseConv(                                                                                                            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv2): BaseConv(                                                                                                            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                         )                                                                                                                           (2): Bottleneck(                                                                                                              (conv1): BaseConv(                                                                                                            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv2): BaseConv(                                                                                                            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                         )                                                                                                                         )                                                                                                                         )                                                                                                                         )                                                                                                                           (dark5): Sequential(                                                                                                          (0): BaseConv(                                                                                                                (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)                                     (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (1): SPPBottleneck(                                                                                                           (conv1): BaseConv(                                                                                                            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (m): ModuleList(                                                                                                              (0): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)                                             (1): MaxPool2d(kernel_size=9, stride=1, padding=4, dilation=1, ceil_mode=False)                                             (2): MaxPool2d(kernel_size=13, stride=1, padding=6, dilation=1, ceil_mode=False)                                          )                                                                                                                           (conv2): BaseConv(                                                                                                            (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                    (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                         )                                                                                                                           (2): CSPLayer(                                                                                                                (conv1): BaseConv(                                                                                                            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv2): BaseConv(                                                                                                            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv3): BaseConv(                                                                                                            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (m): Sequential(                                                                                                              (0): Bottleneck(                                                                                                              (conv1): BaseConv(                                                                                                            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv2): BaseConv(                                                                                                            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                                     (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                         )                                                                                                                         )                                                                                                                         )                                                                                                                         )                                                                                                                         )                                                                                                                           (upsample): Upsample(scale_factor=2.0, mode='nearest')                                                                      (lateral_conv0): BaseConv(                                                                                                    (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (C3_p4): CSPLayer(                                                                                                            (conv1): BaseConv(                                                                                                            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv2): BaseConv(                                                                                                            (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv3): BaseConv(                                                                                                            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (m): Sequential(                                                                                                              (0): Bottleneck(                                                                                                              (conv1): BaseConv(                                                                                                            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv2): BaseConv(                                                                                                            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                         )                                                                                                                         )                                                                                                                         )                                                                                                                           (reduce_conv1): BaseConv(                                                                                                     (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (C3_p3): CSPLayer(                                                                                                            (conv1): BaseConv(                                                                                                            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                      (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv2): BaseConv(                                                                                                            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                      (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv3): BaseConv(                                                                                                            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (m): Sequential(                                                                                                              (0): Bottleneck(                                                                                                              (conv1): BaseConv(                                                                                                            (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                       (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                      (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv2): BaseConv(                                                                                                            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                                       (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                      (act): SiLU(inplace=True)                                                                                                 )                                                                                                                         )                                                                                                                         )                                                                                                                         )                                                                                                                           (bu_conv2): BaseConv(                                                                                                         (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (C3_n3): CSPLayer(                                                                                                            (conv1): BaseConv(                                                                                                            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv2): BaseConv(                                                                                                            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv3): BaseConv(                                                                                                            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (m): Sequential(                                                                                                              (0): Bottleneck(                                                                                                              (conv1): BaseConv(                                                                                                            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv2): BaseConv(                                                                                                            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                         )                                                                                                                         )                                                                                                                         )                                                                                                                           (bu_conv1): BaseConv(                                                                                                         (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)                                     (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (C3_n4): CSPLayer(                                                                                                            (conv1): BaseConv(                                                                                                            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv2): BaseConv(                                                                                                            (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv3): BaseConv(                                                                                                            (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (m): Sequential(                                                                                                              (0): Bottleneck(                                                                                                              (conv1): BaseConv(                                                                                                            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (conv2): BaseConv(                                                                                                            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                                     (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                         )                                                                                                                         )                                                                                                                         )                                                                                                                         )                                                                                                                           (head): YOLOXHead(                                                                                                            (cls_convs): ModuleList(                                                                                                      (0-2): 3 x Sequential(                                                                                                        (0): BaseConv(                                                                                                                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (1): BaseConv(                                                                                                                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                         )                                                                                                                         )                                                                                                                           (reg_convs): ModuleList(                                                                                                      (0-2): 3 x Sequential(                                                                                                        (0): BaseConv(                                                                                                                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (1): BaseConv(                                                                                                                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                         )                                                                                                                         )                                                                                                                           (cls_preds): ModuleList(                                                                                                      (0-2): 3 x Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1))                                                             )                                                                                                                           (reg_preds): ModuleList(                                                                                                      (0-2): 3 x Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1))                                                              )                                                                                                                           (obj_preds): ModuleList(                                                                                                      (0-2): 3 x Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))                                                              )                                                                                                                           (stems): ModuleList(                                                                                                          (0): BaseConv(                                                                                                                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (1): BaseConv(                                                                                                                (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                           (2): BaseConv(                                                                                                                (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)                                                     (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)                                     (act): SiLU(inplace=True)                                                                                                 )                                                                                                                         )                                                                                                                           (l1_loss): L1Loss()                                                                                                         (bcewithlog_loss): BCEWithLogitsLoss()                                                                                      (iou_loss): IOUloss()                                                                                                     )                                                                                                                         )                                                                                                       \n",
      "==================================================\n",
      "Clean Code:\n",
      "def load(self):\n",
      "        with self.cfgfile.open() as fop:\n",
      "            fc = fop.readlines()\n",
      "        cdict = {}\n",
      "        for line in fc:\n",
      "            line = line.strip()\n",
      "            var, val = line.split(\"=\", 1)\n",
      "            cdict[var.lower().strip()] = val.strip()\n",
      "        return cdict\n",
      "load(self=<pyjibe.settings.SettingsFile object at 0x7fa836e42ac0>, self.cfgfile=PosixPath('/home/XXX/.config/pyjibe.cfg'), self.defaults={}, self.working_directories={})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "with self.cfgfile.open() as fop:\n",
      "State:\n",
      "<_io.TextIOWrapper name='/home/XXX/.config/pyjibe.cfg' mode='r' encoding='UTF-8'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def load(self):\n",
      "        with self.cfgfile.open() as fop:\n",
      "            fc = fop.readlines()\n",
      "        cdict = {}\n",
      "        for line in fc:\n",
      "            line = line.strip()\n",
      "            var, val = line.split(\"=\", 1)\n",
      "            cdict[var.lower().strip()] = val.strip()\n",
      "        return cdict\n",
      "load(self=<pyjibe.settings.SettingsFile object at 0x7fa836e42ac0>, self.cfgfile=PosixPath('/home/XXX/.config/pyjibe.cfg'), self.defaults={}, self.working_directories={})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "fc = fop.readlines()\n",
      "State:\n",
      "['path peter = /local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/AFM-analysis+PyJibe\\n']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _add_typecode(tc, sizes_dict):\n",
      "    dt_le = np.dtype('<' + tc)\n",
      "    dt_be = np.dtype('>' + tc)\n",
      "    entries = sizes_dict.setdefault(dt_le.itemsize, [])\n",
      "    entries.append((h5t.py_create(dt_le), dt_le.name))\n",
      "    entries.append((h5t.py_create(dt_be), dt_be.name + ' (big-endian)'))\n",
      "_add_typecode(tc='e', sizes_dict={})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "dt_le = np.dtype('<' + tc)\n",
      "State:\n",
      "dtype('float16')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _add_typecode(tc, sizes_dict):\n",
      "    dt_le = np.dtype('<' + tc)\n",
      "    dt_be = np.dtype('>' + tc)\n",
      "    entries = sizes_dict.setdefault(dt_le.itemsize, [])\n",
      "    entries.append((h5t.py_create(dt_le), dt_le.name))\n",
      "    entries.append((h5t.py_create(dt_be), dt_be.name + ' (big-endian)'))\n",
      "_add_typecode(tc='e', sizes_dict={})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "dt_be = np.dtype('>' + tc)\n",
      "State:\n",
      "dtype('>f2')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _add_typecode(tc, sizes_dict):\n",
      "    dt_le = np.dtype('<' + tc)\n",
      "    dt_be = np.dtype('>' + tc)\n",
      "    entries = sizes_dict.setdefault(dt_le.itemsize, [])\n",
      "    entries.append((h5t.py_create(dt_le), dt_le.name))\n",
      "    entries.append((h5t.py_create(dt_be), dt_be.name + ' (big-endian)'))\n",
      "_add_typecode(tc='e', sizes_dict={})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "entries = sizes_dict.setdefault(dt_le.itemsize, [])\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def dtype_description(hdf_dt):\n",
      "    size = hdf_dt.get_size()\n",
      "    if isinstance(hdf_dt, h5t.TypeIntegerID):\n",
      "        for candidate, descr in int_types_by_size().get(size, ()):\n",
      "            if hdf_dt == candidate:\n",
      "                un = 'un' if hdf_dt.get_sign() == h5t.SGN_NONE else ''\n",
      "                return '{}-bit {}signed integer'.format(size * 8, un)\n",
      "    elif isinstance(hdf_dt, h5t.TypeFloatID):\n",
      "        for candidate, descr in float_types_by_size().get(size, ()):\n",
      "            if hdf_dt == candidate:\n",
      "                return '{}-bit floating point'.format(size * 8)\n",
      "    return None\n",
      "dtype_description(hdf_dt=REPR FAILED)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "size = hdf_dt.get_size()\n",
      "State:\n",
      "4\n",
      "==================================================\n",
      "Clean Code:\n",
      "def make_fragment(obj):\n",
      "    if utils.is_group(obj):\n",
      "        name = file_or_grp_name(obj)\n",
      "        ct = make_list(item_for_group(name, obj))\n",
      "    elif isinstance(obj, (str, Path)) and h5py.is_hdf5(obj):\n",
      "        with h5py.File(obj, 'r') as f:\n",
      "            return make_fragment(f)\n",
      "    else:\n",
      "        raise TypeError(\"Unknown object type: {!r}\".format(obj))\n",
      "    first_chkbx = ct.children.children[0].children.children[0]\n",
      "    first_chkbx.checked = True\n",
      "    tv = Division(ct)\n",
      "    tv.add_css_classes(\"h5glance-css-treeview\")\n",
      "    tv.id = next(treeview_ids)\n",
      "    return tv\n",
      "make_fragment(obj=<HDF5 file \"example.h5\" (mode r+)>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "name = file_or_grp_name(obj)\n",
      "State:\n",
      "'/tmp/tmphwyn_kzo/example.h5'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def item_for_group(gname, grp):\n",
      "    subgroups, datasets = [], []\n",
      "    for name, obj in sorted(grp.items()):\n",
      "        if utils.is_group(obj):\n",
      "            subgroups.append((name, obj))\n",
      "        else:\n",
      "            datasets.append((name, obj))\n",
      "    return ListItem(*checkbox_w_label(gname), make_list(\n",
      "        *[item_for_group(n, g) for n, g in subgroups],\n",
      "        *[item_for_dataset(n, d) for n, d in datasets],\n",
      "    ))\n",
      "item_for_group(gname='group1', grp=<HDF5 group \"/group1\" (4 members)>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "subgroups, datasets = [], []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def object_node(self, obj, name, max_depth=numpy.inf):\n",
      "        color_stop = self.colors.reset\n",
      "        if isinstance(obj, h5py.Dataset):\n",
      "            color_start = self.colors.dataset\n",
      "        elif isinstance(obj, h5py.Group):\n",
      "            color_start = self.colors.group\n",
      "        else:\n",
      "            color_start = ''\n",
      "        obj_id = h5py.h5o.get_info(obj.id).addr\n",
      "        if obj_id in self.visited:\n",
      "            first_link = self.visited[obj_id]\n",
      "            return (color_start + name + color_stop + '\\t= ' + first_link), []\n",
      "        self.visited[obj_id] = obj.name\n",
      "        children = []\n",
      "        detail = attr_detail = ''\n",
      "        if self.expand_attrs:\n",
      "            children += attrs_tree_nodes(obj)\n",
      "        else:\n",
      "            n = len(obj.attrs)\n",
      "            attr_detail = ' ({} attributes)'.format(n) if n else ''\n",
      "        if isinstance(obj, h5py.Dataset):\n",
      "            detail = '\\t[{dt}: {shape}]'.format(\n",
      "                dt=fmt_dtype(obj.id.get_type()),\n",
      "                shape=fmt_shape(obj.shape),\n",
      "            )\n",
      "            if obj.id.get_create_plist().get_layout() == h5py.h5d.VIRTUAL:\n",
      "                detail += ' virtual'\n",
      "        elif isinstance(obj, h5py.Group):\n",
      "            if max_depth >= 1:\n",
      "                children += [self.group_item_node(obj, key, max_depth - 1)\n",
      "                             for key in obj]\n",
      "            else:\n",
      "                detail = f'\\t({len(obj)} children)'\n",
      "        else:\n",
      "            detail = ' (unknown h5py type)'\n",
      "        return (color_start + name + color_stop + detail + attr_detail), children\n",
      "object_node(self=<h5glance.terminal.TreeViewBuilder object at 0x7f59ca28d970>, obj=<HDF5 dataset \"compound\": shape (2,), type \"|V12\">, name='compound', max_depth=inf, self.colors=<class 'h5glance.terminal.ColorsNone'>, self.expand_attrs=False, self.visited={96: '/'})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "color_stop = self.colors.reset\n",
      "State:\n",
      "''\n",
      "==================================================\n",
      "Clean Code:\n",
      "def object_node(self, obj, name, max_depth=numpy.inf):\n",
      "        color_stop = self.colors.reset\n",
      "        if isinstance(obj, h5py.Dataset):\n",
      "            color_start = self.colors.dataset\n",
      "        elif isinstance(obj, h5py.Group):\n",
      "            color_start = self.colors.group\n",
      "        else:\n",
      "            color_start = ''\n",
      "        obj_id = h5py.h5o.get_info(obj.id).addr\n",
      "        if obj_id in self.visited:\n",
      "            first_link = self.visited[obj_id]\n",
      "            return (color_start + name + color_stop + '\\t= ' + first_link), []\n",
      "        self.visited[obj_id] = obj.name\n",
      "        children = []\n",
      "        detail = attr_detail = ''\n",
      "        if self.expand_attrs:\n",
      "            children += attrs_tree_nodes(obj)\n",
      "        else:\n",
      "            n = len(obj.attrs)\n",
      "            attr_detail = ' ({} attributes)'.format(n) if n else ''\n",
      "        if isinstance(obj, h5py.Dataset):\n",
      "            detail = '\\t[{dt}: {shape}]'.format(\n",
      "                dt=fmt_dtype(obj.id.get_type()),\n",
      "                shape=fmt_shape(obj.shape),\n",
      "            )\n",
      "            if obj.id.get_create_plist().get_layout() == h5py.h5d.VIRTUAL:\n",
      "                detail += ' virtual'\n",
      "        elif isinstance(obj, h5py.Group):\n",
      "            if max_depth >= 1:\n",
      "                children += [self.group_item_node(obj, key, max_depth - 1)\n",
      "                             for key in obj]\n",
      "            else:\n",
      "                detail = f'\\t({len(obj)} children)'\n",
      "        else:\n",
      "            detail = ' (unknown h5py type)'\n",
      "        return (color_start + name + color_stop + detail + attr_detail), children\n",
      "object_node(self=<h5glance.terminal.TreeViewBuilder object at 0x7f59ca28d970>, obj=<HDF5 dataset \"compound\": shape (2,), type \"|V12\">, name='compound', max_depth=inf, self.colors=<class 'h5glance.terminal.ColorsNone'>, self.expand_attrs=False, self.visited={96: '/'})\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "color_start = self.colors.dataset\n",
      "State:\n",
      "''\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_rtv_file(config):\n",
      "        rtv = {}\n",
      "        if config.has_section('rtv'):\n",
      "            rtv = dict(config.items('rtv'))\n",
      "        params = {\n",
      "            'ascii': partial(config.getboolean, 'rtv'),\n",
      "            'monochrome': partial(config.getboolean, 'rtv'),\n",
      "            'clear_auth': partial(config.getboolean, 'rtv'),\n",
      "            'persistent': partial(config.getboolean, 'rtv'),\n",
      "            'enable_media': partial(config.getboolean, 'rtv'),\n",
      "            'history_size': partial(config.getint, 'rtv'),\n",
      "            'oauth_redirect_port': partial(config.getint, 'rtv'),\n",
      "            'oauth_scope': lambda x: rtv[x].split(','),\n",
      "            'max_comment_cols': partial(config.getint, 'rtv'),\n",
      "            'hide_username': partial(config.getboolean, 'rtv'),\n",
      "            'flash': partial(config.getboolean, 'rtv')\n",
      "        }\n",
      "        for key, func in params.items():\n",
      "            if key in rtv:\n",
      "                rtv[key] = func(key)\n",
      "        bindings = {}\n",
      "        if config.has_section('bindings'):\n",
      "            bindings = dict(config.items('bindings'))\n",
      "        for name, keys in bindings.items():\n",
      "            bindings[name] = [key.strip() for key in keys.split(',')]\n",
      "        return rtv, bindings\n",
      "_parse_rtv_file(config={_sections={'rtv': {'ascii': 'False', 'monochrome': 'False', 'flash': 'True', 'subreddit': 'front', 'persistent': 'True', 'clear_auth': 'False', 'history_size': '200', 'enable_media': 'False', 'max_comment_cols': '120', 'hide_username': 'False', 'oauth_client_id': 'E2oEtRQfdfAfNQ', 'oauth_client_secret': 'praw_gapfill', 'oauth_redirect_uri': 'http://127.0.0.1:65000/', 'oauth_redirect_port': '65000', 'oauth_scope': 'edit,history,identity,mysubreddits,privatemessages,read,report,save,submit,subscribe,vote', 'imgur_client_id': '93396265f59dec9'}, 'bindings': {'exit': 'q', 'force_exit': 'Q', 'help': '?', 'sort_hot': '1', 'sort_top': '2', 'sort_rising': '3', 'sort_new': '4', 'sort_controversial': '5', 'move_up': 'k, <KEY_UP>', 'move_down': 'j, <KEY_DOWN>', 'previous_theme': '<KEY_F2>', 'next_theme': '<KEY_F3>', 'page_up': 'm, <KEY_PPAGE>, <NAK>', 'page_down': 'n, <KEY_NPAGE>, <EOT>', 'page_top': 'gg', 'page_bottom': 'G', 'upvote': 'a', 'downvote': 'z', 'login': 'u', 'delete': 'd', 'edit': 'e', 'inbox': 'i', 'refresh': 'r, <KEY_F5>', 'prompt': '/', 'save': 'w', 'copy_permalink': 'y', 'copy_url': 'Y', 'submission_toggle_comment': '0x20', 'submission_open_in_browser': 'o, <LF>, <KEY_ENTER>', 'submission_post': 'c', 'submission_exit': 'h, <KEY_LEFT>', 'submission_open_in_pager': 'l, <KEY_RIGHT>', 'submission_open_in_urlviewer': 'b', 'submission_goto_parent': 'K', 'submission_goto_sibling': 'J', 'subreddit_search': 'f', 'subreddit_post': 'c', 'subreddit_open': 'l, <KEY_RIGHT>', 'subreddit_open_in_browser': 'o, <LF>, <KEY_ENTER>', 'subreddit_open_subscriptions': 's', 'subreddit_open_multireddits': 'S', 'subreddit_frontpage': 'p', 'subscription_select': 'l, <LF>, <KEY_ENTER>, <KEY_RIGHT>', 'subscription_exit': 'h, s, S, <ESC>, <KEY_LEFT>'}}, _defaults={}, _converters=<configparser.ConverterMapping object at 0x7f091b295c40>, _proxies={'DEFAULT': <Section: DEFAULT>, 'rtv': <Section: rtv>, 'bindings': <Section: bindings>}, _delimiters=('=', ':'), _optcre=re.compile('\\n        (?P<option>.*?)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "rtv = dict(config.items('rtv'))\n",
      "State:\n",
      "{'ascii': 'False', 'monochrome': 'False', 'flash': 'True', 'subreddit': 'front', 'persistent': 'True', 'clear_auth': 'False', 'history_size': '200', 'enable_media': 'False', 'max_comment_cols': '120', 'hide_username': 'False', 'oauth_client_id': 'E2oEtRQfdfAfNQ', 'oauth_client_secret': 'praw_gapfill', 'oauth_redirect_uri': 'http://127.0.0.1:65000/', 'oauth_redirect_port': '65000', 'oauth_scope': 'edit,history,identity,mysubreddits,privatemessages,read,report,save,submit,subscribe,vote', 'imgur_client_id': '93396265f59dec9'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_bindings(self, bindings):\n",
      "        new_keymap = {}\n",
      "        for command, keys in bindings.items():\n",
      "            if not isinstance(command, Command):\n",
      "                command = Command(command)\n",
      "            new_keymap[command] = keys\n",
      "        if not self._keymap:\n",
      "            self._keymap = new_keymap\n",
      "        else:\n",
      "            self._keymap.update(new_keymap)\n",
      "set_bindings(self=<rtv.objects.KeyMap object at 0x7f091abc8cd0>, bindings={'exit': ['q'], 'force_exit': ['Q'], 'help': ['?'], 'sort_hot': ['1'], 'sort_top': ['2'], 'sort_rising': ['3'], 'sort_new': ['4'], 'sort_controversial': ['5'], 'move_up': ['k', '<KEY_UP>'], 'move_down': ['j', '<KEY_DOWN>'], 'previous_theme': ['<KEY_F2>'], 'next_theme': ['<KEY_F3>'], 'page_up': ['m', '<KEY_PPAGE>', '<NAK>'], 'page_down': ['n', '<KEY_NPAGE>', '<EOT>'], 'page_top': ['gg'], 'page_bottom': ['G'], 'upvote': ['a'], 'downvote': ['z'], 'login': ['u'], 'delete': ['d'], 'edit': ['e'], 'inbox': ['i'], 'refresh': ['r', '<KEY_F5>'], 'prompt': ['/'], 'save': ['w'], 'copy_permalink': ['y'], 'copy_url': ['Y'], 'submission_toggle_comment': ['0x20'], 'submission_open_in_browser': ['o', '<LF>', '<KEY_ENTER>'], 'submission_post': ['c'], 'submission_exit': ['h', '<KEY_LEFT>'], 'submission_open_in_pager': ['l', '<KEY_RIGHT>'], 'submission_open_in_urlviewer': ['b'], 'submission_goto_parent': ['K'], 'submission_goto_sibling': ['J'], 'subreddit_search': ['f'], 'subreddit_post': ['c'], 'subreddit_open': ['l', '<KEY_RIGHT>'], 'subreddit_open_in_browser': ['o', '<LF>', '<KEY_ENTER>'], 'subreddit_open_subscriptions': ['s'], 'subreddit_open_multireddits': ['S'], 'subreddit_frontpage': ['p'], 'subscription_select': ['l', '<LF>', '<KEY_ENTER>', '<KEY_RIGHT>'], 'subscription_exit': ['h', 's', 'S', '<ESC>', '<KEY_LEFT>']}, self._keymap=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "new_keymap = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def update(self, **kwargs):\n",
      "        self.config.update(kwargs)\n",
      "update(self=<rtv.config.Config object at 0x7f091b295d90>, kwargs={'subreddit': 'cfb', 'new_value': 2.0}, self.config={}, self.default={'ascii': False, 'monochrome': False, 'flash': True, 'subreddit': 'front', 'persistent': True, 'clear_auth': False, 'history_size': 200, 'enable_media': False, 'max_comment_cols': 120, 'hide_username': False, 'oauth_client_id': 'E2oEtRQfdfAfNQ', 'oauth_client_secret': 'praw_gapfill', 'oauth_redirect_uri': 'http://127.0.0.1:65000/', 'oauth_redirect_port': 65000, 'oauth_scope': ['edit', 'history', 'identity', 'mysubreddits', 'privatemessages', 'read', 'report', 'save', 'submit', 'subscribe', 'vote'], 'imgur_client_id': '93396265f59dec9'}, self.history=<rtv.config.OrderedSet object at 0x7f091abc8e80>, self.history_file='/home/XXX/.local/share/rtv/history.log', self.keymap=<rtv.objects.KeyMap object at 0x7f091abc8cd0>, self.refresh_token=None, self.token_file='/home/XXX/.local/share/rtv/refresh-token')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.config.update(kwargs)\n",
      "State:\n",
      "{'subreddit': 'cfb', 'new_value': 2.0}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def load_refresh_token(self):\n",
      "        if os.path.exists(self.token_file):\n",
      "            with open(self.token_file) as fp:\n",
      "                self.refresh_token = fp.read().strip()\n",
      "        else:\n",
      "            self.refresh_token = None\n",
      "load_refresh_token(self=<rtv.config.Config object at 0x7f091ae5b850>, self.config={}, self.default={'ascii': False, 'monochrome': False, 'flash': True, 'subreddit': 'front', 'persistent': True, 'clear_auth': False, 'history_size': 200, 'enable_media': False, 'max_comment_cols': 120, 'hide_username': False, 'oauth_client_id': 'E2oEtRQfdfAfNQ', 'oauth_client_secret': 'praw_gapfill', 'oauth_redirect_uri': 'http://127.0.0.1:65000/', 'oauth_redirect_port': 65000, 'oauth_scope': ['edit', 'history', 'identity', 'mysubreddits', 'privatemessages', 'read', 'report', 'save', 'submit', 'subscribe', 'vote'], 'imgur_client_id': '93396265f59dec9'}, self.history=<rtv.config.OrderedSet object at 0x7f091b16d6a0>, self.history_file='/home/XXX/.local/share/rtv/history.log', self.keymap=<rtv.objects.KeyMap object at 0x7f091ae5b670>, self.refresh_token=None, self.token_file='/tmp/tmpi9fhdn65')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.refresh_token = fp.read().strip()\n",
      "State:\n",
      "'secret_value'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def delete_refresh_token(self):\n",
      "        if os.path.exists(self.token_file):\n",
      "            os.remove(self.token_file)\n",
      "        self.refresh_token = None\n",
      "delete_refresh_token(self=<rtv.config.Config object at 0x7f091ae5b850>, self.config={}, self.default={'ascii': False, 'monochrome': False, 'flash': True, 'subreddit': 'front', 'persistent': True, 'clear_auth': False, 'history_size': 200, 'enable_media': False, 'max_comment_cols': 120, 'hide_username': False, 'oauth_client_id': 'E2oEtRQfdfAfNQ', 'oauth_client_secret': 'praw_gapfill', 'oauth_redirect_uri': 'http://127.0.0.1:65000/', 'oauth_redirect_port': 65000, 'oauth_scope': ['edit', 'history', 'identity', 'mysubreddits', 'privatemessages', 'read', 'report', 'save', 'submit', 'subscribe', 'vote'], 'imgur_client_id': '93396265f59dec9'}, self.history=<rtv.config.OrderedSet object at 0x7f091b16d6a0>, self.history_file='/home/XXX/.local/share/rtv/history.log', self.keymap=<rtv.objects.KeyMap object at 0x7f091ae5b670>, self.refresh_token='secret_value', self.token_file='/tmp/tmpi9fhdn65')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.refresh_token = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def wrap_text(text, width):\n",
      "        out = []\n",
      "        for paragraph in text.splitlines():\n",
      "            lines = wrap(paragraph, width=width) or ['']\n",
      "            out.extend(lines)\n",
      "        return out\n",
      "wrap_text(text='four score\\nand seven\\n\\n', width=6)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "out = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def build_key_items(url, params, data, auth, files, method):\n",
      "            request = _prepare_request(self, url, params, data, auth, files,\n",
      "                                       method)\n",
      "            key_items = []\n",
      "            oauth = request.headers.get('Authorization', None)\n",
      "            for key_value in (params, data, request.cookies, auth, oauth):\n",
      "                if isinstance(key_value, dict):\n",
      "                    key_items.append(tuple(key_value.items()))\n",
      "                elif isinstance(key_value, http_cookiejar.CookieJar):\n",
      "                    key_items.append(tuple(key_value.get_dict().items()))\n",
      "                else:\n",
      "                    key_items.append(key_value)\n",
      "            kwargs = {'_rate_domain': self.config.domain,\n",
      "                      '_rate_delay': int(self.config.api_request_delay),\n",
      "                      '_cache_ignore': bool(files) or raw_response,\n",
      "                      '_cache_timeout': int(self.config.cache_timeout)}\n",
      "            return (request, key_items, kwargs)\n",
      "build_key_items(url='https://www.reddit.com/r/CollegeBasketball/comments/31owr1.json', params={'sort': 'top'}, data=None, auth=None, files=None, method=None, raw_response=False, self=<rtv.packages.praw.Reddit object at 0x7f091abc0af0>, self._authentication=None, self._unique_count=1, self._use_oauth=False, self.access_token=None, self.client_id=None, self.client_secret=None, self.config=<rtv.packages.praw.Config object at 0x7f091abc03d0>, self.handler=<rtv.content.RequestHeaderRateLimiter object at 0x7f091afa9610>, self.http=<requests.sessions.Session object at 0x7f091abc08b0>, self.modhash=None, self.redirect_uri=None, self.refresh_token=None, self.user=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "request = _prepare_request(self, url, params, data, auth, files,\n",
      "State:\n",
      "<Request [GET]>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def handle_redirect():\n",
      "            response = None\n",
      "            url = request.url\n",
      "            while url:\n",
      "                request.url = url\n",
      "                kwargs['_cache_key'] = (normalize_url(request.url),\n",
      "                                        tuple(key_items))\n",
      "                response = self.handler.request(\n",
      "                    request=request.prepare(),\n",
      "                    proxies=self.http.proxies,\n",
      "                    timeout=timeout,\n",
      "                    verify=self.http.validate_certs, **kwargs)\n",
      "                if self.config.log_requests >= 2:\n",
      "                    msg = 'status: {0}\\n'.format(response.status_code)\n",
      "                    sys.stderr.write(msg)\n",
      "                url = _raise_redirect_exceptions(response)\n",
      "                assert url != request.url\n",
      "            return response\n",
      "handle_redirect(key_items=[(('sort', 'top'),), None, (), None, None], kwargs={'_rate_domain': 'www.reddit.com', '_rate_delay': 0, '_cache_ignore': False, '_cache_timeout': 30}, request=<Request [GET]>, self=<rtv.packages.praw.Reddit object at 0x7f091abc0af0>, timeout=45.0, self._authentication=None, self._unique_count=1, self._use_oauth=False, self.access_token=None, self.client_id=None, self.client_secret=None, self.config=<rtv.packages.praw.Config object at 0x7f091abc03d0>, self.handler=<rtv.content.RequestHeaderRateLimiter object at 0x7f091afa9610>, self.http=<requests.sessions.Session object at 0x7f091abc08b0>, self.modhash=None, self.redirect_uri=None, self.refresh_token=None, self.user=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "response = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def handle_redirect():\n",
      "            response = None\n",
      "            url = request.url\n",
      "            while url:\n",
      "                request.url = url\n",
      "                kwargs['_cache_key'] = (normalize_url(request.url),\n",
      "                                        tuple(key_items))\n",
      "                response = self.handler.request(\n",
      "                    request=request.prepare(),\n",
      "                    proxies=self.http.proxies,\n",
      "                    timeout=timeout,\n",
      "                    verify=self.http.validate_certs, **kwargs)\n",
      "                if self.config.log_requests >= 2:\n",
      "                    msg = 'status: {0}\\n'.format(response.status_code)\n",
      "                    sys.stderr.write(msg)\n",
      "                url = _raise_redirect_exceptions(response)\n",
      "                assert url != request.url\n",
      "            return response\n",
      "handle_redirect(key_items=[(('sort', 'top'),), None, (), None, None], kwargs={'_rate_domain': 'www.reddit.com', '_rate_delay': 0, '_cache_ignore': False, '_cache_timeout': 30}, request=<Request [GET]>, self=<rtv.packages.praw.Reddit object at 0x7f091abc0af0>, timeout=45.0, self._authentication=None, self._unique_count=1, self._use_oauth=False, self.access_token=None, self.client_id=None, self.client_secret=None, self.config=<rtv.packages.praw.Config object at 0x7f091abc03d0>, self.handler=<rtv.content.RequestHeaderRateLimiter object at 0x7f091afa9610>, self.http=<requests.sessions.Session object at 0x7f091abc08b0>, self.modhash=None, self.redirect_uri=None, self.refresh_token=None, self.user=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "url = request.url\n",
      "State:\n",
      "'https://www.reddit.com/r/CollegeBasketball/comments/31owr1.json'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def handle_redirect():\n",
      "            response = None\n",
      "            url = request.url\n",
      "            while url:\n",
      "                request.url = url\n",
      "                kwargs['_cache_key'] = (normalize_url(request.url),\n",
      "                                        tuple(key_items))\n",
      "                response = self.handler.request(\n",
      "                    request=request.prepare(),\n",
      "                    proxies=self.http.proxies,\n",
      "                    timeout=timeout,\n",
      "                    verify=self.http.validate_certs, **kwargs)\n",
      "                if self.config.log_requests >= 2:\n",
      "                    msg = 'status: {0}\\n'.format(response.status_code)\n",
      "                    sys.stderr.write(msg)\n",
      "                url = _raise_redirect_exceptions(response)\n",
      "                assert url != request.url\n",
      "            return response\n",
      "handle_redirect(key_items=[(('sort', 'top'),), None, (), None, None], kwargs={'_rate_domain': 'www.reddit.com', '_rate_delay': 0, '_cache_ignore': False, '_cache_timeout': 30}, request=<Request [GET]>, self=<rtv.packages.praw.Reddit object at 0x7f091abc0af0>, timeout=45.0, self._authentication=None, self._unique_count=1, self._use_oauth=False, self.access_token=None, self.client_id=None, self.client_secret=None, self.config=<rtv.packages.praw.Config object at 0x7f091abc03d0>, self.handler=<rtv.content.RequestHeaderRateLimiter object at 0x7f091afa9610>, self.http=<requests.sessions.Session object at 0x7f091abc08b0>, self.modhash=None, self.redirect_uri=None, self.refresh_token=None, self.user=None)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "kwargs['_cache_key'] = (normalize_url(request.url),\n",
      "State:\n",
      "{'_rate_domain': 'www.reddit.com', '_rate_delay': 0, '_cache_ignore': False, '_cache_timeout': 30, '_cache_key': ('https://www.reddit.com/r/CollegeBasketball/comments/31owr1', ((('sort', 'top'),), None, (), None, None))}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _populate(self, json_dict, fetch):\n",
      "        if json_dict is None:\n",
      "            json_dict = self._get_json_dict() if fetch else {}\n",
      "        if self.reddit_session.config.store_json_result is True:\n",
      "            self.json_dict = json_dict\n",
      "        else:\n",
      "            self.json_dict = None\n",
      "        if isinstance(json_dict, list):\n",
      "            json_dict = {'_tmp': json_dict}\n",
      "        for name, value in six.iteritems(json_dict):\n",
      "            if self._underscore_names and name in self._underscore_names:\n",
      "                name = '_' + name\n",
      "            setattr(self, name, value)\n",
      "        self._post_populate(fetch)\n",
      "        return bool(json_dict) or fetch\n",
      "_populate(self=REPR FAILED, json_dict=None, fetch=False)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "json_dict = self._get_json_dict() if fetch else {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_theme(self, theme=None):\n",
      "        terminal_colors = curses.COLORS if curses.has_colors() else 0\n",
      "        default_theme = Theme(use_color=bool(terminal_colors))\n",
      "        if theme is None:\n",
      "            theme = default_theme\n",
      "        elif theme.required_color_pairs > curses.COLOR_PAIRS:\n",
      "            _logger.warning(\n",
      "                'Theme `%s` requires %s color pairs, but $TERM=%s only '\n",
      "                'supports %s color pairs, switching to default theme',\n",
      "                theme.name, theme.required_color_pairs, self._term,\n",
      "                curses.COLOR_PAIRS)\n",
      "            theme = default_theme\n",
      "        elif theme.required_colors > terminal_colors:\n",
      "            _logger.warning(\n",
      "                'Theme `%s` requires %s colors, but $TERM=%s only '\n",
      "                'supports %s colors, switching to default theme',\n",
      "                theme.name, theme.required_colors, self._term,\n",
      "                curses.COLORS)\n",
      "            theme = default_theme\n",
      "        theme.bind_curses()\n",
      "        self.theme = theme\n",
      "        self.stdscr.bkgd(str(' '), self.attr('Normal'))\n",
      "set_theme(self=<rtv.terminal.Terminal object at 0x7f090b1fbe80>, theme=None, self._display=None, self._mailcap_dict={}, self._term='screen', self.config=<rtv.config.Config object at 0x7f090dddcdc0>, self.loader=<rtv.objects.LoadScreen object at 0x7f091abcb700>, self.stdscr=<MockStdscr id='139677065541088'>, self.theme=None, self.theme_list=<rtv.theme.ThemeList object at 0x7f091abcb4c0>)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "terminal_colors = curses.COLORS if curses.has_colors() else 0\n",
      "State:\n",
      "256\n",
      "==================================================\n",
      "Clean Code:\n",
      "def strip_praw_comment(cls, comment):\n",
      "        data = {}\n",
      "        data['object'] = comment\n",
      "        if isinstance(comment, praw.objects.MoreComments):\n",
      "            data['type'] = 'MoreComments'\n",
      "            data['level'] = comment.nested_level\n",
      "            data['count'] = comment.count\n",
      "            data['body'] = 'More comments'\n",
      "            data['hidden'] = True\n",
      "        elif hasattr(comment, 'nested_level'):\n",
      "            author = getattr(comment, 'author', '[deleted]')\n",
      "            name = getattr(author, 'name', '[deleted]')\n",
      "            sub = getattr(comment, 'submission', '[deleted]')\n",
      "            sub_author = getattr(sub, 'author', '[deleted]')\n",
      "            sub_name = getattr(sub_author, 'name', '[deleted]')\n",
      "            flair = getattr(comment, 'author_flair_text', '')\n",
      "            permalink = getattr(comment, 'permalink', None)\n",
      "            stickied = getattr(comment, 'stickied', False)\n",
      "            data['type'] = 'Comment'\n",
      "            data['level'] = comment.nested_level\n",
      "            data['body'] = comment.body\n",
      "            data['created'] = cls.humanize_timestamp(comment.created_utc)\n",
      "            data['score'] = '{0} pts'.format(\n",
      "                '-' if comment.score_hidden else comment.score)\n",
      "            data['author'] = name\n",
      "            data['is_author'] = (name == sub_name)\n",
      "            data['flair'] = flair\n",
      "            data['likes'] = comment.likes\n",
      "            data['gold'] = comment.gilded > 0\n",
      "            data['permalink'] = permalink\n",
      "            data['stickied'] = stickied\n",
      "            data['hidden'] = False\n",
      "            data['saved'] = comment.saved\n",
      "        else:\n",
      "            author = getattr(comment, 'author', '[deleted]')\n",
      "            stickied = getattr(comment, 'stickied', False)\n",
      "            flair = getattr(comment, 'author_flair_text', '')\n",
      "            data['type'] = 'SavedComment'\n",
      "            data['level'] = None\n",
      "            data['title'] = '[Comment] {0}'.format(comment.body)\n",
      "            data['comments'] = None\n",
      "            data['url_full'] = comment._fast_permalink\n",
      "            data['url'] = comment._fast_permalink\n",
      "            data['permalink'] = comment._fast_permalink\n",
      "            data['nsfw'] = comment.over_18\n",
      "            data['subreddit'] = six.text_type(comment.subreddit)\n",
      "            data['url_type'] = 'selfpost'\n",
      "            data['score'] = '{0} pts'.format(\n",
      "                '-' if comment.score_hidden else comment.score)\n",
      "            data['likes'] = comment.likes\n",
      "            data['created'] = cls.humanize_timestamp(comment.created_utc)\n",
      "            data['saved'] = comment.saved\n",
      "            data['stickied'] = stickied\n",
      "            data['gold'] = comment.gilded > 0\n",
      "            data['author'] = author\n",
      "            data['flair'] = flair\n",
      "        return data\n",
      "strip_praw_comment(cls=<class 'rtv.content.SubmissionContent'>, comment={_info_url='https://api.reddit.com/api/info/', reddit_session=<rtv.packages.praw.Reddit object at 0x7f090b1fbee0>, _underscore_names=['replies'], _uniq=None, json_dict=None, subreddit_id='t5_2qh0y', approved_at_utc=None, banned_by=None, removal_reason=None, link_id='t3_2xmo63', likes=None, _replies=[<rtv.packages.praw.objects.Comment object at 0x7f0919d75820>, <rtv.packages.praw.objects.Comment object at 0x7f091944b0a0>], user_reports=[], saved=False, id='cp1h67d', banned_at_utc=None, gilded=0, archived=True, report_reasons=None, author=Redditor(user_name='civilization_phaze_3'), can_mod_post=False, ups=21, parent_id='t3_2xmo63', score=21, approved_by=None, downs=0, body=\"Hi all,\\n\\nI've put together a small python project that facilitates browsing Reddit through a terminal window. It works great for text and q&a based subreddits. I made this because I wanted something that I could attach to a tmux session and keep in the background while working on other things. I found similar projects (e.g. AlienFeed), but they didn't support browsing through submission comments, which is IMO the most important part! I would love to get any feedback that you guys might have.\", edited=False, author_flair_css_class=None, collapsed=False, is_submitter=True, collapsed_reason=None, body_html='<div class=\"md\"><p>Hi all,</p>\\n\\n<p>I&\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "sub_author = getattr(sub, 'author', '[deleted]')\n",
      "State:\n",
      "Redditor(user_name='civilization_phaze_3')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _enable(self, name: str, **options) -> None:\n",
      "        if name not in self._plugins:\n",
      "            try:\n",
      "                (ep,) = [\n",
      "                    ep\n",
      "                    for ep in importlib_metadata_get(self.entry_point_group)\n",
      "                    if ep.name == name\n",
      "                ]\n",
      "            except ValueError as err:\n",
      "                if name in self.entrypoint_err_messages:\n",
      "                    raise ValueError(self.entrypoint_err_messages[name]) from err\n",
      "                else:\n",
      "                    raise NoSuchEntryPoint(self.entry_point_group, name) from err\n",
      "            value = cast(PluginType, ep.load())\n",
      "            self.register(name, value)\n",
      "        self._active_name = name\n",
      "        self._active = self._plugins[name]\n",
      "        for key in set(options.keys()) & set(self._global_settings.keys()):\n",
      "            self._global_settings[key] = options.pop(key)\n",
      "        self._options = options\n",
      "_enable(self=DataTransformerRegistry(active='', registered=['csv', 'default', 'json', 'vegafusion']), name='default', options={}, self._active=None, self._active_name='', self._global_settings={'consolidate_datasets': True}, self._options={}, self._plugins={'default': <function default_data_transformer at 0x7fc2ed64adc0>, 'json': <function to_json at 0x7fc2ed6543a0>, 'csv': <function to_csv at 0x7fc2ed654430>, 'vegafusion': <function vegafusion_data_transformer at 0x7fc2ed654e50>}, self.entry_point_group='altair.vegalite.v5.data_transformer', self.plugin_type=<class 'object'>)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "self._active_name = name\n",
      "State:\n",
      "DataTransformerRegistry(active='default', registered=['csv', 'default', 'json', 'vegafusion'])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _deep_copy(obj, ignore: Optional[list] = None):\n",
      "            if ignore is None:\n",
      "                ignore = []\n",
      "            if isinstance(obj, SchemaBase):\n",
      "                args = tuple(_deep_copy(arg) for arg in obj._args)\n",
      "                kwds = {\n",
      "                    k: (_deep_copy(v, ignore=ignore) if k not in ignore else v)\n",
      "                    for k, v in obj._kwds.items()\n",
      "                }\n",
      "                with debug_mode(False):\n",
      "                    return obj.__class__(*args, **kwds)\n",
      "            elif isinstance(obj, list):\n",
      "                return [_deep_copy(v, ignore=ignore) for v in obj]\n",
      "            elif isinstance(obj, dict):\n",
      "                return {\n",
      "                    k: (_deep_copy(v, ignore=ignore) if k not in ignore else v)\n",
      "                    for k, v in obj.items()\n",
      "                }\n",
      "            else:\n",
      "                return obj\n",
      "_deep_copy(obj=(datum.xxx * 2), ignore=[], _deep_copy=<function SchemaBase.copy.<locals>._deep_copy at 0x7fc2eb850a60>)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "kwds = {\n",
      "State:\n",
      "{'op': '*', 'lhs': datum.xxx, 'rhs': 2}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def eval_block(code, namespace=None, filename=\"<string>\"):\n",
      "    tree = ast.parse(code, filename=\"<ast>\", mode=\"exec\")\n",
      "    if namespace is None:\n",
      "        namespace = {}\n",
      "    catch_display = _CatchDisplay()\n",
      "    if isinstance(tree.body[-1], ast.Expr):\n",
      "        to_exec, to_eval = tree.body[:-1], tree.body[-1:]\n",
      "    else:\n",
      "        to_exec, to_eval = tree.body, []\n",
      "    for node in to_exec:\n",
      "        compiled = compile(ast.Module([node], []), filename=filename, mode=\"exec\")\n",
      "        exec(compiled, namespace)\n",
      "    with catch_display:\n",
      "        for node in to_eval:\n",
      "            compiled = compile(\n",
      "                ast.Interactive([node]), filename=filename, mode=\"single\"\n",
      "            )\n",
      "            exec(compiled, namespace)\n",
      "    return catch_display.output\n",
      "eval_block(code=b'\"\"\"\\nConnections Among U.S. Airports Interactive\\n-------------------------------------------\\nThis example shows all the connections between major U.S. airports. Lookup transformations \\nare used to find the coordinates of each airport and connecting airports. Connections \\nare displayed on mouseover via a single selection.\\n\"\"\"\\n\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "catch_display = _CatchDisplay()\n",
      "State:\n",
      "{output=None}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_logger(name: str) -> logging.Logger:\n",
      "    if name.startswith(\"electrum.\"):\n",
      "        name = name[9:]\n",
      "    return electrum_logger.getChild(name)\n",
      "get_logger(name='electrum.logging')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "name = name[9:]\n",
      "State:\n",
      "'logging'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def load_library():\n",
      "    tested_libversions = [2, 1, 0, ]\n",
      "    libnames = []\n",
      "    if sys.platform == 'darwin':\n",
      "        for v in tested_libversions:\n",
      "            libnames.append(f\"libsecp256k1.{v}.dylib\")\n",
      "    elif sys.platform in ('windows', 'win32'):\n",
      "        for v in tested_libversions:\n",
      "            libnames.append(f\"libsecp256k1-{v}.dll\")\n",
      "    elif 'ANDROID_DATA' in os.environ:\n",
      "        libnames = ['libsecp256k1.so', ]\n",
      "    else:\n",
      "        for v in tested_libversions:\n",
      "            libnames.append(f\"libsecp256k1.so.{v}\")\n",
      "    library_paths = []\n",
      "    for libname in libnames:\n",
      "        library_paths.append(os.path.join(os.path.dirname(__file__), libname))\n",
      "    for libname in libnames:\n",
      "        library_paths.append(libname)\n",
      "    exceptions = []\n",
      "    secp256k1 = None\n",
      "    for libpath in library_paths:\n",
      "        try:\n",
      "            secp256k1 = ctypes.cdll.LoadLibrary(libpath)\n",
      "        except BaseException as e:\n",
      "            exceptions.append(e)\n",
      "        else:\n",
      "            break\n",
      "    if not secp256k1:\n",
      "        _logger.error(f'libsecp256k1 library failed to load. exceptions: {repr(exceptions)}')\n",
      "        return None\n",
      "    try:\n",
      "        secp256k1.secp256k1_context_create.argtypes = [c_uint]\n",
      "        secp256k1.secp256k1_context_create.restype = c_void_p\n",
      "        secp256k1.secp256k1_context_randomize.argtypes = [c_void_p, c_char_p]\n",
      "        secp256k1.secp256k1_context_randomize.restype = c_int\n",
      "        secp256k1.secp256k1_ec_pubkey_create.argtypes = [c_void_p, c_void_p, c_char_p]\n",
      "        secp256k1.secp256k1_ec_pubkey_create.restype = c_int\n",
      "        secp256k1.secp256k1_ecdsa_sign.argtypes = [c_void_p, c_char_p, c_char_p, c_char_p, c_void_p, c_void_p]\n",
      "        secp256k1.secp256k1_ecdsa_sign.restype = c_int\n",
      "        secp256k1.secp256k1_ecdsa_verify.argtypes = [c_void_p, c_char_p, c_char_p, c_char_p]\n",
      "        secp256k1.secp256k1_ecdsa_verify.restype = c_int\n",
      "        secp256k1.secp256k1_ec_pubkey_parse.argtypes = [c_void_p, c_char_p, c_char_p, c_size_t]\n",
      "        secp256k1.secp256k1_ec_pubkey_parse.restype = c_int\n",
      "        secp256k1.secp256k1_ec_pubkey_serialize.argtypes = [c_void_p, c_char_p, c_void_p, c_char_p, c_uint]\n",
      "        secp256k1.secp256k1_ec_pubkey_serialize.restype = c_int\n",
      "        secp256k1.secp256k1_ecdsa_signature_parse_compact.argtypes = [c_void_p, c_char_p, c_char_p]\n",
      "        secp256k1.secp256k1_ecdsa_signature_parse_compact.restype = c_int\n",
      "        secp256k1.secp256k1_ecdsa_signature_normalize.argtypes = [c_void_p, c_char_p, c_char_p]\n",
      "        secp256k1.secp256k1_ecdsa_signature_normalize.restype = c_int\n",
      "        secp256k1.secp256k1_ecdsa_signature_serialize_compact.argtypes = [c_void_p, c_char_p, c_char_p]\n",
      "        secp256k1.secp256k1_ecdsa_signature_serialize_compact.restype = c_int\n",
      "        secp256k1.secp256k1_ecdsa_signature_parse_der.argtypes = [c_void_p, c_char_p, c_char_p, c_size_t]\n",
      "        secp256k1.secp256k1_ecdsa_signature_parse_der.restype = c_int\n",
      "        secp256k1.secp256k1_ecdsa_signature_serialize_der.argtypes = [c_void_p, c_char_p, c_void_p, c_char_p]\n",
      "        secp256k1.secp256k1_ecdsa_signature_serialize_der.restype = c_int\n",
      "        secp256k1.secp256k1_ec_pubkey_tweak_mul.argtypes = [c_void_p, c_char_p, c_char_p]\n",
      "        secp256k1.secp256k1_ec_pubkey_tweak_mul.restype = c_int\n",
      "        secp256k1.secp256k1_ec_pubkey_combine.argtypes = [c_void_p, c_char_p, c_void_p, c_size_t]\n",
      "        secp256k1.secp256k1_ec_pubkey_combine.restype = c_int\n",
      "        try:\n",
      "            secp256k1.secp256k1_ecdsa_recover.argtypes = [c_void_p, c_char_p, c_char_p, c_char_p]\n",
      "            secp256k1.secp256k1_ecdsa_recover.restype = c_int\n",
      "            secp256k1.secp256k1_ecdsa_recoverable_signature_parse_compact.argtypes = [c_void_p, c_char_p, c_char_p, c_int]\n",
      "            secp256k1.secp256k1_ecdsa_recoverable_signature_parse_compact.restype = c_int\n",
      "        except (OSError, AttributeError):\n",
      "            raise LibModuleMissing('libsecp256k1 library found but it was built '\n",
      "                                   'without required module (--enable-module-recovery)')\n",
      "        secp256k1.ctx = secp256k1.secp256k1_context_create(SECP256K1_CONTEXT_SIGN | SECP256K1_CONTEXT_VERIFY)\n",
      "        ret = secp256k1.secp256k1_context_randomize(secp256k1.ctx, os.urandom(32))\n",
      "        if not ret:\n",
      "            _logger.error('secp256k1_context_randomize failed')\n",
      "            return None\n",
      "        return secp256k1\n",
      "    except (OSError, AttributeError) as e:\n",
      "        _logger.error(f'libsecp256k1 library was found and loaded but there was an error when using it: {repr(e)}')\n",
      "        return None\n",
      "load_library()\n",
      "\n",
      "n:\n",
      "25\n",
      "Statement:\n",
      "secp256k1 = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _format(self, callFrame, *args):\n",
      "        prefix = callOrValue(self.prefix)\n",
      "        context = self._formatContext(callFrame)\n",
      "        if not args:\n",
      "            time = self._formatTime()\n",
      "            out = prefix + context + time\n",
      "        else:\n",
      "            if not self.includeContext:\n",
      "                context = ''\n",
      "            out = self._formatArgs(\n",
      "                callFrame, prefix, context, args)\n",
      "        return out\n",
      "_format(self=<icecream.icecream.IceCreamDebugger object at 0x7f8e5f34fd00>, callFrame=<frame at 0x55c68a87d3d0, file '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/gruns+icecream/gruns+icecream/tests/test_icecream.py', line 391, code testArgToStringFunction>, args=('ein',), self._pairDelimiter='| ', self.argToStringFunction=<function TestIceCream.testArgToStringFunction.<locals>.hello at 0x7f8e5f253ee0>, self.contextAbsPath=False, self.enabled=True, self.includeContext=False, self.outputFunction=<function stderrPrint at 0x7f8e5f302940>, self.prefix='ic| ')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "prefix = callOrValue(self.prefix)\n",
      "State:\n",
      "'ic| '\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _format(self, callFrame, *args):\n",
      "        prefix = callOrValue(self.prefix)\n",
      "        context = self._formatContext(callFrame)\n",
      "        if not args:\n",
      "            time = self._formatTime()\n",
      "            out = prefix + context + time\n",
      "        else:\n",
      "            if not self.includeContext:\n",
      "                context = ''\n",
      "            out = self._formatArgs(\n",
      "                callFrame, prefix, context, args)\n",
      "        return out\n",
      "_format(self=<icecream.icecream.IceCreamDebugger object at 0x7f8e5f34fd00>, callFrame=<frame at 0x55c68a87d3d0, file '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/gruns+icecream/gruns+icecream/tests/test_icecream.py', line 391, code testArgToStringFunction>, args=('ein',), self._pairDelimiter='| ', self.argToStringFunction=<function TestIceCream.testArgToStringFunction.<locals>.hello at 0x7f8e5f253ee0>, self.contextAbsPath=False, self.enabled=True, self.includeContext=False, self.outputFunction=<function stderrPrint at 0x7f8e5f302940>, self.prefix='ic| ')\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "out = self._formatArgs(\n",
      "State:\n",
      "'ic| eins: zwei'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _formatContext(self, callFrame):\n",
      "        filename, lineNumber, parentFunction = self._getContext(callFrame)\n",
      "        if parentFunction != '<module>':\n",
      "            parentFunction = '%s()' % parentFunction\n",
      "        context = '%s:%s in %s' % (filename, lineNumber, parentFunction)\n",
      "        return context\n",
      "_formatContext(self=<icecream.icecream.IceCreamDebugger object at 0x7f8e5f34fd00>, callFrame=<frame at 0x55c68a87d3d0, file '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/gruns+icecream/gruns+icecream/tests/test_icecream.py', line 391, code testArgToStringFunction>, self._pairDelimiter='| ', self.argToStringFunction=<function TestIceCream.testArgToStringFunction.<locals>.hello at 0x7f8e5f253ee0>, self.contextAbsPath=False, self.enabled=True, self.includeContext=False, self.outputFunction=<function stderrPrint at 0x7f8e5f302940>, self.prefix='ic| ')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "filename, lineNumber, parentFunction = self._getContext(callFrame)\n",
      "State:\n",
      "'test_icecream.py'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _formatContext(self, callFrame):\n",
      "        filename, lineNumber, parentFunction = self._getContext(callFrame)\n",
      "        if parentFunction != '<module>':\n",
      "            parentFunction = '%s()' % parentFunction\n",
      "        context = '%s:%s in %s' % (filename, lineNumber, parentFunction)\n",
      "        return context\n",
      "_formatContext(self=<icecream.icecream.IceCreamDebugger object at 0x7f8e5f34fd00>, callFrame=<frame at 0x55c68a87d3d0, file '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/gruns+icecream/gruns+icecream/tests/test_icecream.py', line 391, code testArgToStringFunction>, self._pairDelimiter='| ', self.argToStringFunction=<function TestIceCream.testArgToStringFunction.<locals>.hello at 0x7f8e5f253ee0>, self.contextAbsPath=False, self.enabled=True, self.includeContext=False, self.outputFunction=<function stderrPrint at 0x7f8e5f302940>, self.prefix='ic| ')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "parentFunction = '%s()' % parentFunction\n",
      "State:\n",
      "'testArgToStringFunction()'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _formatArgs(self, callFrame, prefix, context, args):\n",
      "        callNode = Source.executing(callFrame).node\n",
      "        if callNode is not None:\n",
      "            source = Source.for_frame(callFrame)\n",
      "            sanitizedArgStrs = [\n",
      "                source.get_text_with_indentation(arg)\n",
      "                for arg in callNode.args]\n",
      "        else:\n",
      "            warnings.warn(NO_SOURCE_AVAILABLE_WARNING_MESSAGE,\n",
      "                          category=RuntimeWarning, stacklevel=4)\n",
      "            sanitizedArgStrs = [_arg_source_missing] * len(args)\n",
      "        pairs = list(zip(sanitizedArgStrs, args))\n",
      "        out = self._constructArgumentOutput(prefix, context, pairs)\n",
      "        return out\n",
      "_formatArgs(self=<icecream.icecream.IceCreamDebugger object at 0x7f8e5f34fd00>, callFrame=<frame at 0x55c68a87d3d0, file '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/gruns+icecream/gruns+icecream/tests/test_icecream.py', line 391, code testArgToStringFunction>, prefix='ic| ', context='', args=('ein',), self._pairDelimiter='| ', self.argToStringFunction=<function TestIceCream.testArgToStringFunction.<locals>.hello at 0x7f8e5f253ee0>, self.contextAbsPath=False, self.enabled=True, self.includeContext=False, self.outputFunction=<function stderrPrint at 0x7f8e5f302940>, self.prefix='ic| ')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "source = Source.for_frame(callFrame)\n",
      "State:\n",
      "{filename='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/gruns+icecream/gruns+icecream/tests/test_icecream.py', text='# -*- coding: utf-8 -*-\\n\\n#\\n# IceCream - Never use print() to debug again\\n#\\n# Ansgar Grunseid\\n# grunseid.com\\n# grunseid@gmail.com\\n#\\n# License: MIT\\n#\\n\\nimport functools\\nimport sys\\nimport unittest\\nimport warnings\\n\\ntry:  # Python 2.x.\\n    from StringIO import StringIO\\nexcept ImportError:  # Python 3.x.\\n    from io import StringIO\\nfrom contextlib import contextmanager\\nfrom os.path import basename, splitext, realpath\\n\\nimport icecream\\nfrom icecream import ic, argumentToString, stderrPrint, NO_SOURCE_AVAILABLE_WARNING_MESSAGE\\n\\nTEST_PAIR_DELIMITER = \\'| \\'\\nMY_FILENAME = basename(__file__)\\nMY_FILEPATH = realpath(__file__)\\n\\n\\na = 1\\nb = 2\\nc = 3\\n\\n\\ndef noop(*args, **kwargs):\\n    return\\n\\n\\ndef hasAnsiEscapeCodes(s):\\n    # Oversimplified, but \\\\_()_/. TODO(grun): Test with regex.\\n    return \\'\\\\x1b[\\' in s\\n\\n\\nclass FakeTeletypeBuffer(StringIO):\\n    \"\"\"\\n    Extend StringIO to act like a TTY so ANSI control codes aren\\'t stripped\\n    when wrapped with colorama\\'s wrap_stream().\\n    \"\"\"\\n    def isatty(self):\\n        return True\\n\\n\\n@contextmanager\\ndef disableColoring():\\n    originalOutputFunction = ic.outputFunction\\n\\n    ic.configureOutput(outputFunction=stderrPrint)\\n    yield\\n    ic.configureOutput(outputFunction=originalOutputFunction)\\n\\n\\n@contextmanager\\ndef configureIcecreamOutput(prefix=None, outputFunction=None,\\n                            argToStringFunction=None, includeContext=None,\\n                            contextAbsPath=None):\\n    oldPrefix = ic.prefix\\n    oldOutputFunction = ic.outputFunction\\n    oldArgToStringFunction = ic.argToStringFunction\\n    oldIncludeContext = ic.includeContext\\n    oldContextAbsPath = ic.contextAbsPath\\n\\n    if prefix:\\n        ic.configureOutput(prefix=prefix)\\n    if outputFunction:\\n        ic.configureOutput(outputFunction=outputFunction)\\n    if argToStringFunction:\\n        ic.configureOutput(argToStringFunction=argToStringFunction)\\n    if includeContext:\\n        ic.configureOutput(includeContext=includeContext)\\n    if contextAbsPath:\\n        ic.configureOutput(contextAbsPath=contextAbsPath)\\n\\n    yield\\n\\n    ic.configureOutput(\\n        oldPrefix, oldOutputFunction, oldArgToStringFunction,\\n        oldIncludeContext, oldContextAbsPath)\\n\\n\\n@contextmanager\\ndef captureStandardStreams():\\n    realStdout = sys.stdout\\n    realStderr = sys.stderr\\n    newStdout = FakeTeletypeBuffer()\\n    newStderr = FakeTeletypeBuffer()\\n    try:\\n        sys.stdout = newStdout\\n        sys.stderr = newStderr\\n        yield newStdout, newStderr\\n    finally:\\n        sys.stdout = realStdout\\n        sys.stderr = realStderr\\n\\n\\ndef stripPrefix(line):\\n    if line.startswith(ic.prefix):\\n        line = line.strip()[len(ic.prefix):]\\n    return line\\n\\n\\ndef lineIsContextAndTime(line):\\n    line = stripPrefix(line)  # ic| f.py:33 in foo() at 08:08:51.389\\n    context, time = line.split(\\' at \\')\\n\\n    return (\\n        lineIsContext(context) and\\n        len(time.split(\\':\\')) == 3 and\\n        len(time.split(\\'.\\')) == 2)\\n\\n\\ndef lineIsContext(line):\\n    line = stripPrefix(line)  # ic| f.py:33 in foo()\\n    sourceLocation, function = line.split(\\' in \\')  # f.py:33 in foo()\\n    filename, lineNumber = sourceLocation.split(\\':\\')  # f.py:33\\n    name, ext = splitext(filename)\\n\\n    return (\\n        int(lineNumber) > 0 and\\n        ext in [\\'.py\\', \\'.pyc\\', \\'.pyo\\'] and\\n        name == splitext(MY_FILENAME)[0] and\\n        (function == \\'<module>\\' or function.endswith(\\'()\\')))\\n\\ndef lineIsAbsPathContext(line):\\n    line = stripPrefix(line)  # ic| /absolute/path/to/f.py:33 in foo()\\n    sourceLocation, function = line.split(\\' in \\')  # /absolute/path/to/f.py:33 in foo()\\n    filepath, lineNumber = sourceLocation.split(\\':\\')  # /absolute/path/to/f.py:33\\n    path, ext = splitext(filepath)\\n\\n    return (\\n        int(lineNumber) > 0 and\\n        ext in [\\'.py\\', \\'.pyc\\', \\'.pyo\\'] and\\n       ...2): 'hasAnsiEscapeCodes', ('FakeTeletypeBuffer', 47): 'FakeTeletypeBuffer', ('isatty', 52): 'FakeTeletypeBuffer.isatty', ('disableColoring', 56): 'disableColoring', ('configureIcecreamOutput', 65): 'configureIcecreamOutput', ('captureStandardStreams', 93): 'captureStandardStreams', ('stripPrefix', 108): 'stripPrefix', ('lineIsContextAndTime', 114): 'lineIsContextAndTime', ('lineIsContext', 124): 'lineIsContext', ('lineIsAbsPathContext', 136): 'lineIsAbsPathContext', ('lineAfterContext', 148): 'lineAfterContext', ('parseOutputIntoPairs', 160): 'parseOutputIntoPairs', ('TestIceCream', 201): 'TestIceCream', ('setUp', 202): 'TestIceCream.setUp', ('testMetadata', 205): 'TestIceCream.testMetadata', ('is_non_empty_string', 206): 'TestIceCream.testMetadata.<locals>.is_non_empty_string', ('testWithoutArgs', 216): 'TestIceCream.testWithoutArgs', ('testAsArgument', 221): 'TestIceCream.testAsArgument', ('testSingleArgument', 235): 'TestIceCream.testSingleArgument', ('testMultipleArguments', 240): 'TestIceCream.testMultipleArguments', ('testNestedMultiline', 246): 'TestIceCream.testNestedMultiline', ('testExpressionArguments', 263): 'TestIceCream.testExpressionArguments', ('klass', 264): 'TestIceCream.testExpressionArguments.<locals>.klass', ('testMultipleCallsOnSameLine', 278): 'TestIceCream.testMultipleCallsOnSameLine', ('testCallSurroundedByExpressions', 285): 'TestIceCream.testCallSurroundedByExpressions', ('testComments', 290): 'TestIceCream.testComments', ('testMethodArguments', 295): 'TestIceCream.testMethodArguments', ('Foo', 296): 'TestIceCream.testMethodArguments.<locals>.Foo', ('foo', 297): 'TestIceCream.testMethodArguments.<locals>.Foo.foo', ('testComplicated', 304): 'TestIceCream.testComplicated', ('testReturnValue', 315): 'TestIceCream.testReturnValue', ('testDifferentName', 321): 'TestIceCream.testDifferentName', ('testPrefixConfiguration', 333): 'TestIceCream.testPrefixConfiguration', ('prefixFunction', 341): 'TestIceCream.testPrefixConfiguration.<locals>.prefixFunction', ('testOutputFunction', 349): 'TestIceCream.testOutputFunction', ('appendTo', 352): 'TestIceCream.testOutputFunction.<locals>.appendTo', ('testEnableDisable', 368): 'TestIceCream.testEnableDisable', ('testArgToStringFunction', 384): 'TestIceCream.testArgToStringFunction', ('hello', 385): 'TestIceCream.testArgToStringFunction.<locals>.hello', ('testSingledispatchArgumentToString', 395): 'TestIceCream.testSingledispatchArgumentToString', ('argumentToString_tuple', 396): 'TestIceCream.testSingledispatchArgumentToString.<locals>.argumentToString_tuple', ('testSingleArgumentLongLineNotWrapped', 422): 'TestIceCream.testSingleArgumentLongLineNotWrapped', ('testMultipleArgumentsLongLineWrapped', 431): 'TestIceCream.testMultipleArgumentsLongLineWrapped', ('testMultilineValueWrapped', 450): 'TestIceCream.testMultilineValueWrapped', ('testIncludeContextSingleLine', 458): 'TestIceCream.testIncludeContextSingleLine', ('testContextAbsPathSingleLine', 467): 'TestIceCream.testContextAbsPathSingleLine', ('testValues', 476): 'TestIceCream.testValues', ('testIncludeContextMultiLine', 485): 'TestIceCream.testIncludeContextMultiLine', ('testContextAbsPathMultiLine', 497): 'TestIceCream.testContextAbsPathMultiLine', ('testFormat', 509): 'TestIceCream.testFormat', ('testMultilineInvocationWithComments', 517): 'TestIceCream.testMultilineInvocationWithComments', ('testNoSourceAvailablePrintsValues', 532): 'TestIceCream.testNoSourceAvailablePrintsValues', ('testNoSourceAvailablePrintsMultiline', 540): 'TestIceCream.testNoSourceAvailablePrintsMultiline', ('testNoSourceAvailableIssuesExactlyOneWarning', 552): 'TestIceCream.testNoSourceAvailableIssuesExactlyOneWarning', ('testSingleTupleArgument', 560): 'TestIceCream.testSingleTupleArgument', ('testMultilineContainerArgs', 567): 'TestIceCream.testMultilineContainerArgs', ('testMultipleTupleArguments', 609): 'TestIceCream.testMultipleTupleArguments', ('testColoring', 617): 'TestIceCream.testColoring', ('testConfigureOutputWithNoParameters', 623): 'TestIceCream.testConfigureOutputWithNoParameters'}, _asttokens=None, _asttext=None}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _formatArgs(self, callFrame, prefix, context, args):\n",
      "        callNode = Source.executing(callFrame).node\n",
      "        if callNode is not None:\n",
      "            source = Source.for_frame(callFrame)\n",
      "            sanitizedArgStrs = [\n",
      "                source.get_text_with_indentation(arg)\n",
      "                for arg in callNode.args]\n",
      "        else:\n",
      "            warnings.warn(NO_SOURCE_AVAILABLE_WARNING_MESSAGE,\n",
      "                          category=RuntimeWarning, stacklevel=4)\n",
      "            sanitizedArgStrs = [_arg_source_missing] * len(args)\n",
      "        pairs = list(zip(sanitizedArgStrs, args))\n",
      "        out = self._constructArgumentOutput(prefix, context, pairs)\n",
      "        return out\n",
      "_formatArgs(self=<icecream.icecream.IceCreamDebugger object at 0x7f8e5f34fd00>, callFrame=<frame at 0x55c68a87d3d0, file '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/gruns+icecream/gruns+icecream/tests/test_icecream.py', line 391, code testArgToStringFunction>, prefix='ic| ', context='', args=('ein',), self._pairDelimiter='| ', self.argToStringFunction=<function TestIceCream.testArgToStringFunction.<locals>.hello at 0x7f8e5f253ee0>, self.contextAbsPath=False, self.enabled=True, self.includeContext=False, self.outputFunction=<function stderrPrint at 0x7f8e5f302940>, self.prefix='ic| ')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "sanitizedArgStrs = [\n",
      "State:\n",
      "['eins']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _constructArgumentOutput(self, prefix, context, pairs):\n",
      "        def argPrefix(arg):\n",
      "            return '%s: ' % arg\n",
      "        pairs = [(arg, self.argToStringFunction(val)) for arg, val in pairs]\n",
      "        pairStrs = [\n",
      "            val\n",
      "            if (isLiteral(arg) or arg is _arg_source_missing)\n",
      "            else (argPrefix(arg) + val)\n",
      "            for arg, val in pairs]\n",
      "        allArgsOnOneLine = self._pairDelimiter.join(pairStrs)\n",
      "        multilineArgs = len(allArgsOnOneLine.splitlines()) > 1\n",
      "        contextDelimiter = self.contextDelimiter if context else ''\n",
      "        allPairs = prefix + context + contextDelimiter + allArgsOnOneLine\n",
      "        firstLineTooLong = len(allPairs.splitlines()[0]) > self.lineWrapWidth\n",
      "        if multilineArgs or firstLineTooLong:\n",
      "            if context:\n",
      "                lines = [prefix + context] + [\n",
      "                    format_pair(len(prefix) * ' ', arg, value)\n",
      "                    for arg, value in pairs\n",
      "                ]\n",
      "            else:\n",
      "                arg_lines = [\n",
      "                    format_pair('', arg, value)\n",
      "                    for arg, value in pairs\n",
      "                ]\n",
      "                lines = indented_lines(prefix, '\\n'.join(arg_lines))\n",
      "        else:\n",
      "            lines = [prefix + context + contextDelimiter + allArgsOnOneLine]\n",
      "        return '\\n'.join(lines)\n",
      "_constructArgumentOutput(self=<icecream.icecream.IceCreamDebugger object at 0x7f8e5f34fd00>, prefix='ic| ', context='', pairs=[('eins', 'ein')], self._pairDelimiter='| ', self.argToStringFunction=<function TestIceCream.testArgToStringFunction.<locals>.hello at 0x7f8e5f253ee0>, self.contextAbsPath=False, self.enabled=True, self.includeContext=False, self.outputFunction=<function stderrPrint at 0x7f8e5f302940>, self.prefix='ic| ')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "pairs = [(arg, self.argToStringFunction(val)) for arg, val in pairs]\n",
      "State:\n",
      "[('eins', 'zwei')]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _constructArgumentOutput(self, prefix, context, pairs):\n",
      "        def argPrefix(arg):\n",
      "            return '%s: ' % arg\n",
      "        pairs = [(arg, self.argToStringFunction(val)) for arg, val in pairs]\n",
      "        pairStrs = [\n",
      "            val\n",
      "            if (isLiteral(arg) or arg is _arg_source_missing)\n",
      "            else (argPrefix(arg) + val)\n",
      "            for arg, val in pairs]\n",
      "        allArgsOnOneLine = self._pairDelimiter.join(pairStrs)\n",
      "        multilineArgs = len(allArgsOnOneLine.splitlines()) > 1\n",
      "        contextDelimiter = self.contextDelimiter if context else ''\n",
      "        allPairs = prefix + context + contextDelimiter + allArgsOnOneLine\n",
      "        firstLineTooLong = len(allPairs.splitlines()[0]) > self.lineWrapWidth\n",
      "        if multilineArgs or firstLineTooLong:\n",
      "            if context:\n",
      "                lines = [prefix + context] + [\n",
      "                    format_pair(len(prefix) * ' ', arg, value)\n",
      "                    for arg, value in pairs\n",
      "                ]\n",
      "            else:\n",
      "                arg_lines = [\n",
      "                    format_pair('', arg, value)\n",
      "                    for arg, value in pairs\n",
      "                ]\n",
      "                lines = indented_lines(prefix, '\\n'.join(arg_lines))\n",
      "        else:\n",
      "            lines = [prefix + context + contextDelimiter + allArgsOnOneLine]\n",
      "        return '\\n'.join(lines)\n",
      "_constructArgumentOutput(self=<icecream.icecream.IceCreamDebugger object at 0x7f8e5f34fd00>, prefix='ic| ', context='', pairs=[('eins', 'ein')], self._pairDelimiter='| ', self.argToStringFunction=<function TestIceCream.testArgToStringFunction.<locals>.hello at 0x7f8e5f253ee0>, self.contextAbsPath=False, self.enabled=True, self.includeContext=False, self.outputFunction=<function stderrPrint at 0x7f8e5f302940>, self.prefix='ic| ')\n",
      "\n",
      "n:\n",
      "25\n",
      "Statement:\n",
      "multilineArgs = len(allArgsOnOneLine.splitlines()) > 1\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def lineAfterContext(line, prefix):\n",
      "    if line.startswith(prefix):\n",
      "        line = line[len(prefix):]\n",
      "    toks = line.split(' in ', 1)\n",
      "    if len(toks) == 2:\n",
      "        rest = toks[1].split(' ')\n",
      "        line = ' '.join(rest[1:])\n",
      "    return line\n",
      "lineAfterContext(line='ic| eins: zwei', prefix='ic| ')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "line = line[len(prefix):]\n",
      "State:\n",
      "'eins: zwei'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def argumentToString(obj):\n",
      "    s = DEFAULT_ARG_TO_STRING_FUNCTION(obj)\n",
      "    s = s.replace('\\\\n', '\\n')\n",
      "    return s\n",
      "argumentToString(obj=1)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "s = DEFAULT_ARG_TO_STRING_FUNCTION(obj)\n",
      "State:\n",
      "'1'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def lineIsContextAndTime(line):\n",
      "    line = stripPrefix(line)\n",
      "    context, time = line.split(' at ')\n",
      "    return (\n",
      "        lineIsContext(context) and\n",
      "        len(time.split(':')) == 3 and\n",
      "        len(time.split('.')) == 2)\n",
      "lineIsContextAndTime(line='ic| test_icecream.py:229 in testAsArgument() at 01:15:24.779')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "line = stripPrefix(line)  # ic| f.py:33 in foo() at 08:08:51.389\n",
      "State:\n",
      "'test_icecream.py:229 in testAsArgument() at 01:15:24.779'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def lineIsContextAndTime(line):\n",
      "    line = stripPrefix(line)\n",
      "    context, time = line.split(' at ')\n",
      "    return (\n",
      "        lineIsContext(context) and\n",
      "        len(time.split(':')) == 3 and\n",
      "        len(time.split('.')) == 2)\n",
      "lineIsContextAndTime(line='ic| test_icecream.py:229 in testAsArgument() at 01:15:24.779')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "context, time = line.split(' at ')\n",
      "State:\n",
      "'test_icecream.py:229 in testAsArgument()'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def stripPrefix(line):\n",
      "    if line.startswith(ic.prefix):\n",
      "        line = line.strip()[len(ic.prefix):]\n",
      "    return line\n",
      "stripPrefix(line='ic| test_icecream.py:229 in testAsArgument() at 01:15:24.779')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "line = line.strip()[len(ic.prefix):]\n",
      "State:\n",
      "'test_icecream.py:229 in testAsArgument() at 01:15:24.779'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def lineIsContext(line):\n",
      "    line = stripPrefix(line)\n",
      "    sourceLocation, function = line.split(' in ')\n",
      "    filename, lineNumber = sourceLocation.split(':')\n",
      "    name, ext = splitext(filename)\n",
      "    return (\n",
      "        int(lineNumber) > 0 and\n",
      "        ext in ['.py', '.pyc', '.pyo'] and\n",
      "        name == splitext(MY_FILENAME)[0] and\n",
      "        (function == '<module>' or function.endswith('()')))\n",
      "lineIsContext(line='test_icecream.py:229 in testAsArgument()')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "sourceLocation, function = line.split(' in ')  # f.py:33 in foo()\n",
      "State:\n",
      "'test_icecream.py:229'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def lineIsContext(line):\n",
      "    line = stripPrefix(line)\n",
      "    sourceLocation, function = line.split(' in ')\n",
      "    filename, lineNumber = sourceLocation.split(':')\n",
      "    name, ext = splitext(filename)\n",
      "    return (\n",
      "        int(lineNumber) > 0 and\n",
      "        ext in ['.py', '.pyc', '.pyo'] and\n",
      "        name == splitext(MY_FILENAME)[0] and\n",
      "        (function == '<module>' or function.endswith('()')))\n",
      "lineIsContext(line='test_icecream.py:229 in testAsArgument()')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "filename, lineNumber = sourceLocation.split(':')  # f.py:33\n",
      "State:\n",
      "'test_icecream.py'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def lineIsContext(line):\n",
      "    line = stripPrefix(line)\n",
      "    sourceLocation, function = line.split(' in ')\n",
      "    filename, lineNumber = sourceLocation.split(':')\n",
      "    name, ext = splitext(filename)\n",
      "    return (\n",
      "        int(lineNumber) > 0 and\n",
      "        ext in ['.py', '.pyc', '.pyo'] and\n",
      "        name == splitext(MY_FILENAME)[0] and\n",
      "        (function == '<module>' or function.endswith('()')))\n",
      "lineIsContext(line='test_icecream.py:229 in testAsArgument()')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "name, ext = splitext(filename)\n",
      "State:\n",
      "'test_icecream'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def format_pair(prefix, arg, value):\n",
      "    if arg is _arg_source_missing:\n",
      "        arg_lines = []\n",
      "        value_prefix = prefix\n",
      "    else:\n",
      "        arg_lines = indented_lines(prefix, arg)\n",
      "        value_prefix = arg_lines[-1] + ': '\n",
      "    looksLikeAString = value[0] + value[-1] in [\"''\", '\"\"']\n",
      "    if looksLikeAString:\n",
      "        value = prefixLinesAfterFirst(' ', value)\n",
      "    value_lines = indented_lines(value_prefix, value)\n",
      "    lines = arg_lines[:-1] + value_lines\n",
      "    return '\\n'.join(lines)\n",
      "format_pair(prefix='    ', arg='multilineStr', value=\"'line1\\nline2'\")\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "arg_lines = indented_lines(prefix, arg)\n",
      "State:\n",
      "['    multilineStr']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def format_pair(prefix, arg, value):\n",
      "    if arg is _arg_source_missing:\n",
      "        arg_lines = []\n",
      "        value_prefix = prefix\n",
      "    else:\n",
      "        arg_lines = indented_lines(prefix, arg)\n",
      "        value_prefix = arg_lines[-1] + ': '\n",
      "    looksLikeAString = value[0] + value[-1] in [\"''\", '\"\"']\n",
      "    if looksLikeAString:\n",
      "        value = prefixLinesAfterFirst(' ', value)\n",
      "    value_lines = indented_lines(value_prefix, value)\n",
      "    lines = arg_lines[:-1] + value_lines\n",
      "    return '\\n'.join(lines)\n",
      "format_pair(prefix='    ', arg='multilineStr', value=\"'line1\\nline2'\")\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "value = prefixLinesAfterFirst(' ', value)\n",
      "State:\n",
      "\"'line1\\n line2'\"\n",
      "==================================================\n",
      "Clean Code:\n",
      "def prefixLinesAfterFirst(prefix, s):\n",
      "    lines = s.splitlines(True)\n",
      "    for i in range(1, len(lines)):\n",
      "        lines[i] = prefix + lines[i]\n",
      "    return ''.join(lines)\n",
      "prefixLinesAfterFirst(prefix=' ', s=\"'line1\\nline2'\")\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "lines = s.splitlines(True)\n",
      "State:\n",
      "[\"'line1\\n\", \"line2'\"]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def prefixLinesAfterFirst(prefix, s):\n",
      "    lines = s.splitlines(True)\n",
      "    for i in range(1, len(lines)):\n",
      "        lines[i] = prefix + lines[i]\n",
      "    return ''.join(lines)\n",
      "prefixLinesAfterFirst(prefix=' ', s=\"'line1\\nline2'\")\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "lines[i] = prefix + lines[i]\n",
      "State:\n",
      "[\"'line1\\n\", \" line2'\"]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def appendTo(s):\n",
      "            lst.append(s)\n",
      "appendTo(s='ic| a: 1', lst=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "lst.append(s)\n",
      "State:\n",
      "['ic| a: 1']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def from_function_params(cls, func):\n",
      "        params = NumpyDocString(pydoc.getdoc(func))[\"Parameters\"]\n",
      "        comp_dict = {}\n",
      "        for p in params:\n",
      "            name = p.name\n",
      "            type = p.type\n",
      "            desc = \"\\n    \".join(p.desc)\n",
      "            comp_dict[name] = f\"{name} : {type}\\n    {desc}\"\n",
      "        return cls(comp_dict)\n",
      "from_function_params(cls=<class 'seaborn._docstrings.DocstringComponents'>, func=<function EstimateAggregator.__init__ at 0x7f45d4ea13a0>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "params = NumpyDocString(pydoc.getdoc(func))[\"Parameters\"]\n",
      "State:\n",
      "[Parameter(name='estimator', type='callable or string', desc=['Function (or method name) that maps a vector to a scalar.']), Parameter(name='errorbar', type='string, (string, number) tuple, or callable', desc=['Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple', 'with a method name and a level parameter, or a function that maps from a', 'vector to a (min, max) interval, or None to hide errorbar. See the', ':doc:`errorbar tutorial </tutorial/error_bars>` for more information.']), Parameter(name='boot_kws', type='', desc=['Additional keywords are passed to bootstrap when error_method is \"ci\".'])]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse(self):\n",
      "        self._doc.reset()\n",
      "        self._parse_summary()\n",
      "        sections = list(self._read_sections())\n",
      "        section_names = {section for section, content in sections}\n",
      "        has_returns = 'Returns' in section_names\n",
      "        has_yields = 'Yields' in section_names\n",
      "        if has_returns and has_yields:\n",
      "            msg = 'Docstring contains both a Returns and Yields section.'\n",
      "            raise ValueError(msg)\n",
      "        if not has_yields and 'Receives' in section_names:\n",
      "            msg = 'Docstring contains a Receives section but not Yields.'\n",
      "            raise ValueError(msg)\n",
      "        for (section, content) in sections:\n",
      "            if not section.startswith('..'):\n",
      "                section = (s.capitalize() for s in section.split(' '))\n",
      "                section = ' '.join(section)\n",
      "                if self.get(section):\n",
      "                    self._error_location(f\"The section {section} appears twice\")\n",
      "            if section in ('Parameters', 'Other Parameters', 'Attributes',\n",
      "                           'Methods'):\n",
      "                self[section] = self._parse_param_list(content)\n",
      "            elif section in ('Returns', 'Yields', 'Raises', 'Warns', 'Receives'):\n",
      "                self[section] = self._parse_param_list(\n",
      "                    content, single_element_is_type=True)\n",
      "            elif section.startswith('.. index::'):\n",
      "                self['index'] = self._parse_index(section, content)\n",
      "            elif section == 'See Also':\n",
      "                self['See Also'] = self._parse_see_also(content)\n",
      "            else:\n",
      "                self[section] = content\n",
      "_parse(self=<seaborn.external.docscrape.NumpyDocString object at 0x7f45d9d15c10>, self['Attributes']=[], self['Examples']='', self['Extended Summary']=[], self['Methods']=[], self['Notes']=[], self['Other Parameters']=[], self['Parameters']=[], self['Raises']=[], self['Receives']=[], self['References']='', self['Returns']=[], self['See Also']=[], self['Signature']='', self['Summary']=[''], self['Warnings']=[], self['Warns']=[], self['Yields']=[], self['index']={})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self._parse_summary()\n",
      "State:\n",
      "['Data aggregator that produces an estimate and error bar interval.']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse(self):\n",
      "        self._doc.reset()\n",
      "        self._parse_summary()\n",
      "        sections = list(self._read_sections())\n",
      "        section_names = {section for section, content in sections}\n",
      "        has_returns = 'Returns' in section_names\n",
      "        has_yields = 'Yields' in section_names\n",
      "        if has_returns and has_yields:\n",
      "            msg = 'Docstring contains both a Returns and Yields section.'\n",
      "            raise ValueError(msg)\n",
      "        if not has_yields and 'Receives' in section_names:\n",
      "            msg = 'Docstring contains a Receives section but not Yields.'\n",
      "            raise ValueError(msg)\n",
      "        for (section, content) in sections:\n",
      "            if not section.startswith('..'):\n",
      "                section = (s.capitalize() for s in section.split(' '))\n",
      "                section = ' '.join(section)\n",
      "                if self.get(section):\n",
      "                    self._error_location(f\"The section {section} appears twice\")\n",
      "            if section in ('Parameters', 'Other Parameters', 'Attributes',\n",
      "                           'Methods'):\n",
      "                self[section] = self._parse_param_list(content)\n",
      "            elif section in ('Returns', 'Yields', 'Raises', 'Warns', 'Receives'):\n",
      "                self[section] = self._parse_param_list(\n",
      "                    content, single_element_is_type=True)\n",
      "            elif section.startswith('.. index::'):\n",
      "                self['index'] = self._parse_index(section, content)\n",
      "            elif section == 'See Also':\n",
      "                self['See Also'] = self._parse_see_also(content)\n",
      "            else:\n",
      "                self[section] = content\n",
      "_parse(self=<seaborn.external.docscrape.NumpyDocString object at 0x7f45d9d15c10>, self['Attributes']=[], self['Examples']='', self['Extended Summary']=[], self['Methods']=[], self['Notes']=[], self['Other Parameters']=[], self['Parameters']=[], self['Raises']=[], self['Receives']=[], self['References']='', self['Returns']=[], self['See Also']=[], self['Signature']='', self['Summary']=[''], self['Warnings']=[], self['Warns']=[], self['Yields']=[], self['index']={})\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "section_names = {section for section, content in sections}\n",
      "State:\n",
      "{'Parameters'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse(self):\n",
      "        self._doc.reset()\n",
      "        self._parse_summary()\n",
      "        sections = list(self._read_sections())\n",
      "        section_names = {section for section, content in sections}\n",
      "        has_returns = 'Returns' in section_names\n",
      "        has_yields = 'Yields' in section_names\n",
      "        if has_returns and has_yields:\n",
      "            msg = 'Docstring contains both a Returns and Yields section.'\n",
      "            raise ValueError(msg)\n",
      "        if not has_yields and 'Receives' in section_names:\n",
      "            msg = 'Docstring contains a Receives section but not Yields.'\n",
      "            raise ValueError(msg)\n",
      "        for (section, content) in sections:\n",
      "            if not section.startswith('..'):\n",
      "                section = (s.capitalize() for s in section.split(' '))\n",
      "                section = ' '.join(section)\n",
      "                if self.get(section):\n",
      "                    self._error_location(f\"The section {section} appears twice\")\n",
      "            if section in ('Parameters', 'Other Parameters', 'Attributes',\n",
      "                           'Methods'):\n",
      "                self[section] = self._parse_param_list(content)\n",
      "            elif section in ('Returns', 'Yields', 'Raises', 'Warns', 'Receives'):\n",
      "                self[section] = self._parse_param_list(\n",
      "                    content, single_element_is_type=True)\n",
      "            elif section.startswith('.. index::'):\n",
      "                self['index'] = self._parse_index(section, content)\n",
      "            elif section == 'See Also':\n",
      "                self['See Also'] = self._parse_see_also(content)\n",
      "            else:\n",
      "                self[section] = content\n",
      "_parse(self=<seaborn.external.docscrape.NumpyDocString object at 0x7f45d9d15c10>, self['Attributes']=[], self['Examples']='', self['Extended Summary']=[], self['Methods']=[], self['Notes']=[], self['Other Parameters']=[], self['Parameters']=[], self['Raises']=[], self['Receives']=[], self['References']='', self['Returns']=[], self['See Also']=[], self['Signature']='', self['Summary']=[''], self['Warnings']=[], self['Warns']=[], self['Yields']=[], self['index']={})\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "section = ' '.join(section)\n",
      "State:\n",
      "'Parameters'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def read_to_condition(self, condition_func):\n",
      "        start = self._l\n",
      "        for line in self[start:]:\n",
      "            if condition_func(line):\n",
      "                return self[start:self._l]\n",
      "            self._l += 1\n",
      "            if self.eof():\n",
      "                return self[start:self._l+1]\n",
      "        return []\n",
      "read_to_condition(self=<seaborn.external.docscrape.Reader object at 0x7f453a7a3790>, condition_func=<function Reader.read_to_next_empty_line.<locals>.is_empty at 0x7f45d4da2b80>, self._l=0, self._str=['Data aggregator that produces an estimate and error bar interval.', '', 'Parameters', '----------', 'estimator : callable or string', '    Function (or method name) that maps a vector to a scalar.', 'errorbar : string, (string, number) tuple, or callable', '    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple', '    with a method name and a level parameter, or a function that maps from a', '    vector to a (min, max) interval, or None to hide errorbar. See the', '    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.', 'boot_kws', '    Additional keywords are passed to bootstrap when error_method is \"ci\".'])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "start = self._l\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def read_to_condition(self, condition_func):\n",
      "        start = self._l\n",
      "        for line in self[start:]:\n",
      "            if condition_func(line):\n",
      "                return self[start:self._l]\n",
      "            self._l += 1\n",
      "            if self.eof():\n",
      "                return self[start:self._l+1]\n",
      "        return []\n",
      "read_to_condition(self=<seaborn.external.docscrape.Reader object at 0x7f453a7a3790>, condition_func=<function Reader.read_to_next_empty_line.<locals>.is_empty at 0x7f45d4da2b80>, self._l=0, self._str=['Data aggregator that produces an estimate and error bar interval.', '', 'Parameters', '----------', 'estimator : callable or string', '    Function (or method name) that maps a vector to a scalar.', 'errorbar : string, (string, number) tuple, or callable', '    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple', '    with a method name and a level parameter, or a function that maps from a', '    vector to a (min, max) interval, or None to hide errorbar. See the', '    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.', 'boot_kws', '    Additional keywords are passed to bootstrap when error_method is \"ci\".'])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "self._l += 1\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __setitem__(self, key, val):\n",
      "        if key not in self._parsed_data:\n",
      "            self._error_location(f\"Unknown section {key}\", error=False)\n",
      "        else:\n",
      "            self._parsed_data[key] = val\n",
      "__setitem__(self=<seaborn.external.docscrape.NumpyDocString object at 0x7f45d9d15c10>, key='Summary', val=['Data aggregator that produces an estimate and error bar interval.'], self['Attributes']=[], self['Examples']='', self['Extended Summary']=[], self['Methods']=[], self['Notes']=[], self['Other Parameters']=[], self['Parameters']=[], self['Raises']=[], self['Receives']=[], self['References']='', self['Returns']=[], self['See Also']=[], self['Signature']='', self['Summary']=[''], self['Warnings']=[], self['Warns']=[], self['Yields']=[], self['index']={})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "self._parsed_data[key] = val\n",
      "State:\n",
      "['Data aggregator that produces an estimate and error bar interval.']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _read_to_next_section(self):\n",
      "        section = self._doc.read_to_next_empty_line()\n",
      "        while not self._is_at_section() and not self._doc.eof():\n",
      "            if not self._doc.peek(-1).strip():\n",
      "                section += ['']\n",
      "            section += self._doc.read_to_next_empty_line()\n",
      "        return section\n",
      "_read_to_next_section(self=<seaborn.external.docscrape.NumpyDocString object at 0x7f45d9d15c10>, self['Attributes']=[], self['Examples']='', self['Extended Summary']=[], self['Methods']=[], self['Notes']=[], self['Other Parameters']=[], self['Parameters']=[], self['Raises']=[], self['Receives']=[], self['References']='', self['Returns']=[], self['See Also']=[], self['Signature']='', self['Summary']=['Data aggregator that produces an estimate and error bar interval.'], self['Warnings']=[], self['Warns']=[], self['Yields']=[], self['index']={})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "section = self._doc.read_to_next_empty_line()\n",
      "State:\n",
      "['Parameters', '----------', 'estimator : callable or string', '    Function (or method name) that maps a vector to a scalar.', 'errorbar : string, (string, number) tuple, or callable', '    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple', '    with a method name and a level parameter, or a function that maps from a', '    vector to a (min, max) interval, or None to hide errorbar. See the', '    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.', 'boot_kws', '    Additional keywords are passed to bootstrap when error_method is \"ci\".']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _strip(self, doc):\n",
      "        i = 0\n",
      "        j = 0\n",
      "        for i, line in enumerate(doc):\n",
      "            if line.strip():\n",
      "                break\n",
      "        for j, line in enumerate(doc[::-1]):\n",
      "            if line.strip():\n",
      "                break\n",
      "        return doc[i:len(doc)-j]\n",
      "_strip(self=<seaborn.external.docscrape.NumpyDocString object at 0x7f45d9d15c10>, doc=['estimator : callable or string', '    Function (or method name) that maps a vector to a scalar.', 'errorbar : string, (string, number) tuple, or callable', '    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple', '    with a method name and a level parameter, or a function that maps from a', '    vector to a (min, max) interval, or None to hide errorbar. See the', '    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.', 'boot_kws', '    Additional keywords are passed to bootstrap when error_method is \"ci\".'], self['Attributes']=[], self['Examples']='', self['Extended Summary']=[], self['Methods']=[], self['Notes']=[], self['Other Parameters']=[], self['Parameters']=[], self['Raises']=[], self['Receives']=[], self['References']='', self['Returns']=[], self['See Also']=[], self['Signature']='', self['Summary']=['Data aggregator that produces an estimate and error bar interval.'], self['Warnings']=[], self['Warns']=[], self['Yields']=[], self['index']={})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "i = 0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _strip(self, doc):\n",
      "        i = 0\n",
      "        j = 0\n",
      "        for i, line in enumerate(doc):\n",
      "            if line.strip():\n",
      "                break\n",
      "        for j, line in enumerate(doc[::-1]):\n",
      "            if line.strip():\n",
      "                break\n",
      "        return doc[i:len(doc)-j]\n",
      "_strip(self=<seaborn.external.docscrape.NumpyDocString object at 0x7f45d9d15c10>, doc=['estimator : callable or string', '    Function (or method name) that maps a vector to a scalar.', 'errorbar : string, (string, number) tuple, or callable', '    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple', '    with a method name and a level parameter, or a function that maps from a', '    vector to a (min, max) interval, or None to hide errorbar. See the', '    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.', 'boot_kws', '    Additional keywords are passed to bootstrap when error_method is \"ci\".'], self['Attributes']=[], self['Examples']='', self['Extended Summary']=[], self['Methods']=[], self['Notes']=[], self['Other Parameters']=[], self['Parameters']=[], self['Raises']=[], self['Receives']=[], self['References']='', self['Returns']=[], self['See Also']=[], self['Signature']='', self['Summary']=['Data aggregator that produces an estimate and error bar interval.'], self['Warnings']=[], self['Warns']=[], self['Yields']=[], self['index']={})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "j = 0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_param_list(self, content, single_element_is_type=False):\n",
      "        r = Reader(content)\n",
      "        params = []\n",
      "        while not r.eof():\n",
      "            header = r.read().strip()\n",
      "            if ' : ' in header:\n",
      "                arg_name, arg_type = header.split(' : ')[:2]\n",
      "            else:\n",
      "                if single_element_is_type:\n",
      "                    arg_name, arg_type = '', header\n",
      "                else:\n",
      "                    arg_name, arg_type = header, ''\n",
      "            desc = r.read_to_next_unindented_line()\n",
      "            desc = dedent_lines(desc)\n",
      "            desc = strip_blank_lines(desc)\n",
      "            params.append(Parameter(arg_name, arg_type, desc))\n",
      "        return params\n",
      "_parse_param_list(self=<seaborn.external.docscrape.NumpyDocString object at 0x7f45d9d15c10>, content=['estimator : callable or string', '    Function (or method name) that maps a vector to a scalar.', 'errorbar : string, (string, number) tuple, or callable', '    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple', '    with a method name and a level parameter, or a function that maps from a', '    vector to a (min, max) interval, or None to hide errorbar. See the', '    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.', 'boot_kws', '    Additional keywords are passed to bootstrap when error_method is \"ci\".'], single_element_is_type=False, self['Attributes']=[], self['Examples']='', self['Extended Summary']=[], self['Methods']=[], self['Notes']=[], self['Other Parameters']=[], self['Parameters']=[], self['Raises']=[], self['Receives']=[], self['References']='', self['Returns']=[], self['See Also']=[], self['Signature']='', self['Summary']=['Data aggregator that produces an estimate and error bar interval.'], self['Warnings']=[], self['Warns']=[], self['Yields']=[], self['index']={})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "r = Reader(content)\n",
      "State:\n",
      "{_str=['estimator : callable or string', '    Function (or method name) that maps a vector to a scalar.', 'errorbar : string, (string, number) tuple, or callable', '    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple', '    with a method name and a level parameter, or a function that maps from a', '    vector to a (min, max) interval, or None to hide errorbar. See the', '    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.', 'boot_kws', '    Additional keywords are passed to bootstrap when error_method is \"ci\".'], _l=0}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_param_list(self, content, single_element_is_type=False):\n",
      "        r = Reader(content)\n",
      "        params = []\n",
      "        while not r.eof():\n",
      "            header = r.read().strip()\n",
      "            if ' : ' in header:\n",
      "                arg_name, arg_type = header.split(' : ')[:2]\n",
      "            else:\n",
      "                if single_element_is_type:\n",
      "                    arg_name, arg_type = '', header\n",
      "                else:\n",
      "                    arg_name, arg_type = header, ''\n",
      "            desc = r.read_to_next_unindented_line()\n",
      "            desc = dedent_lines(desc)\n",
      "            desc = strip_blank_lines(desc)\n",
      "            params.append(Parameter(arg_name, arg_type, desc))\n",
      "        return params\n",
      "_parse_param_list(self=<seaborn.external.docscrape.NumpyDocString object at 0x7f45d9d15c10>, content=['estimator : callable or string', '    Function (or method name) that maps a vector to a scalar.', 'errorbar : string, (string, number) tuple, or callable', '    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple', '    with a method name and a level parameter, or a function that maps from a', '    vector to a (min, max) interval, or None to hide errorbar. See the', '    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.', 'boot_kws', '    Additional keywords are passed to bootstrap when error_method is \"ci\".'], single_element_is_type=False, self['Attributes']=[], self['Examples']='', self['Extended Summary']=[], self['Methods']=[], self['Notes']=[], self['Other Parameters']=[], self['Parameters']=[], self['Raises']=[], self['Receives']=[], self['References']='', self['Returns']=[], self['See Also']=[], self['Signature']='', self['Summary']=['Data aggregator that produces an estimate and error bar interval.'], self['Warnings']=[], self['Warns']=[], self['Yields']=[], self['index']={})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "params = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def read(self):\n",
      "        if not self.eof():\n",
      "            out = self[self._l]\n",
      "            self._l += 1\n",
      "            return out\n",
      "        else:\n",
      "            return ''\n",
      "read(self=<seaborn.external.docscrape.Reader object at 0x7f45d516c9a0>, self._l=0, self._str=['estimator : callable or string', '    Function (or method name) that maps a vector to a scalar.', 'errorbar : string, (string, number) tuple, or callable', '    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple', '    with a method name and a level parameter, or a function that maps from a', '    vector to a (min, max) interval, or None to hide errorbar. See the', '    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.', 'boot_kws', '    Additional keywords are passed to bootstrap when error_method is \"ci\".'])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "out = self[self._l]\n",
      "State:\n",
      "'estimator : callable or string'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def read(self):\n",
      "        if not self.eof():\n",
      "            out = self[self._l]\n",
      "            self._l += 1\n",
      "            return out\n",
      "        else:\n",
      "            return ''\n",
      "read(self=<seaborn.external.docscrape.Reader object at 0x7f45d516c9a0>, self._l=0, self._str=['estimator : callable or string', '    Function (or method name) that maps a vector to a scalar.', 'errorbar : string, (string, number) tuple, or callable', '    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple', '    with a method name and a level parameter, or a function that maps from a', '    vector to a (min, max) interval, or None to hide errorbar. See the', '    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.', 'boot_kws', '    Additional keywords are passed to bootstrap when error_method is \"ci\".'])\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self._l += 1\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def read_to_next_unindented_line(self):\n",
      "        def is_unindented(line):\n",
      "            return (line.strip() and (len(line.lstrip()) == len(line)))\n",
      "        return self.read_to_condition(is_unindented)\n",
      "read_to_next_unindented_line(self=<seaborn.external.docscrape.Reader object at 0x7f45d516c9a0>, self._l=1, self._str=['estimator : callable or string', '    Function (or method name) that maps a vector to a scalar.', 'errorbar : string, (string, number) tuple, or callable', '    Name of errorbar method (either \"ci\", \"pi\", \"se\", or \"sd\"), or a tuple', '    with a method name and a level parameter, or a function that maps from a', '    vector to a (min, max) interval, or None to hide errorbar. See the', '    :doc:`errorbar tutorial </tutorial/error_bars>` for more information.', 'boot_kws', '    Additional keywords are passed to bootstrap when error_method is \"ci\".'])\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "return self.read_to_condition(is_unindented)\n",
      "State:\n",
      "2\n",
      "==================================================\n",
      "Clean Code:\n",
      "def strip_blank_lines(l):\n",
      "    \"Remove leading and trailing blank lines from a list of lines\"\n",
      "    while l and not l[0].strip():\n",
      "        del l[0]\n",
      "    while l and not l[-1].strip():\n",
      "        del l[-1]\n",
      "    return l\n",
      "strip_blank_lines(l=['Aggregate statistic to compute in each bin.', '', '- `count`: show the number of observations in each bin', '- `frequency`: show the number of observations divided by the bin width', '- `probability` or `proportion`: normalize such that bar heights sum to 1', '- `percent`: normalize such that bar heights sum to 100', '- `density`: normalize such that the total area of the histogram equals 1', ''])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "del l[-1]\n",
      "State:\n",
      "['Aggregate statistic to compute in each bin.', '', '- `count`: show the number of observations in each bin', '- `frequency`: show the number of observations divided by the bin width', '- `probability` or `proportion`: normalize such that bar heights sum to 1', '- `percent`: normalize such that bar heights sum to 100', '- `density`: normalize such that the total area of the histogram equals 1']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_cluster_initialized_without_leader(leader=False, failover=None, sync=None, cluster_config=None, failsafe=False):\n",
      "    m1 = Member(0, 'leader', 28, {'conn_url': 'postgres://replicator:rep-pass@127.0.0.1:5435/postgres',\n",
      "                                  'api_url': 'http://127.0.0.1:8008/patroni', 'xlog_location': 4,\n",
      "                                  'role': 'primary', 'state': 'running'})\n",
      "    leader = Leader(0, 0, m1 if leader else Member(0, '', 28, {}))\n",
      "    m2 = Member(0, 'other', 28, {'conn_url': 'postgres://replicator:rep-pass@127.0.0.1:5436/postgres',\n",
      "                                 'api_url': 'http://127.0.0.1:8011/patroni',\n",
      "                                 'state': 'running',\n",
      "                                 'pause': True,\n",
      "                                 'tags': {'clonefrom': True},\n",
      "                                 'scheduled_restart': {'schedule': \"2100-01-01 10:53:07.560445+00:00\",\n",
      "                                                       'postgres_version': '99.0.0'}})\n",
      "    syncstate = SyncState(0 if sync else None, sync and sync[0], sync and sync[1])\n",
      "    failsafe = {m.name: m.api_url for m in (m1, m2)} if failsafe else None\n",
      "    return get_cluster(SYSID, leader, [m1, m2], failover, syncstate, cluster_config, failsafe)\n",
      "get_cluster_initialized_without_leader(leader=True, failover=None, sync=None, cluster_config=None, failsafe=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "m1 = Member(0, 'leader', 28, {'conn_url': 'postgres://replicator:rep-pass@127.0.0.1:5435/postgres',\n",
      "State:\n",
      "Member(version=0, name='leader', session=28, data={'conn_url': 'postgres://replicator:rep-pass@127.0.0.1:5435/postgres', 'api_url': 'http://127.0.0.1:8008/patroni', 'xlog_location': 4, 'role': 'primary', 'state': 'running'})\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_cluster_initialized_without_leader(leader=False, failover=None, sync=None, cluster_config=None, failsafe=False):\n",
      "    m1 = Member(0, 'leader', 28, {'conn_url': 'postgres://replicator:rep-pass@127.0.0.1:5435/postgres',\n",
      "                                  'api_url': 'http://127.0.0.1:8008/patroni', 'xlog_location': 4,\n",
      "                                  'role': 'primary', 'state': 'running'})\n",
      "    leader = Leader(0, 0, m1 if leader else Member(0, '', 28, {}))\n",
      "    m2 = Member(0, 'other', 28, {'conn_url': 'postgres://replicator:rep-pass@127.0.0.1:5436/postgres',\n",
      "                                 'api_url': 'http://127.0.0.1:8011/patroni',\n",
      "                                 'state': 'running',\n",
      "                                 'pause': True,\n",
      "                                 'tags': {'clonefrom': True},\n",
      "                                 'scheduled_restart': {'schedule': \"2100-01-01 10:53:07.560445+00:00\",\n",
      "                                                       'postgres_version': '99.0.0'}})\n",
      "    syncstate = SyncState(0 if sync else None, sync and sync[0], sync and sync[1])\n",
      "    failsafe = {m.name: m.api_url for m in (m1, m2)} if failsafe else None\n",
      "    return get_cluster(SYSID, leader, [m1, m2], failover, syncstate, cluster_config, failsafe)\n",
      "get_cluster_initialized_without_leader(leader=True, failover=None, sync=None, cluster_config=None, failsafe=False)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "leader = Leader(0, 0, m1 if leader else Member(0, '', 28, {}))\n",
      "State:\n",
      "Leader(version=0, session=0, member=Member(version=0, name='leader', session=28, data={'conn_url': 'postgres://replicator:rep-pass@127.0.0.1:5435/postgres', 'api_url': 'http://127.0.0.1:8008/patroni', 'xlog_location': 4, 'role': 'primary', 'state': 'running'}))\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_cluster_initialized_without_leader(leader=False, failover=None, sync=None, cluster_config=None, failsafe=False):\n",
      "    m1 = Member(0, 'leader', 28, {'conn_url': 'postgres://replicator:rep-pass@127.0.0.1:5435/postgres',\n",
      "                                  'api_url': 'http://127.0.0.1:8008/patroni', 'xlog_location': 4,\n",
      "                                  'role': 'primary', 'state': 'running'})\n",
      "    leader = Leader(0, 0, m1 if leader else Member(0, '', 28, {}))\n",
      "    m2 = Member(0, 'other', 28, {'conn_url': 'postgres://replicator:rep-pass@127.0.0.1:5436/postgres',\n",
      "                                 'api_url': 'http://127.0.0.1:8011/patroni',\n",
      "                                 'state': 'running',\n",
      "                                 'pause': True,\n",
      "                                 'tags': {'clonefrom': True},\n",
      "                                 'scheduled_restart': {'schedule': \"2100-01-01 10:53:07.560445+00:00\",\n",
      "                                                       'postgres_version': '99.0.0'}})\n",
      "    syncstate = SyncState(0 if sync else None, sync and sync[0], sync and sync[1])\n",
      "    failsafe = {m.name: m.api_url for m in (m1, m2)} if failsafe else None\n",
      "    return get_cluster(SYSID, leader, [m1, m2], failover, syncstate, cluster_config, failsafe)\n",
      "get_cluster_initialized_without_leader(leader=True, failover=None, sync=None, cluster_config=None, failsafe=False)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "m2 = Member(0, 'other', 28, {'conn_url': 'postgres://replicator:rep-pass@127.0.0.1:5436/postgres',\n",
      "State:\n",
      "Member(version=0, name='other', session=28, data={'conn_url': 'postgres://replicator:rep-pass@127.0.0.1:5436/postgres', 'api_url': 'http://127.0.0.1:8011/patroni', 'state': 'running', 'pause': True, 'tags': {'clonefrom': True}, 'scheduled_restart': {'schedule': '2100-01-01 10:53:07.560445+00:00', 'postgres_version': '99.0.0'}})\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_cluster(initialize, leader, members, failover, sync, cluster_config=None, failsafe=None):\n",
      "    t = datetime.datetime.now().isoformat()\n",
      "    history = TimelineHistory(1, '[[1,67197376,\"no recovery target specified\",\"' + t + '\",\"foo\"]]',\n",
      "                              [(1, 67197376, 'no recovery target specified', t, 'foo')])\n",
      "    cluster_config = cluster_config or ClusterConfig(1, {'check_timeline': True}, 1)\n",
      "    return Cluster(initialize, cluster_config, leader, Status(10, None), members, failover, sync, history, failsafe)\n",
      "get_cluster(initialize='12345678901', leader=Leader(version=0, session=0, member=Member(version=0, name='leader', session=28, data={'conn_url': 'postgres://replicator:rep-pass@127.0.0.1:5435/postgres', 'api_url': 'http://127.0.0.1:8008/patroni', 'xlog_location': 4, 'role': 'primary', 'state': 'running'})), members=[Member(version=0, name='leader', session=28, data={'conn_url': 'postgres://replicator:rep-pass@127.0.0.1:5435/postgres', 'api_url': 'http://127.0.0.1:8008/patroni', 'xlog_location': 4, 'role': 'primary', 'state': 'running'}), Member(version=0, name='other', session=28, data={'conn_url': 'postgres://replicator:rep-pass@127.0.0.1:5436/postgres', 'api_url': 'http://127.0.0.1:8011/patroni', 'state': 'running', 'pause': True, 'tags': {'clonefrom': True}, 'scheduled_restart': {'schedule': '2100-01-01 10:53:07.560445+00:00', 'postgres_version': '99.0.0'}})], failover=None, sync=SyncState(version=None, leader=None, sync_standby=None), cluster_config=None, failsafe=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "t = datetime.datetime.now().isoformat()\n",
      "State:\n",
      "'2024-04-04T12:10:52.963799'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_cluster(initialize, leader, members, failover, sync, cluster_config=None, failsafe=None):\n",
      "    t = datetime.datetime.now().isoformat()\n",
      "    history = TimelineHistory(1, '[[1,67197376,\"no recovery target specified\",\"' + t + '\",\"foo\"]]',\n",
      "                              [(1, 67197376, 'no recovery target specified', t, 'foo')])\n",
      "    cluster_config = cluster_config or ClusterConfig(1, {'check_timeline': True}, 1)\n",
      "    return Cluster(initialize, cluster_config, leader, Status(10, None), members, failover, sync, history, failsafe)\n",
      "get_cluster(initialize='12345678901', leader=Leader(version=0, session=0, member=Member(version=0, name='leader', session=28, data={'conn_url': 'postgres://replicator:rep-pass@127.0.0.1:5435/postgres', 'api_url': 'http://127.0.0.1:8008/patroni', 'xlog_location': 4, 'role': 'primary', 'state': 'running'})), members=[Member(version=0, name='leader', session=28, data={'conn_url': 'postgres://replicator:rep-pass@127.0.0.1:5435/postgres', 'api_url': 'http://127.0.0.1:8008/patroni', 'xlog_location': 4, 'role': 'primary', 'state': 'running'}), Member(version=0, name='other', session=28, data={'conn_url': 'postgres://replicator:rep-pass@127.0.0.1:5436/postgres', 'api_url': 'http://127.0.0.1:8011/patroni', 'state': 'running', 'pause': True, 'tags': {'clonefrom': True}, 'scheduled_restart': {'schedule': '2100-01-01 10:53:07.560445+00:00', 'postgres_version': '99.0.0'}})], failover=None, sync=SyncState(version=None, leader=None, sync_standby=None), cluster_config=None, failsafe=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "history = TimelineHistory(1, '[[1,67197376,\"no recovery target specified\",\"' + t + '\",\"foo\"]]',\n",
      "State:\n",
      "TimelineHistory(version=1, value='[[1,67197376,\"no recovery target specified\",\"2024-04-04T12:10:52.963799\",\"foo\"]]', lines=[(1, 67197376, 'no recovery target specified', '2024-04-04T12:10:52.963799', 'foo')])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_cluster(initialize, leader, members, failover, sync, cluster_config=None, failsafe=None):\n",
      "    t = datetime.datetime.now().isoformat()\n",
      "    history = TimelineHistory(1, '[[1,67197376,\"no recovery target specified\",\"' + t + '\",\"foo\"]]',\n",
      "                              [(1, 67197376, 'no recovery target specified', t, 'foo')])\n",
      "    cluster_config = cluster_config or ClusterConfig(1, {'check_timeline': True}, 1)\n",
      "    return Cluster(initialize, cluster_config, leader, Status(10, None), members, failover, sync, history, failsafe)\n",
      "get_cluster(initialize='12345678901', leader=Leader(version=0, session=0, member=Member(version=0, name='leader', session=28, data={'conn_url': 'postgres://replicator:rep-pass@127.0.0.1:5435/postgres', 'api_url': 'http://127.0.0.1:8008/patroni', 'xlog_location': 4, 'role': 'primary', 'state': 'running'})), members=[Member(version=0, name='leader', session=28, data={'conn_url': 'postgres://replicator:rep-pass@127.0.0.1:5435/postgres', 'api_url': 'http://127.0.0.1:8008/patroni', 'xlog_location': 4, 'role': 'primary', 'state': 'running'}), Member(version=0, name='other', session=28, data={'conn_url': 'postgres://replicator:rep-pass@127.0.0.1:5436/postgres', 'api_url': 'http://127.0.0.1:8011/patroni', 'state': 'running', 'pause': True, 'tags': {'clonefrom': True}, 'scheduled_restart': {'schedule': '2100-01-01 10:53:07.560445+00:00', 'postgres_version': '99.0.0'}})], failover=None, sync=SyncState(version=None, leader=None, sync_standby=None), cluster_config=None, failsafe=None)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "cluster_config = cluster_config or ClusterConfig(1, {'check_timeline': True}, 1)\n",
      "State:\n",
      "ClusterConfig(version=1, data={'check_timeline': True}, modify_version=1)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def matches_condition(self, condition):\n",
      "        result = EventMatchResult(is_match=False, parsed_args=self.args)\n",
      "        match_scores = []\n",
      "        if not isinstance(self, condition.type):\n",
      "            return result\n",
      "        for (attr, value) in condition.args.items():\n",
      "            if attr not in self.args:\n",
      "                return result\n",
      "            if isinstance(self.args[attr], str):\n",
      "                arg_result = self._matches_argument(argname=attr, condition_value=value)\n",
      "                if arg_result.is_match:\n",
      "                    match_scores.append(arg_result.score)\n",
      "                    for (parsed_arg, parsed_value) in arg_result.parsed_args.items():\n",
      "                        result.parsed_args[parsed_arg] = parsed_value\n",
      "                else:\n",
      "                    return result\n",
      "            elif self.args[attr] != value:\n",
      "                return result\n",
      "        result.is_match = True\n",
      "        if match_scores:\n",
      "            result.score = sum(match_scores) / float(len(match_scores))\n",
      "        return result\n",
      "matches_condition(self=<platypush.message.event.ping.PingEvent object at 0x7fdf0276b550>, condition={args={'message': 'This is (the)? answer: ${answer}'}, parsed_args={}, priority=None}, self.args={'message': 'GARBAGE GARBAGE this is the answer: 42'}, self.disable_logging=False, self.disable_web_clients_notification=False, self.id='2acb458f5d180c435e474a042dd85c80', self.message='GARBAGE GARBAGE this is the answer: 42', self.origin='thor', self.target='thor', self.timestamp=1712171911.5562408, self.type='platypush.message.event.ping.PingEvent')\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "match_scores.append(arg_result.score)\n",
      "State:\n",
      "[4.75]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def register(self, serializer: ProtoSerializer) -> None:\n",
      "        self._serializers[serializer.supported_format] = serializer\n",
      "        self._extension_to_format.update(\n",
      "            {ext: serializer.supported_format for ext in serializer.file_extensions}\n",
      "        )\n",
      "register(self=<onnx.serialization._Registry object at 0x7f2c88f6ee80>, serializer={}, self._extension_to_format={}, self._serializers={})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self._extension_to_format.update(\n",
      "State:\n",
      "{'.pb': 'protobuf', '.onnx': 'protobuf'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _is_scope_subset(needle_scope, haystack_scope):\n",
      "        needle_scope = set(needle_scope.split()) if needle_scope else set()\n",
      "        haystack_scope = (\n",
      "            set(haystack_scope.split()) if haystack_scope else set()\n",
      "        )\n",
      "        return needle_scope <= haystack_scope\n",
      "_is_scope_subset(needle_scope='playlist-modify-private', haystack_scope='playlist-modify-public')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "needle_scope = set(needle_scope.split()) if needle_scope else set()\n",
      "State:\n",
      "{'playlist-modify-private'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _is_scope_subset(needle_scope, haystack_scope):\n",
      "        needle_scope = set(needle_scope.split()) if needle_scope else set()\n",
      "        haystack_scope = (\n",
      "            set(haystack_scope.split()) if haystack_scope else set()\n",
      "        )\n",
      "        return needle_scope <= haystack_scope\n",
      "_is_scope_subset(needle_scope='playlist-modify-private', haystack_scope='playlist-modify-public')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "haystack_scope = (\n",
      "State:\n",
      "{'playlist-modify-public'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_authorize_url(self, state=None):\n",
      "        payload = {\n",
      "            \"client_id\": self.client_id,\n",
      "            \"response_type\": \"code\",\n",
      "            \"redirect_uri\": self.redirect_uri,\n",
      "        }\n",
      "        if self.scope:\n",
      "            payload[\"scope\"] = self.scope\n",
      "        if state is None:\n",
      "            state = self.state\n",
      "        if state is not None:\n",
      "            payload[\"state\"] = state\n",
      "        if self.show_dialog:\n",
      "            payload[\"show_dialog\"] = True\n",
      "        urlparams = urllibparse.urlencode(payload)\n",
      "        return \"%s?%s\" % (self.OAUTH_AUTHORIZE_URL, urlparams)\n",
      "get_authorize_url(self=<spotipy.oauth2.SpotifyOAuth object at 0x7f1bd5dec8b0>, state=None, self._client_id='CLID', self._client_secret='CLISEC', self._redirect_uri='REDIR', self._session=<requests.sessions.Session object at 0x7f1bd5dec820>, self.cache_handler=<spotipy.cache_handler.CacheFileHandler object at 0x7f1bd5decd90>, self.open_browser=True, self.proxies=None, self.requests_timeout=None, self.scope=None, self.show_dialog=False, self.state=None)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "payload = {\n",
      "State:\n",
      "{'client_id': 'CLID', 'response_type': 'code', 'redirect_uri': 'REDIR'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_pkce_handshake_parameters(self):\n",
      "        self.code_verifier = self._get_code_verifier()\n",
      "        self.code_challenge = self._get_code_challenge()\n",
      "get_pkce_handshake_parameters(self=<spotipy.oauth2.SpotifyPKCE object at 0x7f1bd5f049a0>, self._client_id='CLID', self._code_challenge_method='S256', self._redirect_uri='REDIR', self._session=<requests.sessions.Session object at 0x7f1bd5f04d00>, self.authorization_code=None, self.cache_handler=<spotipy.cache_handler.CacheFileHandler object at 0x7f1bd59a7f70>, self.code_challenge=None, self.code_verifier=None, self.open_browser=True, self.proxies=None, self.requests_timeout=None, self.scope=None, self.state=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.code_verifier = self._get_code_verifier()\n",
      "State:\n",
      "'BCc_VANl0DLnXX7nG6bu5uiaScLjEaAulB04vnAPCnLwXBmHqe0eV9jdNiQt01GHvADzcjJauVXAKbNrbAoonw'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_pkce_handshake_parameters(self):\n",
      "        self.code_verifier = self._get_code_verifier()\n",
      "        self.code_challenge = self._get_code_challenge()\n",
      "get_pkce_handshake_parameters(self=<spotipy.oauth2.SpotifyPKCE object at 0x7f1bd5f049a0>, self._client_id='CLID', self._code_challenge_method='S256', self._redirect_uri='REDIR', self._session=<requests.sessions.Session object at 0x7f1bd5f04d00>, self.authorization_code=None, self.cache_handler=<spotipy.cache_handler.CacheFileHandler object at 0x7f1bd59a7f70>, self.code_challenge=None, self.code_verifier=None, self.open_browser=True, self.proxies=None, self.requests_timeout=None, self.scope=None, self.state=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.code_challenge = self._get_code_challenge()\n",
      "State:\n",
      "'O3YCxENAclCtYO1Zuodb2RqCB6FUmbBEdDG_VqAtA_E'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def call_on_comment(*args, **kwargs):\n",
      "    global events\n",
      "    repo_full_name = kwargs[\"repo_full_name\"]\n",
      "    pr_id = kwargs[\"pr_number\"]\n",
      "    key = f\"{repo_full_name}-{pr_id}\"\n",
      "    thread = events.get(key, None)\n",
      "    if thread:\n",
      "        terminate_thread(thread)\n",
      "    thread = threading.Thread(target=run_comment, args=args, kwargs=kwargs)\n",
      "    events[key] = thread\n",
      "    thread.start()\n",
      "call_on_comment(args=(), kwargs={'repo_full_name': 'exampleRepo', 'pr_number': 1})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "repo_full_name = kwargs[\"repo_full_name\"]\n",
      "State:\n",
      "'exampleRepo'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def call_on_comment(*args, **kwargs):\n",
      "    global events\n",
      "    repo_full_name = kwargs[\"repo_full_name\"]\n",
      "    pr_id = kwargs[\"pr_number\"]\n",
      "    key = f\"{repo_full_name}-{pr_id}\"\n",
      "    thread = events.get(key, None)\n",
      "    if thread:\n",
      "        terminate_thread(thread)\n",
      "    thread = threading.Thread(target=run_comment, args=args, kwargs=kwargs)\n",
      "    events[key] = thread\n",
      "    thread.start()\n",
      "call_on_comment(args=(), kwargs={'repo_full_name': 'exampleRepo', 'pr_number': 1})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "pr_id = kwargs[\"pr_number\"]\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def call_on_comment(*args, **kwargs):\n",
      "    global events\n",
      "    repo_full_name = kwargs[\"repo_full_name\"]\n",
      "    pr_id = kwargs[\"pr_number\"]\n",
      "    key = f\"{repo_full_name}-{pr_id}\"\n",
      "    thread = events.get(key, None)\n",
      "    if thread:\n",
      "        terminate_thread(thread)\n",
      "    thread = threading.Thread(target=run_comment, args=args, kwargs=kwargs)\n",
      "    events[key] = thread\n",
      "    thread.start()\n",
      "call_on_comment(args=(), kwargs={'repo_full_name': 'exampleRepo', 'pr_number': 1})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "key = f\"{repo_full_name}-{pr_id}\"\n",
      "State:\n",
      "'exampleRepo-1'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def driver():\n",
      "    TEST_BROWSER = os.environ.get(\"TEST_BROWSER\", \"chrome\").lower()\n",
      "    if TEST_BROWSER == \"chrome\":\n",
      "        options = webdriver.ChromeOptions()\n",
      "        options.headless = True\n",
      "        capabilities = DesiredCapabilities.CHROME\n",
      "        capabilities[\"goog:loggingPrefs\"] = {\"browser\": \"ALL\"}\n",
      "        if platform.system() == \"Windows\":\n",
      "            options.binary_location = \"C:/Program Files/Google/Chrome/Application/chrome.exe\"\n",
      "        driver = webdriver.Chrome(\n",
      "            ChromeDriverManager().install(),\n",
      "            options=options,\n",
      "            desired_capabilities=capabilities,\n",
      "            service_log_path=os.path.devnull,\n",
      "        )\n",
      "    else:\n",
      "        raise ValueError(f\"Unsupported browser for testing: {TEST_BROWSER}\")\n",
      "    with mock.patch(\"eel.browsers.open\"):\n",
      "        yield driver\n",
      "driver()\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "TEST_BROWSER = os.environ.get(\"TEST_BROWSER\", \"chrome\").lower()\n",
      "State:\n",
      "'chrome'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def driver():\n",
      "    TEST_BROWSER = os.environ.get(\"TEST_BROWSER\", \"chrome\").lower()\n",
      "    if TEST_BROWSER == \"chrome\":\n",
      "        options = webdriver.ChromeOptions()\n",
      "        options.headless = True\n",
      "        capabilities = DesiredCapabilities.CHROME\n",
      "        capabilities[\"goog:loggingPrefs\"] = {\"browser\": \"ALL\"}\n",
      "        if platform.system() == \"Windows\":\n",
      "            options.binary_location = \"C:/Program Files/Google/Chrome/Application/chrome.exe\"\n",
      "        driver = webdriver.Chrome(\n",
      "            ChromeDriverManager().install(),\n",
      "            options=options,\n",
      "            desired_capabilities=capabilities,\n",
      "            service_log_path=os.path.devnull,\n",
      "        )\n",
      "    else:\n",
      "        raise ValueError(f\"Unsupported browser for testing: {TEST_BROWSER}\")\n",
      "    with mock.patch(\"eel.browsers.open\"):\n",
      "        yield driver\n",
      "driver()\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "options = webdriver.ChromeOptions()\n",
      "State:\n",
      "{_caps={'browserName': 'chrome', 'goog:loggingPrefs': {'browser': 'ALL'}, 'pageLoadStrategy': <PageLoadStrategy.normal: 'normal'>}, _proxy=None, mobile_options=None, _arguments=[], _ignore_local_proxy=False, _binary_location='', _extension_files=[], _extensions=[], _experimental_options={}, _debugger_address=None}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def driver():\n",
      "    TEST_BROWSER = os.environ.get(\"TEST_BROWSER\", \"chrome\").lower()\n",
      "    if TEST_BROWSER == \"chrome\":\n",
      "        options = webdriver.ChromeOptions()\n",
      "        options.headless = True\n",
      "        capabilities = DesiredCapabilities.CHROME\n",
      "        capabilities[\"goog:loggingPrefs\"] = {\"browser\": \"ALL\"}\n",
      "        if platform.system() == \"Windows\":\n",
      "            options.binary_location = \"C:/Program Files/Google/Chrome/Application/chrome.exe\"\n",
      "        driver = webdriver.Chrome(\n",
      "            ChromeDriverManager().install(),\n",
      "            options=options,\n",
      "            desired_capabilities=capabilities,\n",
      "            service_log_path=os.path.devnull,\n",
      "        )\n",
      "    else:\n",
      "        raise ValueError(f\"Unsupported browser for testing: {TEST_BROWSER}\")\n",
      "    with mock.patch(\"eel.browsers.open\"):\n",
      "        yield driver\n",
      "driver()\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "options.headless = True\n",
      "State:\n",
      "{_caps={'browserName': 'chrome', 'goog:loggingPrefs': {'browser': 'ALL'}, 'pageLoadStrategy': <PageLoadStrategy.normal: 'normal'>}, _proxy=None, mobile_options=None, _arguments=[], _ignore_local_proxy=False, _binary_location='', _extension_files=[], _extensions=[], _experimental_options={}, _debugger_address=None, headless=True}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def after_bind(self, node, kw):\n",
      "        if not self.get(\"header\"):\n",
      "            self[\"header\"] = HeaderSchema()\n",
      "        if not self.get(\"querystring\"):\n",
      "            self[\"querystring\"] = QuerySchema()\n",
      "after_bind(self=<kinto.core.resource.schema.PayloadRequestSchema object at 140040699356256 (named )>, node=<kinto.core.resource.schema.PayloadRequestSchema object at 140040699356256 (named )>, kw={'header': <kinto.core.resource.schema.PatchHeaderSchema object at 140040699356112 (named header)>}, self._order=55, self.bindings={'header': <kinto.core.resource.schema.PatchHeaderSchema object at 140040699356112 (named header)>}, self.body=<colander.deferred object at 0x7f5dc423a5e0>, self.children=[<kinto.core.resource.schema.PatchHeaderSchema object at 140040699356112 (named header)>], self.querystring=None, self.title='', self.typ=<colander.Mapping object at 0x7f5dc423a2e0>)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "self[\"querystring\"] = QuerySchema()\n",
      "State:\n",
      "[<kinto.core.resource.schema.PatchHeaderSchema object at 140040699356112 (named header)>, <kinto.core.resource.schema.QuerySchema object at 140040699356352 (named querystring)>]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def hmac_digest(secret, message, encoding=\"utf-8\"):\n",
      "    if isinstance(secret, str):\n",
      "        secret = secret.encode(encoding)\n",
      "    return hmac.new(secret, message.encode(encoding), hashlib.sha256).hexdigest()\n",
      "hmac_digest(secret='secret_hmac_for_userids', message='mat:secret', encoding='utf-8')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "secret = secret.encode(encoding)\n",
      "State:\n",
      "b'secret_hmac_for_userids'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def hex_to_rgb(hex_):\n",
      "            hex_ = hex_.lstrip('\n",
      "            r, g, b = tuple(int(hex_[i:i + 2], 16) for i in (0, 2, 4))\n",
      "            return svgwrite.utils.rgb(r, g, b)\n",
      "hex_to_rgb(hex_='\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "hex_ = hex_.lstrip('#')\n",
      "State:\n",
      "'ffffff'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def translate(self, x, y=None):\n",
      "        if y is None:\n",
      "            y = x\n",
      "        self.x += x\n",
      "        self.y += y\n",
      "translate(self=Point, x=3, y=2, self.x=4.0, self.y=2.1)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.x += x\n",
      "State:\n",
      "7.0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def translate(self, x, y=None):\n",
      "        if y is None:\n",
      "            y = x\n",
      "        self.x += x\n",
      "        self.y += y\n",
      "translate(self=Point, x=3, y=2, self.x=4.0, self.y=2.1)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "self.y += y\n",
      "State:\n",
      "4.1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse_csv(csv):\n",
      "    lines = csv.strip().split('\\n')\n",
      "    lines = [line for line in lines if line.strip()]\n",
      "    csv = '\\n'.join(lines)\n",
      "    if autoeq_pattern.match(csv):\n",
      "        columns = lines[0].split(',')\n",
      "        return {column: [float(line.split(',')[i]) for line in lines[1:]] for i, column in enumerate(columns)}\n",
      "    if rew_pattern.match(csv) or crinacle_pattern.match(csv):\n",
      "        csv = '\\n'.join([re.sub(r'(?:, ?| |\\t)', '\\t', line) for line in lines if numeric_start.match(line) and '?' not in line])\n",
      "        lines = csv.split('\\n')\n",
      "    column_separator, decimal_separator = find_csv_separators(csv)\n",
      "    columns = find_csv_columns(csv, column_separator)\n",
      "    if columns is None:\n",
      "        ixs = {'frequency': 0, 'raw': 1}\n",
      "    else:\n",
      "        ixs = {'frequency': None, 'raw': None}\n",
      "        for i, column in enumerate(columns):\n",
      "            if re.match(r'^freq', column, flags=re.IGNORECASE):\n",
      "                ixs['frequency'] = i\n",
      "            if re.match(r'^(?:spl|gain|ampl|raw)', column, flags=re.IGNORECASE):\n",
      "                ixs['raw'] = i\n",
      "        if ixs['frequency'] is None:\n",
      "            if len(columns) == 2:\n",
      "                ixs = {'frequency': 0, 'raw': 1}\n",
      "            else:\n",
      "                raise CsvParseError('Failed to find frequency column')\n",
      "        if ixs['raw'] is None:\n",
      "            raise CsvParseError('Failed to find SPL column')\n",
      "    data_line_pattern = re.compile(rf'^-?\\d+(?:{column_separator}\\d+)?')\n",
      "    data = {'frequency': [], 'raw': []}\n",
      "    for line in lines:\n",
      "        if not data_line_pattern.match(line):\n",
      "            continue\n",
      "        cells = line.split(column_separator)\n",
      "        if decimal_separator == ',':\n",
      "            cells = [float(cell.replace(',', '.')) for cell in cells]\n",
      "        else:\n",
      "            cells = [float(cell) for cell in cells]\n",
      "        for column, ix in ixs.items():\n",
      "            data[column].append(cells[ix])\n",
      "    return data\n",
      "parse_csv(csv='frequency,raw\\n20,0\\n1000,3\\n20000,0\\n')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "lines = csv.strip().split('\\n')\n",
      "State:\n",
      "['frequency,raw', '20,0', '1000,3', '20000,0']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse_csv(csv):\n",
      "    lines = csv.strip().split('\\n')\n",
      "    lines = [line for line in lines if line.strip()]\n",
      "    csv = '\\n'.join(lines)\n",
      "    if autoeq_pattern.match(csv):\n",
      "        columns = lines[0].split(',')\n",
      "        return {column: [float(line.split(',')[i]) for line in lines[1:]] for i, column in enumerate(columns)}\n",
      "    if rew_pattern.match(csv) or crinacle_pattern.match(csv):\n",
      "        csv = '\\n'.join([re.sub(r'(?:, ?| |\\t)', '\\t', line) for line in lines if numeric_start.match(line) and '?' not in line])\n",
      "        lines = csv.split('\\n')\n",
      "    column_separator, decimal_separator = find_csv_separators(csv)\n",
      "    columns = find_csv_columns(csv, column_separator)\n",
      "    if columns is None:\n",
      "        ixs = {'frequency': 0, 'raw': 1}\n",
      "    else:\n",
      "        ixs = {'frequency': None, 'raw': None}\n",
      "        for i, column in enumerate(columns):\n",
      "            if re.match(r'^freq', column, flags=re.IGNORECASE):\n",
      "                ixs['frequency'] = i\n",
      "            if re.match(r'^(?:spl|gain|ampl|raw)', column, flags=re.IGNORECASE):\n",
      "                ixs['raw'] = i\n",
      "        if ixs['frequency'] is None:\n",
      "            if len(columns) == 2:\n",
      "                ixs = {'frequency': 0, 'raw': 1}\n",
      "            else:\n",
      "                raise CsvParseError('Failed to find frequency column')\n",
      "        if ixs['raw'] is None:\n",
      "            raise CsvParseError('Failed to find SPL column')\n",
      "    data_line_pattern = re.compile(rf'^-?\\d+(?:{column_separator}\\d+)?')\n",
      "    data = {'frequency': [], 'raw': []}\n",
      "    for line in lines:\n",
      "        if not data_line_pattern.match(line):\n",
      "            continue\n",
      "        cells = line.split(column_separator)\n",
      "        if decimal_separator == ',':\n",
      "            cells = [float(cell.replace(',', '.')) for cell in cells]\n",
      "        else:\n",
      "            cells = [float(cell) for cell in cells]\n",
      "        for column, ix in ixs.items():\n",
      "            data[column].append(cells[ix])\n",
      "    return data\n",
      "parse_csv(csv='frequency,raw\\n20,0\\n1000,3\\n20000,0\\n')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "csv = '\\n'.join(lines)\n",
      "State:\n",
      "'frequency,raw\\n20,0\\n1000,3\\n20000,0'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse_csv(csv):\n",
      "    lines = csv.strip().split('\\n')\n",
      "    lines = [line for line in lines if line.strip()]\n",
      "    csv = '\\n'.join(lines)\n",
      "    if autoeq_pattern.match(csv):\n",
      "        columns = lines[0].split(',')\n",
      "        return {column: [float(line.split(',')[i]) for line in lines[1:]] for i, column in enumerate(columns)}\n",
      "    if rew_pattern.match(csv) or crinacle_pattern.match(csv):\n",
      "        csv = '\\n'.join([re.sub(r'(?:, ?| |\\t)', '\\t', line) for line in lines if numeric_start.match(line) and '?' not in line])\n",
      "        lines = csv.split('\\n')\n",
      "    column_separator, decimal_separator = find_csv_separators(csv)\n",
      "    columns = find_csv_columns(csv, column_separator)\n",
      "    if columns is None:\n",
      "        ixs = {'frequency': 0, 'raw': 1}\n",
      "    else:\n",
      "        ixs = {'frequency': None, 'raw': None}\n",
      "        for i, column in enumerate(columns):\n",
      "            if re.match(r'^freq', column, flags=re.IGNORECASE):\n",
      "                ixs['frequency'] = i\n",
      "            if re.match(r'^(?:spl|gain|ampl|raw)', column, flags=re.IGNORECASE):\n",
      "                ixs['raw'] = i\n",
      "        if ixs['frequency'] is None:\n",
      "            if len(columns) == 2:\n",
      "                ixs = {'frequency': 0, 'raw': 1}\n",
      "            else:\n",
      "                raise CsvParseError('Failed to find frequency column')\n",
      "        if ixs['raw'] is None:\n",
      "            raise CsvParseError('Failed to find SPL column')\n",
      "    data_line_pattern = re.compile(rf'^-?\\d+(?:{column_separator}\\d+)?')\n",
      "    data = {'frequency': [], 'raw': []}\n",
      "    for line in lines:\n",
      "        if not data_line_pattern.match(line):\n",
      "            continue\n",
      "        cells = line.split(column_separator)\n",
      "        if decimal_separator == ',':\n",
      "            cells = [float(cell.replace(',', '.')) for cell in cells]\n",
      "        else:\n",
      "            cells = [float(cell) for cell in cells]\n",
      "        for column, ix in ixs.items():\n",
      "            data[column].append(cells[ix])\n",
      "    return data\n",
      "parse_csv(csv='frequency,raw\\n20,0\\n1000,3\\n20000,0\\n')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "columns = lines[0].split(',')\n",
      "State:\n",
      "['frequency', 'raw']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def find_csv_columns(csv, column_separator):\n",
      "    lines = csv.strip().split('\\n')\n",
      "    numeric_lines = [line for line in lines if column_separator in line and numeric_start.search(line)]\n",
      "    n_columns = list(set([len(line.split(column_separator)) for line in numeric_lines]))\n",
      "    if len(n_columns) != 1:\n",
      "        raise CsvParseError('Numeric lines have different number of columns')\n",
      "    n_columns = n_columns[0]\n",
      "    for line in lines:\n",
      "        if not numeric_start.search(line) and len(line.split(column_separator)) == n_columns:\n",
      "            return [cell.strip() for cell in line.split(column_separator)]\n",
      "find_csv_columns(csv='20.000\\t68.334\\t0\\n20.250\\t68.335\\t0\\n19998.498\\t27.402\\t0', column_separator='\\t')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "lines = csv.strip().split('\\n')\n",
      "State:\n",
      "['20.000\\t68.334\\t0', '20.250\\t68.335\\t0', '19998.498\\t27.402\\t0']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_logger(self):\n",
      "        log = logging.getLogger(self.name)\n",
      "        log.setLevel(logging.DEBUG)\n",
      "        ch = logging.StreamHandler()\n",
      "        formatter = logging.Formatter(\n",
      "                '%(asctime)s - %(name)s - %(message)s', '%H:%M:%S')\n",
      "        ch.setFormatter(formatter)\n",
      "        ch.setLevel(self.config['log_level_console'])\n",
      "        log.addHandler(ch)\n",
      "        fh = logging.FileHandler(self.config['log_file'])\n",
      "        formatter = logging.Formatter(\n",
      "            '%(asctime)s-%(name)s-%(levelname)s-%(message)s')\n",
      "        fh.setFormatter(formatter)\n",
      "        fh.setLevel(self.config['log_level_file'])\n",
      "        log.addHandler(fh)\n",
      "        return log\n",
      "get_logger(self=<aao.spiders.spider_888sport.Spider888sport object at 0x7f28ae0a2be0>, self.config={'log_level_console': 'CRITICAL', 'log_level_file': 'DEBUG', 'log_file': 'log_888sport.log', 'browser': 'CHROME', 'explicit_wait': 5, 'implicitly_wait': 5, 'headless': False, 'proxy': None})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "log = logging.getLogger(self.name)\n",
      "State:\n",
      "<Logger 888sport (WARNING)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_logger(self):\n",
      "        log = logging.getLogger(self.name)\n",
      "        log.setLevel(logging.DEBUG)\n",
      "        ch = logging.StreamHandler()\n",
      "        formatter = logging.Formatter(\n",
      "                '%(asctime)s - %(name)s - %(message)s', '%H:%M:%S')\n",
      "        ch.setFormatter(formatter)\n",
      "        ch.setLevel(self.config['log_level_console'])\n",
      "        log.addHandler(ch)\n",
      "        fh = logging.FileHandler(self.config['log_file'])\n",
      "        formatter = logging.Formatter(\n",
      "            '%(asctime)s-%(name)s-%(levelname)s-%(message)s')\n",
      "        fh.setFormatter(formatter)\n",
      "        fh.setLevel(self.config['log_level_file'])\n",
      "        log.addHandler(fh)\n",
      "        return log\n",
      "get_logger(self=<aao.spiders.spider_888sport.Spider888sport object at 0x7f28ae0a2be0>, self.config={'log_level_console': 'CRITICAL', 'log_level_file': 'DEBUG', 'log_file': 'log_888sport.log', 'browser': 'CHROME', 'explicit_wait': 5, 'implicitly_wait': 5, 'headless': False, 'proxy': None})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "log.setLevel(logging.DEBUG)\n",
      "State:\n",
      "<Logger 888sport (DEBUG)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_logger(self):\n",
      "        log = logging.getLogger(self.name)\n",
      "        log.setLevel(logging.DEBUG)\n",
      "        ch = logging.StreamHandler()\n",
      "        formatter = logging.Formatter(\n",
      "                '%(asctime)s - %(name)s - %(message)s', '%H:%M:%S')\n",
      "        ch.setFormatter(formatter)\n",
      "        ch.setLevel(self.config['log_level_console'])\n",
      "        log.addHandler(ch)\n",
      "        fh = logging.FileHandler(self.config['log_file'])\n",
      "        formatter = logging.Formatter(\n",
      "            '%(asctime)s-%(name)s-%(levelname)s-%(message)s')\n",
      "        fh.setFormatter(formatter)\n",
      "        fh.setLevel(self.config['log_level_file'])\n",
      "        log.addHandler(fh)\n",
      "        return log\n",
      "get_logger(self=<aao.spiders.spider_888sport.Spider888sport object at 0x7f28ae0a2be0>, self.config={'log_level_console': 'CRITICAL', 'log_level_file': 'DEBUG', 'log_file': 'log_888sport.log', 'browser': 'CHROME', 'explicit_wait': 5, 'implicitly_wait': 5, 'headless': False, 'proxy': None})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "ch = logging.StreamHandler()\n",
      "State:\n",
      "<StreamHandler <stderr> (NOTSET)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_logger(self):\n",
      "        log = logging.getLogger(self.name)\n",
      "        log.setLevel(logging.DEBUG)\n",
      "        ch = logging.StreamHandler()\n",
      "        formatter = logging.Formatter(\n",
      "                '%(asctime)s - %(name)s - %(message)s', '%H:%M:%S')\n",
      "        ch.setFormatter(formatter)\n",
      "        ch.setLevel(self.config['log_level_console'])\n",
      "        log.addHandler(ch)\n",
      "        fh = logging.FileHandler(self.config['log_file'])\n",
      "        formatter = logging.Formatter(\n",
      "            '%(asctime)s-%(name)s-%(levelname)s-%(message)s')\n",
      "        fh.setFormatter(formatter)\n",
      "        fh.setLevel(self.config['log_level_file'])\n",
      "        log.addHandler(fh)\n",
      "        return log\n",
      "get_logger(self=<aao.spiders.spider_888sport.Spider888sport object at 0x7f28ae0a2be0>, self.config={'log_level_console': 'CRITICAL', 'log_level_file': 'DEBUG', 'log_file': 'log_888sport.log', 'browser': 'CHROME', 'explicit_wait': 5, 'implicitly_wait': 5, 'headless': False, 'proxy': None})\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "fh.setLevel(self.config['log_level_file'])\n",
      "State:\n",
      "<FileHandler /local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/S1M0N38+aao/S1M0N38+aao/log_888sport.log (DEBUG)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_browser(self):\n",
      "        if self.config['browser'] == 'CHROME':\n",
      "            user_agent = ('Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 ('\n",
      "                          'KHTML, like Gecko) Chrome/60.0.3112.50 Safari/537.36')\n",
      "            options = chrome_options()\n",
      "            options.add_argument('--no-sandbox')\n",
      "            options.add_argument('--window-size=1920,1080')\n",
      "            if self.config['proxy'] is not None:\n",
      "                proxy = self.config[\"proxy\"]\n",
      "                options.add_argument(f'--proxy-server={proxy}')\n",
      "            if self.config['headless']:\n",
      "                options.add_argument('--headless')\n",
      "            options.add_argument(f'user-agent={user_agent}')\n",
      "            options.add_argument('--lang=en')\n",
      "            browser = webdriver.Chrome(options=options)\n",
      "            browser.implicitly_wait(self.config['implicitly_wait'])\n",
      "            return browser\n",
      "        else:\n",
      "            raise KeyError(\n",
      "                'browser name error: choose form \"CHROME\"')\n",
      "get_browser(self=<aao.spiders.spider_888sport.Spider888sport object at 0x7f28ae0a2be0>, self.config={'log_level_console': 'CRITICAL', 'log_level_file': 'DEBUG', 'log_file': 'log_888sport.log', 'browser': 'CHROME', 'explicit_wait': 5, 'implicitly_wait': 5, 'headless': False, 'proxy': None}, self.log=<Logger 888sport (DEBUG)>, self.table={'soccer': {'countries': {'england': 'england', 'italy': 'italy', 'spain': 'spain', 'world_cup_2018': 'world_cup_2018', 'test_country_foo': 'foo_country', 'test_country_null': None}, 'leagues': {'england': {'premier_league': 'premier_league', 'efl_championship': 'the_championship', 'efl_league_one': 'league_one', 'efl_league_two': 'league_two', 'national_league': 'national_league', 'national_league_north': None, 'national_league_south': None, 'northern_premier_league': None, 'southern_premier_league': None, 'isthmian_league': None, 'premier_league_2': None, 'fa_cup': 'fa_cup', 'efl_cup': None, 'efl_trophy': None, 'fa_trophy': 'fa_trophy', 'fa_vase': None, 'fa_inter-league_cup': None}, 'italy': {'serie_a': 'serie_a', 'serie_b': 'serie_b', 'serie_c_group_a': 'serie_c_girone_a', 'serie_c_group_b': 'serie_c_girone_b', 'serie_c_group_c': 'serie_c_girone_c', 'primavera_1': None, 'primavera_2_group_a': None, 'primavera_2_group_b': None, 'coppa_italia': 'tim_cup', 'supercoppa_italiana': None}, 'spain': {'la_liga': 'laliga', 'la_liga_2': 'laliga2', 'segunda_division_b_group_1': 'segunda_b-1', 'segunda_division_b_group_2': 'segunda_b-2', 'segunda_division_b_group_3': 'segunda_b-3', 'segunda_division_b_group_4': 'segunda_b-4', 'tercera_division_group_1': 'tercera_division_1', 'tercera_division_group_2': 'tercera_division_2', 'tercera_division_group_3': 'tercera_division_3', 'tercera_division_group_4': 'tercera_division_4', 'tercera_division_group_5': 'tercera_division_5', 'tercera_division_group_6': 'tercera_division_6', 'tercera_division_group_7': 'tercera_division_7', 'tercera_division_group_8': 'tercera_division_8', 'tercera_division_group_9': 'tercera_division_9', 'tercera_division_group_10': 'tercera_division_10', 'tercera_division_group_11': 'tercera_division_11', 'tercera_division_group_12': 'tercera_division_12', 'tercera_division_group_13': 'tercera_division_13', 'tercera_division_group_14': 'tercera_division_14', 'tercera_division_group_15': 'tercera_division_15', 'tercera_division_group_16': 'tercera_division_16', 'tercera_division_group_17': 'tercera_division_17', 'tercera_division_group_18': 'tercera_division_18', 'premiera_division_women': 'superliga_femenina__w_', 'segunda_division_women': None, 'youth_league': None, 'copa_del_rey': 'copa_del_rey', 'copa_federacion': None, 'super_cup': None, 'copa_de_la_reina_women': None}, 'world_cup_2018': {'world_cup_2018': '', 'group_a': None, 'group_b': None, 'group_c': None, 'group_d': None, 'group_e': None, 'group_f': None, 'group_g': None, 'group_h': None}, 'test_country_foo': {'test_league_foo': 'foo_league', 'test_league_null': None}}}})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "user_agent = ('Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 ('\n",
      "State:\n",
      "'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.50 Safari/537.36'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_browser(self):\n",
      "        if self.config['browser'] == 'CHROME':\n",
      "            user_agent = ('Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 ('\n",
      "                          'KHTML, like Gecko) Chrome/60.0.3112.50 Safari/537.36')\n",
      "            options = chrome_options()\n",
      "            options.add_argument('--no-sandbox')\n",
      "            options.add_argument('--window-size=1920,1080')\n",
      "            if self.config['proxy'] is not None:\n",
      "                proxy = self.config[\"proxy\"]\n",
      "                options.add_argument(f'--proxy-server={proxy}')\n",
      "            if self.config['headless']:\n",
      "                options.add_argument('--headless')\n",
      "            options.add_argument(f'user-agent={user_agent}')\n",
      "            options.add_argument('--lang=en')\n",
      "            browser = webdriver.Chrome(options=options)\n",
      "            browser.implicitly_wait(self.config['implicitly_wait'])\n",
      "            return browser\n",
      "        else:\n",
      "            raise KeyError(\n",
      "                'browser name error: choose form \"CHROME\"')\n",
      "get_browser(self=<aao.spiders.spider_888sport.Spider888sport object at 0x7f28ae0a2be0>, self.config={'log_level_console': 'CRITICAL', 'log_level_file': 'DEBUG', 'log_file': 'log_888sport.log', 'browser': 'CHROME', 'explicit_wait': 5, 'implicitly_wait': 5, 'headless': False, 'proxy': None}, self.log=<Logger 888sport (DEBUG)>, self.table={'soccer': {'countries': {'england': 'england', 'italy': 'italy', 'spain': 'spain', 'world_cup_2018': 'world_cup_2018', 'test_country_foo': 'foo_country', 'test_country_null': None}, 'leagues': {'england': {'premier_league': 'premier_league', 'efl_championship': 'the_championship', 'efl_league_one': 'league_one', 'efl_league_two': 'league_two', 'national_league': 'national_league', 'national_league_north': None, 'national_league_south': None, 'northern_premier_league': None, 'southern_premier_league': None, 'isthmian_league': None, 'premier_league_2': None, 'fa_cup': 'fa_cup', 'efl_cup': None, 'efl_trophy': None, 'fa_trophy': 'fa_trophy', 'fa_vase': None, 'fa_inter-league_cup': None}, 'italy': {'serie_a': 'serie_a', 'serie_b': 'serie_b', 'serie_c_group_a': 'serie_c_girone_a', 'serie_c_group_b': 'serie_c_girone_b', 'serie_c_group_c': 'serie_c_girone_c', 'primavera_1': None, 'primavera_2_group_a': None, 'primavera_2_group_b': None, 'coppa_italia': 'tim_cup', 'supercoppa_italiana': None}, 'spain': {'la_liga': 'laliga', 'la_liga_2': 'laliga2', 'segunda_division_b_group_1': 'segunda_b-1', 'segunda_division_b_group_2': 'segunda_b-2', 'segunda_division_b_group_3': 'segunda_b-3', 'segunda_division_b_group_4': 'segunda_b-4', 'tercera_division_group_1': 'tercera_division_1', 'tercera_division_group_2': 'tercera_division_2', 'tercera_division_group_3': 'tercera_division_3', 'tercera_division_group_4': 'tercera_division_4', 'tercera_division_group_5': 'tercera_division_5', 'tercera_division_group_6': 'tercera_division_6', 'tercera_division_group_7': 'tercera_division_7', 'tercera_division_group_8': 'tercera_division_8', 'tercera_division_group_9': 'tercera_division_9', 'tercera_division_group_10': 'tercera_division_10', 'tercera_division_group_11': 'tercera_division_11', 'tercera_division_group_12': 'tercera_division_12', 'tercera_division_group_13': 'tercera_division_13', 'tercera_division_group_14': 'tercera_division_14', 'tercera_division_group_15': 'tercera_division_15', 'tercera_division_group_16': 'tercera_division_16', 'tercera_division_group_17': 'tercera_division_17', 'tercera_division_group_18': 'tercera_division_18', 'premiera_division_women': 'superliga_femenina__w_', 'segunda_division_women': None, 'youth_league': None, 'copa_del_rey': 'copa_del_rey', 'copa_federacion': None, 'super_cup': None, 'copa_de_la_reina_women': None}, 'world_cup_2018': {'world_cup_2018': '', 'group_a': None, 'group_b': None, 'group_c': None, 'group_d': None, 'group_e': None, 'group_f': None, 'group_g': None, 'group_h': None}, 'test_country_foo': {'test_league_foo': 'foo_league', 'test_league_null': None}}}})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "options = chrome_options()\n",
      "State:\n",
      "{_caps={'browserName': 'chrome', 'pageLoadStrategy': <PageLoadStrategy.normal: 'normal'>}, _proxy=None, mobile_options=None, _arguments=[], _ignore_local_proxy=False, _binary_location='', _extension_files=[], _extensions=[], _experimental_options={}, _debugger_address=None}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_browser(self):\n",
      "        if self.config['browser'] == 'CHROME':\n",
      "            user_agent = ('Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 ('\n",
      "                          'KHTML, like Gecko) Chrome/60.0.3112.50 Safari/537.36')\n",
      "            options = chrome_options()\n",
      "            options.add_argument('--no-sandbox')\n",
      "            options.add_argument('--window-size=1920,1080')\n",
      "            if self.config['proxy'] is not None:\n",
      "                proxy = self.config[\"proxy\"]\n",
      "                options.add_argument(f'--proxy-server={proxy}')\n",
      "            if self.config['headless']:\n",
      "                options.add_argument('--headless')\n",
      "            options.add_argument(f'user-agent={user_agent}')\n",
      "            options.add_argument('--lang=en')\n",
      "            browser = webdriver.Chrome(options=options)\n",
      "            browser.implicitly_wait(self.config['implicitly_wait'])\n",
      "            return browser\n",
      "        else:\n",
      "            raise KeyError(\n",
      "                'browser name error: choose form \"CHROME\"')\n",
      "get_browser(self=<aao.spiders.spider_888sport.Spider888sport object at 0x7f28ae0a2be0>, self.config={'log_level_console': 'CRITICAL', 'log_level_file': 'DEBUG', 'log_file': 'log_888sport.log', 'browser': 'CHROME', 'explicit_wait': 5, 'implicitly_wait': 5, 'headless': False, 'proxy': None}, self.log=<Logger 888sport (DEBUG)>, self.table={'soccer': {'countries': {'england': 'england', 'italy': 'italy', 'spain': 'spain', 'world_cup_2018': 'world_cup_2018', 'test_country_foo': 'foo_country', 'test_country_null': None}, 'leagues': {'england': {'premier_league': 'premier_league', 'efl_championship': 'the_championship', 'efl_league_one': 'league_one', 'efl_league_two': 'league_two', 'national_league': 'national_league', 'national_league_north': None, 'national_league_south': None, 'northern_premier_league': None, 'southern_premier_league': None, 'isthmian_league': None, 'premier_league_2': None, 'fa_cup': 'fa_cup', 'efl_cup': None, 'efl_trophy': None, 'fa_trophy': 'fa_trophy', 'fa_vase': None, 'fa_inter-league_cup': None}, 'italy': {'serie_a': 'serie_a', 'serie_b': 'serie_b', 'serie_c_group_a': 'serie_c_girone_a', 'serie_c_group_b': 'serie_c_girone_b', 'serie_c_group_c': 'serie_c_girone_c', 'primavera_1': None, 'primavera_2_group_a': None, 'primavera_2_group_b': None, 'coppa_italia': 'tim_cup', 'supercoppa_italiana': None}, 'spain': {'la_liga': 'laliga', 'la_liga_2': 'laliga2', 'segunda_division_b_group_1': 'segunda_b-1', 'segunda_division_b_group_2': 'segunda_b-2', 'segunda_division_b_group_3': 'segunda_b-3', 'segunda_division_b_group_4': 'segunda_b-4', 'tercera_division_group_1': 'tercera_division_1', 'tercera_division_group_2': 'tercera_division_2', 'tercera_division_group_3': 'tercera_division_3', 'tercera_division_group_4': 'tercera_division_4', 'tercera_division_group_5': 'tercera_division_5', 'tercera_division_group_6': 'tercera_division_6', 'tercera_division_group_7': 'tercera_division_7', 'tercera_division_group_8': 'tercera_division_8', 'tercera_division_group_9': 'tercera_division_9', 'tercera_division_group_10': 'tercera_division_10', 'tercera_division_group_11': 'tercera_division_11', 'tercera_division_group_12': 'tercera_division_12', 'tercera_division_group_13': 'tercera_division_13', 'tercera_division_group_14': 'tercera_division_14', 'tercera_division_group_15': 'tercera_division_15', 'tercera_division_group_16': 'tercera_division_16', 'tercera_division_group_17': 'tercera_division_17', 'tercera_division_group_18': 'tercera_division_18', 'premiera_division_women': 'superliga_femenina__w_', 'segunda_division_women': None, 'youth_league': None, 'copa_del_rey': 'copa_del_rey', 'copa_federacion': None, 'super_cup': None, 'copa_de_la_reina_women': None}, 'world_cup_2018': {'world_cup_2018': '', 'group_a': None, 'group_b': None, 'group_c': None, 'group_d': None, 'group_e': None, 'group_f': None, 'group_g': None, 'group_h': None}, 'test_country_foo': {'test_league_foo': 'foo_league', 'test_league_null': None}}}})\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "options.add_argument('--no-sandbox')\n",
      "State:\n",
      "{_caps={'browserName': 'chrome', 'pageLoadStrategy': <PageLoadStrategy.normal: 'normal'>}, _proxy=None, mobile_options=None, _arguments=['--no-sandbox'], _ignore_local_proxy=False, _binary_location='', _extension_files=[], _extensions=[], _experimental_options={}, _debugger_address=None}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def exchange(a, i, j):\n",
      "    tmp = a[j]\n",
      "    a[j] = a[i]\n",
      "    a[i] = tmp\n",
      "exchange(a=[1, 2, 13, 22, 123], i=0, j=1)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "tmp = a[j]\n",
      "State:\n",
      "2\n",
      "==================================================\n",
      "Clean Code:\n",
      "def exchange(a, i, j):\n",
      "    tmp = a[j]\n",
      "    a[j] = a[i]\n",
      "    a[i] = tmp\n",
      "exchange(a=[1, 2, 13, 22, 123], i=0, j=1)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "a[j] = a[i]\n",
      "State:\n",
      "[1, 1, 13, 22, 123]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def exchange(a, i, j):\n",
      "    tmp = a[j]\n",
      "    a[j] = a[i]\n",
      "    a[i] = tmp\n",
      "exchange(a=[1, 2, 13, 22, 123], i=0, j=1)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "a[i] = tmp\n",
      "State:\n",
      "[2, 1, 13, 22, 123]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _sort(a, aux, lo, hi):\n",
      "        if lo >= hi:\n",
      "            return\n",
      "        if hi - lo + 1 < MergeSort.CUTOFF:\n",
      "            InsertionSort.sort(a, lo, hi)\n",
      "            return\n",
      "        mid = lo + (hi - lo) // 2\n",
      "        MergeSort._sort(a, aux, lo, mid)\n",
      "        MergeSort._sort(a, aux, mid + 1, hi)\n",
      "        MergeSort._merge(a, aux, lo, mid, hi)\n",
      "_sort(a=[4, 2, 1, 23, 4, 5, 6, 7, 8, 9, 20, 11, 13, 34, 66], aux=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], lo=0, hi=7)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "MergeSort._sort(a, aux, mid + 1, hi)\n",
      "State:\n",
      "[1, 2, 4, 4, 5, 6, 7, 23, 8, 9, 20, 11, 13, 34, 66]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _merge(a, aux, lo, mid, hi):\n",
      "        i = lo\n",
      "        j = mid + 1\n",
      "        for k in range(lo, hi + 1):\n",
      "            aux[k] = a[k]\n",
      "        for k in range(lo, hi + 1):\n",
      "            if i > mid:\n",
      "                a[k] = aux[j]\n",
      "                j += 1\n",
      "            elif j > hi:\n",
      "                a[k] = aux[i]\n",
      "                i += 1\n",
      "            elif util.less(aux[i], aux[j]):\n",
      "                a[k] = aux[i]\n",
      "                i += 1\n",
      "            else:\n",
      "                a[k] = aux[j]\n",
      "                j += 1\n",
      "_merge(a=[1, 2, 4, 4, 5, 6, 7, 23, 8, 9, 20, 11, 13, 34, 66], aux=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], lo=0, mid=3, hi=7)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "i = lo\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _merge(a, aux, lo, mid, hi):\n",
      "        i = lo\n",
      "        j = mid + 1\n",
      "        for k in range(lo, hi + 1):\n",
      "            aux[k] = a[k]\n",
      "        for k in range(lo, hi + 1):\n",
      "            if i > mid:\n",
      "                a[k] = aux[j]\n",
      "                j += 1\n",
      "            elif j > hi:\n",
      "                a[k] = aux[i]\n",
      "                i += 1\n",
      "            elif util.less(aux[i], aux[j]):\n",
      "                a[k] = aux[i]\n",
      "                i += 1\n",
      "            else:\n",
      "                a[k] = aux[j]\n",
      "                j += 1\n",
      "_merge(a=[1, 2, 4, 4, 5, 6, 7, 23, 8, 9, 20, 11, 13, 34, 66], aux=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], lo=0, mid=3, hi=7)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "j = mid + 1\n",
      "State:\n",
      "4\n",
      "==================================================\n",
      "Clean Code:\n",
      "def partition(a, lo, hi):\n",
      "        i = lo\n",
      "        j = hi\n",
      "        while True:\n",
      "            while not util.less(a[lo], a[i]):\n",
      "                i += 1\n",
      "                if i >= hi:\n",
      "                    break\n",
      "            while util.less(a[lo], a[j]):\n",
      "                j -= 1\n",
      "                if j <= lo:\n",
      "                    break\n",
      "            if i >= j:\n",
      "                break\n",
      "            util.exchange(a, i, j)\n",
      "        util.exchange(a, lo, j)\n",
      "        return j\n",
      "partition(a=[4, 2, 1, 23, 4, 5, 6, 7, 8, 9, 20, 11, 13, 34, 66], lo=0, hi=14)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "i = lo\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def partition(a, lo, hi):\n",
      "        i = lo\n",
      "        j = hi\n",
      "        while True:\n",
      "            while not util.less(a[lo], a[i]):\n",
      "                i += 1\n",
      "                if i >= hi:\n",
      "                    break\n",
      "            while util.less(a[lo], a[j]):\n",
      "                j -= 1\n",
      "                if j <= lo:\n",
      "                    break\n",
      "            if i >= j:\n",
      "                break\n",
      "            util.exchange(a, i, j)\n",
      "        util.exchange(a, lo, j)\n",
      "        return j\n",
      "partition(a=[4, 2, 1, 23, 4, 5, 6, 7, 8, 9, 20, 11, 13, 34, 66], lo=0, hi=14)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "j = hi\n",
      "State:\n",
      "14\n",
      "==================================================\n",
      "Clean Code:\n",
      "def swim(self, k):\n",
      "        while k > 1:\n",
      "            parent = k // 2\n",
      "            if less(self.pq[k], self.pq[parent]):\n",
      "                exchange(self.pq, k, parent)\n",
      "                k = parent\n",
      "            else:\n",
      "                break\n",
      "swim(self=<pyalgs.data_structures.commons.priority_queue.MinPQ object at 0x7f4c41128eb0>, k=2, self.N=2, self.pq=[0, 10, 5, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "parent = k // 2\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def swim(self, k):\n",
      "        while k > 1:\n",
      "            parent = k // 2\n",
      "            if less(self.pq[k], self.pq[parent]):\n",
      "                exchange(self.pq, k, parent)\n",
      "                k = parent\n",
      "            else:\n",
      "                break\n",
      "swim(self=<pyalgs.data_structures.commons.priority_queue.MinPQ object at 0x7f4c41128eb0>, k=2, self.N=2, self.pq=[0, 10, 5, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "exchange(self.pq, k, parent)\n",
      "State:\n",
      "[0, 5, 10, 0, 0, 0, 0, 0, 0, 0]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def swim(self, k):\n",
      "        while k > 1:\n",
      "            parent = k // 2\n",
      "            if less(self.pq[k], self.pq[parent]):\n",
      "                exchange(self.pq, k, parent)\n",
      "                k = parent\n",
      "            else:\n",
      "                break\n",
      "swim(self=<pyalgs.data_structures.commons.priority_queue.MinPQ object at 0x7f4c41128eb0>, k=2, self.N=2, self.pq=[0, 10, 5, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "k = parent\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_google_drive_folder_location():\n",
      "    gdrive_db_path = \"Library/Application Support/Google/Drive/sync_config.db\"\n",
      "    yosemite_gdrive_db_path = (\n",
      "        \"Library/Application Support/Google/Drive/\" \"user_default/sync_config.db\"\n",
      "    )\n",
      "    yosemite_gdrive_db = os.path.join(os.environ[\"HOME\"], yosemite_gdrive_db_path)\n",
      "    if os.path.isfile(yosemite_gdrive_db):\n",
      "        gdrive_db_path = yosemite_gdrive_db\n",
      "    googledrive_home = None\n",
      "    gdrive_db = os.path.join(os.environ[\"HOME\"], gdrive_db_path)\n",
      "    if os.path.isfile(gdrive_db):\n",
      "        con = sqlite3.connect(gdrive_db)\n",
      "        if con:\n",
      "            cur = con.cursor()\n",
      "            query = (\n",
      "                \"SELECT data_value \"\n",
      "                \"FROM data \"\n",
      "                \"WHERE entry_key = 'local_sync_root_path';\"\n",
      "            )\n",
      "            cur.execute(query)\n",
      "            data = cur.fetchone()\n",
      "            googledrive_home = str(data[0])\n",
      "            con.close()\n",
      "    if not googledrive_home:\n",
      "        error(\n",
      "            constants.ERROR_UNABLE_TO_FIND_STORAGE.format(\n",
      "                provider=\"Google Drive install\"\n",
      "            )\n",
      "        )\n",
      "    return googledrive_home\n",
      "get_google_drive_folder_location()\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "googledrive_home = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_declared_instances_dict(bases, attrs, collect_cls, instance_attr):\n",
      "    instances = {}\n",
      "    for name, obj in attrs.copy().items():\n",
      "        if isinstance(obj, collect_cls):\n",
      "            instances[name] = attrs.pop(name)\n",
      "    for base in bases[::-1]:\n",
      "        if hasattr(base, instance_attr):\n",
      "            instances.update(getattr(base, instance_attr))\n",
      "    return instances\n",
      "get_declared_instances_dict(bases=(<class 'quilt.cli.parser.SubParsersMixin'>,), attrs={'__module__': 'quilt.cli.parser', '__doc__': '\\n    Main class to create cli parser\\n\\n    Most of the time your parser should be directly be derived from this class.\\n    ', '__init__': <function Parser.__init__ at 0x7f5650407c10>, 'create_argparser': <function Parser.create_argparser at 0x7f5650407d30>, 'parse_args': <function Parser.parse_args at 0x7f5650407dc0>, 'parse_known_args': <function Parser.parse_known_args at 0x7f5650407e50>, 'print_usage': <function Parser.print_usage at 0x7f5650407ee0>, 'print_version': <function Parser.print_version at 0x7f5650407f70>, 'print_help': <function Parser.print_help at 0x7f5650407ca0>, '__qualname__': 'Parser'}, collect_cls=<class 'quilt.cli.parser.Argument'>, instance_attr='base_arguments')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "instances = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __init__(self, patch_name, strip=1, reverse=False):\n",
      "        self.patch_name = patch_name\n",
      "        self.strip = strip\n",
      "        self.reverse = reverse\n",
      "__init__(self=REPR FAILED, patch_name='firstpatch', strip=1, reverse=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.patch_name = patch_name\n",
      "State:\n",
      "'firstpatch'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __init__(self, patch_name, strip=1, reverse=False):\n",
      "        self.patch_name = patch_name\n",
      "        self.strip = strip\n",
      "        self.reverse = reverse\n",
      "__init__(self=REPR FAILED, patch_name='firstpatch', strip=1, reverse=False)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.strip = strip\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __init__(self, patch_name, strip=1, reverse=False):\n",
      "        self.patch_name = patch_name\n",
      "        self.strip = strip\n",
      "        self.reverse = reverse\n",
      "__init__(self=REPR FAILED, patch_name='firstpatch', strip=1, reverse=False)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.reverse = reverse\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def read(self):\n",
      "        self.patchlines = []\n",
      "        self.patch2line = dict()\n",
      "        if self.exists():\n",
      "            with open(self.series_file, \"r\") as f:\n",
      "                for line in f:\n",
      "                    self.add_patch(line)\n",
      "read(self=<quilt.db.PatchSeries object at 0x7f56504e22e0>, self.dirname='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/bjoernricks+python-quilt/bjoernricks+python-quilt/tests/data/db', self.filename='series_test1', self.series_file='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/bjoernricks+python-quilt/bjoernricks+python-quilt/tests/data/db/series_test1')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.patchlines = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add_patch(self, patch):\n",
      "        patchline = PatchLine(patch)\n",
      "        patch = patchline.get_patch()\n",
      "        if patch:\n",
      "            self.patch2line[patch] = patchline\n",
      "        self.patchlines.append(patchline)\n",
      "add_patch(self=<quilt.db.PatchSeries object at 0x7f56504e22e0>, patch='\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "patchline = PatchLine(patch)\n",
      "State:\n",
      "{comment='# this is a comment', patch=None, line='# this is a comment'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_line(self, line):\n",
      "        line = line.rstrip(\"\\r\\n\")\n",
      "        self.line = line\n",
      "        if line.rstrip().startswith(\"\n",
      "            self.comment = line\n",
      "            return\n",
      "        if not line.strip():\n",
      "            return\n",
      "        if \"\n",
      "            patchline, self.comment = line.split(\"\n",
      "        else:\n",
      "            patchline = line\n",
      "        patchline = patchline.strip()\n",
      "        if not patchline:\n",
      "            return\n",
      "        patch_args = None\n",
      "        strip = 1\n",
      "        reverse = False\n",
      "        if \" \" in patchline:\n",
      "            patch_name, patch_args = patchline.split(\" \", 1)\n",
      "        else:\n",
      "            patch_name = patchline\n",
      "        if patch_args:\n",
      "            patch_args = patch_args.split()\n",
      "            try:\n",
      "                opts, args = getopt.getopt(patch_args, \"p:R\", [\"strip=\",\n",
      "                                                               \"reverse\"])\n",
      "                for o, a in opts:\n",
      "                    if o in [\"-p\", \"--strip\"]:\n",
      "                        strip = a\n",
      "                    elif o in [\"-R\", \"--reverse\"]:\n",
      "                        reverse = True\n",
      "            except getopt.GetoptError as err:\n",
      "                print(err, file=sys.stderr)\n",
      "        self.patch = Patch(patch_name, strip, reverse)\n",
      "_parse_line(self=<quilt.db.PatchLine object at 0x7f56504e21c0>, line='\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "line = line.rstrip(\"\\r\\n\")\n",
      "State:\n",
      "'# this is a comment'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_line(self, line):\n",
      "        line = line.rstrip(\"\\r\\n\")\n",
      "        self.line = line\n",
      "        if line.rstrip().startswith(\"\n",
      "            self.comment = line\n",
      "            return\n",
      "        if not line.strip():\n",
      "            return\n",
      "        if \"\n",
      "            patchline, self.comment = line.split(\"\n",
      "        else:\n",
      "            patchline = line\n",
      "        patchline = patchline.strip()\n",
      "        if not patchline:\n",
      "            return\n",
      "        patch_args = None\n",
      "        strip = 1\n",
      "        reverse = False\n",
      "        if \" \" in patchline:\n",
      "            patch_name, patch_args = patchline.split(\" \", 1)\n",
      "        else:\n",
      "            patch_name = patchline\n",
      "        if patch_args:\n",
      "            patch_args = patch_args.split()\n",
      "            try:\n",
      "                opts, args = getopt.getopt(patch_args, \"p:R\", [\"strip=\",\n",
      "                                                               \"reverse\"])\n",
      "                for o, a in opts:\n",
      "                    if o in [\"-p\", \"--strip\"]:\n",
      "                        strip = a\n",
      "                    elif o in [\"-R\", \"--reverse\"]:\n",
      "                        reverse = True\n",
      "            except getopt.GetoptError as err:\n",
      "                print(err, file=sys.stderr)\n",
      "        self.patch = Patch(patch_name, strip, reverse)\n",
      "_parse_line(self=<quilt.db.PatchLine object at 0x7f56504e21c0>, line='\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.line = line\n",
      "State:\n",
      "'# this is a comment'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_line(self, line):\n",
      "        line = line.rstrip(\"\\r\\n\")\n",
      "        self.line = line\n",
      "        if line.rstrip().startswith(\"\n",
      "            self.comment = line\n",
      "            return\n",
      "        if not line.strip():\n",
      "            return\n",
      "        if \"\n",
      "            patchline, self.comment = line.split(\"\n",
      "        else:\n",
      "            patchline = line\n",
      "        patchline = patchline.strip()\n",
      "        if not patchline:\n",
      "            return\n",
      "        patch_args = None\n",
      "        strip = 1\n",
      "        reverse = False\n",
      "        if \" \" in patchline:\n",
      "            patch_name, patch_args = patchline.split(\" \", 1)\n",
      "        else:\n",
      "            patch_name = patchline\n",
      "        if patch_args:\n",
      "            patch_args = patch_args.split()\n",
      "            try:\n",
      "                opts, args = getopt.getopt(patch_args, \"p:R\", [\"strip=\",\n",
      "                                                               \"reverse\"])\n",
      "                for o, a in opts:\n",
      "                    if o in [\"-p\", \"--strip\"]:\n",
      "                        strip = a\n",
      "                    elif o in [\"-R\", \"--reverse\"]:\n",
      "                        reverse = True\n",
      "            except getopt.GetoptError as err:\n",
      "                print(err, file=sys.stderr)\n",
      "        self.patch = Patch(patch_name, strip, reverse)\n",
      "_parse_line(self=<quilt.db.PatchLine object at 0x7f56504e21c0>, line='\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "self.comment = line\n",
      "State:\n",
      "'# this is a comment'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def remove_patch(self, patch):\n",
      "        self._check_patch(patch)\n",
      "        patchline = self.patch2line[patch]\n",
      "        del self.patch2line[patch]\n",
      "        self.patchlines.remove(patchline)\n",
      "remove_patch(self=<quilt.db.PatchSeries object at 0x7f56504e22e0>, patch=<Patch('newlastpatch', 1, False) id=0x7f56504e2100>, self.dirname='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/bjoernricks+python-quilt/bjoernricks+python-quilt/tests/data/db', self.filename='series_test1', self.patch2line={<Patch('firstpatch', 1, False) id=0x7f565055ad30>: <quilt.db.PatchLine object at 0x7f56504e22b0>, <Patch('secondpatch', 1, False) id=0x7f565055ae80>: <quilt.db.PatchLine object at 0x7f565055a130>, <Patch('thirdpatch', 1, False) id=0x7f565055ae50>: <quilt.db.PatchLine object at 0x7f565055ac10>, <Patch('patchwith.patch', 1, False) id=0x7f565055ab50>: <quilt.db.PatchLine object at 0x7f565055ab20>, <Patch('patchwith.diff', 1, False) id=0x7f565055aeb0>: <quilt.db.PatchLine object at 0x7f565055a2e0>, <Patch('patchwith', '1', True) id=0x7f56504d3880>: <quilt.db.PatchLine object at 0x7f56504d37c0>, <Patch('lastpatch', 1, False) id=0x7f56504d36a0>: <quilt.db.PatchLine object at 0x7f56504d3850>, <Patch('newlastpatch', 1, False) id=0x7f56504e2100>: <quilt.db.PatchLine object at 0x7f56504e20d0>}, self.patchlines=[<quilt.db.PatchLine object at 0x7f56504e21c0>, <quilt.db.PatchLine object at 0x7f56504e22b0>, <quilt.db.PatchLine object at 0x7f565055a130>, <quilt.db.PatchLine object at 0x7f565055ac40>, <quilt.db.PatchLine object at 0x7f565055a8b0>, <quilt.db.PatchLine object at 0x7f565055ac10>, <quilt.db.PatchLine object at 0x7f565055ab20>, <quilt.db.PatchLine object at 0x7f565055a2e0>, <quilt.db.PatchLine object at 0x7f56504d37c0>, <quilt.db.PatchLine object at 0x7f56504d3850>, <quilt.db.PatchLine object at 0x7f56504e20d0>], self.series_file='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/bjoernricks+python-quilt/bjoernricks+python-quilt/tests/data/db/series_test1')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "patchline = self.patch2line[patch]\n",
      "State:\n",
      "{comment='', patch=<Patch('newlastpatch', 1, False) id=0x7f56504e2100>, line='newlastpatch'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __call__(self, *args, **kw):\n",
      "        newargs = []\n",
      "        for name, value in zip(self._get_varnames(), args):\n",
      "            if value and name in self.names and not isinstance(value, self.cls):\n",
      "                newargs.append(self.cls(value))\n",
      "            else:\n",
      "                newargs.append(value)\n",
      "        for name in self.names:\n",
      "            value = kw.get(name, None)\n",
      "            if value and not isinstance(value, self.cls):\n",
      "                value = self.cls(value)\n",
      "                kw[name] = value\n",
      "        return self.func(*newargs, **kw)\n",
      "__call__(self=<quilt.utils.FunctionWrapper object at 0x7f56504c5a90>, args=(<quilt.patch.RollbackPatch object at 0x7f5650301c70>, '/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/bjoernricks+python-quilt/bjoernricks+python-quilt/tests/data/pop/tempkj0emjgs/test2', <quilt.utils.Directory object at 0x7f5650301e50>), kw={}, self.cls=<class 'quilt.utils.Directory'>, self.func=<function RollbackPatch.__init__ at 0x7f5650467940>, self.names=['cwd', 'backup_dir'])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "newargs = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def rollback(self, keep=False):\n",
      "        (dirs, files) = self.backup_dir.content()\n",
      "        for dir in dirs:\n",
      "            newdir = self.cwd + dir\n",
      "            if not newdir.exists():\n",
      "                newdir.create()\n",
      "        for file in files:\n",
      "            file = File(file)\n",
      "            backup_file = self.backup_dir + file\n",
      "            rollback_file = self.cwd + file\n",
      "            if not keep:\n",
      "                rollback_file.delete_if_exists()\n",
      "            if not backup_file.is_empty():\n",
      "                backup_file.copy(rollback_file)\n",
      "rollback(self=<quilt.patch.RollbackPatch object at 0x7f5650301c70>, keep=False, self.backup_dir=<quilt.utils.Directory object at 0x7f5650301e50>, self.cwd=<quilt.utils.Directory object at 0x7f56504d3340>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "(dirs, files) = self.backup_dir.content()\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _content(self, startdir, dirname=None):\n",
      "        files = []\n",
      "        dirs = []\n",
      "        if dirname:\n",
      "            dir = os.path.join(startdir, dirname)\n",
      "        else:\n",
      "            dir = startdir\n",
      "        contents = os.listdir(dir)\n",
      "        for content in contents:\n",
      "            if not dirname:\n",
      "                name = content\n",
      "            else:\n",
      "                name = os.path.join(dirname, content)\n",
      "            path = os.path.join(dir, content)\n",
      "            if os.path.isdir(path):\n",
      "                (newdirs, newfiles) = self._content(startdir, name)\n",
      "                dirs.append(name)\n",
      "                files.extend(newfiles)\n",
      "                dirs.extend(newdirs)\n",
      "            else:\n",
      "                files.append(name)\n",
      "        return (dirs, files)\n",
      "_content(self=<quilt.utils.Directory object at 0x7f5650301e50>, startdir='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/bjoernricks+python-quilt/bjoernricks+python-quilt/tests/data/pop/tempkj0emjgs/test2/pc/p2.patch', dirname=None, self.dirname='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/bjoernricks+python-quilt/bjoernricks+python-quilt/tests/data/pop/tempkj0emjgs/test2/pc/p2.patch')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "files = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _content(self, startdir, dirname=None):\n",
      "        files = []\n",
      "        dirs = []\n",
      "        if dirname:\n",
      "            dir = os.path.join(startdir, dirname)\n",
      "        else:\n",
      "            dir = startdir\n",
      "        contents = os.listdir(dir)\n",
      "        for content in contents:\n",
      "            if not dirname:\n",
      "                name = content\n",
      "            else:\n",
      "                name = os.path.join(dirname, content)\n",
      "            path = os.path.join(dir, content)\n",
      "            if os.path.isdir(path):\n",
      "                (newdirs, newfiles) = self._content(startdir, name)\n",
      "                dirs.append(name)\n",
      "                files.extend(newfiles)\n",
      "                dirs.extend(newdirs)\n",
      "            else:\n",
      "                files.append(name)\n",
      "        return (dirs, files)\n",
      "_content(self=<quilt.utils.Directory object at 0x7f5650301e50>, startdir='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/bjoernricks+python-quilt/bjoernricks+python-quilt/tests/data/pop/tempkj0emjgs/test2/pc/p2.patch', dirname=None, self.dirname='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/bjoernricks+python-quilt/bjoernricks+python-quilt/tests/data/pop/tempkj0emjgs/test2/pc/p2.patch')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "dirs = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _content(self, startdir, dirname=None):\n",
      "        files = []\n",
      "        dirs = []\n",
      "        if dirname:\n",
      "            dir = os.path.join(startdir, dirname)\n",
      "        else:\n",
      "            dir = startdir\n",
      "        contents = os.listdir(dir)\n",
      "        for content in contents:\n",
      "            if not dirname:\n",
      "                name = content\n",
      "            else:\n",
      "                name = os.path.join(dirname, content)\n",
      "            path = os.path.join(dir, content)\n",
      "            if os.path.isdir(path):\n",
      "                (newdirs, newfiles) = self._content(startdir, name)\n",
      "                dirs.append(name)\n",
      "                files.extend(newfiles)\n",
      "                dirs.extend(newdirs)\n",
      "            else:\n",
      "                files.append(name)\n",
      "        return (dirs, files)\n",
      "_content(self=<quilt.utils.Directory object at 0x7f5650301e50>, startdir='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/bjoernricks+python-quilt/bjoernricks+python-quilt/tests/data/pop/tempkj0emjgs/test2/pc/p2.patch', dirname=None, self.dirname='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/bjoernricks+python-quilt/bjoernricks+python-quilt/tests/data/pop/tempkj0emjgs/test2/pc/p2.patch')\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "name = content\n",
      "State:\n",
      "'f2'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _content(self, startdir, dirname=None):\n",
      "        files = []\n",
      "        dirs = []\n",
      "        if dirname:\n",
      "            dir = os.path.join(startdir, dirname)\n",
      "        else:\n",
      "            dir = startdir\n",
      "        contents = os.listdir(dir)\n",
      "        for content in contents:\n",
      "            if not dirname:\n",
      "                name = content\n",
      "            else:\n",
      "                name = os.path.join(dirname, content)\n",
      "            path = os.path.join(dir, content)\n",
      "            if os.path.isdir(path):\n",
      "                (newdirs, newfiles) = self._content(startdir, name)\n",
      "                dirs.append(name)\n",
      "                files.extend(newfiles)\n",
      "                dirs.extend(newdirs)\n",
      "            else:\n",
      "                files.append(name)\n",
      "        return (dirs, files)\n",
      "_content(self=<quilt.utils.Directory object at 0x7f5650301e50>, startdir='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/bjoernricks+python-quilt/bjoernricks+python-quilt/tests/data/pop/tempkj0emjgs/test2/pc/p2.patch', dirname=None, self.dirname='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/bjoernricks+python-quilt/bjoernricks+python-quilt/tests/data/pop/tempkj0emjgs/test2/pc/p2.patch')\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "files.append(name)\n",
      "State:\n",
      "['f2']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _apply_patch(self, patch, force=False, quiet=False):\n",
      "        patch_name = patch.get_name()\n",
      "        pc_dir = self.quilt_pc + patch_name\n",
      "        patch_file = self.quilt_patches + File(patch_name)\n",
      "        refresh = File(pc_dir.get_name() + \"~refresh\")\n",
      "        forced = False\n",
      "        self.applying_patch(patch)\n",
      "        if patch_file.exists():\n",
      "            try:\n",
      "                patch.run(self.cwd, patch_dir=self.quilt_patches, backup=True,\n",
      "                          prefix=pc_dir.get_name(), quiet=quiet)\n",
      "            except SubprocessError as e:\n",
      "                if not force:\n",
      "                    patch = RollbackPatch(self.cwd, pc_dir)\n",
      "                    patch.rollback()\n",
      "                    patch.delete_backup()\n",
      "                    raise QuiltError(\"Patch %s does not apply\" % patch_name)\n",
      "                else:\n",
      "                    refresh.touch()\n",
      "                    forced = True\n",
      "        self.db.add_patch(patch)\n",
      "        if pc_dir.exists():\n",
      "            timestamp = pc_dir + File(\".timestamp\")\n",
      "            timestamp.touch()\n",
      "        else:\n",
      "            pc_dir.create()\n",
      "        if not patch_file.exists():\n",
      "            self.applied_empty_patch(patch, False)\n",
      "        elif pc_dir.is_empty():\n",
      "            self.applied_empty_patch(patch, True)\n",
      "        elif forced:\n",
      "            raise QuiltError(\"Applied patch %s (forced; needs refresh)\" %\n",
      "                             patch.get_name())\n",
      "        else:\n",
      "            self.applied_patch(patch)\n",
      "_apply_patch(self=<quilt.push.Push object at 0x7f56504d3730>, patch=<Patch('p1.patch', 1, False) id=0x7f56504d3b80>, force=False, quiet=True, self.cwd='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/bjoernricks+python-quilt/bjoernricks+python-quilt/tests/data/push/temp4tvqal44/test1', self.db=<quilt.db.Db object at 0x7f56504d3a00>, self.quilt_patches=<quilt.utils.Directory object at 0x7f56504d3c10>, self.quilt_pc=<quilt.utils.Directory object at 0x7f56504d37c0>, self.series=<quilt.db.Series object at 0x7f56504d3640>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "patch_name = patch.get_name()\n",
      "State:\n",
      "'p1.patch'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _apply_patch(self, patch, force=False, quiet=False):\n",
      "        patch_name = patch.get_name()\n",
      "        pc_dir = self.quilt_pc + patch_name\n",
      "        patch_file = self.quilt_patches + File(patch_name)\n",
      "        refresh = File(pc_dir.get_name() + \"~refresh\")\n",
      "        forced = False\n",
      "        self.applying_patch(patch)\n",
      "        if patch_file.exists():\n",
      "            try:\n",
      "                patch.run(self.cwd, patch_dir=self.quilt_patches, backup=True,\n",
      "                          prefix=pc_dir.get_name(), quiet=quiet)\n",
      "            except SubprocessError as e:\n",
      "                if not force:\n",
      "                    patch = RollbackPatch(self.cwd, pc_dir)\n",
      "                    patch.rollback()\n",
      "                    patch.delete_backup()\n",
      "                    raise QuiltError(\"Patch %s does not apply\" % patch_name)\n",
      "                else:\n",
      "                    refresh.touch()\n",
      "                    forced = True\n",
      "        self.db.add_patch(patch)\n",
      "        if pc_dir.exists():\n",
      "            timestamp = pc_dir + File(\".timestamp\")\n",
      "            timestamp.touch()\n",
      "        else:\n",
      "            pc_dir.create()\n",
      "        if not patch_file.exists():\n",
      "            self.applied_empty_patch(patch, False)\n",
      "        elif pc_dir.is_empty():\n",
      "            self.applied_empty_patch(patch, True)\n",
      "        elif forced:\n",
      "            raise QuiltError(\"Applied patch %s (forced; needs refresh)\" %\n",
      "                             patch.get_name())\n",
      "        else:\n",
      "            self.applied_patch(patch)\n",
      "_apply_patch(self=<quilt.push.Push object at 0x7f56504d3730>, patch=<Patch('p1.patch', 1, False) id=0x7f56504d3b80>, force=False, quiet=True, self.cwd='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/bjoernricks+python-quilt/bjoernricks+python-quilt/tests/data/push/temp4tvqal44/test1', self.db=<quilt.db.Db object at 0x7f56504d3a00>, self.quilt_patches=<quilt.utils.Directory object at 0x7f56504d3c10>, self.quilt_pc=<quilt.utils.Directory object at 0x7f56504d37c0>, self.series=<quilt.db.Series object at 0x7f56504d3640>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "pc_dir = self.quilt_pc + patch_name\n",
      "State:\n",
      "{dirname='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/bjoernricks+python-quilt/bjoernricks+python-quilt/tests/data/push/temp4tvqal44/test1/pc/p1.patch'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _apply_patch(self, patch, force=False, quiet=False):\n",
      "        patch_name = patch.get_name()\n",
      "        pc_dir = self.quilt_pc + patch_name\n",
      "        patch_file = self.quilt_patches + File(patch_name)\n",
      "        refresh = File(pc_dir.get_name() + \"~refresh\")\n",
      "        forced = False\n",
      "        self.applying_patch(patch)\n",
      "        if patch_file.exists():\n",
      "            try:\n",
      "                patch.run(self.cwd, patch_dir=self.quilt_patches, backup=True,\n",
      "                          prefix=pc_dir.get_name(), quiet=quiet)\n",
      "            except SubprocessError as e:\n",
      "                if not force:\n",
      "                    patch = RollbackPatch(self.cwd, pc_dir)\n",
      "                    patch.rollback()\n",
      "                    patch.delete_backup()\n",
      "                    raise QuiltError(\"Patch %s does not apply\" % patch_name)\n",
      "                else:\n",
      "                    refresh.touch()\n",
      "                    forced = True\n",
      "        self.db.add_patch(patch)\n",
      "        if pc_dir.exists():\n",
      "            timestamp = pc_dir + File(\".timestamp\")\n",
      "            timestamp.touch()\n",
      "        else:\n",
      "            pc_dir.create()\n",
      "        if not patch_file.exists():\n",
      "            self.applied_empty_patch(patch, False)\n",
      "        elif pc_dir.is_empty():\n",
      "            self.applied_empty_patch(patch, True)\n",
      "        elif forced:\n",
      "            raise QuiltError(\"Applied patch %s (forced; needs refresh)\" %\n",
      "                             patch.get_name())\n",
      "        else:\n",
      "            self.applied_patch(patch)\n",
      "_apply_patch(self=<quilt.push.Push object at 0x7f56504d3730>, patch=<Patch('p1.patch', 1, False) id=0x7f56504d3b80>, force=False, quiet=True, self.cwd='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/bjoernricks+python-quilt/bjoernricks+python-quilt/tests/data/push/temp4tvqal44/test1', self.db=<quilt.db.Db object at 0x7f56504d3a00>, self.quilt_patches=<quilt.utils.Directory object at 0x7f56504d3c10>, self.quilt_pc=<quilt.utils.Directory object at 0x7f56504d37c0>, self.series=<quilt.db.Series object at 0x7f56504d3640>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "patch_file = self.quilt_patches + File(patch_name)\n",
      "State:\n",
      "{filename='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/bjoernricks+python-quilt/bjoernricks+python-quilt/tests/data/push/temp4tvqal44/test1/patches/p1.patch'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _apply_patch(self, patch, force=False, quiet=False):\n",
      "        patch_name = patch.get_name()\n",
      "        pc_dir = self.quilt_pc + patch_name\n",
      "        patch_file = self.quilt_patches + File(patch_name)\n",
      "        refresh = File(pc_dir.get_name() + \"~refresh\")\n",
      "        forced = False\n",
      "        self.applying_patch(patch)\n",
      "        if patch_file.exists():\n",
      "            try:\n",
      "                patch.run(self.cwd, patch_dir=self.quilt_patches, backup=True,\n",
      "                          prefix=pc_dir.get_name(), quiet=quiet)\n",
      "            except SubprocessError as e:\n",
      "                if not force:\n",
      "                    patch = RollbackPatch(self.cwd, pc_dir)\n",
      "                    patch.rollback()\n",
      "                    patch.delete_backup()\n",
      "                    raise QuiltError(\"Patch %s does not apply\" % patch_name)\n",
      "                else:\n",
      "                    refresh.touch()\n",
      "                    forced = True\n",
      "        self.db.add_patch(patch)\n",
      "        if pc_dir.exists():\n",
      "            timestamp = pc_dir + File(\".timestamp\")\n",
      "            timestamp.touch()\n",
      "        else:\n",
      "            pc_dir.create()\n",
      "        if not patch_file.exists():\n",
      "            self.applied_empty_patch(patch, False)\n",
      "        elif pc_dir.is_empty():\n",
      "            self.applied_empty_patch(patch, True)\n",
      "        elif forced:\n",
      "            raise QuiltError(\"Applied patch %s (forced; needs refresh)\" %\n",
      "                             patch.get_name())\n",
      "        else:\n",
      "            self.applied_patch(patch)\n",
      "_apply_patch(self=<quilt.push.Push object at 0x7f56504d3730>, patch=<Patch('p1.patch', 1, False) id=0x7f56504d3b80>, force=False, quiet=True, self.cwd='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/bjoernricks+python-quilt/bjoernricks+python-quilt/tests/data/push/temp4tvqal44/test1', self.db=<quilt.db.Db object at 0x7f56504d3a00>, self.quilt_patches=<quilt.utils.Directory object at 0x7f56504d3c10>, self.quilt_pc=<quilt.utils.Directory object at 0x7f56504d37c0>, self.series=<quilt.db.Series object at 0x7f56504d3640>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "refresh = File(pc_dir.get_name() + \"~refresh\")\n",
      "State:\n",
      "{filename='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/bjoernricks+python-quilt/bjoernricks+python-quilt/tests/data/push/temp4tvqal44/test1/pc/p1.patch~refresh'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def equal(self, cwd):\n",
      "        cmd = [\"diff\"]\n",
      "        cmd.append(\"-q\")\n",
      "        cmd.append(self.left.get_name())\n",
      "        cmd.append(self.right.get_name())\n",
      "        try:\n",
      "            Process(cmd).run(cwd=cwd, suppress_output=True)\n",
      "        except SubprocessError as e:\n",
      "            if e.get_returncode() == 1:\n",
      "                return False\n",
      "            else:\n",
      "                raise e\n",
      "        return True\n",
      "equal(self=<quilt.patch.Diff object at 0x7f56504d3a30>, cwd='.', self.left=<quilt.utils.File object at 0x7f56504d3130>, self.right=<quilt.utils.TmpFile object at 0x7f56504d3490>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "cmd = [\"diff\"]\n",
      "State:\n",
      "['diff']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def equal(self, cwd):\n",
      "        cmd = [\"diff\"]\n",
      "        cmd.append(\"-q\")\n",
      "        cmd.append(self.left.get_name())\n",
      "        cmd.append(self.right.get_name())\n",
      "        try:\n",
      "            Process(cmd).run(cwd=cwd, suppress_output=True)\n",
      "        except SubprocessError as e:\n",
      "            if e.get_returncode() == 1:\n",
      "                return False\n",
      "            else:\n",
      "                raise e\n",
      "        return True\n",
      "equal(self=<quilt.patch.Diff object at 0x7f56504d3a30>, cwd='.', self.left=<quilt.utils.File object at 0x7f56504d3130>, self.right=<quilt.utils.TmpFile object at 0x7f56504d3490>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "cmd.append(\"-q\")\n",
      "State:\n",
      "['diff', '-q']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _reset(self):\n",
      "        self.m_event = -1\n",
      "        self.m_lineNumber = -1\n",
      "        self.m_name = -1\n",
      "        self.m_namespaceUri = -1\n",
      "        self.m_attributes = []\n",
      "        self.m_idAttribute = -1\n",
      "        self.m_classAttribute = -1\n",
      "        self.m_styleAttribute = -1\n",
      "_reset(self=<androguard.core.axml.AXMLParser object at 0x7f3dd7542cd0>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.m_event = -1\n",
      "State:\n",
      "-1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _reset(self):\n",
      "        self.m_event = -1\n",
      "        self.m_lineNumber = -1\n",
      "        self.m_name = -1\n",
      "        self.m_namespaceUri = -1\n",
      "        self.m_attributes = []\n",
      "        self.m_idAttribute = -1\n",
      "        self.m_classAttribute = -1\n",
      "        self.m_styleAttribute = -1\n",
      "_reset(self=<androguard.core.axml.AXMLParser object at 0x7f3dd7542cd0>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.m_lineNumber = -1\n",
      "State:\n",
      "-1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _reset(self):\n",
      "        self.m_event = -1\n",
      "        self.m_lineNumber = -1\n",
      "        self.m_name = -1\n",
      "        self.m_namespaceUri = -1\n",
      "        self.m_attributes = []\n",
      "        self.m_idAttribute = -1\n",
      "        self.m_classAttribute = -1\n",
      "        self.m_styleAttribute = -1\n",
      "_reset(self=<androguard.core.axml.AXMLParser object at 0x7f3dd7542cd0>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.m_name = -1\n",
      "State:\n",
      "-1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _reset(self):\n",
      "        self.m_event = -1\n",
      "        self.m_lineNumber = -1\n",
      "        self.m_name = -1\n",
      "        self.m_namespaceUri = -1\n",
      "        self.m_attributes = []\n",
      "        self.m_idAttribute = -1\n",
      "        self.m_classAttribute = -1\n",
      "        self.m_styleAttribute = -1\n",
      "_reset(self=<androguard.core.axml.AXMLParser object at 0x7f3dd7542cd0>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "self.m_namespaceUri = -1\n",
      "State:\n",
      "-1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _reset(self):\n",
      "        self.m_event = -1\n",
      "        self.m_lineNumber = -1\n",
      "        self.m_name = -1\n",
      "        self.m_namespaceUri = -1\n",
      "        self.m_attributes = []\n",
      "        self.m_idAttribute = -1\n",
      "        self.m_classAttribute = -1\n",
      "        self.m_styleAttribute = -1\n",
      "_reset(self=<androguard.core.axml.AXMLParser object at 0x7f3dd7542cd0>)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "self.m_attributes = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _print_namespace(self, uri):\n",
      "        if uri != \"\":\n",
      "            uri = \"{{{}}}\".format(uri)\n",
      "        return uri\n",
      "_print_namespace(self=<androguard.core.axml.AXMLPrinter object at 0x7f3dd75422e0>, uri='http://schemas.android.com/apk/res/android', self.axml=<androguard.core.axml.AXMLParser object at 0x7f3dd7542cd0>, self.packerwarning=False, self.root=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "uri = \"{{{}}}\".format(uri)\n",
      "State:\n",
      "'{http://schemas.android.com/apk/res/android}'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __next__(self):\n",
      "        self._do_next()\n",
      "        return self.m_event\n",
      "__next__(self=<androguard.core.axml.AXMLParser object at 0x7f3dd7542cd0>, self._valid=True, self.axml_tampered=False, self.buff=<_io.BufferedReader>, self.buff_size=8976, self.filesize=8976, self.m_attributes=[], self.m_classAttribute=-1, self.m_event=-1, self.m_idAttribute=-1, self.m_lineNumber=-1, self.m_name=-1, self.m_namespaceUri=-1, self.m_resourceIDs=[], self.m_styleAttribute=-1, self.namespaces=[], self.sb=<StringPool\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self._do_next()\n",
      "State:\n",
      "20\n",
      "==================================================\n",
      "Clean Code:\n",
      "def setLevel(self, level):\n",
      "        self.__level = level\n",
      "setLevel(self=<QlibLogger qlib.timer (WARNING)>, level=20, self._QlibLogger__level=0, self.module_name='qlib.timer')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.__level = level\n",
      "State:\n",
      "<QlibLogger qlib.timer (INFO)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _load_config(self, path=None):\n",
      "        try_files = self.try_files[:]\n",
      "        try:\n",
      "            try_files[0] = try_files[0] % os.environ[\"HOME\"]\n",
      "        except KeyError:\n",
      "            logger.debug(\"Can't get $HOME\")\n",
      "            try_files.pop(0)\n",
      "        if path:\n",
      "            try_files.insert(0, path)\n",
      "        config = None\n",
      "        for i, p in enumerate(try_files):\n",
      "            try:\n",
      "                with open(p, \"r\") as f:\n",
      "                    config = yaml.load(f)\n",
      "            except FileNotFoundError as e:\n",
      "                logger.debug('Can\\'t load config from \"%s\"' % p)\n",
      "                if len(try_files) == 3 and i == 0:\n",
      "                    raise DidwwAptlyCtlError(e)\n",
      "            except yaml.YAMLError as e:\n",
      "                raise DidwwAptlyCtlError(\"Cannot parse config: \", e)\n",
      "            else:\n",
      "                break\n",
      "        return config\n",
      "_load_config(self=<tests.test_Config.TestConfig.dummyConfig.<locals>._dummyConfig object at 0x7f0a616edf70>, path=local('/tmp/pytest-of-XXX/pytest-203/test_loading_nonexistent_confi0/aptly-ctl.conf'))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "try_files = self.try_files[:]\n",
      "State:\n",
      "['%s/.config/aplty-ctl.conf', '/etc/aptly-ctl.conf']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _load_config(self, path=None):\n",
      "        try_files = self.try_files[:]\n",
      "        try:\n",
      "            try_files[0] = try_files[0] % os.environ[\"HOME\"]\n",
      "        except KeyError:\n",
      "            logger.debug(\"Can't get $HOME\")\n",
      "            try_files.pop(0)\n",
      "        if path:\n",
      "            try_files.insert(0, path)\n",
      "        config = None\n",
      "        for i, p in enumerate(try_files):\n",
      "            try:\n",
      "                with open(p, \"r\") as f:\n",
      "                    config = yaml.load(f)\n",
      "            except FileNotFoundError as e:\n",
      "                logger.debug('Can\\'t load config from \"%s\"' % p)\n",
      "                if len(try_files) == 3 and i == 0:\n",
      "                    raise DidwwAptlyCtlError(e)\n",
      "            except yaml.YAMLError as e:\n",
      "                raise DidwwAptlyCtlError(\"Cannot parse config: \", e)\n",
      "            else:\n",
      "                break\n",
      "        return config\n",
      "_load_config(self=<tests.test_Config.TestConfig.dummyConfig.<locals>._dummyConfig object at 0x7f0a616edf70>, path=local('/tmp/pytest-of-XXX/pytest-203/test_loading_nonexistent_confi0/aptly-ctl.conf'))\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "try_files[0] = try_files[0] % os.environ[\"HOME\"]\n",
      "State:\n",
      "['/home/XXX/.config/aplty-ctl.conf', '/etc/aptly-ctl.conf']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_cmd_line_overrides(self, cfg_overrides):\n",
      "        result = dict()\n",
      "        for expr in cfg_overrides:\n",
      "            key, sep, val = expr.partition(\"=\")\n",
      "            if len(sep) == 0 or len(key) == 0:\n",
      "                raise DidwwAptlyCtlError('Wrong configuration key: \"%s\"' % expr)\n",
      "            nested_set(result, key.split(\".\"), val)\n",
      "        else:\n",
      "            return result\n",
      "_parse_cmd_line_overrides(self=<tests.test_Config.TestConfig.dummyConfig.<locals>._dummyConfig object at 0x7f0a616f17f0>, cfg_overrides=['url=http://overriden/api', 'signing.gpg_key=ABCDEFG', 'signing.passphrase=pass=phrase'])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "result = dict()\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def nested_set(dic, keys, value):\n",
      "    for key in keys[:-1]:\n",
      "        dic = dic.setdefault(key, {})\n",
      "    dic[keys[-1]] = value\n",
      "nested_set(dic={}, keys=['url'], value='http://overriden/api')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "dic[keys[-1]] = value\n",
      "State:\n",
      "{'url': 'http://overriden/api'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def version_compare(self, other):\n",
      "        \"Compares version of the form [epoch:]upstream-version[-debian-revision]\" \\\n",
      "        + \" according to Debian package version number format.\"\n",
      "        diff = self.epoch - other.epoch\n",
      "        if diff != 0:\n",
      "            return diff\n",
      "        for slf, othr in (self.upstream_version, other.upstream_version), (self.revision, other.revision):\n",
      "            i = 0\n",
      "            while len(slf) > 0 or len(othr) > 0:\n",
      "                decimal = (i % 2 == 1) \n",
      "                slf_part, slf = self._get_part(slf, decimal=decimal)\n",
      "                othr_part, othr = self._get_part(othr, decimal=decimal)\n",
      "                diff = self._compare_parts(slf_part, othr_part, decimal=decimal)\n",
      "                if diff != 0:\n",
      "                    return diff\n",
      "                i += 1\n",
      "        return 0\n",
      "version_compare(self=2.2.0~rc5, other=2.2.0~rc5, self.epoch=0, self.revision='0', self.revision_allowed_chars=('.', '+', '~'), self.upstream_version='2.2.0~rc5', self.upstream_version_allowed_chars=('.', '+', '~', '-', ':'), self.version='2.2.0~rc5')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "diff = self.epoch - other.epoch\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_part(self, s, decimal):\n",
      "        \"Strips first part of string containing either non-decimal or decimal characters.\" \\\n",
      "        + \" Returns tuple (part, remider).\"\n",
      "        div = 0\n",
      "        for c in s:\n",
      "            if decimal and not c.isdecimal():\n",
      "                break\n",
      "            elif not decimal and c.isdecimal():\n",
      "                break\n",
      "            else:\n",
      "                div += 1\n",
      "        return (s[:div], s[div:])\n",
      "_get_part(self=2.2.0~rc5, s='2.2.0~rc5', decimal=False, self.epoch=0, self.revision='0', self.revision_allowed_chars=('.', '+', '~'), self.upstream_version='2.2.0~rc5', self.upstream_version_allowed_chars=('.', '+', '~', '-', ':'), self.version='2.2.0~rc5')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "div = 0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def update(self, error, state, action):\n",
      "    features = self.basis.features(state)\n",
      "    self.weights[action] += self.alpha * error * features\n",
      "update(self=<all.approximation.action.discrete_linear.DiscreteLinearApproximation object at 0x7f31aa0b30d0>, error=1, state=array([0.5, 1. ]), action=1, self.alpha=0.1, self.basis=<all.approximation.bases.fourier.FourierBasis object at 0x7f31aa297580>, self.weights=array([[0., 0., 0., 0., 0., 0., 0., 0., 0.],       [0., 0., 0., 0., 0., 0., 0., 0., 0.],       [0., 0., 0., 0., 0., 0., 0., 0., 0.]]))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "features = self.basis.features(state)\n",
      "State:\n",
      "array([ 1.0000000e+00,  6.1232340e-17, -1.0000000e+00, -1.0000000e+00,        1.0000000e+00, -1.8369702e-16,  3.0616170e-16,  1.0000000e+00,       -1.0000000e+00])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def update(self, error, state, action):\n",
      "    features = self.basis.features(state)\n",
      "    self.weights[action] += self.alpha * error * features\n",
      "update(self=<all.approximation.action.discrete_linear.DiscreteLinearApproximation object at 0x7f31aa0b30d0>, error=1, state=array([0.5, 1. ]), action=1, self.alpha=0.1, self.basis=<all.approximation.bases.fourier.FourierBasis object at 0x7f31aa297580>, self.weights=array([[0., 0., 0., 0., 0., 0., 0., 0., 0.],       [0., 0., 0., 0., 0., 0., 0., 0., 0.],       [0., 0., 0., 0., 0., 0., 0., 0., 0.]]))\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.weights[action] += self.alpha * error * features\n",
      "State:\n",
      "array([[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,         0.0000000e+00],       [ 1.0000000e-01,  6.1232340e-18, -1.0000000e-01, -1.0000000e-01,         1.0000000e-01, -1.8369702e-17,  3.0616170e-17,  1.0000000e-01,        -1.0000000e-01],       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,         0.0000000e+00]])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def getorcreatesubcontext(self, path):\n",
      "        for name in path:\n",
      "            that = self.resolvables.get(name)\n",
      "            if that is None:\n",
      "                self.resolvables[name] = that = Context(self)\n",
      "            self = that\n",
      "        return self\n",
      "getorcreatesubcontext(self=Context(SuperContext(), False), path=('woo',), self.islist=False, self.parent=SuperContext(), self.resolvables=OrderedDict())\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "that = self.resolvables.get(name)\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def getorcreatesubcontext(self, path):\n",
      "        for name in path:\n",
      "            that = self.resolvables.get(name)\n",
      "            if that is None:\n",
      "                self.resolvables[name] = that = Context(self)\n",
      "            self = that\n",
      "        return self\n",
      "getorcreatesubcontext(self=Context(SuperContext(), False), path=('woo',), self.islist=False, self.parent=SuperContext(), self.resolvables=OrderedDict())\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "self.resolvables[name] = that = Context(self)\n",
      "State:\n",
      "Context(Context(SuperContext(), False), False)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def getorcreatesubcontext(self, path):\n",
      "        for name in path:\n",
      "            that = self.resolvables.get(name)\n",
      "            if that is None:\n",
      "                self.resolvables[name] = that = Context(self)\n",
      "            self = that\n",
      "        return self\n",
      "getorcreatesubcontext(self=Context(SuperContext(), False), path=('woo',), self.islist=False, self.parent=SuperContext(), self.resolvables=OrderedDict())\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "self = that\n",
      "State:\n",
      "Context(Context(SuperContext(), False), False)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _resolved(self, path, kwargs):\n",
      "        if not path:\n",
      "            return self\n",
      "        c = self\n",
      "        for name in path[:-1]:\n",
      "            that = self.resolvables.get(name)\n",
      "            c = Context(c) if that is None else that.resolve(c)\n",
      "        del c\n",
      "        name, tail = path[0], path[1:]\n",
      "        resolvable = self.resolvables.get(name)\n",
      "        if resolvable is None:\n",
      "            raise NoSuchPathException(path)\n",
      "        return resolvable.resolve(self).resolved(*tail, **kwargs) if tail else resolvable.resolve(self, **kwargs)\n",
      "_resolved(self=SuperContext(), path=('hmm',), kwargs={}, self.depth=1, self.parent=<aridimpl.context.SuperContext.EmptyContext object at 0x7fe13ba47a60>, self.resolvables=OrderedDict([(':', Directive(<aridimpl.directives.Colon object at 0x7fe13ba8ccd0>)), ('redirect', Directive(<aridimpl.directives.Redirect object at 0x7fe13ba8cdf0>)), ('write', Directive(<aridimpl.directives.Write object at 0x7fe13ba8cd30>)), ('.', Directive(<aridimpl.directives.Source object at 0x7fe13ba8c4c0>)), ('cd', Directive(<aridimpl.directives.CD object at 0x7fe13ba8ce20>)), ('test', Directive(<aridimpl.directives.Test object at 0x7fe13ba8c310>)), ('=', Directive(<aridimpl.directives.Equals object at 0x7fe13ba8cd60>)), (':=', Directive(<aridimpl.directives.ColonEquals object at 0x7fe13ba8c430>)), ('+=', Directive(<aridimpl.directives.PlusEquals object at 0x7fe13ba8cf70>)), ('<', Directive(<aridimpl.directives.Cat object at 0x7fe13ba8c040>)), (',', Function(<function Functions.aslist at 0x7fe13ba6b790>)), ('div', Function(<function Functions.div at 0x7fe13ba6baf0>)), ('fork', Function(<function Functions.fork at 0x7fe13ba6b940>)), ('get', Function(<function Functions.get at 0x7fe13ba6b5e0>)), ('', Function(<function Functions.get_ at 0x7fe13ba6b700>)), ('groovystr', Function(<function Functions.groovystr at 0x7fe13ba6b1f0>)), ('hclstr', Function(<function Functions.hclstr at 0x7fe13ba6b160>)), ('./', Function(<function Functions.hereslash at 0x7fe13ba6bca0>)), ('java', Function(<function Functions.java at 0x7fe13ba6b820>)), ('join', Function(<function Functions.join at 0x7fe13ba6b550>)), ('jsonquote', Function(<function Functions.jsonquote at 0x7fe13ba6b3a0>)), ('label', Function(<function Functions.label at 0x7fe13ba6b4c0>)), ('list', Function(<function Functions.list at 0x7fe13ba6b8b0>)), ('map', Function(<function Functions.map at 0x7fe13ba6b430>)), ('mul', Function(<function Functions.mul at 0x7fe13ba6b040>)), ('processtemplate', Function(<function Functions.processtemplate at 0x7fe13ba6bd30>)), ('pystr', Function(<function Functions.pystr at 0x7fe13ba6b280>)), ('readfile', Function(<function Functions.readfile at 0x7fe13ba6b9d0>)), ('repr', Function(<function Functions.repr at 0x7fe13ba6bb80>)), ('screenstr', Function(<function Functions.screenstr at 0x7fe13bac1f70>)), ('scstr', Function(<function Functions.scstr at 0x7fe13ba6b0d0>)), ('shstr', Function(<function Functions.shstr at 0x7fe13ba6b310>)), ('str', Function(<function Functions.str at 0x7fe13ba6b670>)), ('try', Function(<function Functions.try_ at 0x7fe13ba6ba60>)), ('~', Text('/home/XXX')), ('LF', Text('\\n')), ('EOL', Text('\\n')), ('stdout', Stream(<_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>)), ('/', Slash()), ('None', Scalar(None))]))\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "c = self\n",
      "State:\n",
      "SuperContext()\n",
      "==================================================\n",
      "Clean Code:\n",
      "def indent(self):\n",
      "        indent = []\n",
      "        for r in self.resolvables:\n",
      "            if not r.ignorable or r.boundary:\n",
      "                break\n",
      "            indent.append(r)\n",
      "        return Concat.unlesssingleton(indent).resolve(None).cat()\n",
      "indent(self=Entry([Text('woo'), Blank(' '), Text('+='), Blank(' '), Text('yay')]), self.resolvables=[Text('woo'), Blank(' '), Text('+='), Blank(' '), Text('yay')])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "indent = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def append(resolvable):\n",
      "                directives.append((resolvable.directivevalue, i))\n",
      "                raise self.Enough\n",
      "append(resolvable=Directive(<aridimpl.directives.Colon object at 0x7fe13ba8ccd0>), directives=[], i=0, self=Context(SuperContext(), False), self.islist=False, self.parent=SuperContext(), self.resolvables=OrderedDict())\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "raise self.Enough\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def trim(end):\n",
      "            while v and v[end].ignorable:\n",
      "                del v[end]\n",
      "trim(end=-1, v=[Text('woo'), Blank(' '), Text('+='), Blank(' ')])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "del v[end]\n",
      "State:\n",
      "[Text('woo'), Blank(' '), Text('+=')]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def resolve(self, context):\n",
      "        c = Context(self.parent, self.islist)\n",
      "        for name, r in self.resolvables.items():\n",
      "            if name is not None:\n",
      "                c.resolvables[name] = r\n",
      "        defaults = self.resolvables.get(None)\n",
      "        if defaults is not None:\n",
      "            for item in c.resolvables:\n",
      "                for dn, dr in defaults.resolvables.items():\n",
      "                    if dn not in item.resolvables.keys():\n",
      "                        item.resolvables[dn] = dr\n",
      "        return c\n",
      "resolve(self=Context(Context(SuperContext(), False), False), context=Context(SuperContext(), False), self.islist=False, self.parent=Context(SuperContext(), False), self.resolvables=OrderedDict([('yay', Text('yay')), ('$(houpla  )', Call('', [Text('houpla'), Blank('  ')], '()'))]))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "c = Context(self.parent, self.islist)\n",
      "State:\n",
      "Context(Context(SuperContext(), False), False)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def resolve(self, context):\n",
      "        c = Context(self.parent, self.islist)\n",
      "        for name, r in self.resolvables.items():\n",
      "            if name is not None:\n",
      "                c.resolvables[name] = r\n",
      "        defaults = self.resolvables.get(None)\n",
      "        if defaults is not None:\n",
      "            for item in c.resolvables:\n",
      "                for dn, dr in defaults.resolvables.items():\n",
      "                    if dn not in item.resolvables.keys():\n",
      "                        item.resolvables[dn] = dr\n",
      "        return c\n",
      "resolve(self=Context(Context(SuperContext(), False), False), context=Context(SuperContext(), False), self.islist=False, self.parent=Context(SuperContext(), False), self.resolvables=OrderedDict([('yay', Text('yay')), ('$(houpla  )', Call('', [Text('houpla'), Blank('  ')], '()'))]))\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "defaults = self.resolvables.get(None)\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def fork(self, text):\n",
      "        context = Context()\n",
      "        for entry in l(text):\n",
      "            context.execute(entry)\n",
      "        ae = self.assertEqual\n",
      "        ae(Text('uno'), context.resolved('v', 'one', '1'))\n",
      "        ae({'1': 'uno'}, context.resolved('v', 'one').unravel())\n",
      "        ae({'hmm': 'yay'}, context.resolved('v', 'two').unravel())\n",
      "        ae(Text('woo'), context.resolved('v', 'one').resolved('hmm'))\n",
      "        ae(Text('yay'), context.resolved('v', 'two').resolved('hmm'))\n",
      "        return context\n",
      "fork(self=<aridimpl.test_context.TestContext testMethod=test_fork>, text='hmm = woo\\nv := $list()\\nv one := $fork()\\nv one 1 = uno\\nv two := $fork()\\n\\r\\r\\nv two hmm = yay', self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fe13bb73e20>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_fork', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_fork=<bound method TestContext.test_fork of <aridimpl.test_context.TestContext testMethod=test_fork>>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "context = Context()\n",
      "State:\n",
      "Context(SuperContext(), False)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def fork(self, text):\n",
      "        context = Context()\n",
      "        for entry in l(text):\n",
      "            context.execute(entry)\n",
      "        ae = self.assertEqual\n",
      "        ae(Text('uno'), context.resolved('v', 'one', '1'))\n",
      "        ae({'1': 'uno'}, context.resolved('v', 'one').unravel())\n",
      "        ae({'hmm': 'yay'}, context.resolved('v', 'two').unravel())\n",
      "        ae(Text('woo'), context.resolved('v', 'one').resolved('hmm'))\n",
      "        ae(Text('yay'), context.resolved('v', 'two').resolved('hmm'))\n",
      "        return context\n",
      "fork(self=<aridimpl.test_context.TestContext testMethod=test_fork>, text='hmm = woo\\nv := $list()\\nv one := $fork()\\nv one 1 = uno\\nv two := $fork()\\n\\r\\r\\nv two hmm = yay', self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fe13bb73e20>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_fork', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_fork=<bound method TestContext.test_fork of <aridimpl.test_context.TestContext testMethod=test_fork>>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "ae = self.assertEqual\n",
      "State:\n",
      "<bound method TestCase.assertEqual of <aridimpl.test_context.TestContext testMethod=test_fork>>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def execute(self, entry):\n",
      "        directives = []\n",
      "        for i, word in enumerate(entry.words()):\n",
      "            def append(resolvable):\n",
      "                directives.append((resolvable.directivevalue, i))\n",
      "                raise self.Enough\n",
      "            try:\n",
      "                self.getresolvables(word.cat(), append)\n",
      "            except (AttributeError, CatNotSupportedException, self.Enough):\n",
      "                pass\n",
      "        if 1 != len(directives):\n",
      "            raise UnsupportedEntryException(\"Expected 1 directive but %s found: %s\" % (directives, entry))\n",
      "        d, i = directives[0]\n",
      "        d(entry.subentry(0, i), entry.phrase(i + 1), self)\n",
      "execute(self=Context(SuperContext(), False), entry=Entry([Text('ns'), Text('woo'), Blank(' '), Text('='), Blank(' '), Text('yay')]), self.islist=False, self.parent=SuperContext(), self.resolvables=OrderedDict([('here', Text('/tmp'))]))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "directives = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def map(context, objs, *args):\n",
      "        if 1 == len(args):\n",
      "            expr, = args\n",
      "            def g():\n",
      "                for k, v in objs.resolve(context).resolvables.items():\n",
      "                    c = v.createchild()\n",
      "                    c.label = Text(k)\n",
      "                    yield expr.resolve(c)\n",
      "            return List(list(g()))\n",
      "        elif 2 == len(args):\n",
      "            name, expr = args\n",
      "            name = name.resolve(context).cat()\n",
      "            def g():\n",
      "                for obj in objs.resolve(context):\n",
      "                    c = context.createchild()\n",
      "                    c[name,] = obj\n",
      "                    yield expr.resolve(c)\n",
      "            return List(list(g()))\n",
      "        else:\n",
      "            kname, vname, expr = args\n",
      "            kname = kname.resolve(context).cat()\n",
      "            vname = vname.resolve(context).cat()\n",
      "            def g():\n",
      "                for k, v in objs.resolve(context).resolvables.items():\n",
      "                    c = context.createchild()\n",
      "                    c[kname,] = Text(k)\n",
      "                    c[vname,] = v\n",
      "                    yield expr.resolve(c)\n",
      "            return List(list(g()))\n",
      "map(context=Context(SuperContext(), False), objs=Call('', [Text('items')], '()'), args=(Call('', [Text('value')], '()'),))\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "expr, = args\n",
      "State:\n",
      "Call('', [Text('value')], '()')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def merge_dict(source, overrides):\n",
      "    merged = source.copy()\n",
      "    merged.update(overrides)\n",
      "    return merged\n",
      "merge_dict(source={}, overrides={})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "merged = source.copy()\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add_to_class(self, model_class, name):\n",
      "        self.name = name\n",
      "        self.model_class = model_class\n",
      "        self.db_column = self.db_column or self.name\n",
      "        if not self.verbose_name:\n",
      "            self.verbose_name = re.sub('_+', ' ', name).title()\n",
      "        model_class._meta.fields[self.name] = self\n",
      "        model_class._meta.columns[self.db_column] = self\n",
      "        setattr(model_class, name, FieldDescriptor(self))\n",
      "        self._is_bound = True\n",
      "add_to_class(self=<peewee.PrimaryKeyField object at 0x7f9ad1c4d550>, model_class=<class 'peewee.NewBase'>, name='id', self._alias=None, self._bind_to=None, self._is_bound=False, self._negated=False, self._order=1, self._ordering=None, self._sort_key=(1, 1), self.choices=None, self.constraints=None, self.db_column=None, self.default=None, self.help_text=None, self.index=False, self.null=False, self.primary_key=True, self.schema=None, self.sequence=None, self.unique=False, self.verbose_name=None)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "self.verbose_name = re.sub('_+', ' ', name).title()\n",
      "State:\n",
      "'Id'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_database(self, backend=None, db_class=None, **kwargs):\n",
      "        backend = backend or self.backend\n",
      "        method = 'get_%s_database' % backend\n",
      "        if db_class is None:\n",
      "            db_class = self.get_database_class(backend)\n",
      "        if not hasattr(self, method):\n",
      "            return db_class(self.database_name, **kwargs)\n",
      "        else:\n",
      "            return getattr(self, method)(db_class, **kwargs)\n",
      "get_database(self=<playhouse.tests.base.DatabaseInitializer object at 0x7f9ad1b98370>, backend=None, db_class=None, kwargs={}, self.backend='sqlite', self.database_name='peewee_test')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "backend = backend or self.backend\n",
      "State:\n",
      "'sqlite'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_database(self, backend=None, db_class=None, **kwargs):\n",
      "        backend = backend or self.backend\n",
      "        method = 'get_%s_database' % backend\n",
      "        if db_class is None:\n",
      "            db_class = self.get_database_class(backend)\n",
      "        if not hasattr(self, method):\n",
      "            return db_class(self.database_name, **kwargs)\n",
      "        else:\n",
      "            return getattr(self, method)(db_class, **kwargs)\n",
      "get_database(self=<playhouse.tests.base.DatabaseInitializer object at 0x7f9ad1b98370>, backend=None, db_class=None, kwargs={}, self.backend='sqlite', self.database_name='peewee_test')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "method = 'get_%s_database' % backend\n",
      "State:\n",
      "'get_sqlite_database'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __getattr__(self, name):\n",
      "        if name == '__bases__':\n",
      "            raise AttributeError\n",
      "        return self._sentinels.setdefault(name, _SentinelObject(name))\n",
      "__getattr__(self=<playhouse.tests.libs.mock._Sentinel object at 0x7f9acfb11d60>, name='DEFAULT', self._sentinels={'__slots__': sentinel.__slots__})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "return self._sentinels.setdefault(name, _SentinelObject(name))\n",
      "State:\n",
      "{'__slots__': sentinel.__slots__, 'DEFAULT': sentinel.DEFAULT}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def register_collation(self, fn, name=None):\n",
      "        name = name or fn.__name__\n",
      "        def _collation(*args):\n",
      "            expressions = args + (SQL('collate %s' % name),)\n",
      "            return Clause(*expressions)\n",
      "        fn.collation = _collation\n",
      "        self._collations[name] = fn\n",
      "        if not self.is_closed():\n",
      "            self._load_collations(self.get_conn())\n",
      "register_collation(self=<playhouse.sqlite_ext.SqliteExtDatabase object at 0x7f9ace563be0>, fn=<function collate_case_insensitive at 0x7f9ace527790>, name=None, self._Database__local=<peewee._ConnectionLocal object at 0x7f9ace572f40>, self._aggregates={}, self._collations={}, self._conn_lock=<unlocked _thread.lock object at 0x7f9ace563c30>, self._extensions=set(), self._functions={'rank': (<function rank at 0x7f9ad154e940>, 1), 'bm25': (<function bm25 at 0x7f9ad154e9d0>, -1)}, self._journal_mode=None, self._row_factory=None, self.autocommit=True, self.autorollback=False, self.connect_kwargs={'timeout': 0.1}, self.database='peewee_test.db', self.deferred=False, self.field_overrides={}, self.op_overrides={'like': 'GLOB', 'ilike': 'LIKE', 'match': 'MATCH'})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "name = name or fn.__name__\n",
      "State:\n",
      "'collate_case_insensitive'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _add_option(self, name, default):\n",
      "    if name in self._values:\n",
      "      raise RuntimeError(f'Config option {name} already defined')\n",
      "    self._values[name] = default\n",
      "_add_option(self=<flax.configurations.Config object at 0x7fce8be94490>, name='flax_filter_frames', default=True, self._values={})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self._values[name] = default\n",
      "State:\n",
      "{'flax_filter_frames': True}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _make_dir(*_, **__):\n",
      "            dir_created.append(1)\n",
      "_make_dir(_=(None,), __={}, dir_created=[])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "dir_created.append(1)\n",
      "State:\n",
      "[1]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _remove(*_, **__):\n",
      "            removed.append(1)\n",
      "_remove(_=(None,), __={}, removed=[])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "removed.append(1)\n",
      "State:\n",
      "[1]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def relpath(self, path):\n",
      "        for root in self.roots:\n",
      "            try:\n",
      "                if isinstance(root, Pattern):\n",
      "                    relative = root.split(path, maxsplit=1)[1]\n",
      "                else:\n",
      "                    relative = path.split(root, 1)[1]\n",
      "                return relative.lstrip('/')\n",
      "            except IndexError:\n",
      "                continue\n",
      "        return path\n",
      "relpath(self=<pycosio.storage.http._HTTPSystem object at 0x7ffa243b4400>, path='http://accelize.com', self._client=None, self._roots=('http://', 'https://'), self._storage_parameters={}, self._unsecure=False)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "for root in self.roots:\n",
      "State:\n",
      "'http://'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def request(method, url, headers=None, **_):\n",
      "            assert url\n",
      "            assert method in ('HEAD', 'GET')\n",
      "            response = Response()\n",
      "            if method == 'HEAD':\n",
      "                return response\n",
      "            try:\n",
      "                response.content = parse_range(headers)\n",
      "            except ValueError:\n",
      "                response.status_code = 416\n",
      "            return response\n",
      "request(method='HEAD', url='http://accelize.com', headers=None, _={'timeout': 5}, Response=<class 'tests.test_http.test_http_raw_io.<locals>.Response'>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "assert method in ('HEAD', 'GET')\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def request(method, url, headers=None, **_):\n",
      "            assert url\n",
      "            assert method in ('HEAD', 'GET')\n",
      "            response = Response()\n",
      "            if method == 'HEAD':\n",
      "                return response\n",
      "            try:\n",
      "                response.content = parse_range(headers)\n",
      "            except ValueError:\n",
      "                response.status_code = 416\n",
      "            return response\n",
      "request(method='HEAD', url='http://accelize.com', headers=None, _={'timeout': 5}, Response=<class 'tests.test_http.test_http_raw_io.<locals>.Response'>)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "response = Response()\n",
      "State:\n",
      "{status_code=200}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def check_raw_read_methods(io_object):\n",
      "    assert io_object.readall() == SIZE * BYTE\n",
      "    assert io_object.tell() == SIZE\n",
      "    assert io_object.seek(10) == 10\n",
      "    assert io_object.readall() == (SIZE - 10) * BYTE\n",
      "    assert io_object.tell() == SIZE\n",
      "    assert io_object.seek(0) == 0\n",
      "    buffer = bytearray(40)\n",
      "    assert io_object.readinto(buffer) == 40\n",
      "    assert bytes(buffer) == 40 * BYTE\n",
      "    assert io_object.tell() == 40\n",
      "    buffer = bytearray(40)\n",
      "    assert io_object.readinto(buffer) == 40\n",
      "    assert bytes(buffer) == 40 * BYTE\n",
      "    assert io_object.tell() == 80\n",
      "    buffer = bytearray(40)\n",
      "    assert io_object.readinto(buffer) == 20\n",
      "    assert bytes(buffer) == 20 * BYTE + b'\\x00' * 20\n",
      "    assert io_object.tell() == SIZE\n",
      "    buffer = bytearray(40)\n",
      "    assert io_object.readinto(buffer) == 0\n",
      "    assert bytes(buffer) == b'\\x00' * 40\n",
      "    assert io_object.tell() == SIZE\n",
      "check_raw_read_methods(io_object=<pycosio.storage.http.HTTPRawIO name='http://accelize.com' mode='r'>)\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "assert io_object.readinto(buffer) == 40\n",
      "State:\n",
      "bytearray(b'0000000000000000000000000000000000000000')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def patched(self, *args, **kwargs):\n",
      "        try:\n",
      "            return self._cache[method_name]\n",
      "        except KeyError:\n",
      "            result = self._cache[method_name] = method(\n",
      "                self, *args, **kwargs)\n",
      "            return result\n",
      "patched(self=<tests.test_io_buffered.DummyRawIO name='name' mode='r'>, args=(), kwargs={}, method=<function ObjectRawIOBase._head at 0x7ffa234fbee0>, method_name='_head', self._cache={}, self._client_kwargs={}, self._is_raw_of_buffered=False, self._mode='r', self._name='name', self._path='name', self._readable=True, self._seek=0, self._seek_lock=<unlocked _thread.lock object at 0x7ffa2009fd20>, self._seekable=True, self._system=<tests.test_io_buffered.test_object_buffered_base_io.<locals>.DummySystem object at 0x7ffa2009f3d0>, self._writable=False)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "return self._cache[method_name]\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _preload_range(self):\n",
      "        queue = self._read_queue\n",
      "        size = self._buffer_size\n",
      "        start = self._seek\n",
      "        end = int(start + size * self._max_buffers)\n",
      "        workers_submit = self._workers.submit\n",
      "        indexes = tuple(range(start, end, size))\n",
      "        for seek in tuple(queue):\n",
      "            if seek not in indexes:\n",
      "                del queue[seek]\n",
      "        read_range = self._read_range\n",
      "        for seek in indexes:\n",
      "            if seek not in queue:\n",
      "                queue[seek] = workers_submit(read_range, seek, seek + size)\n",
      "_preload_range(self=<tests.test_io_buffered.DummyBufferedIO name='name' mode='r'>, self._buffer_size=100, self._cache={}, self._client_kwargs={}, self._max_buffers=100, self._mode='r', self._name='name', self._raw=<tests.test_io_buffered.DummyRawIO name='name' mode='r'>, self._read_queue={}, self._read_range=<bound method test_object_buffered_base_io.<locals>.DummyRawIO._read_range of <tests.test_io_buffered.DummyRawIO name='name' mode='r'>>, self._readable=True, self._seek=0, self._seek_lock=<locked _thread.lock object at 0x7ffa2342a540>, self._seekable=True, self._size=10000, self._workers_count=None, self._workers_pool=None, self._writable=False)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "queue = self._read_queue\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _preload_range(self):\n",
      "        queue = self._read_queue\n",
      "        size = self._buffer_size\n",
      "        start = self._seek\n",
      "        end = int(start + size * self._max_buffers)\n",
      "        workers_submit = self._workers.submit\n",
      "        indexes = tuple(range(start, end, size))\n",
      "        for seek in tuple(queue):\n",
      "            if seek not in indexes:\n",
      "                del queue[seek]\n",
      "        read_range = self._read_range\n",
      "        for seek in indexes:\n",
      "            if seek not in queue:\n",
      "                queue[seek] = workers_submit(read_range, seek, seek + size)\n",
      "_preload_range(self=<tests.test_io_buffered.DummyBufferedIO name='name' mode='r'>, self._buffer_size=100, self._cache={}, self._client_kwargs={}, self._max_buffers=100, self._mode='r', self._name='name', self._raw=<tests.test_io_buffered.DummyRawIO name='name' mode='r'>, self._read_queue={}, self._read_range=<bound method test_object_buffered_base_io.<locals>.DummyRawIO._read_range of <tests.test_io_buffered.DummyRawIO name='name' mode='r'>>, self._readable=True, self._seek=0, self._seek_lock=<locked _thread.lock object at 0x7ffa2342a540>, self._seekable=True, self._size=10000, self._workers_count=None, self._workers_pool=None, self._writable=False)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "size = self._buffer_size\n",
      "State:\n",
      "100\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _preload_range(self):\n",
      "        queue = self._read_queue\n",
      "        size = self._buffer_size\n",
      "        start = self._seek\n",
      "        end = int(start + size * self._max_buffers)\n",
      "        workers_submit = self._workers.submit\n",
      "        indexes = tuple(range(start, end, size))\n",
      "        for seek in tuple(queue):\n",
      "            if seek not in indexes:\n",
      "                del queue[seek]\n",
      "        read_range = self._read_range\n",
      "        for seek in indexes:\n",
      "            if seek not in queue:\n",
      "                queue[seek] = workers_submit(read_range, seek, seek + size)\n",
      "_preload_range(self=<tests.test_io_buffered.DummyBufferedIO name='name' mode='r'>, self._buffer_size=100, self._cache={}, self._client_kwargs={}, self._max_buffers=100, self._mode='r', self._name='name', self._raw=<tests.test_io_buffered.DummyRawIO name='name' mode='r'>, self._read_queue={}, self._read_range=<bound method test_object_buffered_base_io.<locals>.DummyRawIO._read_range of <tests.test_io_buffered.DummyRawIO name='name' mode='r'>>, self._readable=True, self._seek=0, self._seek_lock=<locked _thread.lock object at 0x7ffa2342a540>, self._seekable=True, self._size=10000, self._workers_count=None, self._workers_pool=None, self._writable=False)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "start = self._seek\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _preload_range(self):\n",
      "        queue = self._read_queue\n",
      "        size = self._buffer_size\n",
      "        start = self._seek\n",
      "        end = int(start + size * self._max_buffers)\n",
      "        workers_submit = self._workers.submit\n",
      "        indexes = tuple(range(start, end, size))\n",
      "        for seek in tuple(queue):\n",
      "            if seek not in indexes:\n",
      "                del queue[seek]\n",
      "        read_range = self._read_range\n",
      "        for seek in indexes:\n",
      "            if seek not in queue:\n",
      "                queue[seek] = workers_submit(read_range, seek, seek + size)\n",
      "_preload_range(self=<tests.test_io_buffered.DummyBufferedIO name='name' mode='r'>, self._buffer_size=100, self._cache={}, self._client_kwargs={}, self._max_buffers=100, self._mode='r', self._name='name', self._raw=<tests.test_io_buffered.DummyRawIO name='name' mode='r'>, self._read_queue={}, self._read_range=<bound method test_object_buffered_base_io.<locals>.DummyRawIO._read_range of <tests.test_io_buffered.DummyRawIO name='name' mode='r'>>, self._readable=True, self._seek=0, self._seek_lock=<locked _thread.lock object at 0x7ffa2342a540>, self._seekable=True, self._size=10000, self._workers_count=None, self._workers_pool=None, self._writable=False)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "end = int(start + size * self._max_buffers)\n",
      "State:\n",
      "10000\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_client_kwargs(self, path):\n",
      "            assert path\n",
      "            dummy_client_kwargs['path'] = path\n",
      "            return dummy_client_kwargs\n",
      "get_client_kwargs(self=<tests.test_io_system.test_system_base.<locals>.DummySystem object at 0x7ffa2009ff70>, path='directory/file', dummy_client_kwargs={'arg1': 1, 'arg2': 2}, self._client='client', self._roots=(re.compile('root2://'), 'root://', '://'), self._storage_parameters={'arg3': 3, 'arg4': 4}, self._unsecure=False)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "dummy_client_kwargs['path'] = path\n",
      "State:\n",
      "{'arg1': 1, 'arg2': 2, 'path': 'directory/file'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False):\n",
      "    assert isinstance(commands, list)\n",
      "    p = None\n",
      "    for c in commands:\n",
      "        try:\n",
      "            dispcmd = str([c] + args)\n",
      "            p = subprocess.Popen([c] + args, cwd=cwd, stdout=subprocess.PIPE,\n",
      "                                 stderr=(subprocess.PIPE if hide_stderr\n",
      "                                         else None))\n",
      "            break\n",
      "        except EnvironmentError:\n",
      "            e = sys.exc_info()[1]\n",
      "            if e.errno == errno.ENOENT:\n",
      "                continue\n",
      "            if verbose:\n",
      "                print(\"unable to run %s\" % dispcmd)\n",
      "                print(e)\n",
      "            return None\n",
      "    else:\n",
      "        if verbose:\n",
      "            print(\"unable to find command, tried %s\" % (commands,))\n",
      "        return None\n",
      "    stdout = p.communicate()[0].strip()\n",
      "    if sys.version_info[0] >= 3:\n",
      "        stdout = stdout.decode()\n",
      "    if p.returncode != 0:\n",
      "        if verbose:\n",
      "            print(\"unable to run %s (error)\" % dispcmd)\n",
      "        return None\n",
      "    return stdout\n",
      "run_command(commands=['git'], args=['describe', '--tags', '--dirty', '--always', '--long'], cwd='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/danielfrg+datasciencebox/danielfrg+datasciencebox', verbose=False, hide_stderr=False)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "p = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False):\n",
      "    assert isinstance(commands, list)\n",
      "    p = None\n",
      "    for c in commands:\n",
      "        try:\n",
      "            dispcmd = str([c] + args)\n",
      "            p = subprocess.Popen([c] + args, cwd=cwd, stdout=subprocess.PIPE,\n",
      "                                 stderr=(subprocess.PIPE if hide_stderr\n",
      "                                         else None))\n",
      "            break\n",
      "        except EnvironmentError:\n",
      "            e = sys.exc_info()[1]\n",
      "            if e.errno == errno.ENOENT:\n",
      "                continue\n",
      "            if verbose:\n",
      "                print(\"unable to run %s\" % dispcmd)\n",
      "                print(e)\n",
      "            return None\n",
      "    else:\n",
      "        if verbose:\n",
      "            print(\"unable to find command, tried %s\" % (commands,))\n",
      "        return None\n",
      "    stdout = p.communicate()[0].strip()\n",
      "    if sys.version_info[0] >= 3:\n",
      "        stdout = stdout.decode()\n",
      "    if p.returncode != 0:\n",
      "        if verbose:\n",
      "            print(\"unable to run %s (error)\" % dispcmd)\n",
      "        return None\n",
      "    return stdout\n",
      "run_command(commands=['git'], args=['describe', '--tags', '--dirty', '--always', '--long'], cwd='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/danielfrg+datasciencebox/danielfrg+datasciencebox', verbose=False, hide_stderr=False)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "dispcmd = str([c] + args)\n",
      "State:\n",
      "\"['git', 'describe', '--tags', '--dirty', '--always', '--long']\"\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_logger_name(context=None):\n",
      "        log_names = [root_logger_name, logger_name]\n",
      "        if context is not None:\n",
      "            log_names.insert(1,\n",
      "                             context if isinstance(context,\n",
      "                                                   string_types) else context.__class__.__name__)\n",
      "        return '.'.join(log_names)\n",
      "get_logger_name(context=None, logger_name='State')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "log_names = [root_logger_name, logger_name]\n",
      "State:\n",
      "['lewis', 'State']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def start_server(self):\n",
      "        if self._socket is None:\n",
      "            context = zmq.Context()\n",
      "            self._socket = context.socket(zmq.REP)\n",
      "            self._socket.bind('tcp://{0}:{1}'.format(self.host, self.port))\n",
      "            self.log.info('Listening on %s:%s', self.host, self.port)\n",
      "start_server(self=<lewis.core.control_server.ControlServer object at 0x7fba9d810790>, self._exposed_object=<lewis.core.control_server.ExposedObjectCollection object at 0x7fba9d6c15e0>, self._socket=None, self.host='127.0.0.1', self.port='10001')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "context = zmq.Context()\n",
      "State:\n",
      "<MagicMock name='Context()' id='140439481752112'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_final_state_handlers(self, overrides):\n",
      "        states = self._get_state_handlers()\n",
      "        if overrides is not None:\n",
      "            dict_strict_update(states, overrides)\n",
      "        return states\n",
      "_get_final_state_handlers(self=<lewis.devices.chopper.devices.device.SimulatedChopper object at 0x7fbaa1e458e0>, overrides={'invalid': <lewis.devices.chopper.devices.states.DefaultIdleState object at 0x7fbaa17d86a0>}, self._bearings=<lewis.devices.chopper.devices.device.SimulatedBearings object at 0x7fbaa1e454c0>, self._idle_commanded=False, self._initialized=False, self._park_commanded=False, self._phase_commanded=False, self._processors=[], self._shutdown_commanded=False, self._start_commanded=False, self._stop_commanded=False, self.auto_park=False, self.parking_position=0.0, self.phase=0.0, self.speed=0.0, self.target_parking_position=0.0, self.target_phase=0.0, self.target_speed=0.0)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "dict_strict_update(states, overrides)\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_final_transition_handlers(self, overrides):\n",
      "        transitions = self._get_transition_handlers()\n",
      "        if overrides is not None:\n",
      "            dict_strict_update(transitions, overrides)\n",
      "        return transitions\n",
      "_get_final_transition_handlers(self=<lewis.devices.chopper.devices.device.SimulatedChopper object at 0x7fbaa1782250>, overrides={('idle', 'phase_locking'): <function TestSimulatedChopper.test_invalid_transition_override_fails.<locals>.<lambda> at 0x7fbaa1a5c820>}, self._bearings=<lewis.devices.chopper.devices.device.SimulatedBearings object at 0x7fbaa17821f0>, self._idle_commanded=False, self._initialized=False, self._park_commanded=False, self._phase_commanded=False, self._processors=[], self._shutdown_commanded=False, self._start_commanded=False, self._stop_commanded=False, self.auto_park=False, self.parking_position=0.0, self.phase=0.0, self.speed=0.0, self.target_parking_position=0.0, self.target_phase=0.0, self.target_speed=0.0)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "dict_strict_update(transitions, overrides)\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def limit_checked(obj, new_value):\n",
      "            lower = getattr(obj, self._lower) if isinstance(self._lower,\n",
      "                                                            string_types) else self._lower\n",
      "            upper = getattr(obj, self._upper) if isinstance(self._upper,\n",
      "                                                            string_types) else self._upper\n",
      "            if (lower is None or lower <= new_value) and (upper is None or new_value <= upper):\n",
      "                return f(obj, new_value)\n",
      "            if not self._silent:\n",
      "                raise LimitViolationException(\n",
      "                    '%f is outside limits (%r, %r)' % (new_value, lower, upper))\n",
      "limit_checked(obj={_processors=[<lewis.core.statemachine.StateMachine object at 0x7fbaa17d8760>], _csm=<lewis.core.statemachine.StateMachine object at 0x7fbaa17d8760>}, new_value=10, f=<function SimulatedJulabo.set_external_d at 0x7fbaa1ce4a60>, self=<lewis.core.utils.check_limits object at 0x7fbaa1d41430>, self._lower=0, self._silent=False, self._upper=999)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "lower = getattr(obj, self._lower) if isinstance(self._lower,\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def limit_checked(obj, new_value):\n",
      "            lower = getattr(obj, self._lower) if isinstance(self._lower,\n",
      "                                                            string_types) else self._lower\n",
      "            upper = getattr(obj, self._upper) if isinstance(self._upper,\n",
      "                                                            string_types) else self._upper\n",
      "            if (lower is None or lower <= new_value) and (upper is None or new_value <= upper):\n",
      "                return f(obj, new_value)\n",
      "            if not self._silent:\n",
      "                raise LimitViolationException(\n",
      "                    '%f is outside limits (%r, %r)' % (new_value, lower, upper))\n",
      "limit_checked(obj={_processors=[<lewis.core.statemachine.StateMachine object at 0x7fbaa17d8760>], _csm=<lewis.core.statemachine.StateMachine object at 0x7fbaa17d8760>}, new_value=10, f=<function SimulatedJulabo.set_external_d at 0x7fbaa1ce4a60>, self=<lewis.core.utils.check_limits object at 0x7fbaa1d41430>, self._lower=0, self._silent=False, self._upper=999)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "upper = getattr(obj, self._upper) if isinstance(self._upper,\n",
      "State:\n",
      "999\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _bind_commands(self, cmds):\n",
      "        patterns = set()\n",
      "        bound_commands = []\n",
      "        for cmd in cmds:\n",
      "            bound = cmd.bind(self) or cmd.bind(self.device) or None\n",
      "            if bound is None:\n",
      "                raise RuntimeError(\n",
      "                    'Unable to produce callable object for non-existing member \\'{}\\' '\n",
      "                    'of device or interface.'.format(cmd.func))\n",
      "            for bound_cmd in bound:\n",
      "                if bound_cmd.pattern in patterns:\n",
      "                    raise RuntimeError(\n",
      "                        'The regular expression {} is '\n",
      "                        'associated with multiple commands.'.format(bound_cmd.pattern.pattern))\n",
      "                patterns.add(bound_cmd.pattern)\n",
      "                bound_commands.append(bound_cmd)\n",
      "        return bound_commands\n",
      "_bind_commands(self=<lewis.devices.linkam_t95.interfaces.stream_interface.LinkamT95StreamInterface object at 0x7fbaa1d3edf0>, cmds={<lewis.adapters.stream.Cmd object at 0x7fbaa1cb6c10>, <lewis.adapters.stream.Cmd object at 0x7fbaa1cb6a90>, <lewis.adapters.stream.Cmd object at 0x7fbaa1cb6ca0>, <lewis.adapters.stream.Cmd object at 0x7fbaa1cb6eb0>, <lewis.adapters.stream.Cmd object at 0x7fbaa1cb6af0>, <lewis.adapters.stream.Cmd object at 0x7fbaa1cb6f10>, <lewis.adapters.stream.Cmd object at 0x7fbaa1cb6f70>, <lewis.adapters.stream.Cmd object at 0x7fbaa1cb6b80>, <lewis.adapters.stream.Cmd object at 0x7fbaa1cb61c0>}, self._bound_commands=[], self._device=<lewis.devices.linkam_t95.devices.device.SimulatedLinkamT95 object at 0x7fbaa1d3e0d0>, self._options=Namespace(bind_address='0.0.0.0', port=9999, telnet_mode=False), self._server=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "patterns = set()\n",
      "State:\n",
      "set()\n",
      "==================================================\n",
      "Clean Code:\n",
      "def stop_server(self):\n",
      "        self._running = False\n",
      "stop_server(self=<test.test_core_adapters.DummyAdapter object at 0x7fba9d325ee0>, self._device=None, self._running=True, self.protocol='foo')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self._running = False\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _discover_setups(self, setups_package):\n",
      "        setups = getattr(self._module, 'setups', {})\n",
      "        all_setups = setups if isinstance(setups, dict) else {}\n",
      "        if setups_package is not None:\n",
      "            for name, setup_module in get_submodules(setups_package).items():\n",
      "                existing_setup = all_setups.get(name)\n",
      "                if existing_setup is not None:\n",
      "                    raise RuntimeError(\n",
      "                        'The setup \\'{}\\' is defined twice in device \\'{}\\'.'.format(\n",
      "                            existing_setup, self.name))\n",
      "                all_setups[name] = {\n",
      "                    'device_type': getattr(setup_module, 'device_type', self.default_device_type),\n",
      "                    'parameters': getattr(setup_module, 'parameters', {})\n",
      "                }\n",
      "        if 'default' not in all_setups:\n",
      "            all_setups['default'] = {'device_type': self.default_device_type}\n",
      "        return all_setups\n",
      "_discover_setups(self=<lewis.core.devices.DeviceBuilder object at 0x7fbaa2f019d0>, setups_package=None, self._device_types=[<class 'test.test_core_devices.TestDeviceBuilderSimpleModule.setUpClass.<locals>.DummyDevice'>], self._module=<module 'simple_dummy_module'>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "setups = getattr(self._module, 'setups', {})\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _discover_interfaces(self, interface_package):\n",
      "        all_interfaces = []\n",
      "        if interface_package is not None:\n",
      "            for interface_module in get_submodules(interface_package).values():\n",
      "                all_interfaces += list(get_members(interface_module, is_adapter).values())\n",
      "        all_interfaces += list(get_members(self._module, is_adapter).values())\n",
      "        interfaces = {}\n",
      "        for interface in all_interfaces:\n",
      "            existing_interface = interfaces.get(interface.protocol)\n",
      "            if existing_interface is not None:\n",
      "                raise RuntimeError(\n",
      "                    'The protocol \\'{}\\' is defined in two interfaces for device \\'{}\\':\\n'\n",
      "                    '    {} (in {})\\n'\n",
      "                    '    {} (in {})\\n'\n",
      "                    'One of the protocol names needs to be changed.'.format(\n",
      "                        interface.protocol, self.name, existing_interface.__name__,\n",
      "                        existing_interface.__module__, interface.__name__, interface.__module__))\n",
      "            interfaces[interface.protocol] = interface\n",
      "        return interfaces\n",
      "_discover_interfaces(self=<lewis.core.devices.DeviceBuilder object at 0x7fbaa2f019d0>, interface_package=None, self._device_types=[<class 'test.test_core_devices.TestDeviceBuilderSimpleModule.setUpClass.<locals>.DummyDevice'>], self._module=<module 'simple_dummy_module'>, self._setups={'default': {'device_type': <class 'test.test_core_devices.TestDeviceBuilderSimpleModule.setUpClass.<locals>.DummyDevice'>}})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "all_interfaces = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def pause(self):\n",
      "        if not self._running:\n",
      "            raise RuntimeError('Can only pause a running simulation.')\n",
      "        self.log.info('Pausing simulation')\n",
      "        self._running = False\n",
      "pause(self=<lewis.core.simulation.Simulation object at 0x7fbaa1b6c6d0>, self._adapters=<lewis.core.adapters.AdapterCollection object at 0x7fbaa1b6ca60>, self._control_server=None, self._cycle_delay=0.1, self._cycles=0, self._device=<Mock id='140439553755216'>, self._running=False, self._runtime=0.0, self._speed=1.0, self._start_time=None, self._started=False, self._stop_commanded=False)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "raise RuntimeError('Can only pause a running simulation.')\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def start(self):\n",
      "        self.log.info('Starting simulation')\n",
      "        self._running = True\n",
      "        self._started = True\n",
      "        self._stop_commanded = False\n",
      "        if self._control_server is not None:\n",
      "            self._control_server.start_server()\n",
      "        self._adapters.connect()\n",
      "        self._start_time = datetime.now()\n",
      "        delta = 0.0\n",
      "        while not self._stop_commanded:\n",
      "            delta = self._process_cycle(delta)\n",
      "        self._running = False\n",
      "        self._started = False\n",
      "        self._adapters.disconnect()\n",
      "start(self=<lewis.core.simulation.Simulation object at 0x7fba9d7d1ac0>, self._adapters=<lewis.core.adapters.AdapterCollection object at 0x7fba9d7d1e80>, self._control_server=<Mock id='140439482866800'>, self._cycle_delay=0.1, self._cycles=0, self._device=<Mock id='140439482866224'>, self._process_cycle=<Mock id='140439482865792'>, self._running=False, self._runtime=0.0, self._speed=1.0, self._start_time=None, self._started=False, self._stop_commanded=False)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "self._start_time = datetime.now()\n",
      "State:\n",
      "datetime.datetime(2024, 4, 3, 15, 36, 19, 472058)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_baz(self, new_baz):\n",
      "                self.baz = new_baz\n",
      "set_baz(self=<test.test_utils.TestCheckLimits.test_upper_lower_only.<locals>.Foo object at 0x7fba9c870fa0>, new_baz=0, self.bar=-5)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.baz = new_baz\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _readCharVec(fp: BinaryIO, itemsize:int=None, dtype=None, size:tuple=None, use_unicode=is_unicode):\n",
      "        array = np.chararray(size, itemsize=itemsize, unicode=use_unicode)\n",
      "        Clen = array.itemsize\n",
      "        if \"<U\" in str(dtype):\n",
      "            Clen = Clen // 4\n",
      "        MaxEntry = size[0]\n",
      "        NRec = 100\n",
      "        ndata = 0\n",
      "        dataFormStart = '=4siii'\n",
      "        while NRec > 1:\n",
      "            nbyte = HarFileIO._getEntrySize(fp)\n",
      "            V = HarFileIO._unpack_data(fp, dataFormStart)\n",
      "            if fb(V[0]) != '    ':\n",
      "                raise IOError(\"Encountered characters at first four positions\")\n",
      "            NRec = V[1]\n",
      "            if V[2] != MaxEntry:\n",
      "                raise ValueError(\"Different Size than specified\")\n",
      "            if ndata + V[3] > MaxEntry:\n",
      "                raise RuntimeError(\"More data on Header than declared\")\n",
      "            AllStr = fb(fp.read(V[3] * Clen))\n",
      "            if nbyte != HarFileIO._getEntrySize(fp):\n",
      "                raise IOError(\"I/O Error: sizes on 1C header do not match record length\")\n",
      "            for j, i in enumerate(range(0, V[3] * Clen, Clen)):\n",
      "                array[ndata + j] = AllStr[i:i + Clen]\n",
      "            ndata += V[3]\n",
      "        return np.ascontiguousarray(array)\n",
      "_readCharVec(fp=<_io.BufferedReader name='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/GEMPACKsoftware+HARPY/GEMPACKsoftware+HARPY/test_remove_header_array.har'>, itemsize=70, dtype='<U12', size=(1,), use_unicode=True)\n",
      "\n",
      "n:\n",
      "25\n",
      "Statement:\n",
      "V = HarFileIO._unpack_data(fp, dataFormStart)\n",
      "State:\n",
      "(b'    ', 1, 1, 1)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _setHeaderBaseData(array, coeff_name, hao, long_name) -> None:\n",
      "        if not isinstance(array, (np.ndarray, np.float32, np.int32,np.float64)):\n",
      "            print(type(array))\n",
      "            raise HeaderArrayObj.UnsupportedArrayType(\"'array' must be of numpy.ndarray type.\")\n",
      "        if coeff_name is None:\n",
      "            coeff_name = \" \" * 12\n",
      "        if long_name is None:\n",
      "            long_name = coeff_name\n",
      "        if len(coeff_name) < 12:\n",
      "            coeff_name = coeff_name.ljust(12)\n",
      "        if len(long_name) < 70:\n",
      "            long_name = long_name.ljust(70)\n",
      "        hao.array = array\n",
      "        hao.coeff_name = coeff_name\n",
      "        hao.long_name = long_name\n",
      "_setHeaderBaseData(array=array(['at 2/03/2018 4:22:38 PM                                               '],      dtype='<U70'), coeff_name=None, hao={_coeff_name='', _array=None, _sets=None, _long_name=''}, long_name='Creation Date and Time                                                ')\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "hao.coeff_name = coeff_name\n",
      "State:\n",
      "{_coeff_name='            ', _array=array(['at 2/03/2018 4:22:38 PM                                               '],      dtype='<U70'), _sets=None, _long_name=''}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _read2DArray(fp, data_type: str = \"i\", storage_type: str = None, file_dims: tuple = None):\n",
      "        if storage_type == 'SPSE':\n",
      "            raise TypeError('Sparse storage not allowed for 2D data form.')\n",
      "        if data_type in [\"i\", \"I\"]:\n",
      "            data_type = np.int32\n",
      "            data_type_str = \"i\"\n",
      "        elif data_type in [\"f\", \"F\"]:\n",
      "            data_type = np.float32\n",
      "            data_type_str = \"f\"\n",
      "        else:\n",
      "            raise ValueError(\n",
      "                \"Provided argument 'data_type' must be 'i' (integer) or 'f' (float). '%s' was provided.\" % data_type)\n",
      "        array = np.ndarray(shape=file_dims[0:2], dtype=data_type, order='F')\n",
      "        arraySize = array.size\n",
      "        nread = 0\n",
      "        while nread != arraySize:\n",
      "            nbyte = HarFileIO._getEntrySize(fp)\n",
      "            dataForm = \"=4siiiiiii\"\n",
      "            V = HarFileIO._unpack_data(fp, dataForm)\n",
      "            if fb(V[0]) != '    ':\n",
      "                raise RuntimeError(\"Encountered characters at read2D loop\")\n",
      "            if V[2] != array.shape[0]:\n",
      "                raise ValueError(\"Mismatching row sizes on header\")\n",
      "            if V[3] != array.shape[1]:\n",
      "                raise ValueError(\"Mismatching col sizes on header\")\n",
      "            xsize = V[5] - V[4] + 1\n",
      "            ysize = V[7] - V[6] + 1\n",
      "            ndata = xsize * ysize\n",
      "            nread += ndata\n",
      "            dataForm = \"=\" + str(ndata) + data_type_str\n",
      "            dat = HarFileIO._unpack_data(fp, dataForm)\n",
      "            array[V[4] - 1:V[5], V[6] - 1:V[7]] = np.array(dat).reshape(xsize, ysize, order='F')\n",
      "            if nbyte != HarFileIO._getEntrySize(fp):\n",
      "                raise RuntimeError('Header corrupted.')\n",
      "        array = np.ascontiguousarray(array)\n",
      "        return array\n",
      "_read2DArray(fp=<_io.BufferedReader name='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/GEMPACKsoftware+HARPY/GEMPACKsoftware+HARPY/test_remove_header_array.har'>, data_type='i', storage_type='FULL', file_dims=(4, 4))\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "nbyte = HarFileIO._getEntrySize(fp)\n",
      "State:\n",
      "96\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _readSets(fp: BinaryIO, file_dims=None) -> (str,_HeaderDims):\n",
      "        Coefficient, SetList, SetStatus, ElementList = HarFileIO._readSetElementInfoRecord(fp)\n",
      "        set_names = [name.strip() for name in SetList]\n",
      "        idim = 0\n",
      "        header_sets = []\n",
      "        processedSet = OrderedDict()\n",
      "        for name, status in zip(set_names, SetStatus):\n",
      "            if status == 'k':\n",
      "                if name not in processedSet:\n",
      "                    processedSet[name] = HarFileIO._readCharVec(fp, itemsize=12, use_unicode=is_unicode, size=tuple([file_dims[idim]]), dtype=\"<U12\")\n",
      "                header_sets.append(_HeaderSet(name=name, status=status, dim_desc=[item.strip() for item in processedSet[name]], dim_size= file_dims[idim]))\n",
      "            elif status == 'u':\n",
      "                header_sets.append(_HeaderSet(name=name, status=status, dim_desc=None, dim_size= file_dims[idim]))\n",
      "            elif status == 'e':\n",
      "                header_sets.append(_HeaderSet(name=name, status=status, dim_desc=ElementList.pop(0), dim_size= file_dims[idim]))\n",
      "            idim += 1\n",
      "        return Coefficient, _HeaderDims(header_sets)\n",
      "_readSets(fp=<_io.BufferedReader name='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/GEMPACKsoftware+HARPY/GEMPACKsoftware+HARPY/test_remove_header_array.har'>, file_dims=(2, 2, 1, 1, 1, 1, 1))\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "Coefficient, SetList, SetStatus, ElementList = HarFileIO._readSetElementInfoRecord(fp)\n",
      "State:\n",
      "'Array2D     '\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _readSetElementInfoRecord(fp):\n",
      "        SetNames = []\n",
      "        ElementList = []\n",
      "        SetStatus = []\n",
      "        nbyte = HarFileIO._getEntrySize(fp)\n",
      "        dataForm = '=' + '4siii12si'\n",
      "        V = HarFileIO._unpack_data(fp, dataForm)\n",
      "        if fb(V[0]) != '    ':\n",
      "            raise RuntimeError(\"Encountered characters at first four positions at SetEl\")\n",
      "        NSets = V[3]\n",
      "        Coefficient = fb(V[4])\n",
      "        setKnown=V[5]!=0\n",
      "        if not setKnown:\n",
      "            dataForm = '=i'\n",
      "        else:\n",
      "            dataForm = \"=\" + str(NSets * 12) + 's' + str(NSets) + 's' + str(NSets) + 'i' + 'i'\n",
      "        V = HarFileIO._unpack_data(fp, dataForm)\n",
      "        if setKnown:\n",
      "            SetNames = [fb(V[0][i:i + 12]) for i in range(0, NSets * 12, 12)]\n",
      "            SetStatus = [fb(V[1][i:i + 1]) for i in range(0, NSets)]\n",
      "        Nexplicit = V[-1]\n",
      "        dataForm = '=' + str(Nexplicit * 12) + 's'\n",
      "        V = HarFileIO._unpack_data(fp, dataForm)\n",
      "        if Nexplicit > 0:\n",
      "            dataForm = '=' + str(Nexplicit * 12) + 's'\n",
      "            V = HarFileIO._unpack_data(fp, dataForm)\n",
      "            ElementList = [fb(V[-1][i:i + 12]) for i in range(0, NSets * 12, 12)]\n",
      "        HarFileIO._checkRead(fp, nbyte)\n",
      "        return Coefficient, SetNames, SetStatus, ElementList\n",
      "_readSetElementInfoRecord(fp=<_io.BufferedReader name='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/GEMPACKsoftware+HARPY/GEMPACKsoftware+HARPY/test_remove_header_array.har'>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "SetNames = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _readSetElementInfoRecord(fp):\n",
      "        SetNames = []\n",
      "        ElementList = []\n",
      "        SetStatus = []\n",
      "        nbyte = HarFileIO._getEntrySize(fp)\n",
      "        dataForm = '=' + '4siii12si'\n",
      "        V = HarFileIO._unpack_data(fp, dataForm)\n",
      "        if fb(V[0]) != '    ':\n",
      "            raise RuntimeError(\"Encountered characters at first four positions at SetEl\")\n",
      "        NSets = V[3]\n",
      "        Coefficient = fb(V[4])\n",
      "        setKnown=V[5]!=0\n",
      "        if not setKnown:\n",
      "            dataForm = '=i'\n",
      "        else:\n",
      "            dataForm = \"=\" + str(NSets * 12) + 's' + str(NSets) + 's' + str(NSets) + 'i' + 'i'\n",
      "        V = HarFileIO._unpack_data(fp, dataForm)\n",
      "        if setKnown:\n",
      "            SetNames = [fb(V[0][i:i + 12]) for i in range(0, NSets * 12, 12)]\n",
      "            SetStatus = [fb(V[1][i:i + 1]) for i in range(0, NSets)]\n",
      "        Nexplicit = V[-1]\n",
      "        dataForm = '=' + str(Nexplicit * 12) + 's'\n",
      "        V = HarFileIO._unpack_data(fp, dataForm)\n",
      "        if Nexplicit > 0:\n",
      "            dataForm = '=' + str(Nexplicit * 12) + 's'\n",
      "            V = HarFileIO._unpack_data(fp, dataForm)\n",
      "            ElementList = [fb(V[-1][i:i + 12]) for i in range(0, NSets * 12, 12)]\n",
      "        HarFileIO._checkRead(fp, nbyte)\n",
      "        return Coefficient, SetNames, SetStatus, ElementList\n",
      "_readSetElementInfoRecord(fp=<_io.BufferedReader name='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/GEMPACKsoftware+HARPY/GEMPACKsoftware+HARPY/test_remove_header_array.har'>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "ElementList = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _readSetElementInfoRecord(fp):\n",
      "        SetNames = []\n",
      "        ElementList = []\n",
      "        SetStatus = []\n",
      "        nbyte = HarFileIO._getEntrySize(fp)\n",
      "        dataForm = '=' + '4siii12si'\n",
      "        V = HarFileIO._unpack_data(fp, dataForm)\n",
      "        if fb(V[0]) != '    ':\n",
      "            raise RuntimeError(\"Encountered characters at first four positions at SetEl\")\n",
      "        NSets = V[3]\n",
      "        Coefficient = fb(V[4])\n",
      "        setKnown=V[5]!=0\n",
      "        if not setKnown:\n",
      "            dataForm = '=i'\n",
      "        else:\n",
      "            dataForm = \"=\" + str(NSets * 12) + 's' + str(NSets) + 's' + str(NSets) + 'i' + 'i'\n",
      "        V = HarFileIO._unpack_data(fp, dataForm)\n",
      "        if setKnown:\n",
      "            SetNames = [fb(V[0][i:i + 12]) for i in range(0, NSets * 12, 12)]\n",
      "            SetStatus = [fb(V[1][i:i + 1]) for i in range(0, NSets)]\n",
      "        Nexplicit = V[-1]\n",
      "        dataForm = '=' + str(Nexplicit * 12) + 's'\n",
      "        V = HarFileIO._unpack_data(fp, dataForm)\n",
      "        if Nexplicit > 0:\n",
      "            dataForm = '=' + str(Nexplicit * 12) + 's'\n",
      "            V = HarFileIO._unpack_data(fp, dataForm)\n",
      "            ElementList = [fb(V[-1][i:i + 12]) for i in range(0, NSets * 12, 12)]\n",
      "        HarFileIO._checkRead(fp, nbyte)\n",
      "        return Coefficient, SetNames, SetStatus, ElementList\n",
      "_readSetElementInfoRecord(fp=<_io.BufferedReader name='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/GEMPACKsoftware+HARPY/GEMPACKsoftware+HARPY/test_remove_header_array.har'>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "SetStatus = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _readREFullObj(fp, array, dtype):\n",
      "        nbyte = HarFileIO._getEntrySize(fp)\n",
      "        dataForm = '=4sii'\n",
      "        V = HarFileIO._unpack_data(fp, dataForm)\n",
      "        if fb(V[0]) != '    ':\n",
      "            raise Exception(\"Encountered characters at read7D[1]\")\n",
      "        nrec = V[1]\n",
      "        NDim = V[2]\n",
      "        dataForm = \"=\" + ('i' * NDim)\n",
      "        V = HarFileIO._unpack_data(fp, dataForm)\n",
      "        if nbyte != HarFileIO._getEntrySize(fp):\n",
      "            raise RuntimeError(\"Header corrupted read7D[0] @ %d\" % fp.tell())\n",
      "        oldshape = array.shape\n",
      "        array = array.flatten('F')\n",
      "        idata = 0\n",
      "        while nrec > 1:\n",
      "            nbyte = HarFileIO._getEntrySize(fp)\n",
      "            dataForm = '4s15i'\n",
      "            V = HarFileIO._unpack_data(fp, dataForm)\n",
      "            if fb(V[0]) != '    ':\n",
      "                raise RuntimeError(\"Encountered characters at first four positions at)SetEl\")\n",
      "            if nbyte != HarFileIO._getEntrySize(fp):\n",
      "                raise RuntimeError('read7D data[2] corrupted')\n",
      "            nbyte = HarFileIO._getEntrySize(fp)\n",
      "            ndata = (nbyte - 8) // struct.calcsize(dtype)\n",
      "            dataForm = '4si' + str(ndata) + dtype\n",
      "            V = HarFileIO._unpack_data(fp, dataForm)\n",
      "            if nbyte != HarFileIO._getEntrySize(fp):\n",
      "                raise RuntimeError('read7D data[2])corrupted')\n",
      "            if fb(V[0]) != '    ':\n",
      "                raise RuntimeError(\"Encountered characters at read7D[2]\")\n",
      "            nrec = V[1]\n",
      "            array[idata:idata + ndata] = V[2:]\n",
      "            idata += ndata\n",
      "        array = array.reshape(oldshape, order='F')\n",
      "        return array\n",
      "_readREFullObj(fp=<_io.BufferedReader name='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/GEMPACKsoftware+HARPY/GEMPACKsoftware+HARPY/test_remove_header_array.har'>, array=array([[0., 0.],       [0., 0.]], dtype=float32), dtype='f')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "nbyte = HarFileIO._getEntrySize(fp)\n",
      "State:\n",
      "40\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _readREFullObj(fp, array, dtype):\n",
      "        nbyte = HarFileIO._getEntrySize(fp)\n",
      "        dataForm = '=4sii'\n",
      "        V = HarFileIO._unpack_data(fp, dataForm)\n",
      "        if fb(V[0]) != '    ':\n",
      "            raise Exception(\"Encountered characters at read7D[1]\")\n",
      "        nrec = V[1]\n",
      "        NDim = V[2]\n",
      "        dataForm = \"=\" + ('i' * NDim)\n",
      "        V = HarFileIO._unpack_data(fp, dataForm)\n",
      "        if nbyte != HarFileIO._getEntrySize(fp):\n",
      "            raise RuntimeError(\"Header corrupted read7D[0] @ %d\" % fp.tell())\n",
      "        oldshape = array.shape\n",
      "        array = array.flatten('F')\n",
      "        idata = 0\n",
      "        while nrec > 1:\n",
      "            nbyte = HarFileIO._getEntrySize(fp)\n",
      "            dataForm = '4s15i'\n",
      "            V = HarFileIO._unpack_data(fp, dataForm)\n",
      "            if fb(V[0]) != '    ':\n",
      "                raise RuntimeError(\"Encountered characters at first four positions at)SetEl\")\n",
      "            if nbyte != HarFileIO._getEntrySize(fp):\n",
      "                raise RuntimeError('read7D data[2] corrupted')\n",
      "            nbyte = HarFileIO._getEntrySize(fp)\n",
      "            ndata = (nbyte - 8) // struct.calcsize(dtype)\n",
      "            dataForm = '4si' + str(ndata) + dtype\n",
      "            V = HarFileIO._unpack_data(fp, dataForm)\n",
      "            if nbyte != HarFileIO._getEntrySize(fp):\n",
      "                raise RuntimeError('read7D data[2])corrupted')\n",
      "            if fb(V[0]) != '    ':\n",
      "                raise RuntimeError(\"Encountered characters at read7D[2]\")\n",
      "            nrec = V[1]\n",
      "            array[idata:idata + ndata] = V[2:]\n",
      "            idata += ndata\n",
      "        array = array.reshape(oldshape, order='F')\n",
      "        return array\n",
      "_readREFullObj(fp=<_io.BufferedReader name='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/GEMPACKsoftware+HARPY/GEMPACKsoftware+HARPY/test_remove_header_array.har'>, array=array([[0., 0.],       [0., 0.]], dtype=float32), dtype='f')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "dataForm = '=4sii'\n",
      "State:\n",
      "'=4sii'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _readREFullObj(fp, array, dtype):\n",
      "        nbyte = HarFileIO._getEntrySize(fp)\n",
      "        dataForm = '=4sii'\n",
      "        V = HarFileIO._unpack_data(fp, dataForm)\n",
      "        if fb(V[0]) != '    ':\n",
      "            raise Exception(\"Encountered characters at read7D[1]\")\n",
      "        nrec = V[1]\n",
      "        NDim = V[2]\n",
      "        dataForm = \"=\" + ('i' * NDim)\n",
      "        V = HarFileIO._unpack_data(fp, dataForm)\n",
      "        if nbyte != HarFileIO._getEntrySize(fp):\n",
      "            raise RuntimeError(\"Header corrupted read7D[0] @ %d\" % fp.tell())\n",
      "        oldshape = array.shape\n",
      "        array = array.flatten('F')\n",
      "        idata = 0\n",
      "        while nrec > 1:\n",
      "            nbyte = HarFileIO._getEntrySize(fp)\n",
      "            dataForm = '4s15i'\n",
      "            V = HarFileIO._unpack_data(fp, dataForm)\n",
      "            if fb(V[0]) != '    ':\n",
      "                raise RuntimeError(\"Encountered characters at first four positions at)SetEl\")\n",
      "            if nbyte != HarFileIO._getEntrySize(fp):\n",
      "                raise RuntimeError('read7D data[2] corrupted')\n",
      "            nbyte = HarFileIO._getEntrySize(fp)\n",
      "            ndata = (nbyte - 8) // struct.calcsize(dtype)\n",
      "            dataForm = '4si' + str(ndata) + dtype\n",
      "            V = HarFileIO._unpack_data(fp, dataForm)\n",
      "            if nbyte != HarFileIO._getEntrySize(fp):\n",
      "                raise RuntimeError('read7D data[2])corrupted')\n",
      "            if fb(V[0]) != '    ':\n",
      "                raise RuntimeError(\"Encountered characters at read7D[2]\")\n",
      "            nrec = V[1]\n",
      "            array[idata:idata + ndata] = V[2:]\n",
      "            idata += ndata\n",
      "        array = array.reshape(oldshape, order='F')\n",
      "        return array\n",
      "_readREFullObj(fp=<_io.BufferedReader name='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/GEMPACKsoftware+HARPY/GEMPACKsoftware+HARPY/test_remove_header_array.har'>, array=array([[0., 0.],       [0., 0.]], dtype=float32), dtype='f')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "V = HarFileIO._unpack_data(fp, dataForm)\n",
      "State:\n",
      "(b'    ', 3, 7)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _readREFullObj(fp, array, dtype):\n",
      "        nbyte = HarFileIO._getEntrySize(fp)\n",
      "        dataForm = '=4sii'\n",
      "        V = HarFileIO._unpack_data(fp, dataForm)\n",
      "        if fb(V[0]) != '    ':\n",
      "            raise Exception(\"Encountered characters at read7D[1]\")\n",
      "        nrec = V[1]\n",
      "        NDim = V[2]\n",
      "        dataForm = \"=\" + ('i' * NDim)\n",
      "        V = HarFileIO._unpack_data(fp, dataForm)\n",
      "        if nbyte != HarFileIO._getEntrySize(fp):\n",
      "            raise RuntimeError(\"Header corrupted read7D[0] @ %d\" % fp.tell())\n",
      "        oldshape = array.shape\n",
      "        array = array.flatten('F')\n",
      "        idata = 0\n",
      "        while nrec > 1:\n",
      "            nbyte = HarFileIO._getEntrySize(fp)\n",
      "            dataForm = '4s15i'\n",
      "            V = HarFileIO._unpack_data(fp, dataForm)\n",
      "            if fb(V[0]) != '    ':\n",
      "                raise RuntimeError(\"Encountered characters at first four positions at)SetEl\")\n",
      "            if nbyte != HarFileIO._getEntrySize(fp):\n",
      "                raise RuntimeError('read7D data[2] corrupted')\n",
      "            nbyte = HarFileIO._getEntrySize(fp)\n",
      "            ndata = (nbyte - 8) // struct.calcsize(dtype)\n",
      "            dataForm = '4si' + str(ndata) + dtype\n",
      "            V = HarFileIO._unpack_data(fp, dataForm)\n",
      "            if nbyte != HarFileIO._getEntrySize(fp):\n",
      "                raise RuntimeError('read7D data[2])corrupted')\n",
      "            if fb(V[0]) != '    ':\n",
      "                raise RuntimeError(\"Encountered characters at read7D[2]\")\n",
      "            nrec = V[1]\n",
      "            array[idata:idata + ndata] = V[2:]\n",
      "            idata += ndata\n",
      "        array = array.reshape(oldshape, order='F')\n",
      "        return array\n",
      "_readREFullObj(fp=<_io.BufferedReader name='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/GEMPACKsoftware+HARPY/GEMPACKsoftware+HARPY/test_remove_header_array.har'>, array=array([[0., 0.],       [0., 0.]], dtype=float32), dtype='f')\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "array = array.flatten('F')\n",
      "State:\n",
      "array([0., 0., 0., 0.], dtype=float32)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _readREFullObj(fp, array, dtype):\n",
      "        nbyte = HarFileIO._getEntrySize(fp)\n",
      "        dataForm = '=4sii'\n",
      "        V = HarFileIO._unpack_data(fp, dataForm)\n",
      "        if fb(V[0]) != '    ':\n",
      "            raise Exception(\"Encountered characters at read7D[1]\")\n",
      "        nrec = V[1]\n",
      "        NDim = V[2]\n",
      "        dataForm = \"=\" + ('i' * NDim)\n",
      "        V = HarFileIO._unpack_data(fp, dataForm)\n",
      "        if nbyte != HarFileIO._getEntrySize(fp):\n",
      "            raise RuntimeError(\"Header corrupted read7D[0] @ %d\" % fp.tell())\n",
      "        oldshape = array.shape\n",
      "        array = array.flatten('F')\n",
      "        idata = 0\n",
      "        while nrec > 1:\n",
      "            nbyte = HarFileIO._getEntrySize(fp)\n",
      "            dataForm = '4s15i'\n",
      "            V = HarFileIO._unpack_data(fp, dataForm)\n",
      "            if fb(V[0]) != '    ':\n",
      "                raise RuntimeError(\"Encountered characters at first four positions at)SetEl\")\n",
      "            if nbyte != HarFileIO._getEntrySize(fp):\n",
      "                raise RuntimeError('read7D data[2] corrupted')\n",
      "            nbyte = HarFileIO._getEntrySize(fp)\n",
      "            ndata = (nbyte - 8) // struct.calcsize(dtype)\n",
      "            dataForm = '4si' + str(ndata) + dtype\n",
      "            V = HarFileIO._unpack_data(fp, dataForm)\n",
      "            if nbyte != HarFileIO._getEntrySize(fp):\n",
      "                raise RuntimeError('read7D data[2])corrupted')\n",
      "            if fb(V[0]) != '    ':\n",
      "                raise RuntimeError(\"Encountered characters at read7D[2]\")\n",
      "            nrec = V[1]\n",
      "            array[idata:idata + ndata] = V[2:]\n",
      "            idata += ndata\n",
      "        array = array.reshape(oldshape, order='F')\n",
      "        return array\n",
      "_readREFullObj(fp=<_io.BufferedReader name='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/GEMPACKsoftware+HARPY/GEMPACKsoftware+HARPY/test_remove_header_array.har'>, array=array([[0., 0.],       [0., 0.]], dtype=float32), dtype='f')\n",
      "\n",
      "n:\n",
      "25\n",
      "Statement:\n",
      "V = HarFileIO._unpack_data(fp, dataForm)\n",
      "State:\n",
      "(b'    ', 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_del_and_contains(self):\n",
      "        hfo = HarFileObj._loadFromDisk(TestHarFileObj._dd + \"test.har\")\n",
      "        test_hn = ['XXCD', 'XXCR', 'XXCP', 'XXHS', 'CHST', 'INTA', 'SIMP', 'SIM2', 'NH01', 'ARR7']\n",
      "        self.assertFalse(\"HNOT\" in hfo)\n",
      "        self.assertTrue( \"XXCD\" in hfo)\n",
      "        self.assertTrue(\"xxcd\" in hfo)\n",
      "        del hfo[\"XXCD\"]\n",
      "        self.assertFalse(\"XXCD\" in hfo)\n",
      "        del hfo[\"xxcr\"]\n",
      "        self.assertFalse(\"XXCR\" in hfo)\n",
      "        del_hn = ['XXCP', 'XXHS', 'CHST', 'INTA', 'SIMP', 'SIM2', 'NH01', 'ARR7']\n",
      "        del hfo[del_hn]\n",
      "        self.assertFalse(any([ name in hfo for name in del_hn ]))\n",
      "test_del_and_contains(self=<harpy.tests.test_har_file.TestHarFileObj testMethod=test_del_and_contains>, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fd016334d60>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_del_and_contains', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_del_and_contains=<bound method TestHarFileObj.test_del_and_contains of <harpy.tests.test_har_file.TestHarFileObj testMethod=test_del_and_contains>>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "test_hn = ['XXCD', 'XXCR', 'XXCP', 'XXHS', 'CHST', 'INTA', 'SIMP', 'SIM2', 'NH01', 'ARR7']\n",
      "State:\n",
      "['XXCD', 'XXCR', 'XXCP', 'XXHS', 'CHST', 'INTA', 'SIMP', 'SIM2', 'NH01', 'ARR7']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_get_item(self):\n",
      "        hfo = HarFileObj._loadFromDisk(TestHarFileObj._dd + \"test.har\")\n",
      "        test_hn = ['XXCD', 'XXCR', 'XXCP', 'XXHS', 'CHST', 'INTA', 'SIMP', 'SIM2', 'NH01', 'ARR7']\n",
      "        XXCDHead=hfo[\"XXCD\"]\n",
      "        self.assertTrue(isinstance(XXCDHead,HeaderArrayObj))\n",
      "        xxcdHead=hfo[\"xxcd\"]\n",
      "        self.assertTrue(xxcdHead==XXCDHead)\n",
      "        HeadList=hfo[test_hn]\n",
      "        for i,headid in enumerate(test_hn):\n",
      "            self.assertTrue(hfo[headid] == HeadList[i])\n",
      "        with self.assertRaises(KeyError):\n",
      "            HeadList = hfo[\"NOTH\"]\n",
      "        with self.assertRaises(TypeError):\n",
      "            HeadList = hfo[1]\n",
      "test_get_item(self=<harpy.tests.test_har_file.TestHarFileObj testMethod=test_get_item>, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fd016334160>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_get_item', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_get_item=<bound method TestHarFileObj.test_get_item of <harpy.tests.test_har_file.TestHarFileObj testMethod=test_get_item>>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "test_hn = ['XXCD', 'XXCR', 'XXCP', 'XXHS', 'CHST', 'INTA', 'SIMP', 'SIM2', 'NH01', 'ARR7']\n",
      "State:\n",
      "['XXCD', 'XXCR', 'XXCP', 'XXHS', 'CHST', 'INTA', 'SIMP', 'SIM2', 'NH01', 'ARR7']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def writeToDisk(self, filename: str=None, ha_names=None):\n",
      "        if filename is None and self.filename is None:\n",
      "            raise ValueError(\"No filename specified in write or upon creation, use writeToDisk(filename=YOURFILENAME)\")\n",
      "        if filename is None:\n",
      "            filename=self.filename\n",
      "        if ha_names is None:\n",
      "            ha_names = self.getHeaderArrayNames()\n",
      "        elif isinstance(ha_names, str):\n",
      "            ha_names = [ha_names]\n",
      "        ha_to_write = self._getHeaderArrayObjs(ha_names)\n",
      "        HarFileIO.writeHeaders(filename, ha_names, ha_to_write)\n",
      "        self._hfi.updateMtime()\n",
      "writeToDisk(self=<harpy.har_file.HarFileObj object at 0x7fd01634d400>, filename='test_overwrite_header.har', ha_names=None, self._head_arrs=OrderedDict([('ARR7', <harpy.header_array.HeaderArrayObj object at 0x7fd016391250>)]), self._hfi=<harpy.har_file_io.HarFileInfoObj object at 0x7fd01634d4c0>, self.filename='test_overwrite_header.har')\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "ha_names = self.getHeaderArrayNames()\n",
      "State:\n",
      "['XXCD', 'XXCR', 'XXCP', 'XXHS', 'CHST', 'INTA', 'SIMP', 'SIM2', 'NH01', 'ARR7']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _writeSecondRecord(fp: BinaryIO, inList):\n",
      "        nint = len(inList) - 4\n",
      "        if len(inList[3]) != 70:\n",
      "            raise ValueError(\"'long_name' must be precisely 70 characters long. 'long_name' is: %s (%d characters long).\" % (inList[3], len(inList[3])))\n",
      "        inList = [tb(x) if isinstance(x, str) else x for x in inList]\n",
      "        dataForm = '=i4s2s4s70s' + 'i' * nint + 'i'\n",
      "        byteLen = struct.calcsize(dataForm) - 8\n",
      "        inList.append(byteLen)\n",
      "        inList.insert(0, byteLen)\n",
      "        packed = struct.pack(dataForm, *inList)\n",
      "        fp.write(packed)\n",
      "_writeSecondRecord(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, inList=['    ', '1C', 'FULL', 'Creation Date and Time                                                ', 2, 1, 70])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "nint = len(inList) - 4\n",
      "State:\n",
      "3\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _writeSecondRecord(fp: BinaryIO, inList):\n",
      "        nint = len(inList) - 4\n",
      "        if len(inList[3]) != 70:\n",
      "            raise ValueError(\"'long_name' must be precisely 70 characters long. 'long_name' is: %s (%d characters long).\" % (inList[3], len(inList[3])))\n",
      "        inList = [tb(x) if isinstance(x, str) else x for x in inList]\n",
      "        dataForm = '=i4s2s4s70s' + 'i' * nint + 'i'\n",
      "        byteLen = struct.calcsize(dataForm) - 8\n",
      "        inList.append(byteLen)\n",
      "        inList.insert(0, byteLen)\n",
      "        packed = struct.pack(dataForm, *inList)\n",
      "        fp.write(packed)\n",
      "_writeSecondRecord(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, inList=['    ', '1C', 'FULL', 'Creation Date and Time                                                ', 2, 1, 70])\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "inList.insert(0, byteLen)\n",
      "State:\n",
      "[92, b'    ', b'1C', b'FULL', b'Creation Date and Time                                                ', 2, 1, 70, 92]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write1CArray(fp : BinaryIO, array : np.array, vecDim : int, strLen : int):\n",
      "        maxwrt = 29996\n",
      "        maxPerLine = maxwrt // strLen\n",
      "        nrec = (vecDim - 1) // maxPerLine + 1\n",
      "        nLeft = vecDim\n",
      "        nDone = 0\n",
      "        if nrec==0:\n",
      "            dataForm = '=i4siiii'\n",
      "            fp.write(struct.pack(dataForm, 16, tb('    '), 1, 0, 0,16))\n",
      "        while nrec > 0:\n",
      "            dataForm = '=i4siii'\n",
      "            nOnRec = min(nLeft, maxPerLine)\n",
      "            ndata = 16 + nOnRec * strLen\n",
      "            fp.write(struct.pack(dataForm, ndata, tb('    '), nrec, vecDim, nOnRec))\n",
      "            dataForm = '=' + str(ndata - 16) + 'si'\n",
      "            packStr = tb(''.join([array[i].ljust(strLen) for i in range(nDone,nDone + nOnRec)]))\n",
      "            fp.write(struct.pack(dataForm, packStr, ndata))\n",
      "            nrec -= 1\n",
      "            nLeft -= nOnRec\n",
      "            nDone += nOnRec\n",
      "_write1CArray(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, array=array(['at 2/03/2018 4:22:38 PM                                               '],      dtype='<U70'), vecDim=1, strLen=70)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "maxwrt = 29996\n",
      "State:\n",
      "29996\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write1CArray(fp : BinaryIO, array : np.array, vecDim : int, strLen : int):\n",
      "        maxwrt = 29996\n",
      "        maxPerLine = maxwrt // strLen\n",
      "        nrec = (vecDim - 1) // maxPerLine + 1\n",
      "        nLeft = vecDim\n",
      "        nDone = 0\n",
      "        if nrec==0:\n",
      "            dataForm = '=i4siiii'\n",
      "            fp.write(struct.pack(dataForm, 16, tb('    '), 1, 0, 0,16))\n",
      "        while nrec > 0:\n",
      "            dataForm = '=i4siii'\n",
      "            nOnRec = min(nLeft, maxPerLine)\n",
      "            ndata = 16 + nOnRec * strLen\n",
      "            fp.write(struct.pack(dataForm, ndata, tb('    '), nrec, vecDim, nOnRec))\n",
      "            dataForm = '=' + str(ndata - 16) + 'si'\n",
      "            packStr = tb(''.join([array[i].ljust(strLen) for i in range(nDone,nDone + nOnRec)]))\n",
      "            fp.write(struct.pack(dataForm, packStr, ndata))\n",
      "            nrec -= 1\n",
      "            nLeft -= nOnRec\n",
      "            nDone += nOnRec\n",
      "_write1CArray(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, array=array(['at 2/03/2018 4:22:38 PM                                               '],      dtype='<U70'), vecDim=1, strLen=70)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "maxPerLine = maxwrt // strLen\n",
      "State:\n",
      "428\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write1CArray(fp : BinaryIO, array : np.array, vecDim : int, strLen : int):\n",
      "        maxwrt = 29996\n",
      "        maxPerLine = maxwrt // strLen\n",
      "        nrec = (vecDim - 1) // maxPerLine + 1\n",
      "        nLeft = vecDim\n",
      "        nDone = 0\n",
      "        if nrec==0:\n",
      "            dataForm = '=i4siiii'\n",
      "            fp.write(struct.pack(dataForm, 16, tb('    '), 1, 0, 0,16))\n",
      "        while nrec > 0:\n",
      "            dataForm = '=i4siii'\n",
      "            nOnRec = min(nLeft, maxPerLine)\n",
      "            ndata = 16 + nOnRec * strLen\n",
      "            fp.write(struct.pack(dataForm, ndata, tb('    '), nrec, vecDim, nOnRec))\n",
      "            dataForm = '=' + str(ndata - 16) + 'si'\n",
      "            packStr = tb(''.join([array[i].ljust(strLen) for i in range(nDone,nDone + nOnRec)]))\n",
      "            fp.write(struct.pack(dataForm, packStr, ndata))\n",
      "            nrec -= 1\n",
      "            nLeft -= nOnRec\n",
      "            nDone += nOnRec\n",
      "_write1CArray(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, array=array(['at 2/03/2018 4:22:38 PM                                               '],      dtype='<U70'), vecDim=1, strLen=70)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "nrec = (vecDim - 1) // maxPerLine + 1\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write1CArray(fp : BinaryIO, array : np.array, vecDim : int, strLen : int):\n",
      "        maxwrt = 29996\n",
      "        maxPerLine = maxwrt // strLen\n",
      "        nrec = (vecDim - 1) // maxPerLine + 1\n",
      "        nLeft = vecDim\n",
      "        nDone = 0\n",
      "        if nrec==0:\n",
      "            dataForm = '=i4siiii'\n",
      "            fp.write(struct.pack(dataForm, 16, tb('    '), 1, 0, 0,16))\n",
      "        while nrec > 0:\n",
      "            dataForm = '=i4siii'\n",
      "            nOnRec = min(nLeft, maxPerLine)\n",
      "            ndata = 16 + nOnRec * strLen\n",
      "            fp.write(struct.pack(dataForm, ndata, tb('    '), nrec, vecDim, nOnRec))\n",
      "            dataForm = '=' + str(ndata - 16) + 'si'\n",
      "            packStr = tb(''.join([array[i].ljust(strLen) for i in range(nDone,nDone + nOnRec)]))\n",
      "            fp.write(struct.pack(dataForm, packStr, ndata))\n",
      "            nrec -= 1\n",
      "            nLeft -= nOnRec\n",
      "            nDone += nOnRec\n",
      "_write1CArray(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, array=array(['at 2/03/2018 4:22:38 PM                                               '],      dtype='<U70'), vecDim=1, strLen=70)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "nLeft = vecDim\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write1CArray(fp : BinaryIO, array : np.array, vecDim : int, strLen : int):\n",
      "        maxwrt = 29996\n",
      "        maxPerLine = maxwrt // strLen\n",
      "        nrec = (vecDim - 1) // maxPerLine + 1\n",
      "        nLeft = vecDim\n",
      "        nDone = 0\n",
      "        if nrec==0:\n",
      "            dataForm = '=i4siiii'\n",
      "            fp.write(struct.pack(dataForm, 16, tb('    '), 1, 0, 0,16))\n",
      "        while nrec > 0:\n",
      "            dataForm = '=i4siii'\n",
      "            nOnRec = min(nLeft, maxPerLine)\n",
      "            ndata = 16 + nOnRec * strLen\n",
      "            fp.write(struct.pack(dataForm, ndata, tb('    '), nrec, vecDim, nOnRec))\n",
      "            dataForm = '=' + str(ndata - 16) + 'si'\n",
      "            packStr = tb(''.join([array[i].ljust(strLen) for i in range(nDone,nDone + nOnRec)]))\n",
      "            fp.write(struct.pack(dataForm, packStr, ndata))\n",
      "            nrec -= 1\n",
      "            nLeft -= nOnRec\n",
      "            nDone += nOnRec\n",
      "_write1CArray(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, array=array(['at 2/03/2018 4:22:38 PM                                               '],      dtype='<U70'), vecDim=1, strLen=70)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "nDone = 0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write1CArray(fp : BinaryIO, array : np.array, vecDim : int, strLen : int):\n",
      "        maxwrt = 29996\n",
      "        maxPerLine = maxwrt // strLen\n",
      "        nrec = (vecDim - 1) // maxPerLine + 1\n",
      "        nLeft = vecDim\n",
      "        nDone = 0\n",
      "        if nrec==0:\n",
      "            dataForm = '=i4siiii'\n",
      "            fp.write(struct.pack(dataForm, 16, tb('    '), 1, 0, 0,16))\n",
      "        while nrec > 0:\n",
      "            dataForm = '=i4siii'\n",
      "            nOnRec = min(nLeft, maxPerLine)\n",
      "            ndata = 16 + nOnRec * strLen\n",
      "            fp.write(struct.pack(dataForm, ndata, tb('    '), nrec, vecDim, nOnRec))\n",
      "            dataForm = '=' + str(ndata - 16) + 'si'\n",
      "            packStr = tb(''.join([array[i].ljust(strLen) for i in range(nDone,nDone + nOnRec)]))\n",
      "            fp.write(struct.pack(dataForm, packStr, ndata))\n",
      "            nrec -= 1\n",
      "            nLeft -= nOnRec\n",
      "            nDone += nOnRec\n",
      "_write1CArray(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, array=array(['at 2/03/2018 4:22:38 PM                                               '],      dtype='<U70'), vecDim=1, strLen=70)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "dataForm = '=i4siii'\n",
      "State:\n",
      "'=i4siii'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _writeHeader2D(fp: BinaryIO, hname : str, head_arr_obj: HeaderArrayObj):\n",
      "        HarFileIO._writeHeaderName(fp, hname)\n",
      "        typeString = str(head_arr_obj.array.dtype)\n",
      "        shape2D = [head_arr_obj.array.shape[i] if i < head_arr_obj.array.ndim else 1 for i in range(0, 2)]\n",
      "        if typeString == 'int32':\n",
      "            dtype = 'i'\n",
      "            secRecList = ['    ', '2I', 'FULL', head_arr_obj.long_name, 2]\n",
      "        elif typeString == 'float32':\n",
      "            secRecList = ['    ', '2R', 'FULL', head_arr_obj.long_name, 2]\n",
      "            dtype = 'f'\n",
      "        else:\n",
      "            raise TypeError(\"Can only write 32bit float or int to 2D arrays\")\n",
      "        secRecList.extend(shape2D)\n",
      "        HarFileIO._writeSecondRecord(fp, secRecList)\n",
      "        HarFileIO._write2DArray(fp, np.asfortranarray(head_arr_obj.array), dtype)\n",
      "_writeHeader2D(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, hname='INTA', head_arr_obj={_coeff_name='            ', _array=array([[ 0,  1,  2,  3],       [ 4,  5,  6,  7],       [ 8,  9, 10, 11],       [12, 13, 14, 15]], dtype=int32), _sets=<harpy._header_sets._HeaderDims object at 0x7fd016334820>, _long_name='2D integer array                                                      '})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "typeString = str(head_arr_obj.array.dtype)\n",
      "State:\n",
      "'int32'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _writeHeader2D(fp: BinaryIO, hname : str, head_arr_obj: HeaderArrayObj):\n",
      "        HarFileIO._writeHeaderName(fp, hname)\n",
      "        typeString = str(head_arr_obj.array.dtype)\n",
      "        shape2D = [head_arr_obj.array.shape[i] if i < head_arr_obj.array.ndim else 1 for i in range(0, 2)]\n",
      "        if typeString == 'int32':\n",
      "            dtype = 'i'\n",
      "            secRecList = ['    ', '2I', 'FULL', head_arr_obj.long_name, 2]\n",
      "        elif typeString == 'float32':\n",
      "            secRecList = ['    ', '2R', 'FULL', head_arr_obj.long_name, 2]\n",
      "            dtype = 'f'\n",
      "        else:\n",
      "            raise TypeError(\"Can only write 32bit float or int to 2D arrays\")\n",
      "        secRecList.extend(shape2D)\n",
      "        HarFileIO._writeSecondRecord(fp, secRecList)\n",
      "        HarFileIO._write2DArray(fp, np.asfortranarray(head_arr_obj.array), dtype)\n",
      "_writeHeader2D(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, hname='INTA', head_arr_obj={_coeff_name='            ', _array=array([[ 0,  1,  2,  3],       [ 4,  5,  6,  7],       [ 8,  9, 10, 11],       [12, 13, 14, 15]], dtype=int32), _sets=<harpy._header_sets._HeaderDims object at 0x7fd016334820>, _long_name='2D integer array                                                      '})\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "dtype = 'i'\n",
      "State:\n",
      "'i'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write2DArray(fp, array, dtype):\n",
      "        dataForm = \"=i4siiiiiii\"\n",
      "        maxData = 7991\n",
      "        nrec = (array.size - 1) // maxData + 1\n",
      "        ndata = 0\n",
      "        indexTuple=(None,)\n",
      "        nbyte=0\n",
      "        for st, end in HarFileIO._slice_inds(array, maxData):\n",
      "            if array.ndim == 1:\n",
      "                indexTuple = (slice(st[0], end[0]))\n",
      "                ndata = (end[0] - st[0])\n",
      "                nbyte = ndata * 4 + 32\n",
      "                fp.write(struct.pack(dataForm, nbyte, tb('    '), nrec, array.size, 1, st[0] + 1, end[0], 1, 1))\n",
      "            elif array.ndim == 2:\n",
      "                indexTuple = (slice(st[0], end[0]), slice(st[1], end[1]))\n",
      "                ndata = (end[0] - st[0]) * (end[1] - st[1])\n",
      "                nbyte = ndata * 4 + 32\n",
      "                fp.write(struct.pack(dataForm, nbyte, tb('    '), nrec, array.shape[0],\n",
      "                                         array.shape[1], st[0] + 1, end[0], st[1] + 1, end[1]))\n",
      "            dataForm1 = '=' + str(ndata) + dtype\n",
      "            fp.write(struct.pack(dataForm1, *array[indexTuple].flatten('F')))\n",
      "            fp.write(struct.pack('=i', nbyte))\n",
      "            nrec = nrec - 1\n",
      "_write2DArray(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, array=array([[ 0,  1,  2,  3],       [ 4,  5,  6,  7],       [ 8,  9, 10, 11],       [12, 13, 14, 15]], dtype=int32), dtype='i')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "dataForm = \"=i4siiiiiii\"\n",
      "State:\n",
      "'=i4siiiiiii'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write2DArray(fp, array, dtype):\n",
      "        dataForm = \"=i4siiiiiii\"\n",
      "        maxData = 7991\n",
      "        nrec = (array.size - 1) // maxData + 1\n",
      "        ndata = 0\n",
      "        indexTuple=(None,)\n",
      "        nbyte=0\n",
      "        for st, end in HarFileIO._slice_inds(array, maxData):\n",
      "            if array.ndim == 1:\n",
      "                indexTuple = (slice(st[0], end[0]))\n",
      "                ndata = (end[0] - st[0])\n",
      "                nbyte = ndata * 4 + 32\n",
      "                fp.write(struct.pack(dataForm, nbyte, tb('    '), nrec, array.size, 1, st[0] + 1, end[0], 1, 1))\n",
      "            elif array.ndim == 2:\n",
      "                indexTuple = (slice(st[0], end[0]), slice(st[1], end[1]))\n",
      "                ndata = (end[0] - st[0]) * (end[1] - st[1])\n",
      "                nbyte = ndata * 4 + 32\n",
      "                fp.write(struct.pack(dataForm, nbyte, tb('    '), nrec, array.shape[0],\n",
      "                                         array.shape[1], st[0] + 1, end[0], st[1] + 1, end[1]))\n",
      "            dataForm1 = '=' + str(ndata) + dtype\n",
      "            fp.write(struct.pack(dataForm1, *array[indexTuple].flatten('F')))\n",
      "            fp.write(struct.pack('=i', nbyte))\n",
      "            nrec = nrec - 1\n",
      "_write2DArray(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, array=array([[ 0,  1,  2,  3],       [ 4,  5,  6,  7],       [ 8,  9, 10, 11],       [12, 13, 14, 15]], dtype=int32), dtype='i')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "maxData = 7991\n",
      "State:\n",
      "7991\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write2DArray(fp, array, dtype):\n",
      "        dataForm = \"=i4siiiiiii\"\n",
      "        maxData = 7991\n",
      "        nrec = (array.size - 1) // maxData + 1\n",
      "        ndata = 0\n",
      "        indexTuple=(None,)\n",
      "        nbyte=0\n",
      "        for st, end in HarFileIO._slice_inds(array, maxData):\n",
      "            if array.ndim == 1:\n",
      "                indexTuple = (slice(st[0], end[0]))\n",
      "                ndata = (end[0] - st[0])\n",
      "                nbyte = ndata * 4 + 32\n",
      "                fp.write(struct.pack(dataForm, nbyte, tb('    '), nrec, array.size, 1, st[0] + 1, end[0], 1, 1))\n",
      "            elif array.ndim == 2:\n",
      "                indexTuple = (slice(st[0], end[0]), slice(st[1], end[1]))\n",
      "                ndata = (end[0] - st[0]) * (end[1] - st[1])\n",
      "                nbyte = ndata * 4 + 32\n",
      "                fp.write(struct.pack(dataForm, nbyte, tb('    '), nrec, array.shape[0],\n",
      "                                         array.shape[1], st[0] + 1, end[0], st[1] + 1, end[1]))\n",
      "            dataForm1 = '=' + str(ndata) + dtype\n",
      "            fp.write(struct.pack(dataForm1, *array[indexTuple].flatten('F')))\n",
      "            fp.write(struct.pack('=i', nbyte))\n",
      "            nrec = nrec - 1\n",
      "_write2DArray(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, array=array([[ 0,  1,  2,  3],       [ 4,  5,  6,  7],       [ 8,  9, 10, 11],       [12, 13, 14, 15]], dtype=int32), dtype='i')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "nrec = (array.size - 1) // maxData + 1\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write2DArray(fp, array, dtype):\n",
      "        dataForm = \"=i4siiiiiii\"\n",
      "        maxData = 7991\n",
      "        nrec = (array.size - 1) // maxData + 1\n",
      "        ndata = 0\n",
      "        indexTuple=(None,)\n",
      "        nbyte=0\n",
      "        for st, end in HarFileIO._slice_inds(array, maxData):\n",
      "            if array.ndim == 1:\n",
      "                indexTuple = (slice(st[0], end[0]))\n",
      "                ndata = (end[0] - st[0])\n",
      "                nbyte = ndata * 4 + 32\n",
      "                fp.write(struct.pack(dataForm, nbyte, tb('    '), nrec, array.size, 1, st[0] + 1, end[0], 1, 1))\n",
      "            elif array.ndim == 2:\n",
      "                indexTuple = (slice(st[0], end[0]), slice(st[1], end[1]))\n",
      "                ndata = (end[0] - st[0]) * (end[1] - st[1])\n",
      "                nbyte = ndata * 4 + 32\n",
      "                fp.write(struct.pack(dataForm, nbyte, tb('    '), nrec, array.shape[0],\n",
      "                                         array.shape[1], st[0] + 1, end[0], st[1] + 1, end[1]))\n",
      "            dataForm1 = '=' + str(ndata) + dtype\n",
      "            fp.write(struct.pack(dataForm1, *array[indexTuple].flatten('F')))\n",
      "            fp.write(struct.pack('=i', nbyte))\n",
      "            nrec = nrec - 1\n",
      "_write2DArray(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, array=array([[ 0,  1,  2,  3],       [ 4,  5,  6,  7],       [ 8,  9, 10, 11],       [12, 13, 14, 15]], dtype=int32), dtype='i')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "ndata = 0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write2DArray(fp, array, dtype):\n",
      "        dataForm = \"=i4siiiiiii\"\n",
      "        maxData = 7991\n",
      "        nrec = (array.size - 1) // maxData + 1\n",
      "        ndata = 0\n",
      "        indexTuple=(None,)\n",
      "        nbyte=0\n",
      "        for st, end in HarFileIO._slice_inds(array, maxData):\n",
      "            if array.ndim == 1:\n",
      "                indexTuple = (slice(st[0], end[0]))\n",
      "                ndata = (end[0] - st[0])\n",
      "                nbyte = ndata * 4 + 32\n",
      "                fp.write(struct.pack(dataForm, nbyte, tb('    '), nrec, array.size, 1, st[0] + 1, end[0], 1, 1))\n",
      "            elif array.ndim == 2:\n",
      "                indexTuple = (slice(st[0], end[0]), slice(st[1], end[1]))\n",
      "                ndata = (end[0] - st[0]) * (end[1] - st[1])\n",
      "                nbyte = ndata * 4 + 32\n",
      "                fp.write(struct.pack(dataForm, nbyte, tb('    '), nrec, array.shape[0],\n",
      "                                         array.shape[1], st[0] + 1, end[0], st[1] + 1, end[1]))\n",
      "            dataForm1 = '=' + str(ndata) + dtype\n",
      "            fp.write(struct.pack(dataForm1, *array[indexTuple].flatten('F')))\n",
      "            fp.write(struct.pack('=i', nbyte))\n",
      "            nrec = nrec - 1\n",
      "_write2DArray(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, array=array([[ 0,  1,  2,  3],       [ 4,  5,  6,  7],       [ 8,  9, 10, 11],       [12, 13, 14, 15]], dtype=int32), dtype='i')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "indexTuple=(None,)\n",
      "State:\n",
      "(None,)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write2DArray(fp, array, dtype):\n",
      "        dataForm = \"=i4siiiiiii\"\n",
      "        maxData = 7991\n",
      "        nrec = (array.size - 1) // maxData + 1\n",
      "        ndata = 0\n",
      "        indexTuple=(None,)\n",
      "        nbyte=0\n",
      "        for st, end in HarFileIO._slice_inds(array, maxData):\n",
      "            if array.ndim == 1:\n",
      "                indexTuple = (slice(st[0], end[0]))\n",
      "                ndata = (end[0] - st[0])\n",
      "                nbyte = ndata * 4 + 32\n",
      "                fp.write(struct.pack(dataForm, nbyte, tb('    '), nrec, array.size, 1, st[0] + 1, end[0], 1, 1))\n",
      "            elif array.ndim == 2:\n",
      "                indexTuple = (slice(st[0], end[0]), slice(st[1], end[1]))\n",
      "                ndata = (end[0] - st[0]) * (end[1] - st[1])\n",
      "                nbyte = ndata * 4 + 32\n",
      "                fp.write(struct.pack(dataForm, nbyte, tb('    '), nrec, array.shape[0],\n",
      "                                         array.shape[1], st[0] + 1, end[0], st[1] + 1, end[1]))\n",
      "            dataForm1 = '=' + str(ndata) + dtype\n",
      "            fp.write(struct.pack(dataForm1, *array[indexTuple].flatten('F')))\n",
      "            fp.write(struct.pack('=i', nbyte))\n",
      "            nrec = nrec - 1\n",
      "_write2DArray(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, array=array([[ 0,  1,  2,  3],       [ 4,  5,  6,  7],       [ 8,  9, 10, 11],       [12, 13, 14, 15]], dtype=int32), dtype='i')\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "ndata = (end[0] - st[0]) * (end[1] - st[1])\n",
      "State:\n",
      "16\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write2DArray(fp, array, dtype):\n",
      "        dataForm = \"=i4siiiiiii\"\n",
      "        maxData = 7991\n",
      "        nrec = (array.size - 1) // maxData + 1\n",
      "        ndata = 0\n",
      "        indexTuple=(None,)\n",
      "        nbyte=0\n",
      "        for st, end in HarFileIO._slice_inds(array, maxData):\n",
      "            if array.ndim == 1:\n",
      "                indexTuple = (slice(st[0], end[0]))\n",
      "                ndata = (end[0] - st[0])\n",
      "                nbyte = ndata * 4 + 32\n",
      "                fp.write(struct.pack(dataForm, nbyte, tb('    '), nrec, array.size, 1, st[0] + 1, end[0], 1, 1))\n",
      "            elif array.ndim == 2:\n",
      "                indexTuple = (slice(st[0], end[0]), slice(st[1], end[1]))\n",
      "                ndata = (end[0] - st[0]) * (end[1] - st[1])\n",
      "                nbyte = ndata * 4 + 32\n",
      "                fp.write(struct.pack(dataForm, nbyte, tb('    '), nrec, array.shape[0],\n",
      "                                         array.shape[1], st[0] + 1, end[0], st[1] + 1, end[1]))\n",
      "            dataForm1 = '=' + str(ndata) + dtype\n",
      "            fp.write(struct.pack(dataForm1, *array[indexTuple].flatten('F')))\n",
      "            fp.write(struct.pack('=i', nbyte))\n",
      "            nrec = nrec - 1\n",
      "_write2DArray(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, array=array([[ 0,  1,  2,  3],       [ 4,  5,  6,  7],       [ 8,  9, 10, 11],       [12, 13, 14, 15]], dtype=int32), dtype='i')\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "dataForm1 = '=' + str(ndata) + dtype\n",
      "State:\n",
      "'=16i'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _writeHeader7D(fp: BinaryIO, hname : str, head_arr_obj: HeaderArrayObj):\n",
      "        hasElements = head_arr_obj.sets.defined()\n",
      "        dataFill = float(np.count_nonzero(head_arr_obj.array)) / head_arr_obj.array.size\n",
      "        if dataFill > 0.4:\n",
      "            head_arr_obj.storage_type = 'FULL'\n",
      "        else:\n",
      "            head_arr_obj.storage_type = 'SPSE'\n",
      "        shape7D = [head_arr_obj.array.shape[i] if i < head_arr_obj.array.ndim else 1 for i in range(0, 7)]\n",
      "        HarFileIO._writeHeaderName(fp, hname)\n",
      "        HeaderType = 'RE'\n",
      "        secRecList = ['    ', HeaderType, head_arr_obj.storage_type, head_arr_obj.long_name, 7]\n",
      "        secRecList.extend(shape7D)\n",
      "        HarFileIO._writeSecondRecord(fp, secRecList)\n",
      "        HarFileIO._writeSetElInfo(fp, head_arr_obj)\n",
      "        if head_arr_obj.storage_type == 'FULL':\n",
      "            HarFileIO._write7DFullArray(fp, np.asfortranarray(head_arr_obj.array), 'f')\n",
      "        else:\n",
      "            HarFileIO._write7DSparseArray(fp, np.asfortranarray(head_arr_obj.array), 'f')\n",
      "_writeHeader7D(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, hname='NH01', head_arr_obj={_coeff_name='Array2D     ', _array=array([[1.  , 3.14],       [3.14, 5.  ]], dtype=float32), _sets=<harpy._header_sets._HeaderDims object at 0x7fd0b7249e80>, _long_name='Simple 2 dimensional array                                            '})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "hasElements = head_arr_obj.sets.defined()\n",
      "State:\n",
      "True\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _writeHeader7D(fp: BinaryIO, hname : str, head_arr_obj: HeaderArrayObj):\n",
      "        hasElements = head_arr_obj.sets.defined()\n",
      "        dataFill = float(np.count_nonzero(head_arr_obj.array)) / head_arr_obj.array.size\n",
      "        if dataFill > 0.4:\n",
      "            head_arr_obj.storage_type = 'FULL'\n",
      "        else:\n",
      "            head_arr_obj.storage_type = 'SPSE'\n",
      "        shape7D = [head_arr_obj.array.shape[i] if i < head_arr_obj.array.ndim else 1 for i in range(0, 7)]\n",
      "        HarFileIO._writeHeaderName(fp, hname)\n",
      "        HeaderType = 'RE'\n",
      "        secRecList = ['    ', HeaderType, head_arr_obj.storage_type, head_arr_obj.long_name, 7]\n",
      "        secRecList.extend(shape7D)\n",
      "        HarFileIO._writeSecondRecord(fp, secRecList)\n",
      "        HarFileIO._writeSetElInfo(fp, head_arr_obj)\n",
      "        if head_arr_obj.storage_type == 'FULL':\n",
      "            HarFileIO._write7DFullArray(fp, np.asfortranarray(head_arr_obj.array), 'f')\n",
      "        else:\n",
      "            HarFileIO._write7DSparseArray(fp, np.asfortranarray(head_arr_obj.array), 'f')\n",
      "_writeHeader7D(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, hname='NH01', head_arr_obj={_coeff_name='Array2D     ', _array=array([[1.  , 3.14],       [3.14, 5.  ]], dtype=float32), _sets=<harpy._header_sets._HeaderDims object at 0x7fd0b7249e80>, _long_name='Simple 2 dimensional array                                            '})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "dataFill = float(np.count_nonzero(head_arr_obj.array)) / head_arr_obj.array.size\n",
      "State:\n",
      "1.0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _writeHeader7D(fp: BinaryIO, hname : str, head_arr_obj: HeaderArrayObj):\n",
      "        hasElements = head_arr_obj.sets.defined()\n",
      "        dataFill = float(np.count_nonzero(head_arr_obj.array)) / head_arr_obj.array.size\n",
      "        if dataFill > 0.4:\n",
      "            head_arr_obj.storage_type = 'FULL'\n",
      "        else:\n",
      "            head_arr_obj.storage_type = 'SPSE'\n",
      "        shape7D = [head_arr_obj.array.shape[i] if i < head_arr_obj.array.ndim else 1 for i in range(0, 7)]\n",
      "        HarFileIO._writeHeaderName(fp, hname)\n",
      "        HeaderType = 'RE'\n",
      "        secRecList = ['    ', HeaderType, head_arr_obj.storage_type, head_arr_obj.long_name, 7]\n",
      "        secRecList.extend(shape7D)\n",
      "        HarFileIO._writeSecondRecord(fp, secRecList)\n",
      "        HarFileIO._writeSetElInfo(fp, head_arr_obj)\n",
      "        if head_arr_obj.storage_type == 'FULL':\n",
      "            HarFileIO._write7DFullArray(fp, np.asfortranarray(head_arr_obj.array), 'f')\n",
      "        else:\n",
      "            HarFileIO._write7DSparseArray(fp, np.asfortranarray(head_arr_obj.array), 'f')\n",
      "_writeHeader7D(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, hname='NH01', head_arr_obj={_coeff_name='Array2D     ', _array=array([[1.  , 3.14],       [3.14, 5.  ]], dtype=float32), _sets=<harpy._header_sets._HeaderDims object at 0x7fd0b7249e80>, _long_name='Simple 2 dimensional array                                            '})\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "secRecList.extend(shape7D)\n",
      "State:\n",
      "['    ', 'RE', 'FULL', 'Simple 2 dimensional array                                            ', 7, 2, 2, 1, 1, 1, 1, 1]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _writeSetElInfo(fp, header_arr_obj: HeaderArrayObj):\n",
      "        sets = [setDim.name for setDim in header_arr_obj.sets.dims]\n",
      "        indexTypes = [setDim.status for setDim in header_arr_obj.sets.dims]\n",
      "        Elements = [setDim.dim_desc for setDim in header_arr_obj.sets.dims]\n",
      "        CName = header_arr_obj.coeff_name\n",
      "        tmp = {}\n",
      "        elList = []\n",
      "        if all(item is None for item in sets):\n",
      "            nToWrite = 0\n",
      "            nSets = len(sets)\n",
      "            nElement = 0\n",
      "            setsKnown= 0\n",
      "        else:\n",
      "            setsKnown=1\n",
      "            statusStr = ''\n",
      "            outputElements = []\n",
      "            for i, j, setEls in zip(sets, indexTypes, Elements):\n",
      "                if j == 'k':\n",
      "                    if not i in tmp:\n",
      "                        outputElements.append(setEls)\n",
      "                    tmp[i] = setEls\n",
      "                    statusStr += 'k'\n",
      "                elif j == 'e':\n",
      "                    elList.append(setEls[0])\n",
      "                    statusStr += 'e'\n",
      "                else:\n",
      "                    statusStr += 'u'\n",
      "            nToWrite = len(tmp)\n",
      "            nElement = len(elList)\n",
      "            ElementStr = tb(''.join(elList))\n",
      "            statusStr = tb(statusStr)\n",
      "            SetStr = tb(''.join([item.ljust(12) if not item is None else \" \"*12 for item in sets]))\n",
      "            nSets = len(sets)\n",
      "        dataForm = '=i4siii12si'\n",
      "        if setsKnown == 1: dataForm += str(nSets * 13) + 's' + str(nSets) + 'i'\n",
      "        dataForm += 'i'\n",
      "        if nElement > 0: dataForm += str(nElement * 12)\n",
      "        dataForm += 'i'\n",
      "        nbyte = struct.calcsize(dataForm) - 8\n",
      "        writeList = [nbyte, tb('    '), nToWrite, 1, nSets, tb(CName.ljust(12)), setsKnown]\n",
      "        if setsKnown == 1:\n",
      "            writeList.append(SetStr + statusStr)\n",
      "            writeList.extend([0]*len(Elements) )\n",
      "        writeList.append(nElement)\n",
      "        if nElement > 0: writeList.append(ElementStr)\n",
      "        writeList.append(nbyte)\n",
      "        fp.write(struct.pack(dataForm, *writeList))\n",
      "        if nToWrite > 0:\n",
      "            for Els in outputElements:\n",
      "                array = np.array(Els)\n",
      "                HarFileIO._write1CArray(fp, array, len(Els), 12)\n",
      "_writeSetElInfo(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, header_arr_obj={_coeff_name='Array2D     ', _array=array([[1.  , 3.14],       [3.14, 5.  ]], dtype=float32), _sets=<harpy._header_sets._HeaderDims object at 0x7fd0b7249e80>, _long_name='Simple 2 dimensional array                                            ', storage_type='FULL'})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "Elements = [setDim.dim_desc for setDim in header_arr_obj.sets.dims]\n",
      "State:\n",
      "[['A', 'B'], ['I', 'II']]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _writeSetElInfo(fp, header_arr_obj: HeaderArrayObj):\n",
      "        sets = [setDim.name for setDim in header_arr_obj.sets.dims]\n",
      "        indexTypes = [setDim.status for setDim in header_arr_obj.sets.dims]\n",
      "        Elements = [setDim.dim_desc for setDim in header_arr_obj.sets.dims]\n",
      "        CName = header_arr_obj.coeff_name\n",
      "        tmp = {}\n",
      "        elList = []\n",
      "        if all(item is None for item in sets):\n",
      "            nToWrite = 0\n",
      "            nSets = len(sets)\n",
      "            nElement = 0\n",
      "            setsKnown= 0\n",
      "        else:\n",
      "            setsKnown=1\n",
      "            statusStr = ''\n",
      "            outputElements = []\n",
      "            for i, j, setEls in zip(sets, indexTypes, Elements):\n",
      "                if j == 'k':\n",
      "                    if not i in tmp:\n",
      "                        outputElements.append(setEls)\n",
      "                    tmp[i] = setEls\n",
      "                    statusStr += 'k'\n",
      "                elif j == 'e':\n",
      "                    elList.append(setEls[0])\n",
      "                    statusStr += 'e'\n",
      "                else:\n",
      "                    statusStr += 'u'\n",
      "            nToWrite = len(tmp)\n",
      "            nElement = len(elList)\n",
      "            ElementStr = tb(''.join(elList))\n",
      "            statusStr = tb(statusStr)\n",
      "            SetStr = tb(''.join([item.ljust(12) if not item is None else \" \"*12 for item in sets]))\n",
      "            nSets = len(sets)\n",
      "        dataForm = '=i4siii12si'\n",
      "        if setsKnown == 1: dataForm += str(nSets * 13) + 's' + str(nSets) + 'i'\n",
      "        dataForm += 'i'\n",
      "        if nElement > 0: dataForm += str(nElement * 12)\n",
      "        dataForm += 'i'\n",
      "        nbyte = struct.calcsize(dataForm) - 8\n",
      "        writeList = [nbyte, tb('    '), nToWrite, 1, nSets, tb(CName.ljust(12)), setsKnown]\n",
      "        if setsKnown == 1:\n",
      "            writeList.append(SetStr + statusStr)\n",
      "            writeList.extend([0]*len(Elements) )\n",
      "        writeList.append(nElement)\n",
      "        if nElement > 0: writeList.append(ElementStr)\n",
      "        writeList.append(nbyte)\n",
      "        fp.write(struct.pack(dataForm, *writeList))\n",
      "        if nToWrite > 0:\n",
      "            for Els in outputElements:\n",
      "                array = np.array(Els)\n",
      "                HarFileIO._write1CArray(fp, array, len(Els), 12)\n",
      "_writeSetElInfo(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, header_arr_obj={_coeff_name='Array2D     ', _array=array([[1.  , 3.14],       [3.14, 5.  ]], dtype=float32), _sets=<harpy._header_sets._HeaderDims object at 0x7fd0b7249e80>, _long_name='Simple 2 dimensional array                                            ', storage_type='FULL'})\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "setsKnown=1\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write7DFullArray(fp, array, dtype):\n",
      "        StEndList = []\n",
      "        for i, j in HarFileIO._slice_inds(array, 7996):\n",
      "            StEndList.append([i[:], j[:]])\n",
      "        nrec = len(StEndList) * 2 + 1\n",
      "        dataForm = '=i4sii7ii'\n",
      "        nbyte = struct.calcsize(dataForm) - 8\n",
      "        writeList = [nbyte, tb('    '), nrec, 7]\n",
      "        writeList.extend([array.shape[i] if i < array.ndim else 1 for i in range(0, 7)])\n",
      "        writeList.append(nbyte)\n",
      "        fp.write(struct.pack(dataForm, *writeList))\n",
      "        array1=array.flatten('F')\n",
      "        nWritten=0\n",
      "        for StEnd in StEndList:\n",
      "            nrec = nrec - 1\n",
      "            st = StEnd[0]\n",
      "            end = StEnd[1]\n",
      "            PosList = [[st[i] + 1, end[i]][ind] if i < array.ndim else [1, 1][ind] for i in range(0, 7) for ind in\n",
      "                       range(0, 2)]\n",
      "            dataForm = '=i4s16i'\n",
      "            nbyte = struct.calcsize(dataForm) - 8\n",
      "            writeList = [nbyte, tb('    '), nrec]\n",
      "            writeList.extend(PosList)\n",
      "            writeList.append(nbyte)\n",
      "            fp.write(struct.pack(dataForm, *writeList))\n",
      "            nrec = nrec - 1\n",
      "            ndata = 1\n",
      "            for i, j in zip(st, end): ndata *= (j - i)\n",
      "            if dtype == 'f' or dtype == 'i': nbyte = ndata * 4 + 8\n",
      "            dataForm = '=i4si'\n",
      "            fp.write(struct.pack(dataForm, nbyte, tb('    '), nrec))\n",
      "            dataForm = '=' + str(ndata) + dtype\n",
      "            fp.write(struct.pack(dataForm, *array1[nWritten:nWritten+ndata].flatten('F')))\n",
      "            nWritten+=ndata\n",
      "            dataForm = '=i'\n",
      "            fp.write(struct.pack(dataForm, nbyte))\n",
      "_write7DFullArray(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, array=array([[1.  , 3.14],       [3.14, 5.  ]], dtype=float32), dtype='f')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "StEndList = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write7DFullArray(fp, array, dtype):\n",
      "        StEndList = []\n",
      "        for i, j in HarFileIO._slice_inds(array, 7996):\n",
      "            StEndList.append([i[:], j[:]])\n",
      "        nrec = len(StEndList) * 2 + 1\n",
      "        dataForm = '=i4sii7ii'\n",
      "        nbyte = struct.calcsize(dataForm) - 8\n",
      "        writeList = [nbyte, tb('    '), nrec, 7]\n",
      "        writeList.extend([array.shape[i] if i < array.ndim else 1 for i in range(0, 7)])\n",
      "        writeList.append(nbyte)\n",
      "        fp.write(struct.pack(dataForm, *writeList))\n",
      "        array1=array.flatten('F')\n",
      "        nWritten=0\n",
      "        for StEnd in StEndList:\n",
      "            nrec = nrec - 1\n",
      "            st = StEnd[0]\n",
      "            end = StEnd[1]\n",
      "            PosList = [[st[i] + 1, end[i]][ind] if i < array.ndim else [1, 1][ind] for i in range(0, 7) for ind in\n",
      "                       range(0, 2)]\n",
      "            dataForm = '=i4s16i'\n",
      "            nbyte = struct.calcsize(dataForm) - 8\n",
      "            writeList = [nbyte, tb('    '), nrec]\n",
      "            writeList.extend(PosList)\n",
      "            writeList.append(nbyte)\n",
      "            fp.write(struct.pack(dataForm, *writeList))\n",
      "            nrec = nrec - 1\n",
      "            ndata = 1\n",
      "            for i, j in zip(st, end): ndata *= (j - i)\n",
      "            if dtype == 'f' or dtype == 'i': nbyte = ndata * 4 + 8\n",
      "            dataForm = '=i4si'\n",
      "            fp.write(struct.pack(dataForm, nbyte, tb('    '), nrec))\n",
      "            dataForm = '=' + str(ndata) + dtype\n",
      "            fp.write(struct.pack(dataForm, *array1[nWritten:nWritten+ndata].flatten('F')))\n",
      "            nWritten+=ndata\n",
      "            dataForm = '=i'\n",
      "            fp.write(struct.pack(dataForm, nbyte))\n",
      "_write7DFullArray(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, array=array([[1.  , 3.14],       [3.14, 5.  ]], dtype=float32), dtype='f')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "StEndList.append([i[:], j[:]])\n",
      "State:\n",
      "[[[0, 0], [2, 2]]]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write7DFullArray(fp, array, dtype):\n",
      "        StEndList = []\n",
      "        for i, j in HarFileIO._slice_inds(array, 7996):\n",
      "            StEndList.append([i[:], j[:]])\n",
      "        nrec = len(StEndList) * 2 + 1\n",
      "        dataForm = '=i4sii7ii'\n",
      "        nbyte = struct.calcsize(dataForm) - 8\n",
      "        writeList = [nbyte, tb('    '), nrec, 7]\n",
      "        writeList.extend([array.shape[i] if i < array.ndim else 1 for i in range(0, 7)])\n",
      "        writeList.append(nbyte)\n",
      "        fp.write(struct.pack(dataForm, *writeList))\n",
      "        array1=array.flatten('F')\n",
      "        nWritten=0\n",
      "        for StEnd in StEndList:\n",
      "            nrec = nrec - 1\n",
      "            st = StEnd[0]\n",
      "            end = StEnd[1]\n",
      "            PosList = [[st[i] + 1, end[i]][ind] if i < array.ndim else [1, 1][ind] for i in range(0, 7) for ind in\n",
      "                       range(0, 2)]\n",
      "            dataForm = '=i4s16i'\n",
      "            nbyte = struct.calcsize(dataForm) - 8\n",
      "            writeList = [nbyte, tb('    '), nrec]\n",
      "            writeList.extend(PosList)\n",
      "            writeList.append(nbyte)\n",
      "            fp.write(struct.pack(dataForm, *writeList))\n",
      "            nrec = nrec - 1\n",
      "            ndata = 1\n",
      "            for i, j in zip(st, end): ndata *= (j - i)\n",
      "            if dtype == 'f' or dtype == 'i': nbyte = ndata * 4 + 8\n",
      "            dataForm = '=i4si'\n",
      "            fp.write(struct.pack(dataForm, nbyte, tb('    '), nrec))\n",
      "            dataForm = '=' + str(ndata) + dtype\n",
      "            fp.write(struct.pack(dataForm, *array1[nWritten:nWritten+ndata].flatten('F')))\n",
      "            nWritten+=ndata\n",
      "            dataForm = '=i'\n",
      "            fp.write(struct.pack(dataForm, nbyte))\n",
      "_write7DFullArray(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, array=array([[1.  , 3.14],       [3.14, 5.  ]], dtype=float32), dtype='f')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "nrec = len(StEndList) * 2 + 1\n",
      "State:\n",
      "3\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write7DFullArray(fp, array, dtype):\n",
      "        StEndList = []\n",
      "        for i, j in HarFileIO._slice_inds(array, 7996):\n",
      "            StEndList.append([i[:], j[:]])\n",
      "        nrec = len(StEndList) * 2 + 1\n",
      "        dataForm = '=i4sii7ii'\n",
      "        nbyte = struct.calcsize(dataForm) - 8\n",
      "        writeList = [nbyte, tb('    '), nrec, 7]\n",
      "        writeList.extend([array.shape[i] if i < array.ndim else 1 for i in range(0, 7)])\n",
      "        writeList.append(nbyte)\n",
      "        fp.write(struct.pack(dataForm, *writeList))\n",
      "        array1=array.flatten('F')\n",
      "        nWritten=0\n",
      "        for StEnd in StEndList:\n",
      "            nrec = nrec - 1\n",
      "            st = StEnd[0]\n",
      "            end = StEnd[1]\n",
      "            PosList = [[st[i] + 1, end[i]][ind] if i < array.ndim else [1, 1][ind] for i in range(0, 7) for ind in\n",
      "                       range(0, 2)]\n",
      "            dataForm = '=i4s16i'\n",
      "            nbyte = struct.calcsize(dataForm) - 8\n",
      "            writeList = [nbyte, tb('    '), nrec]\n",
      "            writeList.extend(PosList)\n",
      "            writeList.append(nbyte)\n",
      "            fp.write(struct.pack(dataForm, *writeList))\n",
      "            nrec = nrec - 1\n",
      "            ndata = 1\n",
      "            for i, j in zip(st, end): ndata *= (j - i)\n",
      "            if dtype == 'f' or dtype == 'i': nbyte = ndata * 4 + 8\n",
      "            dataForm = '=i4si'\n",
      "            fp.write(struct.pack(dataForm, nbyte, tb('    '), nrec))\n",
      "            dataForm = '=' + str(ndata) + dtype\n",
      "            fp.write(struct.pack(dataForm, *array1[nWritten:nWritten+ndata].flatten('F')))\n",
      "            nWritten+=ndata\n",
      "            dataForm = '=i'\n",
      "            fp.write(struct.pack(dataForm, nbyte))\n",
      "_write7DFullArray(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, array=array([[1.  , 3.14],       [3.14, 5.  ]], dtype=float32), dtype='f')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "dataForm = '=i4sii7ii'\n",
      "State:\n",
      "'=i4sii7ii'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write7DFullArray(fp, array, dtype):\n",
      "        StEndList = []\n",
      "        for i, j in HarFileIO._slice_inds(array, 7996):\n",
      "            StEndList.append([i[:], j[:]])\n",
      "        nrec = len(StEndList) * 2 + 1\n",
      "        dataForm = '=i4sii7ii'\n",
      "        nbyte = struct.calcsize(dataForm) - 8\n",
      "        writeList = [nbyte, tb('    '), nrec, 7]\n",
      "        writeList.extend([array.shape[i] if i < array.ndim else 1 for i in range(0, 7)])\n",
      "        writeList.append(nbyte)\n",
      "        fp.write(struct.pack(dataForm, *writeList))\n",
      "        array1=array.flatten('F')\n",
      "        nWritten=0\n",
      "        for StEnd in StEndList:\n",
      "            nrec = nrec - 1\n",
      "            st = StEnd[0]\n",
      "            end = StEnd[1]\n",
      "            PosList = [[st[i] + 1, end[i]][ind] if i < array.ndim else [1, 1][ind] for i in range(0, 7) for ind in\n",
      "                       range(0, 2)]\n",
      "            dataForm = '=i4s16i'\n",
      "            nbyte = struct.calcsize(dataForm) - 8\n",
      "            writeList = [nbyte, tb('    '), nrec]\n",
      "            writeList.extend(PosList)\n",
      "            writeList.append(nbyte)\n",
      "            fp.write(struct.pack(dataForm, *writeList))\n",
      "            nrec = nrec - 1\n",
      "            ndata = 1\n",
      "            for i, j in zip(st, end): ndata *= (j - i)\n",
      "            if dtype == 'f' or dtype == 'i': nbyte = ndata * 4 + 8\n",
      "            dataForm = '=i4si'\n",
      "            fp.write(struct.pack(dataForm, nbyte, tb('    '), nrec))\n",
      "            dataForm = '=' + str(ndata) + dtype\n",
      "            fp.write(struct.pack(dataForm, *array1[nWritten:nWritten+ndata].flatten('F')))\n",
      "            nWritten+=ndata\n",
      "            dataForm = '=i'\n",
      "            fp.write(struct.pack(dataForm, nbyte))\n",
      "_write7DFullArray(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, array=array([[1.  , 3.14],       [3.14, 5.  ]], dtype=float32), dtype='f')\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "PosList = [[st[i] + 1, end[i]][ind] if i < array.ndim else [1, 1][ind] for i in range(0, 7) for ind in\n",
      "State:\n",
      "[1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write7DFullArray(fp, array, dtype):\n",
      "        StEndList = []\n",
      "        for i, j in HarFileIO._slice_inds(array, 7996):\n",
      "            StEndList.append([i[:], j[:]])\n",
      "        nrec = len(StEndList) * 2 + 1\n",
      "        dataForm = '=i4sii7ii'\n",
      "        nbyte = struct.calcsize(dataForm) - 8\n",
      "        writeList = [nbyte, tb('    '), nrec, 7]\n",
      "        writeList.extend([array.shape[i] if i < array.ndim else 1 for i in range(0, 7)])\n",
      "        writeList.append(nbyte)\n",
      "        fp.write(struct.pack(dataForm, *writeList))\n",
      "        array1=array.flatten('F')\n",
      "        nWritten=0\n",
      "        for StEnd in StEndList:\n",
      "            nrec = nrec - 1\n",
      "            st = StEnd[0]\n",
      "            end = StEnd[1]\n",
      "            PosList = [[st[i] + 1, end[i]][ind] if i < array.ndim else [1, 1][ind] for i in range(0, 7) for ind in\n",
      "                       range(0, 2)]\n",
      "            dataForm = '=i4s16i'\n",
      "            nbyte = struct.calcsize(dataForm) - 8\n",
      "            writeList = [nbyte, tb('    '), nrec]\n",
      "            writeList.extend(PosList)\n",
      "            writeList.append(nbyte)\n",
      "            fp.write(struct.pack(dataForm, *writeList))\n",
      "            nrec = nrec - 1\n",
      "            ndata = 1\n",
      "            for i, j in zip(st, end): ndata *= (j - i)\n",
      "            if dtype == 'f' or dtype == 'i': nbyte = ndata * 4 + 8\n",
      "            dataForm = '=i4si'\n",
      "            fp.write(struct.pack(dataForm, nbyte, tb('    '), nrec))\n",
      "            dataForm = '=' + str(ndata) + dtype\n",
      "            fp.write(struct.pack(dataForm, *array1[nWritten:nWritten+ndata].flatten('F')))\n",
      "            nWritten+=ndata\n",
      "            dataForm = '=i'\n",
      "            fp.write(struct.pack(dataForm, nbyte))\n",
      "_write7DFullArray(fp=<_io.BufferedWriter name='test_overwrite_header.har'>, array=array([[1.  , 3.14],       [3.14, 5.  ]], dtype=float32), dtype='f')\n",
      "\n",
      "n:\n",
      "25\n",
      "Statement:\n",
      "writeList.extend(PosList)\n",
      "State:\n",
      "[64, b'    ', 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def updateMtime(self):\n",
      "        self._mtime = os.path.getmtime(self.filename)\n",
      "updateMtime(self=<harpy.har_file_io.HarFileInfoObj object at 0x7fd01634d4c0>, self._ha_infos=OrderedDict([('XXCD', <harpy.har_file_io.HarFileInfoObj._HAInfo object at 0x7fd01634dc70>), ('XXCR', <harpy.har_file_io.HarFileInfoObj._HAInfo object at 0x7fd01634d8b0>), ('XXCP', <harpy.har_file_io.HarFileInfoObj._HAInfo object at 0x7fd01634dc10>), ('XXHS', <harpy.har_file_io.HarFileInfoObj._HAInfo object at 0x7fd01634d4f0>), ('CHST', <harpy.har_file_io.HarFileInfoObj._HAInfo object at 0x7fd01634d220>), ('INTA', <harpy.har_file_io.HarFileInfoObj._HAInfo object at 0x7fd016307f40>), ('SIMP', <harpy.har_file_io.HarFileInfoObj._HAInfo object at 0x7fd016307d30>), ('SIM2', <harpy.har_file_io.HarFileInfoObj._HAInfo object at 0x7fd016307b80>), ('NH01', <harpy.har_file_io.HarFileInfoObj._HAInfo object at 0x7fd016307a00>), ('ARR7', <harpy.har_file_io.HarFileInfoObj._HAInfo object at 0x7fd016307070>)]), self._mtime=1708906163.399167, self.filename='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/GEMPACKsoftware+HARPY/GEMPACKsoftware+HARPY/test_overwrite_header.har')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self._mtime = os.path.getmtime(self.filename)\n",
      "State:\n",
      "1712174310.5007823\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_set_item(self):\n",
      "        hfo = HarFileObj._loadFromDisk(TestHarFileObj._dd + \"test.har\")\n",
      "        test_hn = ['XXCD', 'XXCR', 'XXCP', 'XXHS', 'CHST', 'INTA', 'SIMP', 'SIM2', 'NH01', 'ARR7']\n",
      "        xxcdHead = hfo[\"xxcd\"]\n",
      "        hfo[\"xxc1\"]=xxcdHead\n",
      "        self.assertTrue(\"XXC1\" in hfo)\n",
      "        hfo[[\"xxc2\",\"xxc3\"]]=[xxcdHead]*2\n",
      "        self.assertTrue(all(name in hfo for name in [\"XXC2\",\"XXC3\"]))\n",
      "        with self.assertRaises(HarFileObj.InvalidHeaderArrayName):\n",
      "            hfo[\"TOO LONG NAME\"] = xxcdHead\n",
      "        with self.assertRaises(TypeError):\n",
      "            hfo[1] = xxcdHead\n",
      "test_set_item(self=<harpy.tests.test_har_file.TestHarFileObj testMethod=test_set_item>, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fd016378610>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_set_item', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_set_item=<bound method TestHarFileObj.test_set_item of <harpy.tests.test_har_file.TestHarFileObj testMethod=test_set_item>>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "test_hn = ['XXCD', 'XXCR', 'XXCP', 'XXHS', 'CHST', 'INTA', 'SIMP', 'SIM2', 'NH01', 'ARR7']\n",
      "State:\n",
      "['XXCD', 'XXCR', 'XXCP', 'XXHS', 'CHST', 'INTA', 'SIMP', 'SIM2', 'NH01', 'ARR7']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_array_operation(self):\n",
      "        array_2d = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
      "        hao1 = HeaderArrayObj.HeaderArrayFromData( array=array_2d)\n",
      "        hao2 = HeaderArrayObj.HeaderArrayFromData( array=array_2d)\n",
      "        hao3 = hao1 + hao2\n",
      "        self.assertTrue(np.allclose(hao3.array, array_2d*2))\n",
      "        hao3 = hao1 - hao2\n",
      "        self.assertTrue(np.allclose(hao3.array, np.array([[0, 0], [0, 0]])))\n",
      "        hao3 = hao1 * hao2\n",
      "        self.assertTrue(np.allclose(hao3.array, np.array([[1, 4], [9, 16]])))\n",
      "        hao3 = hao1 / hao2\n",
      "        self.assertTrue(np.allclose(hao3.array, np.array([[1, 1], [1, 1]])))\n",
      "test_array_operation(self=<harpy.tests.test_header_array.TestHeaderArray testMethod=test_array_operation>, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fd016350b80>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_array_operation', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_array_operation=<bound method TestHeaderArray.test_array_operation of <harpy.tests.test_header_array.TestHeaderArray testMethod=test_array_operation>>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "array_2d = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
      "State:\n",
      "array([[1., 2.],       [3., 4.]])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def matchSets(self,sets=None, shape:tuple=None):\n",
      "        if sets is None and shape is None : raise KeyError(\"Only one argument allowed\")\n",
      "        newSets = []\n",
      "        if not sets is None:\n",
      "            iset=len(self.dims)-1; jset=len(sets.dims)-1\n",
      "            while iset >=0 and jset >=0:\n",
      "                if jset < 0 :\n",
      "                    newSets.append(self.dims[iset])\n",
      "                    iset -=1\n",
      "                elif iset < 0 :\n",
      "                    newSets.append(sets.dims[jset])\n",
      "                    jset -=1\n",
      "                if self.dims[iset].dim_size == sets.dims[jset].dim_size or self.dims[iset].dim_size == 1 or sets.dims[jset].dim_size == 1:\n",
      "                    if self.dims[iset].status != 'n':\n",
      "                        newSets.append(self.dims[iset])\n",
      "                    else:\n",
      "                        newSets.append(sets.dims[jset])\n",
      "                    iset-= 1 ; jset -=1\n",
      "            newSets.reverse()\n",
      "        elif not shape is None:\n",
      "            iset = len(self.dims) - 1; jset=len(shape)-1\n",
      "            while iset >=0 and jset >=0:\n",
      "                if jset < 0 :\n",
      "                    newSets.append(self.dims[iset])\n",
      "                    iset -=1\n",
      "                elif iset < 0 :\n",
      "                    newSets.append(_HeaderSet(None , 'n' , None, shape[jset]))\n",
      "                    jset -=1\n",
      "                if self.dims[iset].dim_size == shape[jset] or self.dims[iset].dim_size == 1 or shape[jset] == 1:\n",
      "                    newSets.append(self.dims[iset])\n",
      "                    iset-= 1 ; jset -=1\n",
      "        else:\n",
      "            return KeyError(\"Either sets o shape have to be defined\")\n",
      "        return _HeaderDims(newSets)\n",
      "matchSets(self=<harpy._header_sets._HeaderDims object at 0x7fd016350250>, sets={_dims=[<harpy._header_sets._HeaderSet object at 0x7fd016391ca0>, <harpy._header_sets._HeaderSet object at 0x7fd016391bb0>]}, shape=None, self._dims=[<harpy._header_sets._HeaderSet object at 0x7fd0163913d0>, <harpy._header_sets._HeaderSet object at 0x7fd016391e80>])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "newSets = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_array_operation_float(self):\n",
      "        array_2d = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
      "        hao1 = HeaderArrayObj.HeaderArrayFromData(array=array_2d)\n",
      "        hao3 = hao1 + 1.5\n",
      "        self.assertTrue(np.allclose(hao3.array, array_2d + 1.5))\n",
      "        hao3 = hao1 - 0.5\n",
      "        self.assertTrue(np.allclose(hao3.array, array_2d - 0.5))\n",
      "        hao3 = hao1 * 3.14\n",
      "        self.assertTrue(np.allclose(hao3.array, array_2d * 3.14))\n",
      "        hao3 = hao1 / -2.5\n",
      "        self.assertTrue(np.allclose(hao3.array, array_2d / -2.5))\n",
      "        hao3 = 1.5 + hao1\n",
      "        self.assertTrue(np.allclose(hao3.array, array_2d + 1.5))\n",
      "        hao3 = 0.5 - hao1\n",
      "        self.assertTrue(np.allclose(hao3.array, 0.5 - array_2d))\n",
      "        hao3 = 3.14 * hao1\n",
      "        self.assertTrue(np.allclose(hao3.array, array_2d * 3.14))\n",
      "        hao3 = -2.5 / hao1\n",
      "        self.assertTrue(np.allclose(hao3.array, -2.5 / array_2d))\n",
      "test_array_operation_float(self=<harpy.tests.test_header_array.TestHeaderArray testMethod=test_array_operation_float>, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fd016350df0>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_array_operation_float', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_array_operation_float=<bound method TestHeaderArray.test_array_operation_float of <harpy.tests.test_header_array.TestHeaderArray testMethod=test_array_operation_float>>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "array_2d = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
      "State:\n",
      "array([[1., 2.],       [3., 4.]])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_array_operation_int(self):\n",
      "        array_2d = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
      "        hao1 = HeaderArrayObj.HeaderArrayFromData(array=array_2d)\n",
      "        hao3 = hao1 + 2\n",
      "        self.assertTrue(np.allclose(hao3.array, array_2d + 2))\n",
      "        hao3 = hao1 - 1\n",
      "        self.assertTrue(np.allclose(hao3.array, array_2d - 1))\n",
      "        hao3 = hao1 * 3\n",
      "        self.assertTrue(np.allclose(hao3.array, array_2d * 3))\n",
      "        hao3 = hao1 / 2\n",
      "        self.assertTrue(np.allclose(hao3.array, array_2d / 2))\n",
      "        hao3 = 2 + hao1\n",
      "        self.assertTrue(np.allclose(hao3.array, array_2d + 2))\n",
      "        hao3 = 1 - hao1\n",
      "        self.assertTrue(np.allclose(hao3.array, 1 - array_2d ))\n",
      "        hao3 = 3 * hao1\n",
      "        self.assertTrue(np.allclose(hao3.array, array_2d * 3))\n",
      "        hao3 = 2 / hao1\n",
      "        self.assertTrue(np.allclose(hao3.array, 2 / array_2d ))\n",
      "test_array_operation_int(self=<harpy.tests.test_header_array.TestHeaderArray testMethod=test_array_operation_int>, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fd016391ca0>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_array_operation_int', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_array_operation_int=<bound method TestHeaderArray.test_array_operation_int of <harpy.tests.test_header_array.TestHeaderArray testMethod=test_array_operation_int>>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "array_2d = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
      "State:\n",
      "array([[1., 2.],       [3., 4.]])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_array_operation_ndarray(self):\n",
      "        array_2d = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
      "        hao1 = HeaderArrayObj.HeaderArrayFromData(array=array_2d)\n",
      "        hao3 = hao1 + array_2d\n",
      "        self.assertTrue(np.allclose(hao3.array, array_2d*2))\n",
      "        hao3 = hao1 - array_2d\n",
      "        self.assertTrue(np.allclose(hao3.array, np.array([[0, 0], [0, 0]])))\n",
      "        hao3 = hao1 * array_2d\n",
      "        self.assertTrue(np.allclose(hao3.array, np.array([[1, 4], [9, 16]])))\n",
      "        hao3 = hao1 / array_2d\n",
      "        self.assertTrue(np.allclose(hao3.array, np.array([[1, 1], [1, 1]])))\n",
      "        hao3 = array_2d + hao1\n",
      "        self.assertTrue(np.allclose(hao3.array, array_2d*2))\n",
      "        hao3 = array_2d - hao1\n",
      "        self.assertTrue(np.allclose(hao3.array, np.array([[0, 0], [0, 0]])))\n",
      "        hao3 = array_2d * hao1\n",
      "        self.assertTrue(np.allclose(hao3.array, np.array([[1, 4], [9, 16]])))\n",
      "        hao3 = array_2d / hao1\n",
      "        self.assertTrue(np.allclose(hao3.array, np.array([[1, 1], [1, 1]])))\n",
      "test_array_operation_ndarray(self=<harpy.tests.test_header_array.TestHeaderArray testMethod=test_array_operation_ndarray>, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fd0163913d0>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_array_operation_ndarray', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_array_operation_ndarray=<bound method TestHeaderArray.test_array_operation_ndarray of <harpy.tests.test_header_array.TestHeaderArray testMethod=test_array_operation_ndarray>>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "array_2d = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
      "State:\n",
      "array([[1., 2.],       [3., 4.]])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_attributes_style(self):\n",
      "        hao = HeaderArrayObj()\n",
      "        hao.name = \"ABC\"\n",
      "        hao.coeff_name = \"ABCDEF\"\n",
      "        hao.long_name = \"A test header array object.\"\n",
      "        hao.array = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
      "        self.assertEqual(hao.name, \"ABC\")\n",
      "        self.assertEqual(hao.coeff_name, \"ABCDEF\")\n",
      "        self.assertEqual(hao.long_name, \"A test header array object.\")\n",
      "        self.assertTrue(np.allclose(hao.array, np.array([[1.0, 2.0], [3.0, 4.0]])))\n",
      "test_attributes_style(self=<harpy.tests.test_header_array.TestHeaderArray testMethod=test_attributes_style>, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fd016330be0>, self._subtest=None, self._testMethodDoc='Note that the values set by the setter methods are NOT guaranteed to be consistent or legitimate. Implementation of further checks with the attribute-style referencing may cause this test to fail.', self._testMethodName='test_attributes_style', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_attributes_style=<bound method TestHeaderArray.test_attributes_style of <harpy.tests.test_header_array.TestHeaderArray testMethod=test_attributes_style>>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "hao = HeaderArrayObj()\n",
      "State:\n",
      "{_coeff_name='', _array=None, _sets=None, _long_name=''}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _makeNPIndex(indexList):\n",
      "        newinds = []\n",
      "        for i, item in enumerate(indexList):\n",
      "            if isinstance(item, list):\n",
      "                newinds.append(item)\n",
      "            elif isinstance(item,int):\n",
      "                newinds.append([item])\n",
      "        numpyInd = list(np.ix_(*newinds))\n",
      "        newinds=[]\n",
      "        for item in indexList:\n",
      "            if not item is None:\n",
      "                newinds.append(numpyInd.pop(0))\n",
      "            else:\n",
      "                newinds.append(None)\n",
      "        return tuple(newinds)\n",
      "_makeNPIndex(indexList=[[0, 1], [0, 1]])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "newinds = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _newname(self):\n",
      "        self._genSetID+=1\n",
      "        return \"S@\"+str(self._genSetID)\n",
      "_newname(self=<harpy._header_sets._HeaderSet object at 0x7fd016315dc0>, self.dim_desc=['A', 'B'], self.dim_size=2, self.elemPosDict={'a': 0, 'b': 1}, self.name='SimpleSet', self.status='k')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self._genSetID+=1\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_header_array_from_data(self):\n",
      "        array_1d = np.array([1.0, 2.0, 3.0])\n",
      "        array_2d = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
      "        array_7d = np.array(list(range(2**7)))\n",
      "        array_7d.reshape((2, 2, 2, 2, 2, 2, 2))\n",
      "        hao = HeaderArrayObj.HeaderArrayFromData(array=array_1d)\n",
      "        self.assertTrue(hao.is_valid())\n",
      "        hao = HeaderArrayObj.HeaderArrayFromData(array=array_2d)\n",
      "        self.assertTrue(hao.is_valid())\n",
      "        hao = HeaderArrayObj.HeaderArrayFromData(array=array_7d)\n",
      "        self.assertTrue(hao.is_valid())\n",
      "test_header_array_from_data(self=<harpy.tests.test_header_array.TestHeaderArray testMethod=test_header_array_from_data>, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fd016330be0>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_header_array_from_data', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_header_array_from_data=<bound method TestHeaderArray.test_header_array_from_data of <harpy.tests.test_header_array.TestHeaderArray testMethod=test_header_array_from_data>>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "array_1d = np.array([1.0, 2.0, 3.0])\n",
      "State:\n",
      "array([1., 2., 3.])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_header_array_from_data(self):\n",
      "        array_1d = np.array([1.0, 2.0, 3.0])\n",
      "        array_2d = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
      "        array_7d = np.array(list(range(2**7)))\n",
      "        array_7d.reshape((2, 2, 2, 2, 2, 2, 2))\n",
      "        hao = HeaderArrayObj.HeaderArrayFromData(array=array_1d)\n",
      "        self.assertTrue(hao.is_valid())\n",
      "        hao = HeaderArrayObj.HeaderArrayFromData(array=array_2d)\n",
      "        self.assertTrue(hao.is_valid())\n",
      "        hao = HeaderArrayObj.HeaderArrayFromData(array=array_7d)\n",
      "        self.assertTrue(hao.is_valid())\n",
      "test_header_array_from_data(self=<harpy.tests.test_header_array.TestHeaderArray testMethod=test_header_array_from_data>, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fd016330be0>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_header_array_from_data', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_header_array_from_data=<bound method TestHeaderArray.test_header_array_from_data of <harpy.tests.test_header_array.TestHeaderArray testMethod=test_header_array_from_data>>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "array_2d = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
      "State:\n",
      "array([[1., 2.],       [3., 4.]])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_header_array_from_data(self):\n",
      "        array_1d = np.array([1.0, 2.0, 3.0])\n",
      "        array_2d = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
      "        array_7d = np.array(list(range(2**7)))\n",
      "        array_7d.reshape((2, 2, 2, 2, 2, 2, 2))\n",
      "        hao = HeaderArrayObj.HeaderArrayFromData(array=array_1d)\n",
      "        self.assertTrue(hao.is_valid())\n",
      "        hao = HeaderArrayObj.HeaderArrayFromData(array=array_2d)\n",
      "        self.assertTrue(hao.is_valid())\n",
      "        hao = HeaderArrayObj.HeaderArrayFromData(array=array_7d)\n",
      "        self.assertTrue(hao.is_valid())\n",
      "test_header_array_from_data(self=<harpy.tests.test_header_array.TestHeaderArray testMethod=test_header_array_from_data>, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fd016330be0>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_header_array_from_data', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_header_array_from_data=<bound method TestHeaderArray.test_header_array_from_data of <harpy.tests.test_header_array.TestHeaderArray testMethod=test_header_array_from_data>>)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "array_7d = np.array(list(range(2**7)))\n",
      "State:\n",
      "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def fromSetShape(sets, setElDict, shape):\n",
      "        setObjList=[]\n",
      "        lowerDict=dict(zip([key.strip().lower() for key in setElDict.keys()], setElDict.keys() ))\n",
      "        for idim, setName in enumerate(sets):\n",
      "            lowSet=setName.strip().lower()\n",
      "            if lowSet in lowerDict:\n",
      "                setObjList.append(_HeaderSet(setName,'k',setElDict[lowerDict[lowSet]],shape[idim]))\n",
      "            else:\n",
      "                setObjList.append(_HeaderSet(setName, 'u', None, shape[idim]))\n",
      "        return _HeaderDims(setObjList)\n",
      "fromSetShape(sets=['sect'], setElDict={'sect': ['s1          ', 's2          ']}, shape=(2,))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "setObjList=[]\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def fromSetShape(sets, setElDict, shape):\n",
      "        setObjList=[]\n",
      "        lowerDict=dict(zip([key.strip().lower() for key in setElDict.keys()], setElDict.keys() ))\n",
      "        for idim, setName in enumerate(sets):\n",
      "            lowSet=setName.strip().lower()\n",
      "            if lowSet in lowerDict:\n",
      "                setObjList.append(_HeaderSet(setName,'k',setElDict[lowerDict[lowSet]],shape[idim]))\n",
      "            else:\n",
      "                setObjList.append(_HeaderSet(setName, 'u', None, shape[idim]))\n",
      "        return _HeaderDims(setObjList)\n",
      "fromSetShape(sets=['sect'], setElDict={'sect': ['s1          ', 's2          ']}, shape=(2,))\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "lowSet=setName.strip().lower()\n",
      "State:\n",
      "'sect'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def generateSetDictEntry(self, i, resultsSet, setNames, setPos, varDims, varSetDict, varSetPtr):\n",
      "        ndim = varDims.array[i, 0]\n",
      "        if ndim > 0:\n",
      "            varSetDict[self._variables[i].strip().lower()] = setNames.array[[j - 1 for j in varSetPtr.array[setPos:setPos + ndim, 0]]].tolist()\n",
      "            varSetDict[self._variables[i].strip().lower()] = [name.strip() for name in varSetDict[self._variables[i].strip().lower()]]\n",
      "            setPos += ndim\n",
      "        else:\n",
      "            varSetDict[self._variables[i].strip().lower()] = []\n",
      "        if len(resultsSet) > 1: varSetDict[self._variables[i].strip().lower()].append(\"\n",
      "        return setPos\n",
      "generateSetDictEntry(self=<harpy.sl4.SL4 object at 0x7fd016330670>, i=0, resultsSet=['Cumulative', 'Effect of la', 'Effect of ca'], setNames={_coeff_name='            ', _array=array(['SECT        ', 'FAC         ', 'NUM_SECT    '], dtype='<U12'), _sets=<harpy._header_sets._HeaderDims object at 0x7fd016391b50>, _long_name='STNAM(NUMST) - names of the sets                                      '}, setPos=0, varDims={_coeff_name='            ', _array=array([[0],       [1],       [1],       [1],       [1],       [1],       [2],       [2],       [2],       [2],       [1]], dtype=int32), _sets=<harpy._header_sets._HeaderDims object at 0x7fd016307d60>, _long_name='VCNIND - number of arguments for variables (condensed+backsolved)     '}, varSetDict={}, varSetPtr={_coeff_name='            ', _array=array([[1],       [2],       [1],       [2],       [1],       [1],       [1],       [2],       [1],       [1],       [1],       [2],       [1],       [1]], dtype=int32), _sets=<harpy._header_sets._HeaderDims object at 0x7fd0163025b0>, _long_name='VCSTN - arguments for variables (condensed+backsolved)                '}, self._sets=['sect', 'fac', 'num_sect', '\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "ndim = varDims.array[i, 0]\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def reshapeAndAdd(self, i, outDataList, varLabel, varSetDict):\n",
      "        flatData = np.concatenate(outDataList)\n",
      "        varSets = [thisSet.strip() for thisSet in varSetDict[self._variables[i].strip().lower()]]\n",
      "        simSizes = tuple(\n",
      "            [self.getSet(thisSet).array.shape[0] for thisSet in varSetDict[self._variables[i].strip().lower()]])\n",
      "        setElDict = {}\n",
      "        for myset in varSets:\n",
      "            setElDict[myset.strip().lower()] = self.getSet(myset).array.tolist()\n",
      "        finalData = flatData.reshape(simSizes,order=\"F\").astype(np.float32)\n",
      "        self.variableDict[self._variables[i].strip().lower()] = \\\n",
      "            HeaderArrayObj.HeaderArrayFromData(finalData,self._variables[i].strip()[0:12],varLabel.array[i][0:70], varSets, setElDict)\n",
      "reshapeAndAdd(self=<harpy.sl4.SL4 object at 0x7fd016330670>, i=0, outDataList=[array([13.895816], dtype=float32), array([6.1105247], dtype=float32), array([7.7852945], dtype=float32)], varLabel={_coeff_name='            ', _array=array([' Total nominal household expenditure                                            ',       ' Price of commodity i                                                           ',       ' Price of factor f                                                              ',       ' Total demand for (or supply of) commodity i                                    ',       ' Total demand for (or supply of) factor f                                       ',       ' Household demand for commodity i                                               ',       ' Intermediate inputs of commodity i to industry j                               ',       ' Factor inputs to industry j                                                    ',       ' Dollar value of inputs of commodity i to industry j                            ',       ' Dollar value of factor f used in industry j                                    ',       ' Dollar value of household use of commodity i                                   '],      dtype='<U80'), _sets=<harpy._header_sets._HeaderDims object at 0x7fd016302a00>, _long_name='VCLB - labelling information for variables (condensed+backsolved)     '}, varSetDict={'p_y': ['\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "flatData = np.concatenate(outDataList)\n",
      "State:\n",
      "array([13.895816 ,  6.1105247,  7.7852945], dtype=float32)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def reshapeAndAdd(self, i, outDataList, varLabel, varSetDict):\n",
      "        flatData = np.concatenate(outDataList)\n",
      "        varSets = [thisSet.strip() for thisSet in varSetDict[self._variables[i].strip().lower()]]\n",
      "        simSizes = tuple(\n",
      "            [self.getSet(thisSet).array.shape[0] for thisSet in varSetDict[self._variables[i].strip().lower()]])\n",
      "        setElDict = {}\n",
      "        for myset in varSets:\n",
      "            setElDict[myset.strip().lower()] = self.getSet(myset).array.tolist()\n",
      "        finalData = flatData.reshape(simSizes,order=\"F\").astype(np.float32)\n",
      "        self.variableDict[self._variables[i].strip().lower()] = \\\n",
      "            HeaderArrayObj.HeaderArrayFromData(finalData,self._variables[i].strip()[0:12],varLabel.array[i][0:70], varSets, setElDict)\n",
      "reshapeAndAdd(self=<harpy.sl4.SL4 object at 0x7fd016330670>, i=0, outDataList=[array([13.895816], dtype=float32), array([6.1105247], dtype=float32), array([7.7852945], dtype=float32)], varLabel={_coeff_name='            ', _array=array([' Total nominal household expenditure                                            ',       ' Price of commodity i                                                           ',       ' Price of factor f                                                              ',       ' Total demand for (or supply of) commodity i                                    ',       ' Total demand for (or supply of) factor f                                       ',       ' Household demand for commodity i                                               ',       ' Intermediate inputs of commodity i to industry j                               ',       ' Factor inputs to industry j                                                    ',       ' Dollar value of inputs of commodity i to industry j                            ',       ' Dollar value of factor f used in industry j                                    ',       ' Dollar value of household use of commodity i                                   '],      dtype='<U80'), _sets=<harpy._header_sets._HeaderDims object at 0x7fd016302a00>, _long_name='VCLB - labelling information for variables (condensed+backsolved)     '}, varSetDict={'p_y': ['\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "simSizes = tuple(\n",
      "State:\n",
      "(3,)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def reshapeAndAdd(self, i, outDataList, varLabel, varSetDict):\n",
      "        flatData = np.concatenate(outDataList)\n",
      "        varSets = [thisSet.strip() for thisSet in varSetDict[self._variables[i].strip().lower()]]\n",
      "        simSizes = tuple(\n",
      "            [self.getSet(thisSet).array.shape[0] for thisSet in varSetDict[self._variables[i].strip().lower()]])\n",
      "        setElDict = {}\n",
      "        for myset in varSets:\n",
      "            setElDict[myset.strip().lower()] = self.getSet(myset).array.tolist()\n",
      "        finalData = flatData.reshape(simSizes,order=\"F\").astype(np.float32)\n",
      "        self.variableDict[self._variables[i].strip().lower()] = \\\n",
      "            HeaderArrayObj.HeaderArrayFromData(finalData,self._variables[i].strip()[0:12],varLabel.array[i][0:70], varSets, setElDict)\n",
      "reshapeAndAdd(self=<harpy.sl4.SL4 object at 0x7fd016330670>, i=0, outDataList=[array([13.895816], dtype=float32), array([6.1105247], dtype=float32), array([7.7852945], dtype=float32)], varLabel={_coeff_name='            ', _array=array([' Total nominal household expenditure                                            ',       ' Price of commodity i                                                           ',       ' Price of factor f                                                              ',       ' Total demand for (or supply of) commodity i                                    ',       ' Total demand for (or supply of) factor f                                       ',       ' Household demand for commodity i                                               ',       ' Intermediate inputs of commodity i to industry j                               ',       ' Factor inputs to industry j                                                    ',       ' Dollar value of inputs of commodity i to industry j                            ',       ' Dollar value of factor f used in industry j                                    ',       ' Dollar value of household use of commodity i                                   '],      dtype='<U80'), _sets=<harpy._header_sets._HeaderDims object at 0x7fd016302a00>, _long_name='VCLB - labelling information for variables (condensed+backsolved)     '}, varSetDict={'p_y': ['\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "setElDict = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def insertShocks(flatData, i, nshk, nexo, shockList, shockPtr, shockVal):\n",
      "        if nshk > 0:\n",
      "            start = shockPtr.array[i, 0] - 1\n",
      "            for j in range(0, nshk):\n",
      "                shkInd = start + j\n",
      "                if nshk == nexo:\n",
      "                    varInd=j\n",
      "                else:\n",
      "                    varInd = shockList.array[shkInd, 0] - 1\n",
      "                flatData[varInd] = shockVal.array[shkInd, 0]\n",
      "insertShocks(flatData=array([0., 0.]), i=4, nshk=2, nexo=2, shockList={_coeff_name='            ', _array=array([], shape=(0, 1), dtype=int32), _sets=<harpy._header_sets._HeaderDims object at 0x7fd016378130>, _long_name='LIST for Shocked components (condensed+backsolved)                    '}, shockPtr={_coeff_name='            ', _array=array([[0],       [0],       [0],       [0],       [1],       [0],       [0],       [0],       [0],       [0],       [0]], dtype=int32), _sets=<harpy._header_sets._HeaderDims object at 0x7fd016302f70>, _long_name='Start pos of shocks for vars [header VARS] in shocks [header SHOC]    '}, shockVal={_coeff_name='            ', _array=array([[10.],       [20.]], dtype=float32), _sets=<harpy._header_sets._HeaderDims object at 0x7fd016302850>, _long_name='Shock values (nonzero ones only)                                      '})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "start = shockPtr.array[i, 0] - 1\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add(cls, root: BitParserState, v: Union[int, str], bits: str) -> None:\n",
      "        p: BitParserState = root\n",
      "        b = None\n",
      "        for i in range(len(bits)):\n",
      "            if 0 < i:\n",
      "                assert b is not None\n",
      "                if p[b] is None:\n",
      "                    p[b] = [None, None]\n",
      "                p = p[b]\n",
      "            if bits[i] == \"1\":\n",
      "                b = 1\n",
      "            else:\n",
      "                b = 0\n",
      "        assert b is not None\n",
      "        p[b] = v\n",
      "add(cls=<class 'pdfminer.ccitt.BitParser'>, root=[None, None], v=0, bits='1')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "p: BitParserState = root\n",
      "State:\n",
      "[None, None]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add(cls, root: BitParserState, v: Union[int, str], bits: str) -> None:\n",
      "        p: BitParserState = root\n",
      "        b = None\n",
      "        for i in range(len(bits)):\n",
      "            if 0 < i:\n",
      "                assert b is not None\n",
      "                if p[b] is None:\n",
      "                    p[b] = [None, None]\n",
      "                p = p[b]\n",
      "            if bits[i] == \"1\":\n",
      "                b = 1\n",
      "            else:\n",
      "                b = 0\n",
      "        assert b is not None\n",
      "        p[b] = v\n",
      "add(cls=<class 'pdfminer.ccitt.BitParser'>, root=[None, None], v=0, bits='1')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "b = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add(cls, root: BitParserState, v: Union[int, str], bits: str) -> None:\n",
      "        p: BitParserState = root\n",
      "        b = None\n",
      "        for i in range(len(bits)):\n",
      "            if 0 < i:\n",
      "                assert b is not None\n",
      "                if p[b] is None:\n",
      "                    p[b] = [None, None]\n",
      "                p = p[b]\n",
      "            if bits[i] == \"1\":\n",
      "                b = 1\n",
      "            else:\n",
      "                b = 0\n",
      "        assert b is not None\n",
      "        p[b] = v\n",
      "add(cls=<class 'pdfminer.ccitt.BitParser'>, root=[None, None], v=0, bits='1')\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "b = 1\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_ctm(self, ctm: Matrix) -> None:\n",
      "        self.ctm = ctm\n",
      "set_ctm(self=<PDFDevice>, ctm=[1, 0, 0, 1, 0, 0], self._stack=[], self.ctm=None, self.laparams=None, self.pageno=1, self.rsrcmgr=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.ctm = ctm\n",
      "State:\n",
      "[1, 0, 0, 1, 0, 0]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_bbox(self, bbox: Rect) -> None:\n",
      "        (x0, y0, x1, y1) = bbox\n",
      "        self.x0 = x0\n",
      "        self.y0 = y0\n",
      "        self.x1 = x1\n",
      "        self.y1 = y1\n",
      "        self.width = x1 - x0\n",
      "        self.height = y1 - y0\n",
      "        self.bbox = bbox\n",
      "set_bbox(self=REPR FAILED, bbox=[0, 100, 0, 100])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "(x0, y0, x1, y1) = bbox\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_bbox(self, bbox: Rect) -> None:\n",
      "        (x0, y0, x1, y1) = bbox\n",
      "        self.x0 = x0\n",
      "        self.y0 = y0\n",
      "        self.x1 = x1\n",
      "        self.y1 = y1\n",
      "        self.width = x1 - x0\n",
      "        self.height = y1 - y0\n",
      "        self.bbox = bbox\n",
      "set_bbox(self=REPR FAILED, bbox=[0, 100, 0, 100])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.x0 = x0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_bbox(self, bbox: Rect) -> None:\n",
      "        (x0, y0, x1, y1) = bbox\n",
      "        self.x0 = x0\n",
      "        self.y0 = y0\n",
      "        self.x1 = x1\n",
      "        self.y1 = y1\n",
      "        self.width = x1 - x0\n",
      "        self.height = y1 - y0\n",
      "        self.bbox = bbox\n",
      "set_bbox(self=REPR FAILED, bbox=[0, 100, 0, 100])\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.y0 = y0\n",
      "State:\n",
      "100\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_bbox(self, bbox: Rect) -> None:\n",
      "        (x0, y0, x1, y1) = bbox\n",
      "        self.x0 = x0\n",
      "        self.y0 = y0\n",
      "        self.x1 = x1\n",
      "        self.y1 = y1\n",
      "        self.width = x1 - x0\n",
      "        self.height = y1 - y0\n",
      "        self.bbox = bbox\n",
      "set_bbox(self=REPR FAILED, bbox=[0, 100, 0, 100])\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "self.x1 = x1\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_bbox(self, bbox: Rect) -> None:\n",
      "        (x0, y0, x1, y1) = bbox\n",
      "        self.x0 = x0\n",
      "        self.y0 = y0\n",
      "        self.x1 = x1\n",
      "        self.y1 = y1\n",
      "        self.width = x1 - x0\n",
      "        self.height = y1 - y0\n",
      "        self.bbox = bbox\n",
      "set_bbox(self=REPR FAILED, bbox=[0, 100, 0, 100])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "self.y1 = y1\n",
      "State:\n",
      "100\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_bound(pts: Iterable[Point]) -> Rect:\n",
      "    limit: Rect = (INF, INF, -INF, -INF)\n",
      "    (x0, y0, x1, y1) = limit\n",
      "    for (x, y) in pts:\n",
      "        x0 = min(x0, x)\n",
      "        y0 = min(y0, y)\n",
      "        x1 = max(x1, x)\n",
      "        y1 = max(y1, y)\n",
      "    return x0, y0, x1, y1\n",
      "get_bound(pts=[(6, 7), (7, 7)])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "limit: Rect = (INF, INF, -INF, -INF)\n",
      "State:\n",
      "(2147483647, 2147483647, -2147483647, -2147483647)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _add_token(self, obj: PSBaseParserToken) -> None:\n",
      "        self._tokens.append((self._curtokenpos, obj))\n",
      "        return\n",
      "_add_token(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=208444>, obj=/b'xref', self._curtoken=b'xref', self._curtokenpos=208444, self._parse1=<bound method PSBaseParser._parse_keyword of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=208444>>, self._tokens=[], self.buf=b'xref\\n0 83 \\n0000000000 65535 f \\n0000000015 00000 n \\n0000000102 00000 n \\n0000015379 00000 n \\n0000025830 00000 n \\n0000039162 00000 n \\n0000039211 00000 n \\n0000040383 00000 n \\n0000040521 00000 n \\n0000040701 00000 n \\n0000040736 00000 n \\n0000040768 00000 n \\n0000085321 00000 n \\n0000085400 00000 n \\n0000085446 00000 n \\n0000085504 00000 n \\n0000085671 00000 n \\n0000085837 00000 n \\n0000085939 00000 n \\n0000086006 00000 n \\n0000086203 00000 n \\n0000086235 00000 n \\n0000087055 00000 n \\n0000087090 00000 n \\n0000087140 00000 n \\n0000087209 00000 n \\n0000087234 00000 n \\n0000087441 00000 n \\n0000087634 00000 n \\n0000087827 00000 n \\n0000087936 00000 n \\n0000088132 00000 n \\n0000088295 00000 n \\n0000089023 00000 n \\n0000102886 00000 n \\n0000102943 00000 n \\n0000113208 00000 n \\n0000114380 00000 n \\n0000114694 00000 n \\n0000114857 00000 n \\n0000115097 00000 n \\n0000115151 00000 n \\n0000115213 00000 n \\n0000115994 00000 n \\n0000116196 00000 n \\n0000116355 00000 n \\n0000116398 00000 n \\n0000116462 00000 n \\n0000116515 00000 n \\n0000116546 00000 n \\n0000116721 00000 n \\n0000116887 00000 n \\n0000160698 00000 n \\n0000161042 00000 n \\n0000161209 00000 n \\n0000161392 00000 n \\n0000161424 00000 n \\n0000162597 00000 n \\n0000162764 00000 n \\n0000162969 00000 n \\n0000163132 00000 n \\n0000164306 00000 n \\n0000181236 00000 n \\n0000186572 00000 n \\n0000186632 00000 n \\n0000186673 00000 n \\n0000187843 00000 n \\n0000201641 00000 n \\n0000201807 00000 n \\n0000202576 00000 n \\n0000202632 00000 n \\n0000203809 00000 n \\n0000204551 00000 n \\n0000204706 00000 n \\n0000205428 00000 n \\n0000206202 00000 n \\n0000206368 00000 n \\n0000206494 00000 n \\n0000206611 00000 n \\n0000206643 00000 n \\n0000207560 00000 n \\n0000208280 00000 n \\n0000208327 00000 n \\ntrailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=208444, self.charpos=1, self.context=[], self.curstack=[], self.curtype=None, self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.results=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self._tokens.append((self._curtokenpos, obj))\n",
      "State:\n",
      "[(208444, /b'xref')]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def nextline(self) -> Tuple[int, bytes]:\n",
      "        linebuf = b\"\"\n",
      "        linepos = self.bufpos + self.charpos\n",
      "        eol = False\n",
      "        while 1:\n",
      "            self.fillbuf()\n",
      "            if eol:\n",
      "                c = self.buf[self.charpos : self.charpos + 1]\n",
      "                if c == b\"\\n\":\n",
      "                    linebuf += c\n",
      "                    self.charpos += 1\n",
      "                break\n",
      "            m = EOL.search(self.buf, self.charpos)\n",
      "            if m:\n",
      "                linebuf += self.buf[self.charpos : m.end(0)]\n",
      "                self.charpos = m.end(0)\n",
      "                if linebuf[-1:] == b\"\\r\":\n",
      "                    eol = True\n",
      "                else:\n",
      "                    break\n",
      "            else:\n",
      "                linebuf += self.buf[self.charpos :]\n",
      "                self.charpos = len(self.buf)\n",
      "        log.debug(\"nextline: %r, %r\", linepos, linebuf)\n",
      "        return (linepos, linebuf)\n",
      "nextline(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=208444>, self._curtoken=b'xref', self._curtokenpos=208444, self._parse1=<bound method PSBaseParser._parse_main of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=208444>>, self._tokens=[], self.buf=b'xref\\n0 83 \\n0000000000 65535 f \\n0000000015 00000 n \\n0000000102 00000 n \\n0000015379 00000 n \\n0000025830 00000 n \\n0000039162 00000 n \\n0000039211 00000 n \\n0000040383 00000 n \\n0000040521 00000 n \\n0000040701 00000 n \\n0000040736 00000 n \\n0000040768 00000 n \\n0000085321 00000 n \\n0000085400 00000 n \\n0000085446 00000 n \\n0000085504 00000 n \\n0000085671 00000 n \\n0000085837 00000 n \\n0000085939 00000 n \\n0000086006 00000 n \\n0000086203 00000 n \\n0000086235 00000 n \\n0000087055 00000 n \\n0000087090 00000 n \\n0000087140 00000 n \\n0000087209 00000 n \\n0000087234 00000 n \\n0000087441 00000 n \\n0000087634 00000 n \\n0000087827 00000 n \\n0000087936 00000 n \\n0000088132 00000 n \\n0000088295 00000 n \\n0000089023 00000 n \\n0000102886 00000 n \\n0000102943 00000 n \\n0000113208 00000 n \\n0000114380 00000 n \\n0000114694 00000 n \\n0000114857 00000 n \\n0000115097 00000 n \\n0000115151 00000 n \\n0000115213 00000 n \\n0000115994 00000 n \\n0000116196 00000 n \\n0000116355 00000 n \\n0000116398 00000 n \\n0000116462 00000 n \\n0000116515 00000 n \\n0000116546 00000 n \\n0000116721 00000 n \\n0000116887 00000 n \\n0000160698 00000 n \\n0000161042 00000 n \\n0000161209 00000 n \\n0000161392 00000 n \\n0000161424 00000 n \\n0000162597 00000 n \\n0000162764 00000 n \\n0000162969 00000 n \\n0000163132 00000 n \\n0000164306 00000 n \\n0000181236 00000 n \\n0000186572 00000 n \\n0000186632 00000 n \\n0000186673 00000 n \\n0000187843 00000 n \\n0000201641 00000 n \\n0000201807 00000 n \\n0000202576 00000 n \\n0000202632 00000 n \\n0000203809 00000 n \\n0000204551 00000 n \\n0000204706 00000 n \\n0000205428 00000 n \\n0000206202 00000 n \\n0000206368 00000 n \\n0000206494 00000 n \\n0000206611 00000 n \\n0000206643 00000 n \\n0000207560 00000 n \\n0000208280 00000 n \\n0000208327 00000 n \\ntrailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=208444, self.charpos=4, self.context=[], self.curstack=[], self.curtype=None, self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.results=[])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "linebuf = b\"\"\n",
      "State:\n",
      "b''\n",
      "==================================================\n",
      "Clean Code:\n",
      "def nextline(self) -> Tuple[int, bytes]:\n",
      "        linebuf = b\"\"\n",
      "        linepos = self.bufpos + self.charpos\n",
      "        eol = False\n",
      "        while 1:\n",
      "            self.fillbuf()\n",
      "            if eol:\n",
      "                c = self.buf[self.charpos : self.charpos + 1]\n",
      "                if c == b\"\\n\":\n",
      "                    linebuf += c\n",
      "                    self.charpos += 1\n",
      "                break\n",
      "            m = EOL.search(self.buf, self.charpos)\n",
      "            if m:\n",
      "                linebuf += self.buf[self.charpos : m.end(0)]\n",
      "                self.charpos = m.end(0)\n",
      "                if linebuf[-1:] == b\"\\r\":\n",
      "                    eol = True\n",
      "                else:\n",
      "                    break\n",
      "            else:\n",
      "                linebuf += self.buf[self.charpos :]\n",
      "                self.charpos = len(self.buf)\n",
      "        log.debug(\"nextline: %r, %r\", linepos, linebuf)\n",
      "        return (linepos, linebuf)\n",
      "nextline(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=208444>, self._curtoken=b'xref', self._curtokenpos=208444, self._parse1=<bound method PSBaseParser._parse_main of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=208444>>, self._tokens=[], self.buf=b'xref\\n0 83 \\n0000000000 65535 f \\n0000000015 00000 n \\n0000000102 00000 n \\n0000015379 00000 n \\n0000025830 00000 n \\n0000039162 00000 n \\n0000039211 00000 n \\n0000040383 00000 n \\n0000040521 00000 n \\n0000040701 00000 n \\n0000040736 00000 n \\n0000040768 00000 n \\n0000085321 00000 n \\n0000085400 00000 n \\n0000085446 00000 n \\n0000085504 00000 n \\n0000085671 00000 n \\n0000085837 00000 n \\n0000085939 00000 n \\n0000086006 00000 n \\n0000086203 00000 n \\n0000086235 00000 n \\n0000087055 00000 n \\n0000087090 00000 n \\n0000087140 00000 n \\n0000087209 00000 n \\n0000087234 00000 n \\n0000087441 00000 n \\n0000087634 00000 n \\n0000087827 00000 n \\n0000087936 00000 n \\n0000088132 00000 n \\n0000088295 00000 n \\n0000089023 00000 n \\n0000102886 00000 n \\n0000102943 00000 n \\n0000113208 00000 n \\n0000114380 00000 n \\n0000114694 00000 n \\n0000114857 00000 n \\n0000115097 00000 n \\n0000115151 00000 n \\n0000115213 00000 n \\n0000115994 00000 n \\n0000116196 00000 n \\n0000116355 00000 n \\n0000116398 00000 n \\n0000116462 00000 n \\n0000116515 00000 n \\n0000116546 00000 n \\n0000116721 00000 n \\n0000116887 00000 n \\n0000160698 00000 n \\n0000161042 00000 n \\n0000161209 00000 n \\n0000161392 00000 n \\n0000161424 00000 n \\n0000162597 00000 n \\n0000162764 00000 n \\n0000162969 00000 n \\n0000163132 00000 n \\n0000164306 00000 n \\n0000181236 00000 n \\n0000186572 00000 n \\n0000186632 00000 n \\n0000186673 00000 n \\n0000187843 00000 n \\n0000201641 00000 n \\n0000201807 00000 n \\n0000202576 00000 n \\n0000202632 00000 n \\n0000203809 00000 n \\n0000204551 00000 n \\n0000204706 00000 n \\n0000205428 00000 n \\n0000206202 00000 n \\n0000206368 00000 n \\n0000206494 00000 n \\n0000206611 00000 n \\n0000206643 00000 n \\n0000207560 00000 n \\n0000208280 00000 n \\n0000208327 00000 n \\ntrailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=208444, self.charpos=4, self.context=[], self.curstack=[], self.curtype=None, self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.results=[])\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "linepos = self.bufpos + self.charpos\n",
      "State:\n",
      "208448\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_wopen(self, s: bytes, i: int) -> int:\n",
      "        c = s[i : i + 1]\n",
      "        if c == b\"<\":\n",
      "            self._add_token(KEYWORD_DICT_BEGIN)\n",
      "            self._parse1 = self._parse_main\n",
      "            i += 1\n",
      "        else:\n",
      "            self._parse1 = self._parse_hexstring\n",
      "        return i\n",
      "_parse_wopen(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, s=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', i=9, self._curtoken=b'', self._curtokenpos=210123, self._parse1=<bound method PSBaseParser._parse_wopen of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>, self._tokens=[], self.buf=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=210115, self.charpos=9, self.context=[], self.curstack=[], self.curtype=None, self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.results=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "c = s[i : i + 1]\n",
      "State:\n",
      "b'<'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_wopen(self, s: bytes, i: int) -> int:\n",
      "        c = s[i : i + 1]\n",
      "        if c == b\"<\":\n",
      "            self._add_token(KEYWORD_DICT_BEGIN)\n",
      "            self._parse1 = self._parse_main\n",
      "            i += 1\n",
      "        else:\n",
      "            self._parse1 = self._parse_hexstring\n",
      "        return i\n",
      "_parse_wopen(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, s=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', i=9, self._curtoken=b'', self._curtokenpos=210123, self._parse1=<bound method PSBaseParser._parse_wopen of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>, self._tokens=[], self.buf=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=210115, self.charpos=9, self.context=[], self.curstack=[], self.curtype=None, self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.results=[])\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self._add_token(KEYWORD_DICT_BEGIN)\n",
      "State:\n",
      "[(210123, /b'<<')]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_wopen(self, s: bytes, i: int) -> int:\n",
      "        c = s[i : i + 1]\n",
      "        if c == b\"<\":\n",
      "            self._add_token(KEYWORD_DICT_BEGIN)\n",
      "            self._parse1 = self._parse_main\n",
      "            i += 1\n",
      "        else:\n",
      "            self._parse1 = self._parse_hexstring\n",
      "        return i\n",
      "_parse_wopen(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, s=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', i=9, self._curtoken=b'', self._curtokenpos=210123, self._parse1=<bound method PSBaseParser._parse_wopen of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>, self._tokens=[], self.buf=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=210115, self.charpos=9, self.context=[], self.curstack=[], self.curtype=None, self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.results=[])\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "self._parse1 = self._parse_main\n",
      "State:\n",
      "<bound method PSBaseParser._parse_main of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_wopen(self, s: bytes, i: int) -> int:\n",
      "        c = s[i : i + 1]\n",
      "        if c == b\"<\":\n",
      "            self._add_token(KEYWORD_DICT_BEGIN)\n",
      "            self._parse1 = self._parse_main\n",
      "            i += 1\n",
      "        else:\n",
      "            self._parse1 = self._parse_hexstring\n",
      "        return i\n",
      "_parse_wopen(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, s=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', i=9, self._curtoken=b'', self._curtokenpos=210123, self._parse1=<bound method PSBaseParser._parse_wopen of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>, self._tokens=[], self.buf=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=210115, self.charpos=9, self.context=[], self.curstack=[], self.curtype=None, self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.results=[])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "i += 1\n",
      "State:\n",
      "10\n",
      "==================================================\n",
      "Clean Code:\n",
      "def start_type(self, pos: int, type: str) -> None:\n",
      "        self.context.append((pos, self.curtype, self.curstack))\n",
      "        (self.curtype, self.curstack) = (type, [])\n",
      "        log.debug(\"start_type: pos=%r, type=%r\", pos, type)\n",
      "        return\n",
      "start_type(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, pos=210123, type='d', self._curtoken=b'', self._curtokenpos=210123, self._parse1=<bound method PSBaseParser._parse_main of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>, self._tokens=[], self.buf=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=210115, self.charpos=10, self.context=[], self.curstack=[], self.curtype=None, self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.results=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.context.append((pos, self.curtype, self.curstack))\n",
      "State:\n",
      "[(210123, None, [])]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def start_type(self, pos: int, type: str) -> None:\n",
      "        self.context.append((pos, self.curtype, self.curstack))\n",
      "        (self.curtype, self.curstack) = (type, [])\n",
      "        log.debug(\"start_type: pos=%r, type=%r\", pos, type)\n",
      "        return\n",
      "start_type(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, pos=210123, type='d', self._curtoken=b'', self._curtokenpos=210123, self._parse1=<bound method PSBaseParser._parse_main of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>, self._tokens=[], self.buf=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=210115, self.charpos=10, self.context=[], self.curstack=[], self.curtype=None, self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.results=[])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "(self.curtype, self.curstack) = (type, [])\n",
      "State:\n",
      "'d'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_literal(self, s: bytes, i: int) -> int:\n",
      "        m = END_LITERAL.search(s, i)\n",
      "        if not m:\n",
      "            self._curtoken += s[i:]\n",
      "            return len(s)\n",
      "        j = m.start(0)\n",
      "        self._curtoken += s[i:j]\n",
      "        c = s[j : j + 1]\n",
      "        if c == b\"\n",
      "            self.hex = b\"\"\n",
      "            self._parse1 = self._parse_literal_hex\n",
      "            return j + 1\n",
      "        try:\n",
      "            name: Union[str, bytes] = str(self._curtoken, \"utf-8\")\n",
      "        except Exception:\n",
      "            name = self._curtoken\n",
      "        self._add_token(LIT(name))\n",
      "        self._parse1 = self._parse_main\n",
      "        return j\n",
      "_parse_literal(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, s=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', i=11, self._curtoken=b'', self._curtokenpos=210125, self._parse1=<bound method PSBaseParser._parse_literal of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>, self._tokens=[], self.buf=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=210115, self.charpos=11, self.context=[(210123, None, [])], self.curstack=[], self.curtype='d', self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.results=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "m = END_LITERAL.search(s, i)\n",
      "State:\n",
      "<re.Match object; span=(15, 16), match=b' '>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_literal(self, s: bytes, i: int) -> int:\n",
      "        m = END_LITERAL.search(s, i)\n",
      "        if not m:\n",
      "            self._curtoken += s[i:]\n",
      "            return len(s)\n",
      "        j = m.start(0)\n",
      "        self._curtoken += s[i:j]\n",
      "        c = s[j : j + 1]\n",
      "        if c == b\"\n",
      "            self.hex = b\"\"\n",
      "            self._parse1 = self._parse_literal_hex\n",
      "            return j + 1\n",
      "        try:\n",
      "            name: Union[str, bytes] = str(self._curtoken, \"utf-8\")\n",
      "        except Exception:\n",
      "            name = self._curtoken\n",
      "        self._add_token(LIT(name))\n",
      "        self._parse1 = self._parse_main\n",
      "        return j\n",
      "_parse_literal(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, s=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', i=11, self._curtoken=b'', self._curtokenpos=210125, self._parse1=<bound method PSBaseParser._parse_literal of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>, self._tokens=[], self.buf=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=210115, self.charpos=11, self.context=[(210123, None, [])], self.curstack=[], self.curtype='d', self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.results=[])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "j = m.start(0)\n",
      "State:\n",
      "15\n",
      "==================================================\n",
      "Clean Code:\n",
      "def push(self, *objs: PSStackEntry[ExtraT]) -> None:\n",
      "        self.curstack.extend(objs)\n",
      "        return\n",
      "push(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, objs=((210125, /'Root'),), self._curtoken=b'Root', self._curtokenpos=210125, self._parse1=<bound method PSBaseParser._parse_main of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>, self._tokens=[], self.buf=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=210115, self.charpos=15, self.context=[(210123, None, [])], self.curstack=[], self.curtype='d', self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.results=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.curstack.extend(objs)\n",
      "State:\n",
      "[(210125, /'Root')]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_number(self, s: bytes, i: int) -> int:\n",
      "        m = END_NUMBER.search(s, i)\n",
      "        if not m:\n",
      "            self._curtoken += s[i:]\n",
      "            return len(s)\n",
      "        j = m.start(0)\n",
      "        self._curtoken += s[i:j]\n",
      "        c = s[j : j + 1]\n",
      "        if c == b\".\":\n",
      "            self._curtoken += c\n",
      "            self._parse1 = self._parse_float\n",
      "            return j + 1\n",
      "        try:\n",
      "            self._add_token(int(self._curtoken))\n",
      "        except ValueError:\n",
      "            pass\n",
      "        self._parse1 = self._parse_main\n",
      "        return j\n",
      "_parse_number(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, s=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', i=17, self._curtoken=b'8', self._curtokenpos=210131, self._parse1=<bound method PSBaseParser._parse_number of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>, self._tokens=[], self.buf=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=210115, self.charpos=17, self.context=[(210123, None, [])], self.curstack=[(210125, /'Root')], self.curtype='d', self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.results=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "m = END_NUMBER.search(s, i)\n",
      "State:\n",
      "<re.Match object; span=(18, 19), match=b' '>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_number(self, s: bytes, i: int) -> int:\n",
      "        m = END_NUMBER.search(s, i)\n",
      "        if not m:\n",
      "            self._curtoken += s[i:]\n",
      "            return len(s)\n",
      "        j = m.start(0)\n",
      "        self._curtoken += s[i:j]\n",
      "        c = s[j : j + 1]\n",
      "        if c == b\".\":\n",
      "            self._curtoken += c\n",
      "            self._parse1 = self._parse_float\n",
      "            return j + 1\n",
      "        try:\n",
      "            self._add_token(int(self._curtoken))\n",
      "        except ValueError:\n",
      "            pass\n",
      "        self._parse1 = self._parse_main\n",
      "        return j\n",
      "_parse_number(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, s=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', i=17, self._curtoken=b'8', self._curtokenpos=210131, self._parse1=<bound method PSBaseParser._parse_number of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>, self._tokens=[], self.buf=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=210115, self.charpos=17, self.context=[(210123, None, [])], self.curstack=[(210125, /'Root')], self.curtype='d', self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.results=[])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "j = m.start(0)\n",
      "State:\n",
      "18\n",
      "==================================================\n",
      "Clean Code:\n",
      "def do_keyword(self, pos: int, token: PSKeyword) -> None:\n",
      "        if token in (self.KEYWORD_XREF, self.KEYWORD_STARTXREF):\n",
      "            self.add_results(*self.pop(1))\n",
      "        elif token is self.KEYWORD_ENDOBJ:\n",
      "            self.add_results(*self.pop(4))\n",
      "        elif token is self.KEYWORD_NULL:\n",
      "            self.push((pos, None))\n",
      "        elif token is self.KEYWORD_R:\n",
      "            if len(self.curstack) >= 2:\n",
      "                try:\n",
      "                    ((_, objid), (_, genno)) = self.pop(2)\n",
      "                    (objid, genno) = (int(objid), int(genno))\n",
      "                    assert self.doc is not None\n",
      "                    obj = PDFObjRef(self.doc, objid, genno)\n",
      "                    self.push((pos, obj))\n",
      "                except PSSyntaxError:\n",
      "                    pass\n",
      "        elif token is self.KEYWORD_STREAM:\n",
      "            ((_, dic),) = self.pop(1)\n",
      "            dic = dict_value(dic)\n",
      "            objlen = 0\n",
      "            if not self.fallback:\n",
      "                try:\n",
      "                    objlen = int_value(dic[\"Length\"])\n",
      "                except KeyError:\n",
      "                    if settings.STRICT:\n",
      "                        raise PDFSyntaxError(\"/Length is undefined: %r\" % dic)\n",
      "            self.seek(pos)\n",
      "            try:\n",
      "                (_, line) = self.nextline()\n",
      "            except PSEOF:\n",
      "                if settings.STRICT:\n",
      "                    raise PDFSyntaxError(\"Unexpected EOF\")\n",
      "                return\n",
      "            pos += len(line)\n",
      "            self.fp.seek(pos)\n",
      "            data = bytearray(self.fp.read(objlen))\n",
      "            self.seek(pos + objlen)\n",
      "            while 1:\n",
      "                try:\n",
      "                    (linepos, line) = self.nextline()\n",
      "                except PSEOF:\n",
      "                    if settings.STRICT:\n",
      "                        raise PDFSyntaxError(\"Unexpected EOF\")\n",
      "                    break\n",
      "                if b\"endstream\" in line:\n",
      "                    i = line.index(b\"endstream\")\n",
      "                    objlen += i\n",
      "                    if self.fallback:\n",
      "                        data += line[:i]\n",
      "                    break\n",
      "                objlen += len(line)\n",
      "                if self.fallback:\n",
      "                    data += line\n",
      "            self.seek(pos + objlen)\n",
      "            log.debug(\n",
      "                \"Stream: pos=%d, objlen=%d, dic=%r, data=%r...\",\n",
      "                pos,\n",
      "                objlen,\n",
      "                dic,\n",
      "                data[:10],\n",
      "            )\n",
      "            assert self.doc is not None\n",
      "            stream = PDFStream(dic, bytes(data), self.doc.decipher)\n",
      "            self.push((pos, stream))\n",
      "        else:\n",
      "            self.push((pos, token))\n",
      "do_keyword(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, pos=210136, token=/b'R', self._curtoken=b'R', self._curtokenpos=210136, self._parse1=<bound method PSBaseParser._parse_main of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>, self._tokens=[], self.buf=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=210115, self.charpos=22, self.context=[(210123, None, [])], self.curstack=[(210125, /'Root'), (210131, 82), (210134, 0)], self.curtype='d', self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.results=[])\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "obj = PDFObjRef(self.doc, objid, genno)\n",
      "State:\n",
      "<PDFObjRef:82>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def pop(self, n: int) -> List[PSStackEntry[ExtraT]]:\n",
      "        objs = self.curstack[-n:]\n",
      "        self.curstack[-n:] = []\n",
      "        return objs\n",
      "pop(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, n=2, self._curtoken=b'R', self._curtokenpos=210136, self._parse1=<bound method PSBaseParser._parse_main of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>, self._tokens=[], self.buf=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=210115, self.charpos=22, self.context=[(210123, None, [])], self.curstack=[(210125, /'Root'), (210131, 82), (210134, 0)], self.curtype='d', self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.results=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "objs = self.curstack[-n:]\n",
      "State:\n",
      "[(210131, 82), (210134, 0)]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def pop(self, n: int) -> List[PSStackEntry[ExtraT]]:\n",
      "        objs = self.curstack[-n:]\n",
      "        self.curstack[-n:] = []\n",
      "        return objs\n",
      "pop(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, n=2, self._curtoken=b'R', self._curtokenpos=210136, self._parse1=<bound method PSBaseParser._parse_main of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>, self._tokens=[], self.buf=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=210115, self.charpos=22, self.context=[(210123, None, [])], self.curstack=[(210125, /'Root'), (210131, 82), (210134, 0)], self.curtype='d', self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.results=[])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.curstack[-n:] = []\n",
      "State:\n",
      "[(210125, /'Root')]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_string(self, s: bytes, i: int) -> int:\n",
      "        m = END_STRING.search(s, i)\n",
      "        if not m:\n",
      "            self._curtoken += s[i:]\n",
      "            return len(s)\n",
      "        j = m.start(0)\n",
      "        self._curtoken += s[i:j]\n",
      "        c = s[j : j + 1]\n",
      "        if c == b\"\\\\\":\n",
      "            self.oct = b\"\"\n",
      "            self._parse1 = self._parse_string_1\n",
      "            return j + 1\n",
      "        if c == b\"(\":\n",
      "            self.paren += 1\n",
      "            self._curtoken += c\n",
      "            return j + 1\n",
      "        if c == b\")\":\n",
      "            self.paren -= 1\n",
      "            if self.paren:\n",
      "                self._curtoken += c\n",
      "                return j + 1\n",
      "        self._add_token(self._curtoken)\n",
      "        self._parse1 = self._parse_main\n",
      "        return j + 1\n",
      "_parse_string(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, s=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', i=27, self._curtoken=b'', self._curtokenpos=210141, self._parse1=<bound method PSBaseParser._parse_string of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>, self._tokens=[], self.buf=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=210115, self.charpos=27, self.context=[(210123, None, []), (210140, 'd', [(210125, /'Root'), (210136, <PDFObjRef:82>), (210137, /'ID')])], self.curstack=[], self.curtype='a', self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.paren=1, self.results=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "m = END_STRING.search(s, i)\n",
      "State:\n",
      "<re.Match object; span=(43, 44), match=b')'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_string(self, s: bytes, i: int) -> int:\n",
      "        m = END_STRING.search(s, i)\n",
      "        if not m:\n",
      "            self._curtoken += s[i:]\n",
      "            return len(s)\n",
      "        j = m.start(0)\n",
      "        self._curtoken += s[i:j]\n",
      "        c = s[j : j + 1]\n",
      "        if c == b\"\\\\\":\n",
      "            self.oct = b\"\"\n",
      "            self._parse1 = self._parse_string_1\n",
      "            return j + 1\n",
      "        if c == b\"(\":\n",
      "            self.paren += 1\n",
      "            self._curtoken += c\n",
      "            return j + 1\n",
      "        if c == b\")\":\n",
      "            self.paren -= 1\n",
      "            if self.paren:\n",
      "                self._curtoken += c\n",
      "                return j + 1\n",
      "        self._add_token(self._curtoken)\n",
      "        self._parse1 = self._parse_main\n",
      "        return j + 1\n",
      "_parse_string(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, s=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', i=27, self._curtoken=b'', self._curtokenpos=210141, self._parse1=<bound method PSBaseParser._parse_string of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>, self._tokens=[], self.buf=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=210115, self.charpos=27, self.context=[(210123, None, []), (210140, 'd', [(210125, /'Root'), (210136, <PDFObjRef:82>), (210137, /'ID')])], self.curstack=[], self.curtype='a', self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.paren=1, self.results=[])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "j = m.start(0)\n",
      "State:\n",
      "43\n",
      "==================================================\n",
      "Clean Code:\n",
      "def end_type(self, type: str) -> Tuple[int, List[PSStackType[ExtraT]]]:\n",
      "        if self.curtype != type:\n",
      "            raise PSTypeError(\"Type mismatch: {!r} != {!r}\".format(self.curtype, type))\n",
      "        objs = [obj for (_, obj) in self.curstack]\n",
      "        (pos, self.curtype, self.curstack) = self.context.pop()\n",
      "        log.debug(\"end_type: pos=%r, type=%r, objs=%r\", pos, type, objs)\n",
      "        return (pos, objs)\n",
      "end_type(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, type='a', self._curtoken=b'k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8', self._curtokenpos=210177, self._parse1=<bound method PSBaseParser._parse_main of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>, self._tokens=[], self.buf=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=210115, self.charpos=63, self.context=[(210123, None, []), (210140, 'd', [(210125, /'Root'), (210136, <PDFObjRef:82>), (210137, /'ID')])], self.curstack=[(210141, b'\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9'), (210159, b'k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8')], self.curtype='a', self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.paren=0, self.results=[])\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "(pos, self.curtype, self.curstack) = self.context.pop()\n",
      "State:\n",
      "210140\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_wclose(self, s: bytes, i: int) -> int:\n",
      "        c = s[i : i + 1]\n",
      "        if c == b\">\":\n",
      "            self._add_token(KEYWORD_DICT_END)\n",
      "            i += 1\n",
      "        self._parse1 = self._parse_main\n",
      "        return i\n",
      "_parse_wclose(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, s=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', i=84, self._curtoken=b'', self._curtokenpos=210198, self._parse1=<bound method PSBaseParser._parse_wclose of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>, self._tokens=[], self.buf=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=210115, self.charpos=84, self.context=[(210123, None, [])], self.curstack=[(210125, /'Root'), (210136, <PDFObjRef:82>), (210137, /'ID'), (210140, [b'\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9', b'k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8']), (210178, /'Info'), (210189, <PDFObjRef:39>), (210190, /'Size'), (210196, 83)], self.curtype='d', self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.paren=0, self.results=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "c = s[i : i + 1]\n",
      "State:\n",
      "b'>'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_wclose(self, s: bytes, i: int) -> int:\n",
      "        c = s[i : i + 1]\n",
      "        if c == b\">\":\n",
      "            self._add_token(KEYWORD_DICT_END)\n",
      "            i += 1\n",
      "        self._parse1 = self._parse_main\n",
      "        return i\n",
      "_parse_wclose(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, s=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', i=84, self._curtoken=b'', self._curtokenpos=210198, self._parse1=<bound method PSBaseParser._parse_wclose of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>, self._tokens=[], self.buf=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=210115, self.charpos=84, self.context=[(210123, None, [])], self.curstack=[(210125, /'Root'), (210136, <PDFObjRef:82>), (210137, /'ID'), (210140, [b'\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9', b'k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8']), (210178, /'Info'), (210189, <PDFObjRef:39>), (210190, /'Size'), (210196, 83)], self.curtype='d', self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.paren=0, self.results=[])\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self._add_token(KEYWORD_DICT_END)\n",
      "State:\n",
      "[(210198, /b'>>')]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_wclose(self, s: bytes, i: int) -> int:\n",
      "        c = s[i : i + 1]\n",
      "        if c == b\">\":\n",
      "            self._add_token(KEYWORD_DICT_END)\n",
      "            i += 1\n",
      "        self._parse1 = self._parse_main\n",
      "        return i\n",
      "_parse_wclose(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, s=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', i=84, self._curtoken=b'', self._curtokenpos=210198, self._parse1=<bound method PSBaseParser._parse_wclose of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>, self._tokens=[], self.buf=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=210115, self.charpos=84, self.context=[(210123, None, [])], self.curstack=[(210125, /'Root'), (210136, <PDFObjRef:82>), (210137, /'ID'), (210140, [b'\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9', b'k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8']), (210178, /'Info'), (210189, <PDFObjRef:39>), (210190, /'Size'), (210196, 83)], self.curtype='d', self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.paren=0, self.results=[])\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "i += 1\n",
      "State:\n",
      "85\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_wclose(self, s: bytes, i: int) -> int:\n",
      "        c = s[i : i + 1]\n",
      "        if c == b\">\":\n",
      "            self._add_token(KEYWORD_DICT_END)\n",
      "            i += 1\n",
      "        self._parse1 = self._parse_main\n",
      "        return i\n",
      "_parse_wclose(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, s=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', i=84, self._curtoken=b'', self._curtokenpos=210198, self._parse1=<bound method PSBaseParser._parse_wclose of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>, self._tokens=[], self.buf=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=210115, self.charpos=84, self.context=[(210123, None, [])], self.curstack=[(210125, /'Root'), (210136, <PDFObjRef:82>), (210137, /'ID'), (210140, [b'\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9', b'k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8']), (210178, /'Info'), (210189, <PDFObjRef:39>), (210190, /'Size'), (210196, 83)], self.curtype='d', self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.paren=0, self.results=[])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "self._parse1 = self._parse_main\n",
      "State:\n",
      "<bound method PSBaseParser._parse_main of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add_results(self, *objs: PSStackEntry[ExtraT]) -> None:\n",
      "        try:\n",
      "            log.debug(\"add_results: %r\", objs)\n",
      "        except Exception:\n",
      "            log.debug(\"add_results: (unprintable object)\")\n",
      "        self.results.extend(objs)\n",
      "        return\n",
      "add_results(self=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, objs=((210123, {'Root': <PDFObjRef:82>, 'ID': [b'\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9', b'k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8'], 'Info': <PDFObjRef:39>, 'Size': 83}),), self._curtoken=b'startxref', self._curtokenpos=210201, self._parse1=<bound method PSBaseParser._parse_main of <PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>>, self._tokens=[], self.buf=b'trailer\\n<</Root 82 0 R/ID[(\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9)(k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8)]/Info 39 0 R/Size 83>>\\nstartxref\\n208444\\n%%EOF\\n', self.bufpos=210115, self.charpos=95, self.context=[], self.curstack=[], self.curtype=None, self.doc=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, self.fallback=False, self.fp=<_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, self.paren=0, self.results=[])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "self.results.extend(objs)\n",
      "State:\n",
      "[(210123, {'Root': <PDFObjRef:82>, 'ID': [b'\\xf8\\x00\\x94\\xe8\\x97\\xdc.\\xefy\\x04\\xc0\\xeaL\\x98\\xdb\\xe9', b'k\\x0b\\xb3\\xc4\\xcf\\x8d\\xe6?\\x84\\x10\\x83i\\xb68\\xe2\\xb8'], 'Info': <PDFObjRef:39>, 'Size': 83})]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def dict_value(x: object) -> Dict[Any, Any]:\n",
      "    x = resolve1(x)\n",
      "    if not isinstance(x, dict):\n",
      "        if settings.STRICT:\n",
      "            logger.error(\"PDFTypeError : Dict required: %r\", x)\n",
      "            raise PDFTypeError(\"Dict required: %r\" % x)\n",
      "        return {}\n",
      "    return x\n",
      "dict_value(x=<PDFObjRef:39>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "x = resolve1(x)\n",
      "State:\n",
      "{'Producer': b'iText\\xae 5.5.13 \\xa92000-2018 iText Group NV (Government Publishing Office; licensed version)', 'CreationDate': b'D:20200806095125Z', 'ModDate': b\"D:20200806055139-04'00'\", 'Creator': b'govinfo, U. S. Government Publishing Office'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _getobj_parse(self, pos: int, objid: int) -> object:\n",
      "        assert self._parser is not None\n",
      "        self._parser.seek(pos)\n",
      "        (_, objid1) = self._parser.nexttoken()\n",
      "        (_, genno) = self._parser.nexttoken()\n",
      "        (_, kwd) = self._parser.nexttoken()\n",
      "        if objid1 != objid:\n",
      "            x = []\n",
      "            while kwd is not self.KEYWORD_OBJ:\n",
      "                (_, kwd) = self._parser.nexttoken()\n",
      "                x.append(kwd)\n",
      "            if len(x) >= 2:\n",
      "                objid1 = x[-2]\n",
      "        if objid1 != objid:\n",
      "            raise PDFSyntaxError(\"objid mismatch: {!r}={!r}\".format(objid1, objid))\n",
      "        if kwd != KWD(b\"obj\"):\n",
      "            raise PDFSyntaxError(\"Invalid object spec: offset=%r\" % pos)\n",
      "        (_, obj) = self._parser.nextobject()\n",
      "        return obj\n",
      "_getobj_parse(self=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, pos=114857, objid=39, self._cached_objs={}, self._parsed_objs={}, self._parser=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, self.caching=True, self.catalog={}, self.decipher=None, self.encryption=None, self.info=[], self.is_extractable=True, self.is_modifiable=True, self.is_printable=True, self.xrefs=[<PDFXRef: offsets=dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82])>])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self._parser.seek(pos)\n",
      "State:\n",
      "<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=114857>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _getobj_parse(self, pos: int, objid: int) -> object:\n",
      "        assert self._parser is not None\n",
      "        self._parser.seek(pos)\n",
      "        (_, objid1) = self._parser.nexttoken()\n",
      "        (_, genno) = self._parser.nexttoken()\n",
      "        (_, kwd) = self._parser.nexttoken()\n",
      "        if objid1 != objid:\n",
      "            x = []\n",
      "            while kwd is not self.KEYWORD_OBJ:\n",
      "                (_, kwd) = self._parser.nexttoken()\n",
      "                x.append(kwd)\n",
      "            if len(x) >= 2:\n",
      "                objid1 = x[-2]\n",
      "        if objid1 != objid:\n",
      "            raise PDFSyntaxError(\"objid mismatch: {!r}={!r}\".format(objid1, objid))\n",
      "        if kwd != KWD(b\"obj\"):\n",
      "            raise PDFSyntaxError(\"Invalid object spec: offset=%r\" % pos)\n",
      "        (_, obj) = self._parser.nextobject()\n",
      "        return obj\n",
      "_getobj_parse(self=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, pos=114857, objid=39, self._cached_objs={}, self._parsed_objs={}, self._parser=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, self.caching=True, self.catalog={}, self.decipher=None, self.encryption=None, self.info=[], self.is_extractable=True, self.is_modifiable=True, self.is_printable=True, self.xrefs=[<PDFXRef: offsets=dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82])>])\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "(_, objid1) = self._parser.nexttoken()  # objid\n",
      "State:\n",
      "114857\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _getobj_parse(self, pos: int, objid: int) -> object:\n",
      "        assert self._parser is not None\n",
      "        self._parser.seek(pos)\n",
      "        (_, objid1) = self._parser.nexttoken()\n",
      "        (_, genno) = self._parser.nexttoken()\n",
      "        (_, kwd) = self._parser.nexttoken()\n",
      "        if objid1 != objid:\n",
      "            x = []\n",
      "            while kwd is not self.KEYWORD_OBJ:\n",
      "                (_, kwd) = self._parser.nexttoken()\n",
      "                x.append(kwd)\n",
      "            if len(x) >= 2:\n",
      "                objid1 = x[-2]\n",
      "        if objid1 != objid:\n",
      "            raise PDFSyntaxError(\"objid mismatch: {!r}={!r}\".format(objid1, objid))\n",
      "        if kwd != KWD(b\"obj\"):\n",
      "            raise PDFSyntaxError(\"Invalid object spec: offset=%r\" % pos)\n",
      "        (_, obj) = self._parser.nextobject()\n",
      "        return obj\n",
      "_getobj_parse(self=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, pos=114857, objid=39, self._cached_objs={}, self._parsed_objs={}, self._parser=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, self.caching=True, self.catalog={}, self.decipher=None, self.encryption=None, self.info=[], self.is_extractable=True, self.is_modifiable=True, self.is_printable=True, self.xrefs=[<PDFXRef: offsets=dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82])>])\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "(_, genno) = self._parser.nexttoken()  # genno\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _getobj_parse(self, pos: int, objid: int) -> object:\n",
      "        assert self._parser is not None\n",
      "        self._parser.seek(pos)\n",
      "        (_, objid1) = self._parser.nexttoken()\n",
      "        (_, genno) = self._parser.nexttoken()\n",
      "        (_, kwd) = self._parser.nexttoken()\n",
      "        if objid1 != objid:\n",
      "            x = []\n",
      "            while kwd is not self.KEYWORD_OBJ:\n",
      "                (_, kwd) = self._parser.nexttoken()\n",
      "                x.append(kwd)\n",
      "            if len(x) >= 2:\n",
      "                objid1 = x[-2]\n",
      "        if objid1 != objid:\n",
      "            raise PDFSyntaxError(\"objid mismatch: {!r}={!r}\".format(objid1, objid))\n",
      "        if kwd != KWD(b\"obj\"):\n",
      "            raise PDFSyntaxError(\"Invalid object spec: offset=%r\" % pos)\n",
      "        (_, obj) = self._parser.nextobject()\n",
      "        return obj\n",
      "_getobj_parse(self=<pdfminer.pdfdocument.PDFDocument object at 0x7f1a45c2d9a0>, pos=114857, objid=39, self._cached_objs={}, self._parsed_objs={}, self._parser=<PDFParser: <_io.BufferedReader name='samples/contrib/pr-00530-ml-lines.pdf'>, bufpos=210115>, self.caching=True, self.catalog={}, self.decipher=None, self.encryption=None, self.info=[], self.is_extractable=True, self.is_modifiable=True, self.is_printable=True, self.xrefs=[<PDFXRef: offsets=dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82])>])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "(_, kwd) = self._parser.nexttoken()\n",
      "State:\n",
      "/b'obj'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def stream_value(x: object) -> \"PDFStream\":\n",
      "    x = resolve1(x)\n",
      "    if not isinstance(x, PDFStream):\n",
      "        if settings.STRICT:\n",
      "            raise PDFTypeError(\"PDFStream required: %r\" % x)\n",
      "        return PDFStream({}, b\"\")\n",
      "    return x\n",
      "stream_value(x=<PDFObjRef:52>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "x = resolve1(x)\n",
      "State:\n",
      "<PDFStream(52): raw=275, {'Filter': /'FlateDecode', 'Length': 275}>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_objid(self, objid: int, genno: int) -> None:\n",
      "        self.objid = objid\n",
      "        self.genno = genno\n",
      "set_objid(self=<PDFStream(None): raw=275, {'Filter': /'FlateDecode', 'Length': 275}>, objid=52, genno=0, self.attrs={'Filter': /'FlateDecode', 'Length': 275}, self.data=None, self.decipher=None, self.genno=None, self.objid=None, self.rawdata=b'h\\xdeTQMo\\xc3 \\x0c\\xbd\\xf3+|\\xec\\xb4\\x03\\x84,\\xca&\\xa1H[\\xbbJ9\\xecCk\\xbb;\\x01\\'CZ\\x08\"\\xc9!\\xff~@\\xb2N;\\x00\\xcf\\xf6\\xb3\\x9fm\\xe8\\xbe>\\xd4\\xd6L@\\xdf\\xfd\\xa0N8Ak\\xac\\xf68\\x0e\\xb3W\\x08\\rv\\xc6B\\xc6A\\x1b5mV\\xbaU/\\x1d\\xd0\\x90|Z\\xc6\\t\\xfb\\xda\\xb6\\x03\\x08A\\xe8G\\x08\\x8e\\x93_`w,n\\xd9\\r\\xd07\\xaf\\xd1\\x1b\\xdb\\xc1\\xee\\x9c]>\\x83\\xe34;\\xf7\\x8d=\\xda\\t\\x18T\\x15hl\\t\\xdd\\xbfH\\xf7*{\\x04\\x1a\\xd3\\xfe|\\xe7\\xc5!\\xf0dg\\x9b\\xf0\\xa0qtR\\xa1\\x97\\xb6C\\x10\\x8cW tY\\x01Z\\xfd?F\\xca5\\xa3i\\xd5\\x97\\xf4D\\xb0\\xbb\\xc0d\\xf9\\xc3s\\x15p\\x111{\\x8c\\x98\\xb3\\x84\\xc3CD\\xd1&\\\\\\x1c\\x03\\xbe\\x8f~\\xce8\\x0f\\xb8Y9O\\x91\\x13\\xc5\\x02>\\x94\\x15\\t\\x9a[\\xf5\\xecWk\\x95^\\xdbby\\xaa\\x90\\xf3\\x8d\\xb9\\xc6b\\xa3q{\\xd7\\x99\\xd5\\xec}XGZq\\x9a:\\xcek,^\\x7f\\xc1\\r.\\x8e\\x17\\x0f\\xf9\\x11`\\x00b\\x9b\\x83\\xdc')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.objid = objid\n",
      "State:\n",
      "<PDFStream(52): raw=275, {'Filter': /'FlateDecode', 'Length': 275}>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_objid(self, objid: int, genno: int) -> None:\n",
      "        self.objid = objid\n",
      "        self.genno = genno\n",
      "set_objid(self=<PDFStream(None): raw=275, {'Filter': /'FlateDecode', 'Length': 275}>, objid=52, genno=0, self.attrs={'Filter': /'FlateDecode', 'Length': 275}, self.data=None, self.decipher=None, self.genno=None, self.objid=None, self.rawdata=b'h\\xdeTQMo\\xc3 \\x0c\\xbd\\xf3+|\\xec\\xb4\\x03\\x84,\\xca&\\xa1H[\\xbbJ9\\xecCk\\xbb;\\x01\\'CZ\\x08\"\\xc9!\\xff~@\\xb2N;\\x00\\xcf\\xf6\\xb3\\x9fm\\xe8\\xbe>\\xd4\\xd6L@\\xdf\\xfd\\xa0N8Ak\\xac\\xf68\\x0e\\xb3W\\x08\\rv\\xc6B\\xc6A\\x1b5mV\\xbaU/\\x1d\\xd0\\x90|Z\\xc6\\t\\xfb\\xda\\xb6\\x03\\x08A\\xe8G\\x08\\x8e\\x93_`w,n\\xd9\\r\\xd07\\xaf\\xd1\\x1b\\xdb\\xc1\\xee\\x9c]>\\x83\\xe34;\\xf7\\x8d=\\xda\\t\\x18T\\x15hl\\t\\xdd\\xbfH\\xf7*{\\x04\\x1a\\xd3\\xfe|\\xe7\\xc5!\\xf0dg\\x9b\\xf0\\xa0qtR\\xa1\\x97\\xb6C\\x10\\x8cW tY\\x01Z\\xfd?F\\xca5\\xa3i\\xd5\\x97\\xf4D\\xb0\\xbb\\xc0d\\xf9\\xc3s\\x15p\\x111{\\x8c\\x98\\xb3\\x84\\xc3CD\\xd1&\\\\\\x1c\\x03\\xbe\\x8f~\\xce8\\x0f\\xb8Y9O\\x91\\x13\\xc5\\x02>\\x94\\x15\\t\\x9a[\\xf5\\xecWk\\x95^\\xdbby\\xaa\\x90\\xf3\\x8d\\xb9\\xc6b\\xa3q{\\xd7\\x99\\xd5\\xec}XGZq\\x9a:\\xcek,^\\x7f\\xc1\\r.\\x8e\\x17\\x0f\\xf9\\x11`\\x00b\\x9b\\x83\\xdc')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.genno = genno\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_data(self) -> bytes:\n",
      "        if self.data is None:\n",
      "            self.decode()\n",
      "            assert self.data is not None\n",
      "        return self.data\n",
      "get_data(self=<PDFStream(52): raw=275, {'Filter': /'FlateDecode', 'Length': 275}>, self.attrs={'Filter': /'FlateDecode', 'Length': 275}, self.data=None, self.decipher=None, self.genno=0, self.objid=52, self.rawdata=b'h\\xdeTQMo\\xc3 \\x0c\\xbd\\xf3+|\\xec\\xb4\\x03\\x84,\\xca&\\xa1H[\\xbbJ9\\xecCk\\xbb;\\x01\\'CZ\\x08\"\\xc9!\\xff~@\\xb2N;\\x00\\xcf\\xf6\\xb3\\x9fm\\xe8\\xbe>\\xd4\\xd6L@\\xdf\\xfd\\xa0N8Ak\\xac\\xf68\\x0e\\xb3W\\x08\\rv\\xc6B\\xc6A\\x1b5mV\\xbaU/\\x1d\\xd0\\x90|Z\\xc6\\t\\xfb\\xda\\xb6\\x03\\x08A\\xe8G\\x08\\x8e\\x93_`w,n\\xd9\\r\\xd07\\xaf\\xd1\\x1b\\xdb\\xc1\\xee\\x9c]>\\x83\\xe34;\\xf7\\x8d=\\xda\\t\\x18T\\x15hl\\t\\xdd\\xbfH\\xf7*{\\x04\\x1a\\xd3\\xfe|\\xe7\\xc5!\\xf0dg\\x9b\\xf0\\xa0qtR\\xa1\\x97\\xb6C\\x10\\x8cW tY\\x01Z\\xfd?F\\xca5\\xa3i\\xd5\\x97\\xf4D\\xb0\\xbb\\xc0d\\xf9\\xc3s\\x15p\\x111{\\x8c\\x98\\xb3\\x84\\xc3CD\\xd1&\\\\\\x1c\\x03\\xbe\\x8f~\\xce8\\x0f\\xb8Y9O\\x91\\x13\\xc5\\x02>\\x94\\x15\\t\\x9a[\\xf5\\xecWk\\x95^\\xdbby\\xaa\\x90\\xf3\\x8d\\xb9\\xc6b\\xa3q{\\xd7\\x99\\xd5\\xec}XGZq\\x9a:\\xcek,^\\x7f\\xc1\\r.\\x8e\\x17\\x0f\\xf9\\x11`\\x00b\\x9b\\x83\\xdc')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.decode()\n",
      "State:\n",
      "<PDFStream(52): len=433, {'Filter': /'FlateDecode', 'Length': 275}>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def decode(self) -> None:\n",
      "        assert self.data is None and self.rawdata is not None, str(\n",
      "            (self.data, self.rawdata)\n",
      "        )\n",
      "        data = self.rawdata\n",
      "        if self.decipher:\n",
      "            assert self.objid is not None\n",
      "            assert self.genno is not None\n",
      "            data = self.decipher(self.objid, self.genno, data, self.attrs)\n",
      "        filters = self.get_filters()\n",
      "        if not filters:\n",
      "            self.data = data\n",
      "            self.rawdata = None\n",
      "            return\n",
      "        for (f, params) in filters:\n",
      "            if f in LITERALS_FLATE_DECODE:\n",
      "                try:\n",
      "                    data = zlib.decompress(data)\n",
      "                except zlib.error as e:\n",
      "                    if settings.STRICT:\n",
      "                        error_msg = \"Invalid zlib bytes: {!r}, {!r}\".format(e, data)\n",
      "                        raise PDFException(error_msg)\n",
      "                    try:\n",
      "                        data = decompress_corrupted(data)\n",
      "                    except zlib.error:\n",
      "                        data = b\"\"\n",
      "            elif f in LITERALS_LZW_DECODE:\n",
      "                data = lzwdecode(data)\n",
      "            elif f in LITERALS_ASCII85_DECODE:\n",
      "                data = ascii85decode(data)\n",
      "            elif f in LITERALS_ASCIIHEX_DECODE:\n",
      "                data = asciihexdecode(data)\n",
      "            elif f in LITERALS_RUNLENGTH_DECODE:\n",
      "                data = rldecode(data)\n",
      "            elif f in LITERALS_CCITTFAX_DECODE:\n",
      "                data = ccittfaxdecode(data, params)\n",
      "            elif f in LITERALS_DCT_DECODE:\n",
      "                pass\n",
      "            elif f in LITERALS_JBIG2_DECODE:\n",
      "                pass\n",
      "            elif f in LITERALS_JPX_DECODE:\n",
      "                pass\n",
      "            elif f == LITERAL_CRYPT:\n",
      "                raise PDFNotImplementedError(\"/Crypt filter is unsupported\")\n",
      "            else:\n",
      "                raise PDFNotImplementedError(\"Unsupported filter: %r\" % f)\n",
      "            if params and \"Predictor\" in params:\n",
      "                pred = int_value(params[\"Predictor\"])\n",
      "                if pred == 1:\n",
      "                    pass\n",
      "                elif 10 <= pred:\n",
      "                    colors = int_value(params.get(\"Colors\", 1))\n",
      "                    columns = int_value(params.get(\"Columns\", 1))\n",
      "                    raw_bits_per_component = params.get(\"BitsPerComponent\", 8)\n",
      "                    bitspercomponent = int_value(raw_bits_per_component)\n",
      "                    data = apply_png_predictor(\n",
      "                        pred, colors, columns, bitspercomponent, data\n",
      "                    )\n",
      "                else:\n",
      "                    error_msg = \"Unsupported predictor: %r\" % pred\n",
      "                    raise PDFNotImplementedError(error_msg)\n",
      "        self.data = data\n",
      "        self.rawdata = None\n",
      "        return\n",
      "decode(self=<PDFStream(52): raw=275, {'Filter': /'FlateDecode', 'Length': 275}>, self.attrs={'Filter': /'FlateDecode', 'Length': 275}, self.data=None, self.decipher=None, self.genno=0, self.objid=52, self.rawdata=b'h\\xdeTQMo\\xc3 \\x0c\\xbd\\xf3+|\\xec\\xb4\\x03\\x84,\\xca&\\xa1H[\\xbbJ9\\xecCk\\xbb;\\x01\\'CZ\\x08\"\\xc9!\\xff~@\\xb2N;\\x00\\xcf\\xf6\\xb3\\x9fm\\xe8\\xbe>\\xd4\\xd6L@\\xdf\\xfd\\xa0N8Ak\\xac\\xf68\\x0e\\xb3W\\x08\\rv\\xc6B\\xc6A\\x1b5mV\\xbaU/\\x1d\\xd0\\x90|Z\\xc6\\t\\xfb\\xda\\xb6\\x03\\x08A\\xe8G\\x08\\x8e\\x93_`w,n\\xd9\\r\\xd07\\xaf\\xd1\\x1b\\xdb\\xc1\\xee\\x9c]>\\x83\\xe34;\\xf7\\x8d=\\xda\\t\\x18T\\x15hl\\t\\xdd\\xbfH\\xf7*{\\x04\\x1a\\xd3\\xfe|\\xe7\\xc5!\\xf0dg\\x9b\\xf0\\xa0qtR\\xa1\\x97\\xb6C\\x10\\x8cW tY\\x01Z\\xfd?F\\xca5\\xa3i\\xd5\\x97\\xf4D\\xb0\\xbb\\xc0d\\xf9\\xc3s\\x15p\\x111{\\x8c\\x98\\xb3\\x84\\xc3CD\\xd1&\\\\\\x1c\\x03\\xbe\\x8f~\\xce8\\x0f\\xb8Y9O\\x91\\x13\\xc5\\x02>\\x94\\x15\\t\\x9a[\\xf5\\xecWk\\x95^\\xdbby\\xaa\\x90\\xf3\\x8d\\xb9\\xc6b\\xa3q{\\xd7\\x99\\xd5\\xec}XGZq\\x9a:\\xcek,^\\x7f\\xc1\\r.\\x8e\\x17\\x0f\\xf9\\x11`\\x00b\\x9b\\x83\\xdc')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "data = self.rawdata\n",
      "State:\n",
      "b'h\\xdeTQMo\\xc3 \\x0c\\xbd\\xf3+|\\xec\\xb4\\x03\\x84,\\xca&\\xa1H[\\xbbJ9\\xecCk\\xbb;\\x01\\'CZ\\x08\"\\xc9!\\xff~@\\xb2N;\\x00\\xcf\\xf6\\xb3\\x9fm\\xe8\\xbe>\\xd4\\xd6L@\\xdf\\xfd\\xa0N8Ak\\xac\\xf68\\x0e\\xb3W\\x08\\rv\\xc6B\\xc6A\\x1b5mV\\xbaU/\\x1d\\xd0\\x90|Z\\xc6\\t\\xfb\\xda\\xb6\\x03\\x08A\\xe8G\\x08\\x8e\\x93_`w,n\\xd9\\r\\xd07\\xaf\\xd1\\x1b\\xdb\\xc1\\xee\\x9c]>\\x83\\xe34;\\xf7\\x8d=\\xda\\t\\x18T\\x15hl\\t\\xdd\\xbfH\\xf7*{\\x04\\x1a\\xd3\\xfe|\\xe7\\xc5!\\xf0dg\\x9b\\xf0\\xa0qtR\\xa1\\x97\\xb6C\\x10\\x8cW tY\\x01Z\\xfd?F\\xca5\\xa3i\\xd5\\x97\\xf4D\\xb0\\xbb\\xc0d\\xf9\\xc3s\\x15p\\x111{\\x8c\\x98\\xb3\\x84\\xc3CD\\xd1&\\\\\\x1c\\x03\\xbe\\x8f~\\xce8\\x0f\\xb8Y9O\\x91\\x13\\xc5\\x02>\\x94\\x15\\t\\x9a[\\xf5\\xecWk\\x95^\\xdbby\\xaa\\x90\\xf3\\x8d\\xb9\\xc6b\\xa3q{\\xd7\\x99\\xd5\\xec}XGZq\\x9a:\\xcek,^\\x7f\\xc1\\r.\\x8e\\x17\\x0f\\xf9\\x11`\\x00b\\x9b\\x83\\xdc'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_filters(self) -> List[Tuple[Any, Any]]:\n",
      "        filters = self.get_any((\"F\", \"Filter\"))\n",
      "        params = self.get_any((\"DP\", \"DecodeParms\", \"FDecodeParms\"), {})\n",
      "        if not filters:\n",
      "            return []\n",
      "        if not isinstance(filters, list):\n",
      "            filters = [filters]\n",
      "        if not isinstance(params, list):\n",
      "            params = [params] * len(filters)\n",
      "        if settings.STRICT and len(params) != len(filters):\n",
      "            raise PDFException(\"Parameters len filter mismatch\")\n",
      "        resolved_filters = [resolve1(f) for f in filters]\n",
      "        resolved_params = [resolve1(param) for param in params]\n",
      "        return list(zip(resolved_filters, resolved_params))\n",
      "get_filters(self=<PDFStream(52): raw=275, {'Filter': /'FlateDecode', 'Length': 275}>, self.attrs={'Filter': /'FlateDecode', 'Length': 275}, self.data=None, self.decipher=None, self.genno=0, self.objid=52, self.rawdata=b'h\\xdeTQMo\\xc3 \\x0c\\xbd\\xf3+|\\xec\\xb4\\x03\\x84,\\xca&\\xa1H[\\xbbJ9\\xecCk\\xbb;\\x01\\'CZ\\x08\"\\xc9!\\xff~@\\xb2N;\\x00\\xcf\\xf6\\xb3\\x9fm\\xe8\\xbe>\\xd4\\xd6L@\\xdf\\xfd\\xa0N8Ak\\xac\\xf68\\x0e\\xb3W\\x08\\rv\\xc6B\\xc6A\\x1b5mV\\xbaU/\\x1d\\xd0\\x90|Z\\xc6\\t\\xfb\\xda\\xb6\\x03\\x08A\\xe8G\\x08\\x8e\\x93_`w,n\\xd9\\r\\xd07\\xaf\\xd1\\x1b\\xdb\\xc1\\xee\\x9c]>\\x83\\xe34;\\xf7\\x8d=\\xda\\t\\x18T\\x15hl\\t\\xdd\\xbfH\\xf7*{\\x04\\x1a\\xd3\\xfe|\\xe7\\xc5!\\xf0dg\\x9b\\xf0\\xa0qtR\\xa1\\x97\\xb6C\\x10\\x8cW tY\\x01Z\\xfd?F\\xca5\\xa3i\\xd5\\x97\\xf4D\\xb0\\xbb\\xc0d\\xf9\\xc3s\\x15p\\x111{\\x8c\\x98\\xb3\\x84\\xc3CD\\xd1&\\\\\\x1c\\x03\\xbe\\x8f~\\xce8\\x0f\\xb8Y9O\\x91\\x13\\xc5\\x02>\\x94\\x15\\t\\x9a[\\xf5\\xecWk\\x95^\\xdbby\\xaa\\x90\\xf3\\x8d\\xb9\\xc6b\\xa3q{\\xd7\\x99\\xd5\\xec}XGZq\\x9a:\\xcek,^\\x7f\\xc1\\r.\\x8e\\x17\\x0f\\xf9\\x11`\\x00b\\x9b\\x83\\xdc')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "filters = self.get_any((\"F\", \"Filter\"))\n",
      "State:\n",
      "/'FlateDecode'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_filters(self) -> List[Tuple[Any, Any]]:\n",
      "        filters = self.get_any((\"F\", \"Filter\"))\n",
      "        params = self.get_any((\"DP\", \"DecodeParms\", \"FDecodeParms\"), {})\n",
      "        if not filters:\n",
      "            return []\n",
      "        if not isinstance(filters, list):\n",
      "            filters = [filters]\n",
      "        if not isinstance(params, list):\n",
      "            params = [params] * len(filters)\n",
      "        if settings.STRICT and len(params) != len(filters):\n",
      "            raise PDFException(\"Parameters len filter mismatch\")\n",
      "        resolved_filters = [resolve1(f) for f in filters]\n",
      "        resolved_params = [resolve1(param) for param in params]\n",
      "        return list(zip(resolved_filters, resolved_params))\n",
      "get_filters(self=<PDFStream(52): raw=275, {'Filter': /'FlateDecode', 'Length': 275}>, self.attrs={'Filter': /'FlateDecode', 'Length': 275}, self.data=None, self.decipher=None, self.genno=0, self.objid=52, self.rawdata=b'h\\xdeTQMo\\xc3 \\x0c\\xbd\\xf3+|\\xec\\xb4\\x03\\x84,\\xca&\\xa1H[\\xbbJ9\\xecCk\\xbb;\\x01\\'CZ\\x08\"\\xc9!\\xff~@\\xb2N;\\x00\\xcf\\xf6\\xb3\\x9fm\\xe8\\xbe>\\xd4\\xd6L@\\xdf\\xfd\\xa0N8Ak\\xac\\xf68\\x0e\\xb3W\\x08\\rv\\xc6B\\xc6A\\x1b5mV\\xbaU/\\x1d\\xd0\\x90|Z\\xc6\\t\\xfb\\xda\\xb6\\x03\\x08A\\xe8G\\x08\\x8e\\x93_`w,n\\xd9\\r\\xd07\\xaf\\xd1\\x1b\\xdb\\xc1\\xee\\x9c]>\\x83\\xe34;\\xf7\\x8d=\\xda\\t\\x18T\\x15hl\\t\\xdd\\xbfH\\xf7*{\\x04\\x1a\\xd3\\xfe|\\xe7\\xc5!\\xf0dg\\x9b\\xf0\\xa0qtR\\xa1\\x97\\xb6C\\x10\\x8cW tY\\x01Z\\xfd?F\\xca5\\xa3i\\xd5\\x97\\xf4D\\xb0\\xbb\\xc0d\\xf9\\xc3s\\x15p\\x111{\\x8c\\x98\\xb3\\x84\\xc3CD\\xd1&\\\\\\x1c\\x03\\xbe\\x8f~\\xce8\\x0f\\xb8Y9O\\x91\\x13\\xc5\\x02>\\x94\\x15\\t\\x9a[\\xf5\\xecWk\\x95^\\xdbby\\xaa\\x90\\xf3\\x8d\\xb9\\xc6b\\xa3q{\\xd7\\x99\\xd5\\xec}XGZq\\x9a:\\xcek,^\\x7f\\xc1\\r.\\x8e\\x17\\x0f\\xf9\\x11`\\x00b\\x9b\\x83\\xdc')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "params = self.get_any((\"DP\", \"DecodeParms\", \"FDecodeParms\"), {})\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def popall(self) -> List[PSStackEntry[ExtraT]]:\n",
      "        objs = self.curstack\n",
      "        self.curstack = []\n",
      "        return objs\n",
      "popall(self=<CMapParser: <_io.BytesIO object at 0x7f1a4346d9f0>, bufpos=0>, self._curtoken=b'begincmap', self._curtokenpos=51, self._in_cmap=True, self._parse1=<bound method PSBaseParser._parse_main of <CMapParser: <_io.BytesIO object at 0x7f1a4346d9f0>, bufpos=0>>, self._tokens=[], self._warnings=set(), self.buf=b'/CIDInit /ProcSet findresource begin 12 dict begin begincmap /CIDSystemInfo <<\\n/Registry (F5+0) /Ordering (T1UV) /Supplement 0 >> def\\n/CMapName /F5+0 def\\n/CMapType 2 def\\n1 begincodespacerange <02> <d7> endcodespacerange\\n7 beginbfchar\\n<04> <039E>\\n<05> <00AE>\\n<20> <0020>\\n<5f> <005F>\\n<80> <2022>\\n<b0> <00B0>\\n<d7> <00D7>\\nendbfchar\\n1 beginbfrange\\n<02> <03> <2032>\\nendbfrange\\nendcmap CMapName currentdict /CMap defineresource pop end end\\n', self.bufpos=0, self.charpos=60, self.cmap=<UnicodeMap: None>, self.context=[], self.curstack=[(0, /'CIDInit'), (9, /'ProcSet'), (18, /b'findresource'), (31, /b'begin'), (37, 12), (40, /b'dict'), (45, /b'begin')], self.curtype=None, self.fp=<_io.BytesIO object at 0x7f1a4346d9f0>, self.results=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "objs = self.curstack\n",
      "State:\n",
      "[(0, /'CIDInit'), (9, /'ProcSet'), (18, /b'findresource'), (31, /b'begin'), (37, 12), (40, /b'dict'), (45, /b'begin')]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def popall(self) -> List[PSStackEntry[ExtraT]]:\n",
      "        objs = self.curstack\n",
      "        self.curstack = []\n",
      "        return objs\n",
      "popall(self=<CMapParser: <_io.BytesIO object at 0x7f1a4346d9f0>, bufpos=0>, self._curtoken=b'begincmap', self._curtokenpos=51, self._in_cmap=True, self._parse1=<bound method PSBaseParser._parse_main of <CMapParser: <_io.BytesIO object at 0x7f1a4346d9f0>, bufpos=0>>, self._tokens=[], self._warnings=set(), self.buf=b'/CIDInit /ProcSet findresource begin 12 dict begin begincmap /CIDSystemInfo <<\\n/Registry (F5+0) /Ordering (T1UV) /Supplement 0 >> def\\n/CMapName /F5+0 def\\n/CMapType 2 def\\n1 begincodespacerange <02> <d7> endcodespacerange\\n7 beginbfchar\\n<04> <039E>\\n<05> <00AE>\\n<20> <0020>\\n<5f> <005F>\\n<80> <2022>\\n<b0> <00B0>\\n<d7> <00D7>\\nendbfchar\\n1 beginbfrange\\n<02> <03> <2032>\\nendbfrange\\nendcmap CMapName currentdict /CMap defineresource pop end end\\n', self.bufpos=0, self.charpos=60, self.cmap=<UnicodeMap: None>, self.context=[], self.curstack=[(0, /'CIDInit'), (9, /'ProcSet'), (18, /b'findresource'), (31, /b'begin'), (37, 12), (40, /b'dict'), (45, /b'begin')], self.curtype=None, self.fp=<_io.BytesIO object at 0x7f1a4346d9f0>, self.results=[])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.curstack = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_attr(self, k: str, v: object) -> None:\n",
      "        self.attrs[k] = v\n",
      "set_attr(self=<UnicodeMap: None>, k='CIDSystemInfo', v={'Registry': b'F5+0', 'Ordering': b'T1UV', 'Supplement': 0}, self.attrs={}, self.cid2unichr={})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.attrs[k] = v\n",
      "State:\n",
      "{'CIDSystemInfo': {'Registry': b'F5+0', 'Ordering': b'T1UV', 'Supplement': 0}}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_hexstring(self, s: bytes, i: int) -> int:\n",
      "        m = END_HEX_STRING.search(s, i)\n",
      "        if not m:\n",
      "            self._curtoken += s[i:]\n",
      "            return len(s)\n",
      "        j = m.start(0)\n",
      "        self._curtoken += s[i:j]\n",
      "        token = HEX_PAIR.sub(\n",
      "            lambda m: bytes((int(m.group(0), 16),)), SPC.sub(b\"\", self._curtoken)\n",
      "        )\n",
      "        self._add_token(token)\n",
      "        self._parse1 = self._parse_main\n",
      "        return j\n",
      "_parse_hexstring(self=<CMapParser: <_io.BytesIO object at 0x7f1a4346d9f0>, bufpos=0>, s=b'/CIDInit /ProcSet findresource begin 12 dict begin begincmap /CIDSystemInfo <<\\n/Registry (F5+0) /Ordering (T1UV) /Supplement 0 >> def\\n/CMapName /F5+0 def\\n/CMapType 2 def\\n1 begincodespacerange <02> <d7> endcodespacerange\\n7 beginbfchar\\n<04> <039E>\\n<05> <00AE>\\n<20> <0020>\\n<5f> <005F>\\n<80> <2022>\\n<b0> <00B0>\\n<d7> <00D7>\\nendbfchar\\n1 beginbfrange\\n<02> <03> <2032>\\nendbfrange\\nendcmap CMapName currentdict /CMap defineresource pop end end\\n', i=193, self._curtoken=b'', self._curtokenpos=192, self._in_cmap=True, self._parse1=<bound method PSBaseParser._parse_hexstring of <CMapParser: <_io.BytesIO object at 0x7f1a4346d9f0>, bufpos=0>>, self._tokens=[], self._warnings=set(), self.buf=b'/CIDInit /ProcSet findresource begin 12 dict begin begincmap /CIDSystemInfo <<\\n/Registry (F5+0) /Ordering (T1UV) /Supplement 0 >> def\\n/CMapName /F5+0 def\\n/CMapType 2 def\\n1 begincodespacerange <02> <d7> endcodespacerange\\n7 beginbfchar\\n<04> <039E>\\n<05> <00AE>\\n<20> <0020>\\n<5f> <005F>\\n<80> <2022>\\n<b0> <00B0>\\n<d7> <00D7>\\nendbfchar\\n1 beginbfrange\\n<02> <03> <2032>\\nendbfrange\\nendcmap CMapName currentdict /CMap defineresource pop end end\\n', self.bufpos=0, self.charpos=193, self.cmap=<UnicodeMap: /'F5+0'>, self.context=[], self.curstack=[], self.curtype=None, self.fp=<_io.BytesIO object at 0x7f1a4346d9f0>, self.paren=0, self.results=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "m = END_HEX_STRING.search(s, i)\n",
      "State:\n",
      "<re.Match object; span=(195, 196), match=b'>'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_hexstring(self, s: bytes, i: int) -> int:\n",
      "        m = END_HEX_STRING.search(s, i)\n",
      "        if not m:\n",
      "            self._curtoken += s[i:]\n",
      "            return len(s)\n",
      "        j = m.start(0)\n",
      "        self._curtoken += s[i:j]\n",
      "        token = HEX_PAIR.sub(\n",
      "            lambda m: bytes((int(m.group(0), 16),)), SPC.sub(b\"\", self._curtoken)\n",
      "        )\n",
      "        self._add_token(token)\n",
      "        self._parse1 = self._parse_main\n",
      "        return j\n",
      "_parse_hexstring(self=<CMapParser: <_io.BytesIO object at 0x7f1a4346d9f0>, bufpos=0>, s=b'/CIDInit /ProcSet findresource begin 12 dict begin begincmap /CIDSystemInfo <<\\n/Registry (F5+0) /Ordering (T1UV) /Supplement 0 >> def\\n/CMapName /F5+0 def\\n/CMapType 2 def\\n1 begincodespacerange <02> <d7> endcodespacerange\\n7 beginbfchar\\n<04> <039E>\\n<05> <00AE>\\n<20> <0020>\\n<5f> <005F>\\n<80> <2022>\\n<b0> <00B0>\\n<d7> <00D7>\\nendbfchar\\n1 beginbfrange\\n<02> <03> <2032>\\nendbfrange\\nendcmap CMapName currentdict /CMap defineresource pop end end\\n', i=193, self._curtoken=b'', self._curtokenpos=192, self._in_cmap=True, self._parse1=<bound method PSBaseParser._parse_hexstring of <CMapParser: <_io.BytesIO object at 0x7f1a4346d9f0>, bufpos=0>>, self._tokens=[], self._warnings=set(), self.buf=b'/CIDInit /ProcSet findresource begin 12 dict begin begincmap /CIDSystemInfo <<\\n/Registry (F5+0) /Ordering (T1UV) /Supplement 0 >> def\\n/CMapName /F5+0 def\\n/CMapType 2 def\\n1 begincodespacerange <02> <d7> endcodespacerange\\n7 beginbfchar\\n<04> <039E>\\n<05> <00AE>\\n<20> <0020>\\n<5f> <005F>\\n<80> <2022>\\n<b0> <00B0>\\n<d7> <00D7>\\nendbfchar\\n1 beginbfrange\\n<02> <03> <2032>\\nendbfrange\\nendcmap CMapName currentdict /CMap defineresource pop end end\\n', self.bufpos=0, self.charpos=193, self.cmap=<UnicodeMap: /'F5+0'>, self.context=[], self.curstack=[], self.curtype=None, self.fp=<_io.BytesIO object at 0x7f1a4346d9f0>, self.paren=0, self.results=[])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "j = m.start(0)\n",
      "State:\n",
      "195\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_hexstring(self, s: bytes, i: int) -> int:\n",
      "        m = END_HEX_STRING.search(s, i)\n",
      "        if not m:\n",
      "            self._curtoken += s[i:]\n",
      "            return len(s)\n",
      "        j = m.start(0)\n",
      "        self._curtoken += s[i:j]\n",
      "        token = HEX_PAIR.sub(\n",
      "            lambda m: bytes((int(m.group(0), 16),)), SPC.sub(b\"\", self._curtoken)\n",
      "        )\n",
      "        self._add_token(token)\n",
      "        self._parse1 = self._parse_main\n",
      "        return j\n",
      "_parse_hexstring(self=<CMapParser: <_io.BytesIO object at 0x7f1a4346d9f0>, bufpos=0>, s=b'/CIDInit /ProcSet findresource begin 12 dict begin begincmap /CIDSystemInfo <<\\n/Registry (F5+0) /Ordering (T1UV) /Supplement 0 >> def\\n/CMapName /F5+0 def\\n/CMapType 2 def\\n1 begincodespacerange <02> <d7> endcodespacerange\\n7 beginbfchar\\n<04> <039E>\\n<05> <00AE>\\n<20> <0020>\\n<5f> <005F>\\n<80> <2022>\\n<b0> <00B0>\\n<d7> <00D7>\\nendbfchar\\n1 beginbfrange\\n<02> <03> <2032>\\nendbfrange\\nendcmap CMapName currentdict /CMap defineresource pop end end\\n', i=193, self._curtoken=b'', self._curtokenpos=192, self._in_cmap=True, self._parse1=<bound method PSBaseParser._parse_hexstring of <CMapParser: <_io.BytesIO object at 0x7f1a4346d9f0>, bufpos=0>>, self._tokens=[], self._warnings=set(), self.buf=b'/CIDInit /ProcSet findresource begin 12 dict begin begincmap /CIDSystemInfo <<\\n/Registry (F5+0) /Ordering (T1UV) /Supplement 0 >> def\\n/CMapName /F5+0 def\\n/CMapType 2 def\\n1 begincodespacerange <02> <d7> endcodespacerange\\n7 beginbfchar\\n<04> <039E>\\n<05> <00AE>\\n<20> <0020>\\n<5f> <005F>\\n<80> <2022>\\n<b0> <00B0>\\n<d7> <00D7>\\nendbfchar\\n1 beginbfrange\\n<02> <03> <2032>\\nendbfrange\\nendcmap CMapName currentdict /CMap defineresource pop end end\\n', self.bufpos=0, self.charpos=193, self.cmap=<UnicodeMap: /'F5+0'>, self.context=[], self.curstack=[], self.curtype=None, self.fp=<_io.BytesIO object at 0x7f1a4346d9f0>, self.paren=0, self.results=[])\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "self._add_token(token)\n",
      "State:\n",
      "[(192, b'\\x02')]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def fillfp(self) -> None:\n",
      "        if not self.fp:\n",
      "            if self.istream < len(self.streams):\n",
      "                strm = stream_value(self.streams[self.istream])\n",
      "                self.istream += 1\n",
      "            else:\n",
      "                raise PSEOF(\"Unexpected EOF, file truncated?\")\n",
      "            self.fp = BytesIO(strm.get_data())\n",
      "fillfp(self=REPR FAILED, self.fp=None, self.istream=0, self.streams=[<PDFObjRef:80>, <PDFObjRef:74>, <PDFObjRef:71>, <PDFObjRef:42>, <PDFObjRef:32>, <PDFObjRef:21>, <PDFObjRef:73>, <PDFObjRef:68>])\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "strm = stream_value(self.streams[self.istream])\n",
      "State:\n",
      "<PDFStream(80): raw=651, {'Filter': /'FlateDecode', 'Length': 651}>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def fillfp(self) -> None:\n",
      "        if not self.fp:\n",
      "            if self.istream < len(self.streams):\n",
      "                strm = stream_value(self.streams[self.istream])\n",
      "                self.istream += 1\n",
      "            else:\n",
      "                raise PSEOF(\"Unexpected EOF, file truncated?\")\n",
      "            self.fp = BytesIO(strm.get_data())\n",
      "fillfp(self=REPR FAILED, self.fp=None, self.istream=0, self.streams=[<PDFObjRef:80>, <PDFObjRef:74>, <PDFObjRef:71>, <PDFObjRef:42>, <PDFObjRef:32>, <PDFObjRef:21>, <PDFObjRef:73>, <PDFObjRef:68>])\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "self.istream += 1\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def flush(self) -> None:\n",
      "        self.add_results(*self.popall())\n",
      "flush(self=<PDFContentParser: <_io.BytesIO object at 0x7f1a360decc0>, bufpos=0>, self._curtoken=b'q', self._curtokenpos=0, self._parse1=<bound method PSBaseParser._parse_main of <PDFContentParser: <_io.BytesIO object at 0x7f1a360decc0>, bufpos=0>>, self._tokens=[], self.buf=b'q\\n1 0 0 1 45 744 cm\\n2.2 w \\n/GS0 gs\\n0 0 m\\n522 0 l\\nS\\nQ\\nq\\n1 0 0 1 45 740 cm\\n0.3 w \\n/GS0 gs\\n0 0 m\\n522 0 l\\nS\\nQ\\nq\\n1 0 0 1 45 683 cm\\n0.3 w \\n/GS0 gs\\n0 0 m\\n522 0 l\\nS\\nQ\\nq\\n1 0 0 1 45 600 cm\\n1.2 w \\n/GS0 gs\\n0 0 m\\n168 0 l\\nS\\nQ\\nq\\n1 0 0 1 45 597 cm\\n0.3 w \\n/GS0 gs\\n0 0 m\\n168 0 l\\nS\\nQ\\nBT\\n/GS0 gs\\n/T1_0 1 Tf\\n0.055 Tw 8 0 0 8 45 663 Tm\\n(This section of the FEDERAL REGISTER)Tj\\n0 -1.125 TD\\n(contains notices to the public of the proposed)Tj\\nT*\\n(issuance of rules and regulations. The)Tj\\nT*\\n(purpose of these notices is to give interested)Tj\\nT*\\n(persons an opportunity to participate in the)Tj\\nT*\\n(rule making prior to the adoption of the final)Tj\\n0 Tw T*\\n(rules.)Tj\\n/T1_1 1 Tf\\n0.0544 Tw 18 0 0 18 45 723 Tm\\n(Proposed Rules)Tj\\n/T1_2 1 Tf\\n0.055 Tw 8 0 0 8 399 727 Tm\\n(Federal Register)Tj\\n0 Tw 11 0 0 11 45 750 Tm\\n(47698 )Tj\\n/T1_3 1 Tf\\n0.2175 Tw 8 0 0 8 399 712 Tm\\n(Vol. 85, No. 152 )Tj\\n0 -2.125 TD\\n(Thursday, August 6, 2020 )Tj\\n/T1_1 1 Tf\\n-0.0044 Tw 9 0 0 9 45 581 Tm\\n(DEPARTMENT OF TRANSPORTATION )Tj\\n0 -2 TD\\n(Federal Aviation Administration )Tj\\n0 -2.222 TD\\n(14 CFR Part 39 )Tj\\n-0.0025 Tw 8 0 0 8 45 526.1 Tm\\n([Docket No. FAA\\\\2262020\\\\2260686; Product )Tj\\n0 -1.125 TD\\n(Identifier 2019\\\\226NM\\\\226035\\\\226AD] )Tj\\n0 -2.1 TD\\n(RIN 2120\\\\226AA64 )Tj\\n-0.0044 Tw 9 0 0 9 45 482.3 Tm\\n(Airworthiness Directives; The Boeing )Tj\\n0 -1.111 TD\\n(Company Airplanes )Tj\\n0 Tw 7.2 0 0 7.4445 45 456.3001 Tm\\n ', self.bufpos=0, self.charpos=1, self.context=[], self.curstack=[(0, /b'q')], self.curtype=None, self.fp=<_io.BytesIO object at 0x7f1a360decc0>, self.istream=1, self.results=[], self.streams=[<PDFObjRef:80>, <PDFObjRef:74>, <PDFObjRef:71>, <PDFObjRef:42>, <PDFObjRef:32>, <PDFObjRef:21>, <PDFObjRef:73>, <PDFObjRef:68>])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.add_results(*self.popall())\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def do_q(self) -> None:\n",
      "        self.gstack.append(self.get_current_state())\n",
      "        return\n",
      "do_q(self=<pdfminer.pdfinterp.PDFPageInterpreter object at 0x7f1a47ef73a0>, self.argstack=[], self.csmap=OrderedDict([('DeviceGray', <PDFColorSpace: DeviceGray, ncomponents=1>), ('CalRGB', <PDFColorSpace: CalRGB, ncomponents=3>), ('CalGray', <PDFColorSpace: CalGray, ncomponents=1>), ('Lab', <PDFColorSpace: Lab, ncomponents=3>), ('DeviceRGB', <PDFColorSpace: DeviceRGB, ncomponents=3>), ('DeviceCMYK', <PDFColorSpace: DeviceCMYK, ncomponents=4>), ('Separation', <PDFColorSpace: Separation, ncomponents=1>), ('Indexed', <PDFColorSpace: Indexed, ncomponents=1>), ('Pattern', <PDFColorSpace: Pattern, ncomponents=1>)]), self.ctm=(1, 0, 0, 1, 0, 0), self.curpath=[], self.device=<PDFDevice>, self.fontmap={'T1_0': <PDFType1Font: basefont='Helvetica'>, 'T1_1': <PDFType1Font: basefont='Helvetica-Bold'>, 'T1_2': <PDFType1Font: basefont='Melior-Bold'>, 'T1_3': <PDFType1Font: basefont='Melior'>, 'T1_4': <PDFType1Font: basefont='Symbol'>, 'T1_5': <PDFType1Font: basefont='Melior-Italic'>}, self.graphicstate=<PDFGraphicState: linewidth=0, linecap=None, linejoin=None,  miterlimit=None, dash=None, intent=None, flatness=None,  stroking color=None, non stroking color=None>, self.gstack=[], self.ncs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.resources={'ExtGState': {'GS0': <PDFObjRef:17>}, 'Font': {'T1_0': <PDFObjRef:56>, 'T1_1': <PDFObjRef:70>, 'T1_2': <PDFObjRef:60>, 'T1_3': <PDFObjRef:65>, 'T1_4': <PDFObjRef:36>, 'T1_5': <PDFObjRef:6>}, 'ProcSet': [/'PDF', /'Text']}, self.rsrcmgr=<pdfminer.pdfinterp.PDFResourceManager object at 0x7f1a47ef77f0>, self.scs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.textstate=<PDFTextState: font=None, fontsize=0, charspace=0, wordspace=0, scaling=100, leading=0, render=0, rise=0, matrix=(1, 0, 0, 1, 0, 0), linematrix=(0, 0)>, self.xobjmap={})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.gstack.append(self.get_current_state())\n",
      "State:\n",
      "[((1, 0, 0, 1, 0, 0), <PDFTextState: font=None, fontsize=0, charspace=0, wordspace=0, scaling=100, leading=0, render=0, rise=0, matrix=(1, 0, 0, 1, 0, 0), linematrix=(0, 0)>, <PDFGraphicState: linewidth=0, linecap=None, linejoin=None,  miterlimit=None, dash=None, intent=None, flatness=None,  stroking color=None, non stroking color=None>)]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_float(self, s: bytes, i: int) -> int:\n",
      "        m = END_NUMBER.search(s, i)\n",
      "        if not m:\n",
      "            self._curtoken += s[i:]\n",
      "            return len(s)\n",
      "        j = m.start(0)\n",
      "        self._curtoken += s[i:j]\n",
      "        try:\n",
      "            self._add_token(float(self._curtoken))\n",
      "        except ValueError:\n",
      "            pass\n",
      "        self._parse1 = self._parse_main\n",
      "        return j\n",
      "_parse_float(self=<PDFContentParser: <_io.BytesIO object at 0x7f1a360decc0>, bufpos=0>, s=b'q\\n1 0 0 1 45 744 cm\\n2.2 w \\n/GS0 gs\\n0 0 m\\n522 0 l\\nS\\nQ\\nq\\n1 0 0 1 45 740 cm\\n0.3 w \\n/GS0 gs\\n0 0 m\\n522 0 l\\nS\\nQ\\nq\\n1 0 0 1 45 683 cm\\n0.3 w \\n/GS0 gs\\n0 0 m\\n522 0 l\\nS\\nQ\\nq\\n1 0 0 1 45 600 cm\\n1.2 w \\n/GS0 gs\\n0 0 m\\n168 0 l\\nS\\nQ\\nq\\n1 0 0 1 45 597 cm\\n0.3 w \\n/GS0 gs\\n0 0 m\\n168 0 l\\nS\\nQ\\nBT\\n/GS0 gs\\n/T1_0 1 Tf\\n0.055 Tw 8 0 0 8 45 663 Tm\\n(This section of the FEDERAL REGISTER)Tj\\n0 -1.125 TD\\n(contains notices to the public of the proposed)Tj\\nT*\\n(issuance of rules and regulations. The)Tj\\nT*\\n(purpose of these notices is to give interested)Tj\\nT*\\n(persons an opportunity to participate in the)Tj\\nT*\\n(rule making prior to the adoption of the final)Tj\\n0 Tw T*\\n(rules.)Tj\\n/T1_1 1 Tf\\n0.0544 Tw 18 0 0 18 45 723 Tm\\n(Proposed Rules)Tj\\n/T1_2 1 Tf\\n0.055 Tw 8 0 0 8 399 727 Tm\\n(Federal Register)Tj\\n0 Tw 11 0 0 11 45 750 Tm\\n(47698 )Tj\\n/T1_3 1 Tf\\n0.2175 Tw 8 0 0 8 399 712 Tm\\n(Vol. 85, No. 152 )Tj\\n0 -2.125 TD\\n(Thursday, August 6, 2020 )Tj\\n/T1_1 1 Tf\\n-0.0044 Tw 9 0 0 9 45 581 Tm\\n(DEPARTMENT OF TRANSPORTATION )Tj\\n0 -2 TD\\n(Federal Aviation Administration )Tj\\n0 -2.222 TD\\n(14 CFR Part 39 )Tj\\n-0.0025 Tw 8 0 0 8 45 526.1 Tm\\n([Docket No. FAA\\\\2262020\\\\2260686; Product )Tj\\n0 -1.125 TD\\n(Identifier 2019\\\\226NM\\\\226035\\\\226AD] )Tj\\n0 -2.1 TD\\n(RIN 2120\\\\226AA64 )Tj\\n-0.0044 Tw 9 0 0 9 45 482.3 Tm\\n(Airworthiness Directives; The Boeing )Tj\\n0 -1.111 TD\\n(Company Airplanes )Tj\\n0 Tw 7.2 0 0 7.4445 45 456.3001 Tm\\n ', i=22, self._curtoken=b'2.', self._curtokenpos=20, self._parse1=<bound method PSBaseParser._parse_float of <PDFContentParser: <_io.BytesIO object at 0x7f1a360decc0>, bufpos=0>>, self._tokens=[], self.buf=b'q\\n1 0 0 1 45 744 cm\\n2.2 w \\n/GS0 gs\\n0 0 m\\n522 0 l\\nS\\nQ\\nq\\n1 0 0 1 45 740 cm\\n0.3 w \\n/GS0 gs\\n0 0 m\\n522 0 l\\nS\\nQ\\nq\\n1 0 0 1 45 683 cm\\n0.3 w \\n/GS0 gs\\n0 0 m\\n522 0 l\\nS\\nQ\\nq\\n1 0 0 1 45 600 cm\\n1.2 w \\n/GS0 gs\\n0 0 m\\n168 0 l\\nS\\nQ\\nq\\n1 0 0 1 45 597 cm\\n0.3 w \\n/GS0 gs\\n0 0 m\\n168 0 l\\nS\\nQ\\nBT\\n/GS0 gs\\n/T1_0 1 Tf\\n0.055 Tw 8 0 0 8 45 663 Tm\\n(This section of the FEDERAL REGISTER)Tj\\n0 -1.125 TD\\n(contains notices to the public of the proposed)Tj\\nT*\\n(issuance of rules and regulations. The)Tj\\nT*\\n(purpose of these notices is to give interested)Tj\\nT*\\n(persons an opportunity to participate in the)Tj\\nT*\\n(rule making prior to the adoption of the final)Tj\\n0 Tw T*\\n(rules.)Tj\\n/T1_1 1 Tf\\n0.0544 Tw 18 0 0 18 45 723 Tm\\n(Proposed Rules)Tj\\n/T1_2 1 Tf\\n0.055 Tw 8 0 0 8 399 727 Tm\\n(Federal Register)Tj\\n0 Tw 11 0 0 11 45 750 Tm\\n(47698 )Tj\\n/T1_3 1 Tf\\n0.2175 Tw 8 0 0 8 399 712 Tm\\n(Vol. 85, No. 152 )Tj\\n0 -2.125 TD\\n(Thursday, August 6, 2020 )Tj\\n/T1_1 1 Tf\\n-0.0044 Tw 9 0 0 9 45 581 Tm\\n(DEPARTMENT OF TRANSPORTATION )Tj\\n0 -2 TD\\n(Federal Aviation Administration )Tj\\n0 -2.222 TD\\n(14 CFR Part 39 )Tj\\n-0.0025 Tw 8 0 0 8 45 526.1 Tm\\n([Docket No. FAA\\\\2262020\\\\2260686; Product )Tj\\n0 -1.125 TD\\n(Identifier 2019\\\\226NM\\\\226035\\\\226AD] )Tj\\n0 -2.1 TD\\n(RIN 2120\\\\226AA64 )Tj\\n-0.0044 Tw 9 0 0 9 45 482.3 Tm\\n(Airworthiness Directives; The Boeing )Tj\\n0 -1.111 TD\\n(Company Airplanes )Tj\\n0 Tw 7.2 0 0 7.4445 45 456.3001 Tm\\n ', self.bufpos=0, self.charpos=22, self.context=[], self.curstack=[], self.curtype=None, self.fp=<_io.BytesIO object at 0x7f1a360decc0>, self.istream=1, self.results=[], self.streams=[<PDFObjRef:80>, <PDFObjRef:74>, <PDFObjRef:71>, <PDFObjRef:42>, <PDFObjRef:32>, <PDFObjRef:21>, <PDFObjRef:73>, <PDFObjRef:68>])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "m = END_NUMBER.search(s, i)\n",
      "State:\n",
      "<re.Match object; span=(23, 24), match=b' '>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_float(self, s: bytes, i: int) -> int:\n",
      "        m = END_NUMBER.search(s, i)\n",
      "        if not m:\n",
      "            self._curtoken += s[i:]\n",
      "            return len(s)\n",
      "        j = m.start(0)\n",
      "        self._curtoken += s[i:j]\n",
      "        try:\n",
      "            self._add_token(float(self._curtoken))\n",
      "        except ValueError:\n",
      "            pass\n",
      "        self._parse1 = self._parse_main\n",
      "        return j\n",
      "_parse_float(self=<PDFContentParser: <_io.BytesIO object at 0x7f1a360decc0>, bufpos=0>, s=b'q\\n1 0 0 1 45 744 cm\\n2.2 w \\n/GS0 gs\\n0 0 m\\n522 0 l\\nS\\nQ\\nq\\n1 0 0 1 45 740 cm\\n0.3 w \\n/GS0 gs\\n0 0 m\\n522 0 l\\nS\\nQ\\nq\\n1 0 0 1 45 683 cm\\n0.3 w \\n/GS0 gs\\n0 0 m\\n522 0 l\\nS\\nQ\\nq\\n1 0 0 1 45 600 cm\\n1.2 w \\n/GS0 gs\\n0 0 m\\n168 0 l\\nS\\nQ\\nq\\n1 0 0 1 45 597 cm\\n0.3 w \\n/GS0 gs\\n0 0 m\\n168 0 l\\nS\\nQ\\nBT\\n/GS0 gs\\n/T1_0 1 Tf\\n0.055 Tw 8 0 0 8 45 663 Tm\\n(This section of the FEDERAL REGISTER)Tj\\n0 -1.125 TD\\n(contains notices to the public of the proposed)Tj\\nT*\\n(issuance of rules and regulations. The)Tj\\nT*\\n(purpose of these notices is to give interested)Tj\\nT*\\n(persons an opportunity to participate in the)Tj\\nT*\\n(rule making prior to the adoption of the final)Tj\\n0 Tw T*\\n(rules.)Tj\\n/T1_1 1 Tf\\n0.0544 Tw 18 0 0 18 45 723 Tm\\n(Proposed Rules)Tj\\n/T1_2 1 Tf\\n0.055 Tw 8 0 0 8 399 727 Tm\\n(Federal Register)Tj\\n0 Tw 11 0 0 11 45 750 Tm\\n(47698 )Tj\\n/T1_3 1 Tf\\n0.2175 Tw 8 0 0 8 399 712 Tm\\n(Vol. 85, No. 152 )Tj\\n0 -2.125 TD\\n(Thursday, August 6, 2020 )Tj\\n/T1_1 1 Tf\\n-0.0044 Tw 9 0 0 9 45 581 Tm\\n(DEPARTMENT OF TRANSPORTATION )Tj\\n0 -2 TD\\n(Federal Aviation Administration )Tj\\n0 -2.222 TD\\n(14 CFR Part 39 )Tj\\n-0.0025 Tw 8 0 0 8 45 526.1 Tm\\n([Docket No. FAA\\\\2262020\\\\2260686; Product )Tj\\n0 -1.125 TD\\n(Identifier 2019\\\\226NM\\\\226035\\\\226AD] )Tj\\n0 -2.1 TD\\n(RIN 2120\\\\226AA64 )Tj\\n-0.0044 Tw 9 0 0 9 45 482.3 Tm\\n(Airworthiness Directives; The Boeing )Tj\\n0 -1.111 TD\\n(Company Airplanes )Tj\\n0 Tw 7.2 0 0 7.4445 45 456.3001 Tm\\n ', i=22, self._curtoken=b'2.', self._curtokenpos=20, self._parse1=<bound method PSBaseParser._parse_float of <PDFContentParser: <_io.BytesIO object at 0x7f1a360decc0>, bufpos=0>>, self._tokens=[], self.buf=b'q\\n1 0 0 1 45 744 cm\\n2.2 w \\n/GS0 gs\\n0 0 m\\n522 0 l\\nS\\nQ\\nq\\n1 0 0 1 45 740 cm\\n0.3 w \\n/GS0 gs\\n0 0 m\\n522 0 l\\nS\\nQ\\nq\\n1 0 0 1 45 683 cm\\n0.3 w \\n/GS0 gs\\n0 0 m\\n522 0 l\\nS\\nQ\\nq\\n1 0 0 1 45 600 cm\\n1.2 w \\n/GS0 gs\\n0 0 m\\n168 0 l\\nS\\nQ\\nq\\n1 0 0 1 45 597 cm\\n0.3 w \\n/GS0 gs\\n0 0 m\\n168 0 l\\nS\\nQ\\nBT\\n/GS0 gs\\n/T1_0 1 Tf\\n0.055 Tw 8 0 0 8 45 663 Tm\\n(This section of the FEDERAL REGISTER)Tj\\n0 -1.125 TD\\n(contains notices to the public of the proposed)Tj\\nT*\\n(issuance of rules and regulations. The)Tj\\nT*\\n(purpose of these notices is to give interested)Tj\\nT*\\n(persons an opportunity to participate in the)Tj\\nT*\\n(rule making prior to the adoption of the final)Tj\\n0 Tw T*\\n(rules.)Tj\\n/T1_1 1 Tf\\n0.0544 Tw 18 0 0 18 45 723 Tm\\n(Proposed Rules)Tj\\n/T1_2 1 Tf\\n0.055 Tw 8 0 0 8 399 727 Tm\\n(Federal Register)Tj\\n0 Tw 11 0 0 11 45 750 Tm\\n(47698 )Tj\\n/T1_3 1 Tf\\n0.2175 Tw 8 0 0 8 399 712 Tm\\n(Vol. 85, No. 152 )Tj\\n0 -2.125 TD\\n(Thursday, August 6, 2020 )Tj\\n/T1_1 1 Tf\\n-0.0044 Tw 9 0 0 9 45 581 Tm\\n(DEPARTMENT OF TRANSPORTATION )Tj\\n0 -2 TD\\n(Federal Aviation Administration )Tj\\n0 -2.222 TD\\n(14 CFR Part 39 )Tj\\n-0.0025 Tw 8 0 0 8 45 526.1 Tm\\n([Docket No. FAA\\\\2262020\\\\2260686; Product )Tj\\n0 -1.125 TD\\n(Identifier 2019\\\\226NM\\\\226035\\\\226AD] )Tj\\n0 -2.1 TD\\n(RIN 2120\\\\226AA64 )Tj\\n-0.0044 Tw 9 0 0 9 45 482.3 Tm\\n(Airworthiness Directives; The Boeing )Tj\\n0 -1.111 TD\\n(Company Airplanes )Tj\\n0 Tw 7.2 0 0 7.4445 45 456.3001 Tm\\n ', self.bufpos=0, self.charpos=22, self.context=[], self.curstack=[], self.curtype=None, self.fp=<_io.BytesIO object at 0x7f1a360decc0>, self.istream=1, self.results=[], self.streams=[<PDFObjRef:80>, <PDFObjRef:74>, <PDFObjRef:71>, <PDFObjRef:42>, <PDFObjRef:32>, <PDFObjRef:21>, <PDFObjRef:73>, <PDFObjRef:68>])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "j = m.start(0)\n",
      "State:\n",
      "23\n",
      "==================================================\n",
      "Clean Code:\n",
      "def do_w(self, linewidth: PDFStackT) -> None:\n",
      "        self.graphicstate.linewidth = cast(float, linewidth)\n",
      "        return\n",
      "do_w(self=<pdfminer.pdfinterp.PDFPageInterpreter object at 0x7f1a47ef73a0>, linewidth=2.2, self.argstack=[], self.csmap=OrderedDict([('DeviceGray', <PDFColorSpace: DeviceGray, ncomponents=1>), ('CalRGB', <PDFColorSpace: CalRGB, ncomponents=3>), ('CalGray', <PDFColorSpace: CalGray, ncomponents=1>), ('Lab', <PDFColorSpace: Lab, ncomponents=3>), ('DeviceRGB', <PDFColorSpace: DeviceRGB, ncomponents=3>), ('DeviceCMYK', <PDFColorSpace: DeviceCMYK, ncomponents=4>), ('Separation', <PDFColorSpace: Separation, ncomponents=1>), ('Indexed', <PDFColorSpace: Indexed, ncomponents=1>), ('Pattern', <PDFColorSpace: Pattern, ncomponents=1>)]), self.ctm=(1, 0, 0, 1, 45, 744), self.curpath=[], self.device=<PDFDevice>, self.fontmap={'T1_0': <PDFType1Font: basefont='Helvetica'>, 'T1_1': <PDFType1Font: basefont='Helvetica-Bold'>, 'T1_2': <PDFType1Font: basefont='Melior-Bold'>, 'T1_3': <PDFType1Font: basefont='Melior'>, 'T1_4': <PDFType1Font: basefont='Symbol'>, 'T1_5': <PDFType1Font: basefont='Melior-Italic'>}, self.graphicstate=<PDFGraphicState: linewidth=0, linecap=None, linejoin=None,  miterlimit=None, dash=None, intent=None, flatness=None,  stroking color=None, non stroking color=None>, self.gstack=[((1, 0, 0, 1, 0, 0), <PDFTextState: font=None, fontsize=0, charspace=0, wordspace=0, scaling=100, leading=0, render=0, rise=0, matrix=(1, 0, 0, 1, 0, 0), linematrix=(0, 0)>, <PDFGraphicState: linewidth=0, linecap=None, linejoin=None,  miterlimit=None, dash=None, intent=None, flatness=None,  stroking color=None, non stroking color=None>)], self.ncs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.resources={'ExtGState': {'GS0': <PDFObjRef:17>}, 'Font': {'T1_0': <PDFObjRef:56>, 'T1_1': <PDFObjRef:70>, 'T1_2': <PDFObjRef:60>, 'T1_3': <PDFObjRef:65>, 'T1_4': <PDFObjRef:36>, 'T1_5': <PDFObjRef:6>}, 'ProcSet': [/'PDF', /'Text']}, self.rsrcmgr=<pdfminer.pdfinterp.PDFResourceManager object at 0x7f1a47ef77f0>, self.scs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.textstate=<PDFTextState: font=None, fontsize=0, charspace=0, wordspace=0, scaling=100, leading=0, render=0, rise=0, matrix=(1, 0, 0, 1, 0, 0), linematrix=(0, 0)>, self.xobjmap={})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.graphicstate.linewidth = cast(float, linewidth)\n",
      "State:\n",
      "<PDFGraphicState: linewidth=2.2, linecap=None, linejoin=None,  miterlimit=None, dash=None, intent=None, flatness=None,  stroking color=None, non stroking color=None>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def do_l(self, x: PDFStackT, y: PDFStackT) -> None:\n",
      "        self.curpath.append((\"l\", cast(float, x), cast(float, y)))\n",
      "        return\n",
      "do_l(self=<pdfminer.pdfinterp.PDFPageInterpreter object at 0x7f1a47ef73a0>, x=522, y=0, self.argstack=[], self.csmap=OrderedDict([('DeviceGray', <PDFColorSpace: DeviceGray, ncomponents=1>), ('CalRGB', <PDFColorSpace: CalRGB, ncomponents=3>), ('CalGray', <PDFColorSpace: CalGray, ncomponents=1>), ('Lab', <PDFColorSpace: Lab, ncomponents=3>), ('DeviceRGB', <PDFColorSpace: DeviceRGB, ncomponents=3>), ('DeviceCMYK', <PDFColorSpace: DeviceCMYK, ncomponents=4>), ('Separation', <PDFColorSpace: Separation, ncomponents=1>), ('Indexed', <PDFColorSpace: Indexed, ncomponents=1>), ('Pattern', <PDFColorSpace: Pattern, ncomponents=1>)]), self.ctm=(1, 0, 0, 1, 45, 744), self.curpath=[('m', 0, 0)], self.device=<PDFDevice>, self.fontmap={'T1_0': <PDFType1Font: basefont='Helvetica'>, 'T1_1': <PDFType1Font: basefont='Helvetica-Bold'>, 'T1_2': <PDFType1Font: basefont='Melior-Bold'>, 'T1_3': <PDFType1Font: basefont='Melior'>, 'T1_4': <PDFType1Font: basefont='Symbol'>, 'T1_5': <PDFType1Font: basefont='Melior-Italic'>}, self.graphicstate=<PDFGraphicState: linewidth=2.2, linecap=None, linejoin=None,  miterlimit=None, dash=None, intent=None, flatness=None,  stroking color=None, non stroking color=None>, self.gstack=[((1, 0, 0, 1, 0, 0), <PDFTextState: font=None, fontsize=0, charspace=0, wordspace=0, scaling=100, leading=0, render=0, rise=0, matrix=(1, 0, 0, 1, 0, 0), linematrix=(0, 0)>, <PDFGraphicState: linewidth=0, linecap=None, linejoin=None,  miterlimit=None, dash=None, intent=None, flatness=None,  stroking color=None, non stroking color=None>)], self.ncs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.resources={'ExtGState': {'GS0': <PDFObjRef:17>}, 'Font': {'T1_0': <PDFObjRef:56>, 'T1_1': <PDFObjRef:70>, 'T1_2': <PDFObjRef:60>, 'T1_3': <PDFObjRef:65>, 'T1_4': <PDFObjRef:36>, 'T1_5': <PDFObjRef:6>}, 'ProcSet': [/'PDF', /'Text']}, self.rsrcmgr=<pdfminer.pdfinterp.PDFResourceManager object at 0x7f1a47ef77f0>, self.scs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.textstate=<PDFTextState: font=None, fontsize=0, charspace=0, wordspace=0, scaling=100, leading=0, render=0, rise=0, matrix=(1, 0, 0, 1, 0, 0), linematrix=(0, 0)>, self.xobjmap={})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.curpath.append((\"l\", cast(float, x), cast(float, y)))\n",
      "State:\n",
      "[('m', 0, 0), ('l', 522, 0)]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def do_S(self) -> None:\n",
      "        self.device.paint_path(self.graphicstate, True, False, False, self.curpath)\n",
      "        self.curpath = []\n",
      "        return\n",
      "do_S(self=<pdfminer.pdfinterp.PDFPageInterpreter object at 0x7f1a47ef73a0>, self.argstack=[], self.csmap=OrderedDict([('DeviceGray', <PDFColorSpace: DeviceGray, ncomponents=1>), ('CalRGB', <PDFColorSpace: CalRGB, ncomponents=3>), ('CalGray', <PDFColorSpace: CalGray, ncomponents=1>), ('Lab', <PDFColorSpace: Lab, ncomponents=3>), ('DeviceRGB', <PDFColorSpace: DeviceRGB, ncomponents=3>), ('DeviceCMYK', <PDFColorSpace: DeviceCMYK, ncomponents=4>), ('Separation', <PDFColorSpace: Separation, ncomponents=1>), ('Indexed', <PDFColorSpace: Indexed, ncomponents=1>), ('Pattern', <PDFColorSpace: Pattern, ncomponents=1>)]), self.ctm=(1, 0, 0, 1, 45, 744), self.curpath=[('m', 0, 0), ('l', 522, 0)], self.device=<PDFDevice>, self.fontmap={'T1_0': <PDFType1Font: basefont='Helvetica'>, 'T1_1': <PDFType1Font: basefont='Helvetica-Bold'>, 'T1_2': <PDFType1Font: basefont='Melior-Bold'>, 'T1_3': <PDFType1Font: basefont='Melior'>, 'T1_4': <PDFType1Font: basefont='Symbol'>, 'T1_5': <PDFType1Font: basefont='Melior-Italic'>}, self.graphicstate=<PDFGraphicState: linewidth=2.2, linecap=None, linejoin=None,  miterlimit=None, dash=None, intent=None, flatness=None,  stroking color=None, non stroking color=None>, self.gstack=[((1, 0, 0, 1, 0, 0), <PDFTextState: font=None, fontsize=0, charspace=0, wordspace=0, scaling=100, leading=0, render=0, rise=0, matrix=(1, 0, 0, 1, 0, 0), linematrix=(0, 0)>, <PDFGraphicState: linewidth=0, linecap=None, linejoin=None,  miterlimit=None, dash=None, intent=None, flatness=None,  stroking color=None, non stroking color=None>)], self.ncs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.resources={'ExtGState': {'GS0': <PDFObjRef:17>}, 'Font': {'T1_0': <PDFObjRef:56>, 'T1_1': <PDFObjRef:70>, 'T1_2': <PDFObjRef:60>, 'T1_3': <PDFObjRef:65>, 'T1_4': <PDFObjRef:36>, 'T1_5': <PDFObjRef:6>}, 'ProcSet': [/'PDF', /'Text']}, self.rsrcmgr=<pdfminer.pdfinterp.PDFResourceManager object at 0x7f1a47ef77f0>, self.scs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.textstate=<PDFTextState: font=None, fontsize=0, charspace=0, wordspace=0, scaling=100, leading=0, render=0, rise=0, matrix=(1, 0, 0, 1, 0, 0), linematrix=(0, 0)>, self.xobjmap={})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.curpath = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def do_Q(self) -> None:\n",
      "        if self.gstack:\n",
      "            self.set_current_state(self.gstack.pop())\n",
      "        return\n",
      "do_Q(self=<pdfminer.pdfinterp.PDFPageInterpreter object at 0x7f1a47ef73a0>, self.argstack=[], self.csmap=OrderedDict([('DeviceGray', <PDFColorSpace: DeviceGray, ncomponents=1>), ('CalRGB', <PDFColorSpace: CalRGB, ncomponents=3>), ('CalGray', <PDFColorSpace: CalGray, ncomponents=1>), ('Lab', <PDFColorSpace: Lab, ncomponents=3>), ('DeviceRGB', <PDFColorSpace: DeviceRGB, ncomponents=3>), ('DeviceCMYK', <PDFColorSpace: DeviceCMYK, ncomponents=4>), ('Separation', <PDFColorSpace: Separation, ncomponents=1>), ('Indexed', <PDFColorSpace: Indexed, ncomponents=1>), ('Pattern', <PDFColorSpace: Pattern, ncomponents=1>)]), self.ctm=(1, 0, 0, 1, 45, 744), self.curpath=[], self.device=<PDFDevice>, self.fontmap={'T1_0': <PDFType1Font: basefont='Helvetica'>, 'T1_1': <PDFType1Font: basefont='Helvetica-Bold'>, 'T1_2': <PDFType1Font: basefont='Melior-Bold'>, 'T1_3': <PDFType1Font: basefont='Melior'>, 'T1_4': <PDFType1Font: basefont='Symbol'>, 'T1_5': <PDFType1Font: basefont='Melior-Italic'>}, self.graphicstate=<PDFGraphicState: linewidth=2.2, linecap=None, linejoin=None,  miterlimit=None, dash=None, intent=None, flatness=None,  stroking color=None, non stroking color=None>, self.gstack=[((1, 0, 0, 1, 0, 0), <PDFTextState: font=None, fontsize=0, charspace=0, wordspace=0, scaling=100, leading=0, render=0, rise=0, matrix=(1, 0, 0, 1, 0, 0), linematrix=(0, 0)>, <PDFGraphicState: linewidth=0, linecap=None, linejoin=None,  miterlimit=None, dash=None, intent=None, flatness=None,  stroking color=None, non stroking color=None>)], self.ncs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.resources={'ExtGState': {'GS0': <PDFObjRef:17>}, 'Font': {'T1_0': <PDFObjRef:56>, 'T1_1': <PDFObjRef:70>, 'T1_2': <PDFObjRef:60>, 'T1_3': <PDFObjRef:65>, 'T1_4': <PDFObjRef:36>, 'T1_5': <PDFObjRef:6>}, 'ProcSet': [/'PDF', /'Text']}, self.rsrcmgr=<pdfminer.pdfinterp.PDFResourceManager object at 0x7f1a47ef77f0>, self.scs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.textstate=<PDFTextState: font=None, fontsize=0, charspace=0, wordspace=0, scaling=100, leading=0, render=0, rise=0, matrix=(1, 0, 0, 1, 0, 0), linematrix=(0, 0)>, self.xobjmap={})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.set_current_state(self.gstack.pop())\n",
      "State:\n",
      "(1, 0, 0, 1, 0, 0)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def do_Tj(self, s: PDFStackT) -> None:\n",
      "        self.do_TJ([s])\n",
      "        return\n",
      "do_Tj(self=<pdfminer.pdfinterp.PDFPageInterpreter object at 0x7f1a47ef73a0>, s=b'This section of the FEDERAL REGISTER', self.argstack=[], self.csmap=OrderedDict([('DeviceGray', <PDFColorSpace: DeviceGray, ncomponents=1>), ('CalRGB', <PDFColorSpace: CalRGB, ncomponents=3>), ('CalGray', <PDFColorSpace: CalGray, ncomponents=1>), ('Lab', <PDFColorSpace: Lab, ncomponents=3>), ('DeviceRGB', <PDFColorSpace: DeviceRGB, ncomponents=3>), ('DeviceCMYK', <PDFColorSpace: DeviceCMYK, ncomponents=4>), ('Separation', <PDFColorSpace: Separation, ncomponents=1>), ('Indexed', <PDFColorSpace: Indexed, ncomponents=1>), ('Pattern', <PDFColorSpace: Pattern, ncomponents=1>)]), self.ctm=(1, 0, 0, 1, 0, 0), self.curpath=[], self.device=<PDFDevice>, self.fontmap={'T1_0': <PDFType1Font: basefont='Helvetica'>, 'T1_1': <PDFType1Font: basefont='Helvetica-Bold'>, 'T1_2': <PDFType1Font: basefont='Melior-Bold'>, 'T1_3': <PDFType1Font: basefont='Melior'>, 'T1_4': <PDFType1Font: basefont='Symbol'>, 'T1_5': <PDFType1Font: basefont='Melior-Italic'>}, self.graphicstate=<PDFGraphicState: linewidth=0, linecap=None, linejoin=None,  miterlimit=None, dash=None, intent=None, flatness=None,  stroking color=None, non stroking color=None>, self.gstack=[], self.ncs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.resources={'ExtGState': {'GS0': <PDFObjRef:17>}, 'Font': {'T1_0': <PDFObjRef:56>, 'T1_1': <PDFObjRef:70>, 'T1_2': <PDFObjRef:60>, 'T1_3': <PDFObjRef:65>, 'T1_4': <PDFObjRef:36>, 'T1_5': <PDFObjRef:6>}, 'ProcSet': [/'PDF', /'Text']}, self.rsrcmgr=<pdfminer.pdfinterp.PDFResourceManager object at 0x7f1a47ef77f0>, self.scs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.textstate=<PDFTextState: font=<PDFType1Font: basefont='Helvetica'>, fontsize=1, charspace=0, wordspace=0.055, scaling=100, leading=0, render=0, rise=0, matrix=(8, 0, 0, 8, 45, 663), linematrix=(0, 0)>, self.xobjmap={})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.do_TJ([s])\n",
      "State:\n",
      "<PDFTextState: font=<PDFType1Font: basefont='Helvetica'>, fontsize=1, charspace=0, wordspace=0.055, scaling=100, leading=0, render=0, rise=0, matrix=(8, 0, 0, 8, 45, 663), linematrix=(18.670000000000005, 0)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def do_TD(self, tx: PDFStackT, ty: PDFStackT) -> None:\n",
      "        tx = cast(float, tx)\n",
      "        ty = cast(float, ty)\n",
      "        (a, b, c, d, e, f) = self.textstate.matrix\n",
      "        self.textstate.matrix = (a, b, c, d, tx * a + ty * c + e, tx * b + ty * d + f)\n",
      "        self.textstate.leading = ty\n",
      "        self.textstate.linematrix = (0, 0)\n",
      "        return\n",
      "do_TD(self=<pdfminer.pdfinterp.PDFPageInterpreter object at 0x7f1a47ef73a0>, tx=0, ty=-1.125, self.argstack=[], self.csmap=OrderedDict([('DeviceGray', <PDFColorSpace: DeviceGray, ncomponents=1>), ('CalRGB', <PDFColorSpace: CalRGB, ncomponents=3>), ('CalGray', <PDFColorSpace: CalGray, ncomponents=1>), ('Lab', <PDFColorSpace: Lab, ncomponents=3>), ('DeviceRGB', <PDFColorSpace: DeviceRGB, ncomponents=3>), ('DeviceCMYK', <PDFColorSpace: DeviceCMYK, ncomponents=4>), ('Separation', <PDFColorSpace: Separation, ncomponents=1>), ('Indexed', <PDFColorSpace: Indexed, ncomponents=1>), ('Pattern', <PDFColorSpace: Pattern, ncomponents=1>)]), self.ctm=(1, 0, 0, 1, 0, 0), self.curpath=[], self.device=<PDFDevice>, self.fontmap={'T1_0': <PDFType1Font: basefont='Helvetica'>, 'T1_1': <PDFType1Font: basefont='Helvetica-Bold'>, 'T1_2': <PDFType1Font: basefont='Melior-Bold'>, 'T1_3': <PDFType1Font: basefont='Melior'>, 'T1_4': <PDFType1Font: basefont='Symbol'>, 'T1_5': <PDFType1Font: basefont='Melior-Italic'>}, self.graphicstate=<PDFGraphicState: linewidth=0, linecap=None, linejoin=None,  miterlimit=None, dash=None, intent=None, flatness=None,  stroking color=None, non stroking color=None>, self.gstack=[], self.ncs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.resources={'ExtGState': {'GS0': <PDFObjRef:17>}, 'Font': {'T1_0': <PDFObjRef:56>, 'T1_1': <PDFObjRef:70>, 'T1_2': <PDFObjRef:60>, 'T1_3': <PDFObjRef:65>, 'T1_4': <PDFObjRef:36>, 'T1_5': <PDFObjRef:6>}, 'ProcSet': [/'PDF', /'Text']}, self.rsrcmgr=<pdfminer.pdfinterp.PDFResourceManager object at 0x7f1a47ef77f0>, self.scs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.textstate=<PDFTextState: font=<PDFType1Font: basefont='Helvetica'>, fontsize=1, charspace=0, wordspace=0.055, scaling=100, leading=0, render=0, rise=0, matrix=(8, 0, 0, 8, 45, 663), linematrix=(18.670000000000005, 0)>, self.xobjmap={})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "(a, b, c, d, e, f) = self.textstate.matrix\n",
      "State:\n",
      "8\n",
      "==================================================\n",
      "Clean Code:\n",
      "def do_TD(self, tx: PDFStackT, ty: PDFStackT) -> None:\n",
      "        tx = cast(float, tx)\n",
      "        ty = cast(float, ty)\n",
      "        (a, b, c, d, e, f) = self.textstate.matrix\n",
      "        self.textstate.matrix = (a, b, c, d, tx * a + ty * c + e, tx * b + ty * d + f)\n",
      "        self.textstate.leading = ty\n",
      "        self.textstate.linematrix = (0, 0)\n",
      "        return\n",
      "do_TD(self=<pdfminer.pdfinterp.PDFPageInterpreter object at 0x7f1a47ef73a0>, tx=0, ty=-1.125, self.argstack=[], self.csmap=OrderedDict([('DeviceGray', <PDFColorSpace: DeviceGray, ncomponents=1>), ('CalRGB', <PDFColorSpace: CalRGB, ncomponents=3>), ('CalGray', <PDFColorSpace: CalGray, ncomponents=1>), ('Lab', <PDFColorSpace: Lab, ncomponents=3>), ('DeviceRGB', <PDFColorSpace: DeviceRGB, ncomponents=3>), ('DeviceCMYK', <PDFColorSpace: DeviceCMYK, ncomponents=4>), ('Separation', <PDFColorSpace: Separation, ncomponents=1>), ('Indexed', <PDFColorSpace: Indexed, ncomponents=1>), ('Pattern', <PDFColorSpace: Pattern, ncomponents=1>)]), self.ctm=(1, 0, 0, 1, 0, 0), self.curpath=[], self.device=<PDFDevice>, self.fontmap={'T1_0': <PDFType1Font: basefont='Helvetica'>, 'T1_1': <PDFType1Font: basefont='Helvetica-Bold'>, 'T1_2': <PDFType1Font: basefont='Melior-Bold'>, 'T1_3': <PDFType1Font: basefont='Melior'>, 'T1_4': <PDFType1Font: basefont='Symbol'>, 'T1_5': <PDFType1Font: basefont='Melior-Italic'>}, self.graphicstate=<PDFGraphicState: linewidth=0, linecap=None, linejoin=None,  miterlimit=None, dash=None, intent=None, flatness=None,  stroking color=None, non stroking color=None>, self.gstack=[], self.ncs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.resources={'ExtGState': {'GS0': <PDFObjRef:17>}, 'Font': {'T1_0': <PDFObjRef:56>, 'T1_1': <PDFObjRef:70>, 'T1_2': <PDFObjRef:60>, 'T1_3': <PDFObjRef:65>, 'T1_4': <PDFObjRef:36>, 'T1_5': <PDFObjRef:6>}, 'ProcSet': [/'PDF', /'Text']}, self.rsrcmgr=<pdfminer.pdfinterp.PDFResourceManager object at 0x7f1a47ef77f0>, self.scs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.textstate=<PDFTextState: font=<PDFType1Font: basefont='Helvetica'>, fontsize=1, charspace=0, wordspace=0.055, scaling=100, leading=0, render=0, rise=0, matrix=(8, 0, 0, 8, 45, 663), linematrix=(18.670000000000005, 0)>, self.xobjmap={})\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "self.textstate.matrix = (a, b, c, d, tx * a + ty * c + e, tx * b + ty * d + f)\n",
      "State:\n",
      "<PDFTextState: font=<PDFType1Font: basefont='Helvetica'>, fontsize=1, charspace=0, wordspace=0.055, scaling=100, leading=0, render=0, rise=0, matrix=(8, 0, 0, 8, 45.0, 654.0), linematrix=(18.670000000000005, 0)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def do_T_a(self) -> None:\n",
      "        (a, b, c, d, e, f) = self.textstate.matrix\n",
      "        self.textstate.matrix = (\n",
      "            a,\n",
      "            b,\n",
      "            c,\n",
      "            d,\n",
      "            self.textstate.leading * c + e,\n",
      "            self.textstate.leading * d + f,\n",
      "        )\n",
      "        self.textstate.linematrix = (0, 0)\n",
      "        return\n",
      "do_T_a(self=<pdfminer.pdfinterp.PDFPageInterpreter object at 0x7f1a47ef73a0>, self.argstack=[], self.csmap=OrderedDict([('DeviceGray', <PDFColorSpace: DeviceGray, ncomponents=1>), ('CalRGB', <PDFColorSpace: CalRGB, ncomponents=3>), ('CalGray', <PDFColorSpace: CalGray, ncomponents=1>), ('Lab', <PDFColorSpace: Lab, ncomponents=3>), ('DeviceRGB', <PDFColorSpace: DeviceRGB, ncomponents=3>), ('DeviceCMYK', <PDFColorSpace: DeviceCMYK, ncomponents=4>), ('Separation', <PDFColorSpace: Separation, ncomponents=1>), ('Indexed', <PDFColorSpace: Indexed, ncomponents=1>), ('Pattern', <PDFColorSpace: Pattern, ncomponents=1>)]), self.ctm=(1, 0, 0, 1, 0, 0), self.curpath=[], self.device=<PDFDevice>, self.fontmap={'T1_0': <PDFType1Font: basefont='Helvetica'>, 'T1_1': <PDFType1Font: basefont='Helvetica-Bold'>, 'T1_2': <PDFType1Font: basefont='Melior-Bold'>, 'T1_3': <PDFType1Font: basefont='Melior'>, 'T1_4': <PDFType1Font: basefont='Symbol'>, 'T1_5': <PDFType1Font: basefont='Melior-Italic'>}, self.graphicstate=<PDFGraphicState: linewidth=0, linecap=None, linejoin=None,  miterlimit=None, dash=None, intent=None, flatness=None,  stroking color=None, non stroking color=None>, self.gstack=[], self.ncs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.resources={'ExtGState': {'GS0': <PDFObjRef:17>}, 'Font': {'T1_0': <PDFObjRef:56>, 'T1_1': <PDFObjRef:70>, 'T1_2': <PDFObjRef:60>, 'T1_3': <PDFObjRef:65>, 'T1_4': <PDFObjRef:36>, 'T1_5': <PDFObjRef:6>}, 'ProcSet': [/'PDF', /'Text']}, self.rsrcmgr=<pdfminer.pdfinterp.PDFResourceManager object at 0x7f1a47ef77f0>, self.scs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.textstate=<PDFTextState: font=<PDFType1Font: basefont='Helvetica'>, fontsize=1, charspace=0, wordspace=0.055, scaling=100, leading=-1.125, render=0, rise=0, matrix=(8, 0, 0, 8, 45.0, 654.0), linematrix=(20.45200000000001, 0)>, self.xobjmap={})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "(a, b, c, d, e, f) = self.textstate.matrix\n",
      "State:\n",
      "8\n",
      "==================================================\n",
      "Clean Code:\n",
      "def do_T_a(self) -> None:\n",
      "        (a, b, c, d, e, f) = self.textstate.matrix\n",
      "        self.textstate.matrix = (\n",
      "            a,\n",
      "            b,\n",
      "            c,\n",
      "            d,\n",
      "            self.textstate.leading * c + e,\n",
      "            self.textstate.leading * d + f,\n",
      "        )\n",
      "        self.textstate.linematrix = (0, 0)\n",
      "        return\n",
      "do_T_a(self=<pdfminer.pdfinterp.PDFPageInterpreter object at 0x7f1a47ef73a0>, self.argstack=[], self.csmap=OrderedDict([('DeviceGray', <PDFColorSpace: DeviceGray, ncomponents=1>), ('CalRGB', <PDFColorSpace: CalRGB, ncomponents=3>), ('CalGray', <PDFColorSpace: CalGray, ncomponents=1>), ('Lab', <PDFColorSpace: Lab, ncomponents=3>), ('DeviceRGB', <PDFColorSpace: DeviceRGB, ncomponents=3>), ('DeviceCMYK', <PDFColorSpace: DeviceCMYK, ncomponents=4>), ('Separation', <PDFColorSpace: Separation, ncomponents=1>), ('Indexed', <PDFColorSpace: Indexed, ncomponents=1>), ('Pattern', <PDFColorSpace: Pattern, ncomponents=1>)]), self.ctm=(1, 0, 0, 1, 0, 0), self.curpath=[], self.device=<PDFDevice>, self.fontmap={'T1_0': <PDFType1Font: basefont='Helvetica'>, 'T1_1': <PDFType1Font: basefont='Helvetica-Bold'>, 'T1_2': <PDFType1Font: basefont='Melior-Bold'>, 'T1_3': <PDFType1Font: basefont='Melior'>, 'T1_4': <PDFType1Font: basefont='Symbol'>, 'T1_5': <PDFType1Font: basefont='Melior-Italic'>}, self.graphicstate=<PDFGraphicState: linewidth=0, linecap=None, linejoin=None,  miterlimit=None, dash=None, intent=None, flatness=None,  stroking color=None, non stroking color=None>, self.gstack=[], self.ncs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.resources={'ExtGState': {'GS0': <PDFObjRef:17>}, 'Font': {'T1_0': <PDFObjRef:56>, 'T1_1': <PDFObjRef:70>, 'T1_2': <PDFObjRef:60>, 'T1_3': <PDFObjRef:65>, 'T1_4': <PDFObjRef:36>, 'T1_5': <PDFObjRef:6>}, 'ProcSet': [/'PDF', /'Text']}, self.rsrcmgr=<pdfminer.pdfinterp.PDFResourceManager object at 0x7f1a47ef77f0>, self.scs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.textstate=<PDFTextState: font=<PDFType1Font: basefont='Helvetica'>, fontsize=1, charspace=0, wordspace=0.055, scaling=100, leading=-1.125, render=0, rise=0, matrix=(8, 0, 0, 8, 45.0, 654.0), linematrix=(20.45200000000001, 0)>, self.xobjmap={})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.textstate.matrix = (\n",
      "State:\n",
      "<PDFTextState: font=<PDFType1Font: basefont='Helvetica'>, fontsize=1, charspace=0, wordspace=0.055, scaling=100, leading=-1.125, render=0, rise=0, matrix=(8, 0, 0, 8, 45.0, 645.0), linematrix=(20.45200000000001, 0)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def do_Td(self, tx: PDFStackT, ty: PDFStackT) -> None:\n",
      "        tx = cast(float, tx)\n",
      "        ty = cast(float, ty)\n",
      "        (a, b, c, d, e, f) = self.textstate.matrix\n",
      "        self.textstate.matrix = (a, b, c, d, tx * a + ty * c + e, tx * b + ty * d + f)\n",
      "        self.textstate.linematrix = (0, 0)\n",
      "        return\n",
      "do_Td(self=<pdfminer.pdfinterp.PDFPageInterpreter object at 0x7f1a47ef73a0>, tx=0.607, ty=0, self.argstack=[], self.csmap=OrderedDict([('DeviceGray', <PDFColorSpace: DeviceGray, ncomponents=1>), ('CalRGB', <PDFColorSpace: CalRGB, ncomponents=3>), ('CalGray', <PDFColorSpace: CalGray, ncomponents=1>), ('Lab', <PDFColorSpace: Lab, ncomponents=3>), ('DeviceRGB', <PDFColorSpace: DeviceRGB, ncomponents=3>), ('DeviceCMYK', <PDFColorSpace: DeviceCMYK, ncomponents=4>), ('Separation', <PDFColorSpace: Separation, ncomponents=1>), ('Indexed', <PDFColorSpace: Indexed, ncomponents=1>), ('Pattern', <PDFColorSpace: Pattern, ncomponents=1>)]), self.ctm=(1, 0, 0, 1, 0, 0), self.curpath=[], self.device=<PDFDevice>, self.fontmap={'T1_0': <PDFType1Font: basefont='Helvetica'>, 'T1_1': <PDFType1Font: basefont='Helvetica-Bold'>, 'T1_2': <PDFType1Font: basefont='Melior-Bold'>, 'T1_3': <PDFType1Font: basefont='Melior'>, 'T1_4': <PDFType1Font: basefont='Symbol'>, 'T1_5': <PDFType1Font: basefont='Melior-Italic'>}, self.graphicstate=<PDFGraphicState: linewidth=0, linecap=None, linejoin=None,  miterlimit=None, dash=None, intent=None, flatness=None,  stroking color=None, non stroking color=None>, self.gstack=[], self.ncs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.resources={'ExtGState': {'GS0': <PDFObjRef:17>}, 'Font': {'T1_0': <PDFObjRef:56>, 'T1_1': <PDFObjRef:70>, 'T1_2': <PDFObjRef:60>, 'T1_3': <PDFObjRef:65>, 'T1_4': <PDFObjRef:36>, 'T1_5': <PDFObjRef:6>}, 'ProcSet': [/'PDF', /'Text']}, self.rsrcmgr=<pdfminer.pdfinterp.PDFResourceManager object at 0x7f1a47ef77f0>, self.scs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.textstate=<PDFTextState: font=<PDFType1Font: basefont='Melior'>, fontsize=1, charspace=0, wordspace=-0.0044, scaling=100, leading=-1.111, render=0, rise=0, matrix=(9, 0, 0, 9, 75.8023, 456.3001), linematrix=(0.611, 0)>, self.xobjmap={})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "(a, b, c, d, e, f) = self.textstate.matrix\n",
      "State:\n",
      "9\n",
      "==================================================\n",
      "Clean Code:\n",
      "def do_Td(self, tx: PDFStackT, ty: PDFStackT) -> None:\n",
      "        tx = cast(float, tx)\n",
      "        ty = cast(float, ty)\n",
      "        (a, b, c, d, e, f) = self.textstate.matrix\n",
      "        self.textstate.matrix = (a, b, c, d, tx * a + ty * c + e, tx * b + ty * d + f)\n",
      "        self.textstate.linematrix = (0, 0)\n",
      "        return\n",
      "do_Td(self=<pdfminer.pdfinterp.PDFPageInterpreter object at 0x7f1a47ef73a0>, tx=0.607, ty=0, self.argstack=[], self.csmap=OrderedDict([('DeviceGray', <PDFColorSpace: DeviceGray, ncomponents=1>), ('CalRGB', <PDFColorSpace: CalRGB, ncomponents=3>), ('CalGray', <PDFColorSpace: CalGray, ncomponents=1>), ('Lab', <PDFColorSpace: Lab, ncomponents=3>), ('DeviceRGB', <PDFColorSpace: DeviceRGB, ncomponents=3>), ('DeviceCMYK', <PDFColorSpace: DeviceCMYK, ncomponents=4>), ('Separation', <PDFColorSpace: Separation, ncomponents=1>), ('Indexed', <PDFColorSpace: Indexed, ncomponents=1>), ('Pattern', <PDFColorSpace: Pattern, ncomponents=1>)]), self.ctm=(1, 0, 0, 1, 0, 0), self.curpath=[], self.device=<PDFDevice>, self.fontmap={'T1_0': <PDFType1Font: basefont='Helvetica'>, 'T1_1': <PDFType1Font: basefont='Helvetica-Bold'>, 'T1_2': <PDFType1Font: basefont='Melior-Bold'>, 'T1_3': <PDFType1Font: basefont='Melior'>, 'T1_4': <PDFType1Font: basefont='Symbol'>, 'T1_5': <PDFType1Font: basefont='Melior-Italic'>}, self.graphicstate=<PDFGraphicState: linewidth=0, linecap=None, linejoin=None,  miterlimit=None, dash=None, intent=None, flatness=None,  stroking color=None, non stroking color=None>, self.gstack=[], self.ncs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.resources={'ExtGState': {'GS0': <PDFObjRef:17>}, 'Font': {'T1_0': <PDFObjRef:56>, 'T1_1': <PDFObjRef:70>, 'T1_2': <PDFObjRef:60>, 'T1_3': <PDFObjRef:65>, 'T1_4': <PDFObjRef:36>, 'T1_5': <PDFObjRef:6>}, 'ProcSet': [/'PDF', /'Text']}, self.rsrcmgr=<pdfminer.pdfinterp.PDFResourceManager object at 0x7f1a47ef77f0>, self.scs=<PDFColorSpace: DeviceGray, ncomponents=1>, self.textstate=<PDFTextState: font=<PDFType1Font: basefont='Melior'>, fontsize=1, charspace=0, wordspace=-0.0044, scaling=100, leading=-1.111, render=0, rise=0, matrix=(9, 0, 0, 9, 75.8023, 456.3001), linematrix=(0.611, 0)>, self.xobjmap={})\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "self.textstate.matrix = (a, b, c, d, tx * a + ty * c + e, tx * b + ty * d + f)\n",
      "State:\n",
      "<PDFTextState: font=<PDFType1Font: basefont='Melior'>, fontsize=1, charspace=0, wordspace=-0.0044, scaling=100, leading=-1.111, render=0, rise=0, matrix=(9, 0, 0, 9, 81.2653, 456.3001), linematrix=(0.611, 0)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def fsplit(pred: Callable[[_T], bool], objs: Iterable[_T]) -> Tuple[List[_T], List[_T]]:\n",
      "    t = []\n",
      "    f = []\n",
      "    for obj in objs:\n",
      "        if pred(obj):\n",
      "            t.append(obj)\n",
      "        else:\n",
      "            f.append(obj)\n",
      "    return t, f\n",
      "fsplit(pred=<function LTLayoutContainer.analyze.<locals>.<lambda> at 0x7f1a441ed790>, objs=<LTPage(1) 0.000,0.000,612.000,792.000 rotate=0>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "t = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def step(self, action):\n",
      "        state = self.state\n",
      "        action = np.clip(action, self.action_space.low, self.action_space.high)\n",
      "        self.state = next_state = self._transition_fn(self.state, action)\n",
      "        next_obs = self._get_obs(next_state)\n",
      "        reward = self._reward_fn(state, action, next_state)\n",
      "        done = self._terminal(next_state)\n",
      "        return next_obs, reward, done, {}\n",
      "step(self=<gym_cartpole_swingup.envs.cartpole_swingup.CartPoleSwingUpV0 object at 0x7ff1730d73d0>, action=array([-0.09395223], dtype=float32), self._np_random=Generator(PCG64) at 0x7FF1730609E0, self.action_space=Box(-1.0, 1.0, (1,), float32), self.observation_space=Box(-3.4028235e+38, 3.4028235e+38, (5,), float32), self.params=CartPoleSwingUpParams(gravity=9.82, forcemag=10.0, deltat=0.01, friction=0.1, x_threshold=2.4, cart=CartParams(width=0.3333333333333333, height=0.16666666666666666, mass=0.5), pole=PoleParams(width=0.05, length=0.6, mass=0.5), masstotal=1.0, mpl=0.3), self.spec=EnvSpec(id='CartPoleSwingUp-v0', entry_point='gym_cartpole_swingup.envs.cartpole_swingup:CartPoleSwingUpV0', reward_threshold=None, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CartPoleSwingUp', version=0), self.state=State(x_pos=0.18573049, x_dot=-0.062929064, theta=3.3186514, theta_dot=0.23617047), self.viewer=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "state = self.state\n",
      "State:\n",
      "State(x_pos=0.18573049, x_dot=-0.062929064, theta=3.3186514, theta_dot=0.23617047)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def step(self, action):\n",
      "        state = self.state\n",
      "        action = np.clip(action, self.action_space.low, self.action_space.high)\n",
      "        self.state = next_state = self._transition_fn(self.state, action)\n",
      "        next_obs = self._get_obs(next_state)\n",
      "        reward = self._reward_fn(state, action, next_state)\n",
      "        done = self._terminal(next_state)\n",
      "        return next_obs, reward, done, {}\n",
      "step(self=<gym_cartpole_swingup.envs.cartpole_swingup.CartPoleSwingUpV0 object at 0x7ff1730d73d0>, action=array([-0.09395223], dtype=float32), self._np_random=Generator(PCG64) at 0x7FF1730609E0, self.action_space=Box(-1.0, 1.0, (1,), float32), self.observation_space=Box(-3.4028235e+38, 3.4028235e+38, (5,), float32), self.params=CartPoleSwingUpParams(gravity=9.82, forcemag=10.0, deltat=0.01, friction=0.1, x_threshold=2.4, cart=CartParams(width=0.3333333333333333, height=0.16666666666666666, mass=0.5), pole=PoleParams(width=0.05, length=0.6, mass=0.5), masstotal=1.0, mpl=0.3), self.spec=EnvSpec(id='CartPoleSwingUp-v0', entry_point='gym_cartpole_swingup.envs.cartpole_swingup:CartPoleSwingUpV0', reward_threshold=None, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CartPoleSwingUp', version=0), self.state=State(x_pos=0.18573049, x_dot=-0.062929064, theta=3.3186514, theta_dot=0.23617047), self.viewer=None)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "self.state = next_state = self._transition_fn(self.state, action)\n",
      "State:\n",
      "State(x_pos=0.18510119646787643, x_dot=-0.0675357736946287, theta=3.321013142466545, theta_dot=0.20426602262882)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def step(self, action):\n",
      "        state = self.state\n",
      "        action = np.clip(action, self.action_space.low, self.action_space.high)\n",
      "        self.state = next_state = self._transition_fn(self.state, action)\n",
      "        next_obs = self._get_obs(next_state)\n",
      "        reward = self._reward_fn(state, action, next_state)\n",
      "        done = self._terminal(next_state)\n",
      "        return next_obs, reward, done, {}\n",
      "step(self=<gym_cartpole_swingup.envs.cartpole_swingup.CartPoleSwingUpV0 object at 0x7ff1730d73d0>, action=array([-0.09395223], dtype=float32), self._np_random=Generator(PCG64) at 0x7FF1730609E0, self.action_space=Box(-1.0, 1.0, (1,), float32), self.observation_space=Box(-3.4028235e+38, 3.4028235e+38, (5,), float32), self.params=CartPoleSwingUpParams(gravity=9.82, forcemag=10.0, deltat=0.01, friction=0.1, x_threshold=2.4, cart=CartParams(width=0.3333333333333333, height=0.16666666666666666, mass=0.5), pole=PoleParams(width=0.05, length=0.6, mass=0.5), masstotal=1.0, mpl=0.3), self.spec=EnvSpec(id='CartPoleSwingUp-v0', entry_point='gym_cartpole_swingup.envs.cartpole_swingup:CartPoleSwingUpV0', reward_threshold=None, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CartPoleSwingUp', version=0), self.state=State(x_pos=0.18573049, x_dot=-0.062929064, theta=3.3186514, theta_dot=0.23617047), self.viewer=None)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "next_obs = self._get_obs(next_state)\n",
      "State:\n",
      "array([ 0.1851012 , -0.06753577, -0.9839473 , -0.17845939,  0.20426603],      dtype=float32)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _transition_fn(self, state, action):\n",
      "        action = action[0] * self.params.forcemag\n",
      "        sin_theta = np.sin(state.theta)\n",
      "        cos_theta = np.cos(state.theta)\n",
      "        xdot_update = (\n",
      "            -2 * self.params.mpl * (state.theta_dot ** 2) * sin_theta\n",
      "            + 3 * self.params.pole.mass * self.params.gravity * sin_theta * cos_theta\n",
      "            + 4 * action\n",
      "            - 4 * self.params.friction * state.x_dot\n",
      "        ) / (4 * self.params.masstotal - 3 * self.params.pole.mass * cos_theta ** 2)\n",
      "        thetadot_update = (\n",
      "            -3 * self.params.mpl * (state.theta_dot ** 2) * sin_theta * cos_theta\n",
      "            + 6 * self.params.masstotal * self.params.gravity * sin_theta\n",
      "            + 6 * (action - self.params.friction * state.x_dot) * cos_theta\n",
      "        ) / (\n",
      "            4 * self.params.pole.length * self.params.masstotal\n",
      "            - 3 * self.params.mpl * cos_theta ** 2\n",
      "        )\n",
      "        delta_t = self.params.deltat\n",
      "        return State(\n",
      "            x_pos=state.x_pos + state.x_dot * delta_t,\n",
      "            theta=state.theta + state.theta_dot * delta_t,\n",
      "            x_dot=state.x_dot + xdot_update * delta_t,\n",
      "            theta_dot=state.theta_dot + thetadot_update * delta_t,\n",
      "        )\n",
      "_transition_fn(self=<gym_cartpole_swingup.envs.cartpole_swingup.CartPoleSwingUpV0 object at 0x7ff1730d73d0>, state=State(x_pos=0.18573049, x_dot=-0.062929064, theta=3.3186514, theta_dot=0.23617047), action=array([-0.09395223], dtype=float32), self._np_random=Generator(PCG64) at 0x7FF1730609E0, self.action_space=Box(-1.0, 1.0, (1,), float32), self.observation_space=Box(-3.4028235e+38, 3.4028235e+38, (5,), float32), self.params=CartPoleSwingUpParams(gravity=9.82, forcemag=10.0, deltat=0.01, friction=0.1, x_threshold=2.4, cart=CartParams(width=0.3333333333333333, height=0.16666666666666666, mass=0.5), pole=PoleParams(width=0.05, length=0.6, mass=0.5), masstotal=1.0, mpl=0.3), self.spec=EnvSpec(id='CartPoleSwingUp-v0', entry_point='gym_cartpole_swingup.envs.cartpole_swingup:CartPoleSwingUpV0', reward_threshold=None, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CartPoleSwingUp', version=0), self.state=State(x_pos=0.18573049, x_dot=-0.062929064, theta=3.3186514, theta_dot=0.23617047), self.viewer=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "action = action[0] * self.params.forcemag\n",
      "State:\n",
      "-0.939522311091423\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _transition_fn(self, state, action):\n",
      "        action = action[0] * self.params.forcemag\n",
      "        sin_theta = np.sin(state.theta)\n",
      "        cos_theta = np.cos(state.theta)\n",
      "        xdot_update = (\n",
      "            -2 * self.params.mpl * (state.theta_dot ** 2) * sin_theta\n",
      "            + 3 * self.params.pole.mass * self.params.gravity * sin_theta * cos_theta\n",
      "            + 4 * action\n",
      "            - 4 * self.params.friction * state.x_dot\n",
      "        ) / (4 * self.params.masstotal - 3 * self.params.pole.mass * cos_theta ** 2)\n",
      "        thetadot_update = (\n",
      "            -3 * self.params.mpl * (state.theta_dot ** 2) * sin_theta * cos_theta\n",
      "            + 6 * self.params.masstotal * self.params.gravity * sin_theta\n",
      "            + 6 * (action - self.params.friction * state.x_dot) * cos_theta\n",
      "        ) / (\n",
      "            4 * self.params.pole.length * self.params.masstotal\n",
      "            - 3 * self.params.mpl * cos_theta ** 2\n",
      "        )\n",
      "        delta_t = self.params.deltat\n",
      "        return State(\n",
      "            x_pos=state.x_pos + state.x_dot * delta_t,\n",
      "            theta=state.theta + state.theta_dot * delta_t,\n",
      "            x_dot=state.x_dot + xdot_update * delta_t,\n",
      "            theta_dot=state.theta_dot + thetadot_update * delta_t,\n",
      "        )\n",
      "_transition_fn(self=<gym_cartpole_swingup.envs.cartpole_swingup.CartPoleSwingUpV0 object at 0x7ff1730d73d0>, state=State(x_pos=0.18573049, x_dot=-0.062929064, theta=3.3186514, theta_dot=0.23617047), action=array([-0.09395223], dtype=float32), self._np_random=Generator(PCG64) at 0x7FF1730609E0, self.action_space=Box(-1.0, 1.0, (1,), float32), self.observation_space=Box(-3.4028235e+38, 3.4028235e+38, (5,), float32), self.params=CartPoleSwingUpParams(gravity=9.82, forcemag=10.0, deltat=0.01, friction=0.1, x_threshold=2.4, cart=CartParams(width=0.3333333333333333, height=0.16666666666666666, mass=0.5), pole=PoleParams(width=0.05, length=0.6, mass=0.5), masstotal=1.0, mpl=0.3), self.spec=EnvSpec(id='CartPoleSwingUp-v0', entry_point='gym_cartpole_swingup.envs.cartpole_swingup:CartPoleSwingUpV0', reward_threshold=None, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CartPoleSwingUp', version=0), self.state=State(x_pos=0.18573049, x_dot=-0.062929064, theta=3.3186514, theta_dot=0.23617047), self.viewer=None)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "sin_theta = np.sin(state.theta)\n",
      "State:\n",
      "-0.17613511\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _transition_fn(self, state, action):\n",
      "        action = action[0] * self.params.forcemag\n",
      "        sin_theta = np.sin(state.theta)\n",
      "        cos_theta = np.cos(state.theta)\n",
      "        xdot_update = (\n",
      "            -2 * self.params.mpl * (state.theta_dot ** 2) * sin_theta\n",
      "            + 3 * self.params.pole.mass * self.params.gravity * sin_theta * cos_theta\n",
      "            + 4 * action\n",
      "            - 4 * self.params.friction * state.x_dot\n",
      "        ) / (4 * self.params.masstotal - 3 * self.params.pole.mass * cos_theta ** 2)\n",
      "        thetadot_update = (\n",
      "            -3 * self.params.mpl * (state.theta_dot ** 2) * sin_theta * cos_theta\n",
      "            + 6 * self.params.masstotal * self.params.gravity * sin_theta\n",
      "            + 6 * (action - self.params.friction * state.x_dot) * cos_theta\n",
      "        ) / (\n",
      "            4 * self.params.pole.length * self.params.masstotal\n",
      "            - 3 * self.params.mpl * cos_theta ** 2\n",
      "        )\n",
      "        delta_t = self.params.deltat\n",
      "        return State(\n",
      "            x_pos=state.x_pos + state.x_dot * delta_t,\n",
      "            theta=state.theta + state.theta_dot * delta_t,\n",
      "            x_dot=state.x_dot + xdot_update * delta_t,\n",
      "            theta_dot=state.theta_dot + thetadot_update * delta_t,\n",
      "        )\n",
      "_transition_fn(self=<gym_cartpole_swingup.envs.cartpole_swingup.CartPoleSwingUpV0 object at 0x7ff1730d73d0>, state=State(x_pos=0.18573049, x_dot=-0.062929064, theta=3.3186514, theta_dot=0.23617047), action=array([-0.09395223], dtype=float32), self._np_random=Generator(PCG64) at 0x7FF1730609E0, self.action_space=Box(-1.0, 1.0, (1,), float32), self.observation_space=Box(-3.4028235e+38, 3.4028235e+38, (5,), float32), self.params=CartPoleSwingUpParams(gravity=9.82, forcemag=10.0, deltat=0.01, friction=0.1, x_threshold=2.4, cart=CartParams(width=0.3333333333333333, height=0.16666666666666666, mass=0.5), pole=PoleParams(width=0.05, length=0.6, mass=0.5), masstotal=1.0, mpl=0.3), self.spec=EnvSpec(id='CartPoleSwingUp-v0', entry_point='gym_cartpole_swingup.envs.cartpole_swingup:CartPoleSwingUpV0', reward_threshold=None, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CartPoleSwingUp', version=0), self.state=State(x_pos=0.18573049, x_dot=-0.062929064, theta=3.3186514, theta_dot=0.23617047), self.viewer=None)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "cos_theta = np.cos(state.theta)\n",
      "State:\n",
      "-0.984366\n",
      "==================================================\n",
      "Clean Code:\n",
      "def convert_type(ty, default=None):\n",
      "    guessed_type = False\n",
      "    if ty is None and default is not None:\n",
      "        if isinstance(default, tuple):\n",
      "            ty = tuple(map(type, default))\n",
      "        else:\n",
      "            ty = type(default)\n",
      "        guessed_type = True\n",
      "    if isinstance(ty, tuple):\n",
      "        return Tuple(ty)\n",
      "    if isinstance(ty, ParamType):\n",
      "        return ty\n",
      "    if ty is text_type or ty is str or ty is None:\n",
      "        return STRING\n",
      "    if ty is int:\n",
      "        return INT\n",
      "    if ty is bool and not guessed_type:\n",
      "        return BOOL\n",
      "    if ty is float:\n",
      "        return FLOAT\n",
      "    if guessed_type:\n",
      "        return STRING\n",
      "    if __debug__:\n",
      "        try:\n",
      "            if issubclass(ty, ParamType):\n",
      "                raise AssertionError('Attempted to use an uninstantiated '\n",
      "                                     'parameter type (%s).' % ty)\n",
      "        except TypeError:\n",
      "            pass\n",
      "    return FuncParamType(ty)\n",
      "convert_type(ty=None, default=None)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "guessed_type = False\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _make_command(f, name, attrs, cls):\n",
      "    if isinstance(f, Command):\n",
      "        raise TypeError('Attempted to convert a callback into a '\n",
      "                        'command twice.')\n",
      "    try:\n",
      "        params = f.__click_params__\n",
      "        params.reverse()\n",
      "        del f.__click_params__\n",
      "    except AttributeError:\n",
      "        params = []\n",
      "    help = attrs.get('help')\n",
      "    if help is None:\n",
      "        help = inspect.getdoc(f)\n",
      "        if isinstance(help, bytes):\n",
      "            help = help.decode('utf-8')\n",
      "    else:\n",
      "        help = inspect.cleandoc(help)\n",
      "    attrs['help'] = help\n",
      "    _check_for_unicode_literals()\n",
      "    return cls(name=name or f.__name__.lower(),\n",
      "               callback=f, params=params, **attrs)\n",
      "_make_command(f=<function test_nargs_star.<locals>.copy at 0x7fe6e94318b0>, name=None, attrs={}, cls=<class 'click.core.Command'>)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "help = attrs.get('help')\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _bashcomplete(cmd, prog_name, complete_var=None):\n",
      "    if complete_var is None:\n",
      "        complete_var = '_%s_COMPLETE' % (prog_name.replace('-', '_')).upper()\n",
      "    complete_instr = os.environ.get(complete_var)\n",
      "    if not complete_instr:\n",
      "        return\n",
      "    from ._bashcomplete import bashcomplete\n",
      "    if bashcomplete(cmd, prog_name, complete_var, complete_instr):\n",
      "        sys.exit(1)\n",
      "_bashcomplete(cmd={name='copy', context_settings={}, params=[<click.core.Argument object at 0x7fe6e966a940>, <click.core.Argument object at 0x7fe6e966a610>], help=None, epilog=None, options_metavar='[OPTIONS]', short_help=None, add_help_option=True, __doc__=None}, prog_name='copy', complete_var=None)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "complete_var = '_%s_COMPLETE' % (prog_name.replace('-', '_')).upper()\n",
      "State:\n",
      "'_COPY_COMPLETE'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __enter__(self):\n",
      "        self._depth += 1\n",
      "        push_context(self)\n",
      "        return self\n",
      "__enter__(self=<click.core.Context object at 0x7fe6e93bae50>, self._close_callbacks=[], self._depth=1, self._meta={}, self.allow_extra_args=False, self.allow_interspersed_args=True, self.args=[], self.auto_envvar_prefix=None, self.color=None, self.command=<click.core.Command object at 0x7fe6e92e0ac0>, self.default_map=None, self.help_option_names=['--help'], self.ignore_unknown_options=False, self.info_name='copy', self.invoked_subcommand=None, self.max_content_width=None, self.obj=None, self.params={}, self.parent=None, self.protected_args=[], self.resilient_parsing=False, self.terminal_width=None, self.token_normalize_func=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self._depth += 1\n",
      "State:\n",
      "2\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_params(self, ctx):\n",
      "        rv = self.params\n",
      "        help_option = self.get_help_option(ctx)\n",
      "        if help_option is not None:\n",
      "            rv = rv + [help_option]\n",
      "        return rv\n",
      "get_params(self=<click.core.Command object at 0x7fe6e92e0ac0>, ctx={parent=None, command=<click.core.Command object at 0x7fe6e92e0ac0>, info_name='copy', params={}, args=[], protected_args=[], obj=None, _meta={}, default_map=None, invoked_subcommand=None, terminal_width=None, max_content_width=None, allow_extra_args=False, allow_interspersed_args=True, ignore_unknown_options=False, help_option_names=['--help'], token_normalize_func=None, resilient_parsing=False, auto_envvar_prefix=None, color=None, _close_callbacks=[], _depth=2}, self.__doc__=None, self.add_help_option=True, self.callback=<function test_nargs_star.<locals>.copy at 0x7fe6e94318b0>, self.context_settings={}, self.epilog=None, self.help=None, self.name='copy', self.options_metavar='[OPTIONS]', self.params=[<click.core.Argument object at 0x7fe6e966a940>, <click.core.Argument object at 0x7fe6e966a610>], self.short_help=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "help_option = self.get_help_option(ctx)\n",
      "State:\n",
      "{name='help', opts=['--help'], secondary_opts=[], type=BOOL, required=False, nargs=1, multiple=False, expose_value=False, default=False, is_eager=True, metavar=None, envvar=None, prompt=None, confirmation_prompt=False, hide_input=False, is_flag=True, flag_value=True, is_bool_flag=True, count=False, allow_from_autoenv=True, help='Show this message and exit.', show_default=False}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_decls(self, decls, expose_value):\n",
      "        opts = []\n",
      "        secondary_opts = []\n",
      "        name = None\n",
      "        possible_names = []\n",
      "        for decl in decls:\n",
      "            if isidentifier(decl):\n",
      "                if name is not None:\n",
      "                    raise TypeError('Name defined twice')\n",
      "                name = decl\n",
      "            else:\n",
      "                split_char = decl[:1] == '/' and ';' or '/'\n",
      "                if split_char in decl:\n",
      "                    first, second = decl.split(split_char, 1)\n",
      "                    first = first.rstrip()\n",
      "                    if first:\n",
      "                        possible_names.append(split_opt(first))\n",
      "                        opts.append(first)\n",
      "                    second = second.lstrip()\n",
      "                    if second:\n",
      "                        secondary_opts.append(second.lstrip())\n",
      "                else:\n",
      "                    possible_names.append(split_opt(decl))\n",
      "                    opts.append(decl)\n",
      "        if name is None and possible_names:\n",
      "            possible_names.sort(key=lambda x: len(x[0]))\n",
      "            name = possible_names[-1][1].replace('-', '_').lower()\n",
      "            if not isidentifier(name):\n",
      "                name = None\n",
      "        if name is None:\n",
      "            if not expose_value:\n",
      "                return None, opts, secondary_opts\n",
      "            raise TypeError('Could not determine name for option')\n",
      "        if not opts and not secondary_opts:\n",
      "            raise TypeError('No options defined but a name was passed (%s). '\n",
      "                            'Did you mean to declare an argument instead '\n",
      "                            'of an option?' % name)\n",
      "        return name, opts, secondary_opts\n",
      "_parse_decls(self=<click.core.Option object at 0x7fe6e93bad60>, decls={'--help'}, expose_value=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "opts = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_decls(self, decls, expose_value):\n",
      "        opts = []\n",
      "        secondary_opts = []\n",
      "        name = None\n",
      "        possible_names = []\n",
      "        for decl in decls:\n",
      "            if isidentifier(decl):\n",
      "                if name is not None:\n",
      "                    raise TypeError('Name defined twice')\n",
      "                name = decl\n",
      "            else:\n",
      "                split_char = decl[:1] == '/' and ';' or '/'\n",
      "                if split_char in decl:\n",
      "                    first, second = decl.split(split_char, 1)\n",
      "                    first = first.rstrip()\n",
      "                    if first:\n",
      "                        possible_names.append(split_opt(first))\n",
      "                        opts.append(first)\n",
      "                    second = second.lstrip()\n",
      "                    if second:\n",
      "                        secondary_opts.append(second.lstrip())\n",
      "                else:\n",
      "                    possible_names.append(split_opt(decl))\n",
      "                    opts.append(decl)\n",
      "        if name is None and possible_names:\n",
      "            possible_names.sort(key=lambda x: len(x[0]))\n",
      "            name = possible_names[-1][1].replace('-', '_').lower()\n",
      "            if not isidentifier(name):\n",
      "                name = None\n",
      "        if name is None:\n",
      "            if not expose_value:\n",
      "                return None, opts, secondary_opts\n",
      "            raise TypeError('Could not determine name for option')\n",
      "        if not opts and not secondary_opts:\n",
      "            raise TypeError('No options defined but a name was passed (%s). '\n",
      "                            'Did you mean to declare an argument instead '\n",
      "                            'of an option?' % name)\n",
      "        return name, opts, secondary_opts\n",
      "_parse_decls(self=<click.core.Option object at 0x7fe6e93bad60>, decls={'--help'}, expose_value=False)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "secondary_opts = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_decls(self, decls, expose_value):\n",
      "        opts = []\n",
      "        secondary_opts = []\n",
      "        name = None\n",
      "        possible_names = []\n",
      "        for decl in decls:\n",
      "            if isidentifier(decl):\n",
      "                if name is not None:\n",
      "                    raise TypeError('Name defined twice')\n",
      "                name = decl\n",
      "            else:\n",
      "                split_char = decl[:1] == '/' and ';' or '/'\n",
      "                if split_char in decl:\n",
      "                    first, second = decl.split(split_char, 1)\n",
      "                    first = first.rstrip()\n",
      "                    if first:\n",
      "                        possible_names.append(split_opt(first))\n",
      "                        opts.append(first)\n",
      "                    second = second.lstrip()\n",
      "                    if second:\n",
      "                        secondary_opts.append(second.lstrip())\n",
      "                else:\n",
      "                    possible_names.append(split_opt(decl))\n",
      "                    opts.append(decl)\n",
      "        if name is None and possible_names:\n",
      "            possible_names.sort(key=lambda x: len(x[0]))\n",
      "            name = possible_names[-1][1].replace('-', '_').lower()\n",
      "            if not isidentifier(name):\n",
      "                name = None\n",
      "        if name is None:\n",
      "            if not expose_value:\n",
      "                return None, opts, secondary_opts\n",
      "            raise TypeError('Could not determine name for option')\n",
      "        if not opts and not secondary_opts:\n",
      "            raise TypeError('No options defined but a name was passed (%s). '\n",
      "                            'Did you mean to declare an argument instead '\n",
      "                            'of an option?' % name)\n",
      "        return name, opts, secondary_opts\n",
      "_parse_decls(self=<click.core.Option object at 0x7fe6e93bad60>, decls={'--help'}, expose_value=False)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "name = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_decls(self, decls, expose_value):\n",
      "        opts = []\n",
      "        secondary_opts = []\n",
      "        name = None\n",
      "        possible_names = []\n",
      "        for decl in decls:\n",
      "            if isidentifier(decl):\n",
      "                if name is not None:\n",
      "                    raise TypeError('Name defined twice')\n",
      "                name = decl\n",
      "            else:\n",
      "                split_char = decl[:1] == '/' and ';' or '/'\n",
      "                if split_char in decl:\n",
      "                    first, second = decl.split(split_char, 1)\n",
      "                    first = first.rstrip()\n",
      "                    if first:\n",
      "                        possible_names.append(split_opt(first))\n",
      "                        opts.append(first)\n",
      "                    second = second.lstrip()\n",
      "                    if second:\n",
      "                        secondary_opts.append(second.lstrip())\n",
      "                else:\n",
      "                    possible_names.append(split_opt(decl))\n",
      "                    opts.append(decl)\n",
      "        if name is None and possible_names:\n",
      "            possible_names.sort(key=lambda x: len(x[0]))\n",
      "            name = possible_names[-1][1].replace('-', '_').lower()\n",
      "            if not isidentifier(name):\n",
      "                name = None\n",
      "        if name is None:\n",
      "            if not expose_value:\n",
      "                return None, opts, secondary_opts\n",
      "            raise TypeError('Could not determine name for option')\n",
      "        if not opts and not secondary_opts:\n",
      "            raise TypeError('No options defined but a name was passed (%s). '\n",
      "                            'Did you mean to declare an argument instead '\n",
      "                            'of an option?' % name)\n",
      "        return name, opts, secondary_opts\n",
      "_parse_decls(self=<click.core.Option object at 0x7fe6e93bad60>, decls={'--help'}, expose_value=False)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "possible_names = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def normalize_opt(opt, ctx):\n",
      "    if ctx is None or ctx.token_normalize_func is None:\n",
      "        return opt\n",
      "    prefix, opt = split_opt(opt)\n",
      "    return prefix + ctx.token_normalize_func(opt)\n",
      "normalize_opt(opt='--foo', ctx={parent=None, command=<click.core.Command object at 0x7fe6e9373070>, info_name='cli', params={}, args=[], protected_args=[], obj=None, _meta={}, default_map=None, invoked_subcommand=None, terminal_width=None, max_content_width=None, allow_extra_args=False, allow_interspersed_args=True, ignore_unknown_options=False, help_option_names=['--help'], resilient_parsing=False, auto_envvar_prefix=None, color=None, _close_callbacks=[], _depth=2})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "prefix, opt = split_opt(opt)\n",
      "State:\n",
      "'--'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _unpack_args(args, nargs_spec):\n",
      "    args = deque(args)\n",
      "    nargs_spec = deque(nargs_spec)\n",
      "    rv = []\n",
      "    spos = None\n",
      "    def _fetch(c):\n",
      "        try:\n",
      "            if spos is None:\n",
      "                return c.popleft()\n",
      "            else:\n",
      "                return c.pop()\n",
      "        except IndexError:\n",
      "            return None\n",
      "    while nargs_spec:\n",
      "        nargs = _fetch(nargs_spec)\n",
      "        if nargs == 1:\n",
      "            rv.append(_fetch(args))\n",
      "        elif nargs > 1:\n",
      "            x = [_fetch(args) for _ in range(nargs)]\n",
      "            if spos is not None:\n",
      "                x.reverse()\n",
      "            rv.append(tuple(x))\n",
      "        elif nargs < 0:\n",
      "            if spos is not None:\n",
      "                raise TypeError('Cannot have two nargs < 0')\n",
      "            spos = len(rv)\n",
      "            rv.append(None)\n",
      "    if spos is not None:\n",
      "        rv[spos] = tuple(args)\n",
      "        args = []\n",
      "        rv[spos + 1:] = reversed(rv[spos + 1:])\n",
      "    return tuple(rv), list(args)\n",
      "_unpack_args(args=['foo.txt', 'bar.txt', 'dir'], nargs_spec=[-1, 1])\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "args = deque(args)\n",
      "State:\n",
      "deque(['foo.txt', 'bar.txt', 'dir'])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _fetch(c):\n",
      "        try:\n",
      "            if spos is None:\n",
      "                return c.popleft()\n",
      "            else:\n",
      "                return c.pop()\n",
      "        except IndexError:\n",
      "            return None\n",
      "_fetch(c=deque([-1, 1]), spos=None)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "return c.popleft()\n",
      "State:\n",
      "deque([1])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def handle_parse_result(self, ctx, opts, args):\n",
      "        with augment_usage_errors(ctx, param=self):\n",
      "            value = self.consume_value(ctx, opts)\n",
      "            try:\n",
      "                value = self.full_process_value(ctx, value)\n",
      "            except Exception:\n",
      "                if not ctx.resilient_parsing:\n",
      "                    raise\n",
      "                value = None\n",
      "            if self.callback is not None:\n",
      "                try:\n",
      "                    value = invoke_param_callback(\n",
      "                        self.callback, ctx, self, value)\n",
      "                except Exception:\n",
      "                    if not ctx.resilient_parsing:\n",
      "                        raise\n",
      "        if self.expose_value:\n",
      "            ctx.params[self.name] = value\n",
      "        return value, args\n",
      "handle_parse_result(self=<click.core.Option object at 0x7fe6e93ba190>, ctx={parent=None, command=<click.core.Command object at 0x7fe6e92e0ac0>, info_name='copy', params={}, args=[], protected_args=[], obj=None, _meta={}, default_map=None, invoked_subcommand=None, terminal_width=None, max_content_width=None, allow_extra_args=False, allow_interspersed_args=True, ignore_unknown_options=False, help_option_names=['--help'], token_normalize_func=None, resilient_parsing=False, auto_envvar_prefix=None, color=None, _close_callbacks=[], _depth=2}, opts={'src': ('foo.txt', 'bar.txt'), 'dst': 'dir'}, args=[], self.allow_from_autoenv=True, self.callback=<function Command.get_help_option.<locals>.show_help at 0x7fe6e9431b80>, self.confirmation_prompt=False, self.count=False, self.default=False, self.envvar=None, self.expose_value=False, self.flag_value=True, self.help='Show this message and exit.', self.hide_input=False, self.is_bool_flag=True, self.is_eager=True, self.is_flag=True, self.metavar=None, self.multiple=False, self.name='help', self.nargs=1, self.opts=['--help'], self.prompt=None, self.required=False, self.secondary_opts=[], self.show_default=False, self.type=BOOL)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "value = self.consume_value(ctx, opts)\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def handle_parse_result(self, ctx, opts, args):\n",
      "        with augment_usage_errors(ctx, param=self):\n",
      "            value = self.consume_value(ctx, opts)\n",
      "            try:\n",
      "                value = self.full_process_value(ctx, value)\n",
      "            except Exception:\n",
      "                if not ctx.resilient_parsing:\n",
      "                    raise\n",
      "                value = None\n",
      "            if self.callback is not None:\n",
      "                try:\n",
      "                    value = invoke_param_callback(\n",
      "                        self.callback, ctx, self, value)\n",
      "                except Exception:\n",
      "                    if not ctx.resilient_parsing:\n",
      "                        raise\n",
      "        if self.expose_value:\n",
      "            ctx.params[self.name] = value\n",
      "        return value, args\n",
      "handle_parse_result(self=<click.core.Option object at 0x7fe6e93ba190>, ctx={parent=None, command=<click.core.Command object at 0x7fe6e92e0ac0>, info_name='copy', params={}, args=[], protected_args=[], obj=None, _meta={}, default_map=None, invoked_subcommand=None, terminal_width=None, max_content_width=None, allow_extra_args=False, allow_interspersed_args=True, ignore_unknown_options=False, help_option_names=['--help'], token_normalize_func=None, resilient_parsing=False, auto_envvar_prefix=None, color=None, _close_callbacks=[], _depth=2}, opts={'src': ('foo.txt', 'bar.txt'), 'dst': 'dir'}, args=[], self.allow_from_autoenv=True, self.callback=<function Command.get_help_option.<locals>.show_help at 0x7fe6e9431b80>, self.confirmation_prompt=False, self.count=False, self.default=False, self.envvar=None, self.expose_value=False, self.flag_value=True, self.help='Show this message and exit.', self.hide_input=False, self.is_bool_flag=True, self.is_eager=True, self.is_flag=True, self.metavar=None, self.multiple=False, self.name='help', self.nargs=1, self.opts=['--help'], self.prompt=None, self.required=False, self.secondary_opts=[], self.show_default=False, self.type=BOOL)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "value = self.full_process_value(ctx, value)\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def consume_value(self, ctx, opts):\n",
      "        value = opts.get(self.name)\n",
      "        if value is None:\n",
      "            value = ctx.lookup_default(self.name)\n",
      "        if value is None:\n",
      "            value = self.value_from_envvar(ctx)\n",
      "        return value\n",
      "consume_value(self=<click.core.Option object at 0x7fe6e93ba190>, ctx={parent=None, command=<click.core.Command object at 0x7fe6e92e0ac0>, info_name='copy', params={}, args=[], protected_args=[], obj=None, _meta={}, default_map=None, invoked_subcommand=None, terminal_width=None, max_content_width=None, allow_extra_args=False, allow_interspersed_args=True, ignore_unknown_options=False, help_option_names=['--help'], token_normalize_func=None, resilient_parsing=False, auto_envvar_prefix=None, color=None, _close_callbacks=[], _depth=2}, opts={'src': ('foo.txt', 'bar.txt'), 'dst': 'dir'}, self.allow_from_autoenv=True, self.callback=<function Command.get_help_option.<locals>.show_help at 0x7fe6e9431b80>, self.confirmation_prompt=False, self.count=False, self.default=False, self.envvar=None, self.expose_value=False, self.flag_value=True, self.help='Show this message and exit.', self.hide_input=False, self.is_bool_flag=True, self.is_eager=True, self.is_flag=True, self.metavar=None, self.multiple=False, self.name='help', self.nargs=1, self.opts=['--help'], self.prompt=None, self.required=False, self.secondary_opts=[], self.show_default=False, self.type=BOOL)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "value = opts.get(self.name)\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def exit(self, code=0):\n",
      "        sys.exit(code)\n",
      "exit(self=<click.core.Context object at 0x7fe6e9340940>, code=0, self._close_callbacks=[], self._depth=1, self._meta={}, self.allow_extra_args=False, self.allow_interspersed_args=True, self.args=[], self.auto_envvar_prefix=None, self.color=None, self.command=<click.core.Command object at 0x7fe6e94bee20>, self.default_map=None, self.help_option_names=['--help'], self.ignore_unknown_options=False, self.info_name='cli', self.invoked_subcommand=None, self.max_content_width=None, self.obj=None, self.params={'arg': ((1, 2), (3, 4))}, self.parent=None, self.protected_args=[], self.resilient_parsing=False, self.terminal_width=None, self.token_normalize_func=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "sys.exit(code)\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def func():\n",
      "        stream = src_func()\n",
      "        try:\n",
      "            rv = cache.get(stream)\n",
      "        except Exception:\n",
      "            rv = None\n",
      "        if rv is not None:\n",
      "            return rv\n",
      "        rv = wrapper_func()\n",
      "        try:\n",
      "            cache[stream] = rv\n",
      "        except Exception:\n",
      "            pass\n",
      "        return rv\n",
      "func(cache=<WeakKeyDictionary at 0x7fe6e9750670>, src_func=<function <lambda> at 0x7fe6e9735160>, wrapper_func=<function get_text_stdout at 0x7fe6e97574c0>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "stream = src_func()\n",
      "State:\n",
      "<_io.TextIOWrapper encoding='utf-8'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def func():\n",
      "        stream = src_func()\n",
      "        try:\n",
      "            rv = cache.get(stream)\n",
      "        except Exception:\n",
      "            rv = None\n",
      "        if rv is not None:\n",
      "            return rv\n",
      "        rv = wrapper_func()\n",
      "        try:\n",
      "            cache[stream] = rv\n",
      "        except Exception:\n",
      "            pass\n",
      "        return rv\n",
      "func(cache=<WeakKeyDictionary at 0x7fe6e9750670>, src_func=<function <lambda> at 0x7fe6e9735160>, wrapper_func=<function get_text_stdout at 0x7fe6e97574c0>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "rv = cache.get(stream)\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def format_usage(self, ctx, formatter):\n",
      "        pieces = self.collect_usage_pieces(ctx)\n",
      "        formatter.write_usage(ctx.command_path, ' '.join(pieces))\n",
      "format_usage(self=<click.core.Command object at 0x7fe6e9465040>, ctx={parent=None, command=<click.core.Command object at 0x7fe6e9465040>, info_name='copy', params={'x': 'foo'}, args=[], protected_args=[], obj=None, _meta={}, default_map=None, invoked_subcommand=None, terminal_width=None, max_content_width=None, allow_extra_args=False, allow_interspersed_args=True, ignore_unknown_options=False, help_option_names=['--help'], token_normalize_func=None, resilient_parsing=False, auto_envvar_prefix=None, color=None, _close_callbacks=[], _depth=0}, formatter={indent_increment=2, width=80, current_indent=0, buffer=[]}, self.__doc__=None, self.add_help_option=True, self.callback=<function test_nargs_err.<locals>.copy at 0x7fe6e8acf040>, self.context_settings={}, self.epilog=None, self.help=None, self.name='copy', self.options_metavar='[OPTIONS]', self.params=[<click.core.Argument object at 0x7fe6e9465af0>], self.short_help=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "pieces = self.collect_usage_pieces(ctx)\n",
      "State:\n",
      "['[OPTIONS]', 'X']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def format_usage(self, ctx, formatter):\n",
      "        pieces = self.collect_usage_pieces(ctx)\n",
      "        formatter.write_usage(ctx.command_path, ' '.join(pieces))\n",
      "format_usage(self=<click.core.Command object at 0x7fe6e9465040>, ctx={parent=None, command=<click.core.Command object at 0x7fe6e9465040>, info_name='copy', params={'x': 'foo'}, args=[], protected_args=[], obj=None, _meta={}, default_map=None, invoked_subcommand=None, terminal_width=None, max_content_width=None, allow_extra_args=False, allow_interspersed_args=True, ignore_unknown_options=False, help_option_names=['--help'], token_normalize_func=None, resilient_parsing=False, auto_envvar_prefix=None, color=None, _close_callbacks=[], _depth=0}, formatter={indent_increment=2, width=80, current_indent=0, buffer=[]}, self.__doc__=None, self.add_help_option=True, self.callback=<function test_nargs_err.<locals>.copy at 0x7fe6e8acf040>, self.context_settings={}, self.epilog=None, self.help=None, self.name='copy', self.options_metavar='[OPTIONS]', self.params=[<click.core.Argument object at 0x7fe6e9465af0>], self.short_help=None)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "formatter.write_usage(ctx.command_path, ' '.join(pieces))\n",
      "State:\n",
      "{indent_increment=2, width=80, current_indent=0, buffer=['Usage: copy [OPTIONS] X', '\\n']}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def collect_usage_pieces(self, ctx):\n",
      "        rv = [self.options_metavar]\n",
      "        for param in self.get_params(ctx):\n",
      "            rv.extend(param.get_usage_pieces(ctx))\n",
      "        return rv\n",
      "collect_usage_pieces(self=<click.core.Command object at 0x7fe6e9465040>, ctx={parent=None, command=<click.core.Command object at 0x7fe6e9465040>, info_name='copy', params={'x': 'foo'}, args=[], protected_args=[], obj=None, _meta={}, default_map=None, invoked_subcommand=None, terminal_width=None, max_content_width=None, allow_extra_args=False, allow_interspersed_args=True, ignore_unknown_options=False, help_option_names=['--help'], token_normalize_func=None, resilient_parsing=False, auto_envvar_prefix=None, color=None, _close_callbacks=[], _depth=0}, self.__doc__=None, self.add_help_option=True, self.callback=<function test_nargs_err.<locals>.copy at 0x7fe6e8acf040>, self.context_settings={}, self.epilog=None, self.help=None, self.name='copy', self.options_metavar='[OPTIONS]', self.params=[<click.core.Argument object at 0x7fe6e9465af0>], self.short_help=None)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "rv = [self.options_metavar]\n",
      "State:\n",
      "['[OPTIONS]']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _process_opts(self, arg, state):\n",
      "        explicit_value = None\n",
      "        if '=' in arg:\n",
      "            long_opt, explicit_value = arg.split('=', 1)\n",
      "        else:\n",
      "            long_opt = arg\n",
      "        norm_long_opt = normalize_opt(long_opt, self.ctx)\n",
      "        try:\n",
      "            self._match_long_opt(norm_long_opt, explicit_value, state)\n",
      "        except NoSuchOption:\n",
      "            if arg[:2] not in self._opt_prefixes:\n",
      "                return self._match_short_opt(arg, state)\n",
      "            if not self.ignore_unknown_options:\n",
      "                raise\n",
      "            state.largs.append(arg)\n",
      "_process_opts(self=<click.parser.OptionParser object at 0x7fe6e92cb340>, arg='-f', state={opts={}, largs=[], rargs=['-x', '--', '-foo', 'bar'], order=[]}, self._args=[<click.parser.Argument object at 0x7fe6e92cbf40>], self._long_opt={'--help': <click.parser.Option object at 0x7fe6e9373250>}, self._opt_prefixes={'--', '-'}, self._short_opt={'-f': <click.parser.Option object at 0x7fe6e9373310>}, self.allow_interspersed_args=True, self.ctx=<click.core.Context object at 0x7fe6e92cb4f0>, self.ignore_unknown_options=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "explicit_value = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _process_opts(self, arg, state):\n",
      "        explicit_value = None\n",
      "        if '=' in arg:\n",
      "            long_opt, explicit_value = arg.split('=', 1)\n",
      "        else:\n",
      "            long_opt = arg\n",
      "        norm_long_opt = normalize_opt(long_opt, self.ctx)\n",
      "        try:\n",
      "            self._match_long_opt(norm_long_opt, explicit_value, state)\n",
      "        except NoSuchOption:\n",
      "            if arg[:2] not in self._opt_prefixes:\n",
      "                return self._match_short_opt(arg, state)\n",
      "            if not self.ignore_unknown_options:\n",
      "                raise\n",
      "            state.largs.append(arg)\n",
      "_process_opts(self=<click.parser.OptionParser object at 0x7fe6e92cb340>, arg='-f', state={opts={}, largs=[], rargs=['-x', '--', '-foo', 'bar'], order=[]}, self._args=[<click.parser.Argument object at 0x7fe6e92cbf40>], self._long_opt={'--help': <click.parser.Option object at 0x7fe6e9373250>}, self._opt_prefixes={'--', '-'}, self._short_opt={'-f': <click.parser.Option object at 0x7fe6e9373310>}, self.allow_interspersed_args=True, self.ctx=<click.core.Context object at 0x7fe6e92cb4f0>, self.ignore_unknown_options=False)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "self._match_long_opt(norm_long_opt, explicit_value, state)\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _convert(value, level):\n",
      "            if level == 0:\n",
      "                return self.type(value, self, ctx)\n",
      "            return tuple(_convert(x, level - 1) for x in value or ())\n",
      "_convert(value='bar', level=0, _convert=<function Parameter.type_cast_value.<locals>._convert at 0x7fe6e3fe7e50>, ctx={parent=None, command=<click.core.Command object at 0x7fe6e966a370>, info_name='cli', params={}, args=[], protected_args=[], obj=None, _meta={}, default_map=None, invoked_subcommand=None, terminal_width=None, max_content_width=None, allow_extra_args=False, allow_interspersed_args=True, ignore_unknown_options=False, help_option_names=['--help'], token_normalize_func=None, resilient_parsing=False, auto_envvar_prefix=None, color=None, _close_callbacks=[], _depth=2}, self=<click.core.Option object at 0x7fe6e966a040>, self.allow_from_autoenv=True, self.callback=None, self.confirmation_prompt=False, self.count=False, self.default=42, self.envvar=None, self.expose_value=True, self.flag_value=False, self.help=None, self.hide_input=False, self.is_bool_flag=False, self.is_eager=False, self.is_flag=False, self.metavar=None, self.multiple=False, self.name='foo', self.nargs=1, self.opts=['--foo'], self.prompt=None, self.required=False, self.secondary_opts=[], self.show_default=False, self.type=INT)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "return self.type(value, self, ctx)\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _process_args_for_args(self, state):\n",
      "        pargs, args = _unpack_args(state.largs + state.rargs,\n",
      "                                   [x.nargs for x in self._args])\n",
      "        for idx, arg in enumerate(self._args):\n",
      "            arg.process(pargs[idx], state)\n",
      "        state.largs = args\n",
      "        state.rargs = []\n",
      "_process_args_for_args(self=<click.parser.OptionParser object at 0x7fe6e93707f0>, state={opts={}, largs=[], rargs=[], order=[]}, self._args=[], self._long_opt={'--local-opt': <click.parser.Option object at 0x7fe6e93703d0>, '--help': <click.parser.Option object at 0x7fe6e9370400>}, self._opt_prefixes={'--', '-'}, self._short_opt={}, self.allow_interspersed_args=True, self.ctx=<click.core.Context object at 0x7fe6e9370580>, self.ignore_unknown_options=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "pargs, args = _unpack_args(state.largs + state.rargs,\n",
      "State:\n",
      "()\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add_command(self, cmd, name=None):\n",
      "        name = name or cmd.name\n",
      "        if name is None:\n",
      "            raise TypeError('Command has no name.')\n",
      "        _check_multicommand(self, name, cmd, register=True)\n",
      "        self.commands[name] = cmd\n",
      "add_command(self=<click.core.Group object at 0x7fe6e9370190>, cmd={name='sub', context_settings={}, params=[<click.core.Option object at 0x7fe6e9370670>], help=None, epilog=None, options_metavar='[OPTIONS]', short_help=None, add_help_option=True, __doc__=None}, name=None, self.__doc__=None, self.add_help_option=True, self.callback=<function test_small_chain.<locals>.cli at 0x7fe6e6b9b0d0>, self.chain=False, self.commands={}, self.context_settings={}, self.epilog=None, self.help=None, self.invoke_without_command=False, self.name='cli', self.no_args_is_help=True, self.options_metavar='[OPTIONS]', self.params=[<click.core.Option object at 0x7fe6e9370e50>], self.result_callback=None, self.short_help=None, self.subcommand_metavar='COMMAND [ARGS]...')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "name = name or cmd.name\n",
      "State:\n",
      "'sub'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def make_default_short_help(help, max_length=45):\n",
      "    words = help.split()\n",
      "    total_length = 0\n",
      "    result = []\n",
      "    done = False\n",
      "    for word in words:\n",
      "        if word[-1:] == '.':\n",
      "            done = True\n",
      "        new_length = result and 1 + len(word) or len(word)\n",
      "        if total_length + new_length > max_length:\n",
      "            result.append('...')\n",
      "            done = True\n",
      "        else:\n",
      "            if result:\n",
      "                result.append(' ')\n",
      "            result.append(word)\n",
      "        if done:\n",
      "            break\n",
      "        total_length += new_length\n",
      "    return ''.join(result)\n",
      "make_default_short_help(help='Hello World!', max_length=45)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "words = help.split()\n",
      "State:\n",
      "['Hello', 'World!']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def make_default_short_help(help, max_length=45):\n",
      "    words = help.split()\n",
      "    total_length = 0\n",
      "    result = []\n",
      "    done = False\n",
      "    for word in words:\n",
      "        if word[-1:] == '.':\n",
      "            done = True\n",
      "        new_length = result and 1 + len(word) or len(word)\n",
      "        if total_length + new_length > max_length:\n",
      "            result.append('...')\n",
      "            done = True\n",
      "        else:\n",
      "            if result:\n",
      "                result.append(' ')\n",
      "            result.append(word)\n",
      "        if done:\n",
      "            break\n",
      "        total_length += new_length\n",
      "    return ''.join(result)\n",
      "make_default_short_help(help='Hello World!', max_length=45)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "total_length = 0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def make_default_short_help(help, max_length=45):\n",
      "    words = help.split()\n",
      "    total_length = 0\n",
      "    result = []\n",
      "    done = False\n",
      "    for word in words:\n",
      "        if word[-1:] == '.':\n",
      "            done = True\n",
      "        new_length = result and 1 + len(word) or len(word)\n",
      "        if total_length + new_length > max_length:\n",
      "            result.append('...')\n",
      "            done = True\n",
      "        else:\n",
      "            if result:\n",
      "                result.append(' ')\n",
      "            result.append(word)\n",
      "        if done:\n",
      "            break\n",
      "        total_length += new_length\n",
      "    return ''.join(result)\n",
      "make_default_short_help(help='Hello World!', max_length=45)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "result = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def make_default_short_help(help, max_length=45):\n",
      "    words = help.split()\n",
      "    total_length = 0\n",
      "    result = []\n",
      "    done = False\n",
      "    for word in words:\n",
      "        if word[-1:] == '.':\n",
      "            done = True\n",
      "        new_length = result and 1 + len(word) or len(word)\n",
      "        if total_length + new_length > max_length:\n",
      "            result.append('...')\n",
      "            done = True\n",
      "        else:\n",
      "            if result:\n",
      "                result.append(' ')\n",
      "            result.append(word)\n",
      "        if done:\n",
      "            break\n",
      "        total_length += new_length\n",
      "    return ''.join(result)\n",
      "make_default_short_help(help='Hello World!', max_length=45)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "done = False\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def make_default_short_help(help, max_length=45):\n",
      "    words = help.split()\n",
      "    total_length = 0\n",
      "    result = []\n",
      "    done = False\n",
      "    for word in words:\n",
      "        if word[-1:] == '.':\n",
      "            done = True\n",
      "        new_length = result and 1 + len(word) or len(word)\n",
      "        if total_length + new_length > max_length:\n",
      "            result.append('...')\n",
      "            done = True\n",
      "        else:\n",
      "            if result:\n",
      "                result.append(' ')\n",
      "            result.append(word)\n",
      "        if done:\n",
      "            break\n",
      "        total_length += new_length\n",
      "    return ''.join(result)\n",
      "make_default_short_help(help='Hello World!', max_length=45)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "result.append(' ')\n",
      "State:\n",
      "['Hello', ' ']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def format_help_text(self, ctx, formatter):\n",
      "        if self.help:\n",
      "            formatter.write_paragraph()\n",
      "            with formatter.indentation():\n",
      "                formatter.write_text(self.help)\n",
      "format_help_text(self=<click.core.Command object at 0x7fe6e9373a60>, ctx={parent=None, command=<click.core.Command object at 0x7fe6e9373a60>, info_name='cli', params={}, args=[], protected_args=[], obj=None, _meta={}, default_map=None, invoked_subcommand=None, terminal_width=None, max_content_width=None, allow_extra_args=False, allow_interspersed_args=True, ignore_unknown_options=False, help_option_names=['--help'], token_normalize_func=None, resilient_parsing=False, auto_envvar_prefix=None, color=None, _close_callbacks=[], _depth=2}, formatter={indent_increment=2, width=80, current_indent=0, buffer=['Usage: cli [OPTIONS]', '\\n']}, self.__doc__='Hello World!', self.add_help_option=True, self.callback=<function test_basic_functionality.<locals>.cli at 0x7fe6e7860310>, self.context_settings={}, self.epilog=None, self.help='Hello World!', self.name='cli', self.options_metavar='[OPTIONS]', self.params=[], self.short_help='Hello World!')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "with formatter.indentation():\n",
      "State:\n",
      "{indent_increment=2, width=80, current_indent=2, buffer=['Usage: cli [OPTIONS]', '\\n', '\\n']}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def format_help_text(self, ctx, formatter):\n",
      "        if self.help:\n",
      "            formatter.write_paragraph()\n",
      "            with formatter.indentation():\n",
      "                formatter.write_text(self.help)\n",
      "format_help_text(self=<click.core.Command object at 0x7fe6e9373a60>, ctx={parent=None, command=<click.core.Command object at 0x7fe6e9373a60>, info_name='cli', params={}, args=[], protected_args=[], obj=None, _meta={}, default_map=None, invoked_subcommand=None, terminal_width=None, max_content_width=None, allow_extra_args=False, allow_interspersed_args=True, ignore_unknown_options=False, help_option_names=['--help'], token_normalize_func=None, resilient_parsing=False, auto_envvar_prefix=None, color=None, _close_callbacks=[], _depth=2}, formatter={indent_increment=2, width=80, current_indent=0, buffer=['Usage: cli [OPTIONS]', '\\n']}, self.__doc__='Hello World!', self.add_help_option=True, self.callback=<function test_basic_functionality.<locals>.cli at 0x7fe6e7860310>, self.context_settings={}, self.epilog=None, self.help='Hello World!', self.name='cli', self.options_metavar='[OPTIONS]', self.params=[], self.short_help='Hello World!')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "formatter.write_text(self.help)\n",
      "State:\n",
      "{indent_increment=2, width=80, current_indent=0, buffer=['Usage: cli [OPTIONS]', '\\n', '\\n', '  Hello World!', '\\n']}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def write_paragraph(self):\n",
      "        if self.buffer:\n",
      "            self.write('\\n')\n",
      "write_paragraph(self=<click.formatting.HelpFormatter object at 0x7fe6e92cbe50>, self.buffer=['Usage: cli [OPTIONS]', '\\n'], self.current_indent=0, self.indent_increment=2, self.width=80)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.write('\\n')\n",
      "State:\n",
      "['Usage: cli [OPTIONS]', '\\n', '\\n']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def dedent(self):\n",
      "        self.current_indent -= self.indent_increment\n",
      "dedent(self=<click.formatting.HelpFormatter object at 0x7fe6e92cbe50>, self.buffer=['Usage: cli [OPTIONS]', '\\n', '\\n', '  Hello World!', '\\n'], self.current_indent=2, self.indent_increment=2, self.width=80)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.current_indent -= self.indent_increment\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write_opts(opts):\n",
      "            rv, any_slashes = join_options(opts)\n",
      "            if any_slashes:\n",
      "                any_prefix_is_slash[:] = [True]\n",
      "            if not self.is_flag and not self.count:\n",
      "                rv += ' ' + self.make_metavar()\n",
      "            return rv\n",
      "_write_opts(opts=['--help'], any_prefix_is_slash=[], self=<click.core.Option object at 0x7fe6e92cb160>, self.allow_from_autoenv=True, self.callback=<function Command.get_help_option.<locals>.show_help at 0x7fe6e6b9bee0>, self.confirmation_prompt=False, self.count=False, self.default=False, self.envvar=None, self.expose_value=False, self.flag_value=True, self.help='Show this message and exit.', self.hide_input=False, self.is_bool_flag=True, self.is_eager=True, self.is_flag=True, self.metavar=None, self.multiple=False, self.name='help', self.nargs=1, self.opts=['--help'], self.prompt=None, self.required=False, self.secondary_opts=[], self.show_default=False, self.type=BOOL)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "rv, any_slashes = join_options(opts)\n",
      "State:\n",
      "'--help'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def write_heading(self, heading):\n",
      "        self.write('%*s%s:\\n' % (self.current_indent, '', heading))\n",
      "write_heading(self=<click.formatting.HelpFormatter object at 0x7fe6e92cbe50>, heading='Options', self.buffer=['Usage: cli [OPTIONS]', '\\n', '\\n', '  Hello World!', '\\n', '\\n'], self.current_indent=0, self.indent_increment=2, self.width=80)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.write('%*s%s:\\n' % (self.current_indent, '', heading))\n",
      "State:\n",
      "['Usage: cli [OPTIONS]', '\\n', '\\n', '  Hello World!', '\\n', '\\n', 'Options:\\n']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def write_dl(self, rows, col_max=30, col_spacing=2):\n",
      "        rows = list(rows)\n",
      "        widths = measure_table(rows)\n",
      "        if len(widths) != 2:\n",
      "            raise TypeError('Expected two columns for definition list')\n",
      "        first_col = min(widths[0], col_max) + col_spacing\n",
      "        for first, second in iter_rows(rows, len(widths)):\n",
      "            self.write('%*s%s' % (self.current_indent, '', first))\n",
      "            if not second:\n",
      "                self.write('\\n')\n",
      "                continue\n",
      "            if term_len(first) <= first_col - col_spacing:\n",
      "                self.write(' ' * (first_col - term_len(first)))\n",
      "            else:\n",
      "                self.write('\\n')\n",
      "                self.write(' ' * (first_col + self.current_indent))\n",
      "            text_width = max(self.width - first_col - 2, 10)\n",
      "            lines = iter(wrap_text(second, text_width).splitlines())\n",
      "            if lines:\n",
      "                self.write(next(lines) + '\\n')\n",
      "                for line in lines:\n",
      "                    self.write('%*s%s\\n' % (\n",
      "                        first_col + self.current_indent, '', line))\n",
      "            else:\n",
      "                self.write('\\n')\n",
      "write_dl(self=<click.formatting.HelpFormatter object at 0x7fe6e92cbe50>, rows=[('--help', 'Show this message and exit.')], col_max=30, col_spacing=2, self.buffer=['Usage: cli [OPTIONS]', '\\n', '\\n', '  Hello World!', '\\n', '\\n', 'Options:\\n'], self.current_indent=2, self.indent_increment=2, self.width=80)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "widths = measure_table(rows)\n",
      "State:\n",
      "(6, 27)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def measure_table(rows):\n",
      "    widths = {}\n",
      "    for row in rows:\n",
      "        for idx, col in enumerate(row):\n",
      "            widths[idx] = max(widths.get(idx, 0), term_len(col))\n",
      "    return tuple(y for x, y in sorted(widths.items()))\n",
      "measure_table(rows=[('--help', 'Show this message and exit.')])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "widths = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _process_result(value):\n",
      "            if self.result_callback is not None:\n",
      "                value = ctx.invoke(self.result_callback, value,\n",
      "                                   **ctx.params)\n",
      "            return value\n",
      "_process_result(value=[], ctx={parent=None, command=<click.core.Group object at 0x7fe6e9370fa0>, info_name='cli', params={'input': <_io.TextIOWrapper encoding='utf-8'>}, args=[], protected_args=[], obj=None, _meta={}, default_map=None, invoked_subcommand=None, terminal_width=None, max_content_width=None, allow_extra_args=True, allow_interspersed_args=False, ignore_unknown_options=False, help_option_names=['--help'], token_normalize_func=None, resilient_parsing=False, auto_envvar_prefix=None, color=None, _close_callbacks=[<function safecall.<locals>.wrapper at 0x7fe6df02fe50>], _depth=2}, self=<click.core.Group object at 0x7fe6e9370fa0>, self.__doc__=None, self.add_help_option=True, self.callback=<function test_pipeline.<locals>.cli at 0x7fe6df02f550>, self.chain=True, self.commands={'uppercase': <click.core.Command object at 0x7fe6e92ac100>, 'strip': <click.core.Command object at 0x7fe6e92ac2b0>}, self.context_settings={}, self.epilog=None, self.help=None, self.invoke_without_command=True, self.name='cli', self.no_args_is_help=False, self.options_metavar='[OPTIONS]', self.params=[<click.core.Option object at 0x7fe6e92aceb0>], self.result_callback=<function test_pipeline.<locals>.process_pipeline at 0x7fe6df02f310>, self.short_help=None, self.subcommand_metavar='COMMAND1 [ARGS]... [COMMAND2 [ARGS]...]...')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "value = ctx.invoke(self.result_callback, value,\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _process_args_for_options(self, state):\n",
      "        while state.rargs:\n",
      "            arg = state.rargs.pop(0)\n",
      "            arglen = len(arg)\n",
      "            if arg == '--':\n",
      "                return\n",
      "            elif arg[:1] in self._opt_prefixes and arglen > 1:\n",
      "                self._process_opts(arg, state)\n",
      "            elif self.allow_interspersed_args:\n",
      "                state.largs.append(arg)\n",
      "            else:\n",
      "                state.rargs.insert(0, arg)\n",
      "                return\n",
      "_process_args_for_options(self=<click.parser.OptionParser object at 0x7fe6e94658b0>, state={opts={}, largs=[], rargs=['--foo='], order=[]}, self._args=[], self._long_opt={'--foo': <click.parser.Option object at 0x7fe6e9683c70>, '--help': <click.parser.Option object at 0x7fe6e9683ca0>}, self._opt_prefixes={'--', '-'}, self._short_opt={}, self.allow_interspersed_args=True, self.ctx=<click.core.Context object at 0x7fe6e93ba0a0>, self.ignore_unknown_options=False)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "arg = state.rargs.pop(0)\n",
      "State:\n",
      "'--foo='\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _process_args_for_options(self, state):\n",
      "        while state.rargs:\n",
      "            arg = state.rargs.pop(0)\n",
      "            arglen = len(arg)\n",
      "            if arg == '--':\n",
      "                return\n",
      "            elif arg[:1] in self._opt_prefixes and arglen > 1:\n",
      "                self._process_opts(arg, state)\n",
      "            elif self.allow_interspersed_args:\n",
      "                state.largs.append(arg)\n",
      "            else:\n",
      "                state.rargs.insert(0, arg)\n",
      "                return\n",
      "_process_args_for_options(self=<click.parser.OptionParser object at 0x7fe6e94658b0>, state={opts={}, largs=[], rargs=['--foo='], order=[]}, self._args=[], self._long_opt={'--foo': <click.parser.Option object at 0x7fe6e9683c70>, '--help': <click.parser.Option object at 0x7fe6e9683ca0>}, self._opt_prefixes={'--', '-'}, self._short_opt={}, self.allow_interspersed_args=True, self.ctx=<click.core.Context object at 0x7fe6e93ba0a0>, self.ignore_unknown_options=False)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "arglen = len(arg)\n",
      "State:\n",
      "6\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _match_long_opt(self, opt, explicit_value, state):\n",
      "        if opt not in self._long_opt:\n",
      "            possibilities = [word for word in self._long_opt\n",
      "                             if word.startswith(opt)]\n",
      "            raise NoSuchOption(opt, possibilities=possibilities)\n",
      "        option = self._long_opt[opt]\n",
      "        if option.takes_value:\n",
      "            if explicit_value is not None:\n",
      "                state.rargs.insert(0, explicit_value)\n",
      "            nargs = option.nargs\n",
      "            if len(state.rargs) < nargs:\n",
      "                _error_opt_args(nargs, opt)\n",
      "            elif nargs == 1:\n",
      "                value = state.rargs.pop(0)\n",
      "            else:\n",
      "                value = tuple(state.rargs[:nargs])\n",
      "                del state.rargs[:nargs]\n",
      "        elif explicit_value is not None:\n",
      "            raise BadOptionUsage('%s option does not take a value' % opt)\n",
      "        else:\n",
      "            value = None\n",
      "        option.process(value, state)\n",
      "_match_long_opt(self=<click.parser.OptionParser object at 0x7fe6e94658b0>, opt='--foo', explicit_value='', state={opts={}, largs=[], rargs=[], order=[]}, self._args=[], self._long_opt={'--foo': <click.parser.Option object at 0x7fe6e9683c70>, '--help': <click.parser.Option object at 0x7fe6e9683ca0>}, self._opt_prefixes={'--', '-'}, self._short_opt={}, self.allow_interspersed_args=True, self.ctx=<click.core.Context object at 0x7fe6e93ba0a0>, self.ignore_unknown_options=False)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "nargs = option.nargs\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def memo(ctx, value):\n",
      "        called.append(value)\n",
      "        return value\n",
      "memo(ctx={parent=None, command=<click.core.Command object at 0x7fe6e973ed60>, info_name='cli', params={}, args=[], protected_args=[], obj=None, _meta={}, default_map=None, invoked_subcommand=None, terminal_width=None, max_content_width=None, allow_extra_args=False, allow_interspersed_args=True, ignore_unknown_options=False, help_option_names=['--help'], token_normalize_func=None, resilient_parsing=False, auto_envvar_prefix=None, color=None, _close_callbacks=[], _depth=2}, value='eager2', called=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "called.append(value)\n",
      "State:\n",
      "['eager2']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def command_path(self):\n",
      "        rv = ''\n",
      "        if self.info_name is not None:\n",
      "            rv = self.info_name\n",
      "        if self.parent is not None:\n",
      "            rv = self.parent.command_path + ' ' + rv\n",
      "        return rv.lstrip()\n",
      "command_path(self=<click.core.Context object at 0x7fe6e973e3d0>, self._close_callbacks=[], self._depth=2, self._meta={}, self.allow_extra_args=True, self.allow_interspersed_args=False, self.args=[], self.auto_envvar_prefix=None, self.color=None, self.command=<click.core.Group object at 0x7fe6e973eb80>, self.default_map=None, self.help_option_names=['--help'], self.ignore_unknown_options=False, self.info_name='cli', self.invoked_subcommand='*', self.max_content_width=None, self.obj=None, self.params={}, self.parent=None, self.protected_args=[], self.resilient_parsing=False, self.terminal_width=None, self.token_normalize_func=None)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "rv = ''\n",
      "State:\n",
      "''\n",
      "==================================================\n",
      "Clean Code:\n",
      "def resolve_command(self, ctx, args):\n",
      "        cmd_name = make_str(args[0])\n",
      "        original_cmd_name = cmd_name\n",
      "        cmd = self.get_command(ctx, cmd_name)\n",
      "        if cmd is None and ctx.token_normalize_func is not None:\n",
      "            cmd_name = ctx.token_normalize_func(cmd_name)\n",
      "            cmd = self.get_command(ctx, cmd_name)\n",
      "        if cmd is None:\n",
      "            if split_opt(cmd_name)[0]:\n",
      "                self.parse_args(ctx, ctx.args)\n",
      "            ctx.fail('No such command \"%s\".' % original_cmd_name)\n",
      "        return cmd_name, cmd, args[1:]\n",
      "resolve_command(self=<click.core.Group object at 0x7fe6e9562310>, ctx={parent=None, command=<click.core.Group object at 0x7fe6e9562310>, info_name='cli', params={'obj': 'obj1'}, args=[], protected_args=[], obj=None, _meta={}, default_map=None, invoked_subcommand=None, terminal_width=None, max_content_width=None, allow_extra_args=True, allow_interspersed_args=False, ignore_unknown_options=False, help_option_names=['--help'], token_normalize_func=None, resilient_parsing=False, auto_envvar_prefix=None, color=None, _close_callbacks=[], _depth=2}, args=['move'], self.__doc__=None, self.add_help_option=True, self.callback=<function test_group_with_args.<locals>.cli at 0x7fe6dd873550>, self.chain=False, self.commands={'move': <click.core.Command object at 0x7fe6e9562880>}, self.context_settings={}, self.epilog=None, self.help=None, self.invoke_without_command=False, self.name='cli', self.no_args_is_help=True, self.options_metavar='[OPTIONS]', self.params=[<click.core.Argument object at 0x7fe6e9562970>], self.result_callback=None, self.short_help=None, self.subcommand_metavar='COMMAND [ARGS]...')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "cmd_name = make_str(args[0])\n",
      "State:\n",
      "'move'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def resolve_command(self, ctx, args):\n",
      "        cmd_name = make_str(args[0])\n",
      "        original_cmd_name = cmd_name\n",
      "        cmd = self.get_command(ctx, cmd_name)\n",
      "        if cmd is None and ctx.token_normalize_func is not None:\n",
      "            cmd_name = ctx.token_normalize_func(cmd_name)\n",
      "            cmd = self.get_command(ctx, cmd_name)\n",
      "        if cmd is None:\n",
      "            if split_opt(cmd_name)[0]:\n",
      "                self.parse_args(ctx, ctx.args)\n",
      "            ctx.fail('No such command \"%s\".' % original_cmd_name)\n",
      "        return cmd_name, cmd, args[1:]\n",
      "resolve_command(self=<click.core.Group object at 0x7fe6e9562310>, ctx={parent=None, command=<click.core.Group object at 0x7fe6e9562310>, info_name='cli', params={'obj': 'obj1'}, args=[], protected_args=[], obj=None, _meta={}, default_map=None, invoked_subcommand=None, terminal_width=None, max_content_width=None, allow_extra_args=True, allow_interspersed_args=False, ignore_unknown_options=False, help_option_names=['--help'], token_normalize_func=None, resilient_parsing=False, auto_envvar_prefix=None, color=None, _close_callbacks=[], _depth=2}, args=['move'], self.__doc__=None, self.add_help_option=True, self.callback=<function test_group_with_args.<locals>.cli at 0x7fe6dd873550>, self.chain=False, self.commands={'move': <click.core.Command object at 0x7fe6e9562880>}, self.context_settings={}, self.epilog=None, self.help=None, self.invoke_without_command=False, self.name='cli', self.no_args_is_help=True, self.options_metavar='[OPTIONS]', self.params=[<click.core.Argument object at 0x7fe6e9562970>], self.result_callback=None, self.short_help=None, self.subcommand_metavar='COMMAND [ARGS]...')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "original_cmd_name = cmd_name\n",
      "State:\n",
      "'move'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def resolve_command(self, ctx, args):\n",
      "        cmd_name = make_str(args[0])\n",
      "        original_cmd_name = cmd_name\n",
      "        cmd = self.get_command(ctx, cmd_name)\n",
      "        if cmd is None and ctx.token_normalize_func is not None:\n",
      "            cmd_name = ctx.token_normalize_func(cmd_name)\n",
      "            cmd = self.get_command(ctx, cmd_name)\n",
      "        if cmd is None:\n",
      "            if split_opt(cmd_name)[0]:\n",
      "                self.parse_args(ctx, ctx.args)\n",
      "            ctx.fail('No such command \"%s\".' % original_cmd_name)\n",
      "        return cmd_name, cmd, args[1:]\n",
      "resolve_command(self=<click.core.Group object at 0x7fe6e9562310>, ctx={parent=None, command=<click.core.Group object at 0x7fe6e9562310>, info_name='cli', params={'obj': 'obj1'}, args=[], protected_args=[], obj=None, _meta={}, default_map=None, invoked_subcommand=None, terminal_width=None, max_content_width=None, allow_extra_args=True, allow_interspersed_args=False, ignore_unknown_options=False, help_option_names=['--help'], token_normalize_func=None, resilient_parsing=False, auto_envvar_prefix=None, color=None, _close_callbacks=[], _depth=2}, args=['move'], self.__doc__=None, self.add_help_option=True, self.callback=<function test_group_with_args.<locals>.cli at 0x7fe6dd873550>, self.chain=False, self.commands={'move': <click.core.Command object at 0x7fe6e9562880>}, self.context_settings={}, self.epilog=None, self.help=None, self.invoke_without_command=False, self.name='cli', self.no_args_is_help=True, self.options_metavar='[OPTIONS]', self.params=[<click.core.Argument object at 0x7fe6e9562970>], self.result_callback=None, self.short_help=None, self.subcommand_metavar='COMMAND [ARGS]...')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "cmd = self.get_command(ctx, cmd_name)\n",
      "State:\n",
      "{name='move', context_settings={}, params=[], help=None, epilog=None, options_metavar='[OPTIONS]', short_help=None, add_help_option=True, __doc__=None}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def ensure_object(self, object_type):\n",
      "        rv = self.find_object(object_type)\n",
      "        if rv is None:\n",
      "            self.obj = rv = object_type()\n",
      "        return rv\n",
      "ensure_object(self=<click.core.Context object at 0x7fe6e93405b0>, object_type=<class 'test_context.test_ensure_context_objects.<locals>.Foo'>, self._close_callbacks=[], self._depth=3, self._meta={}, self.allow_extra_args=True, self.allow_interspersed_args=False, self.args=[], self.auto_envvar_prefix=None, self.color=None, self.command=<click.core.Group object at 0x7fe6e9340880>, self.default_map=None, self.help_option_names=['--help'], self.ignore_unknown_options=False, self.info_name='cli', self.invoked_subcommand='test', self.max_content_width=None, self.obj=None, self.params={}, self.parent=None, self.protected_args=[], self.resilient_parsing=False, self.terminal_width=None, self.token_normalize_func=None)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "rv = self.find_object(object_type)\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def indent_only(self, text):\n",
      "        rv = []\n",
      "        for idx, line in enumerate(text.splitlines()):\n",
      "            indent = self.initial_indent\n",
      "            if idx > 0:\n",
      "                indent = self.subsequent_indent\n",
      "            rv.append(indent + line)\n",
      "        return '\\n'.join(rv)\n",
      "indent_only(self=<click._textwrap.TextWrapper object at 0x7fe6e93404f0>, text='This is\\na paragraph\\nwithout rewrapping.', self.break_long_words=True, self.break_on_hyphens=True, self.drop_whitespace=True, self.expand_tabs=True, self.fix_sentence_endings=False, self.initial_indent='  ', self.max_lines=None, self.placeholder=' [...]', self.replace_whitespace=False, self.subsequent_indent='  ', self.tabsize=8, self.width=58)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "rv = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _match_short_opt(self, arg, state):\n",
      "        stop = False\n",
      "        i = 1\n",
      "        prefix = arg[0]\n",
      "        unknown_options = []\n",
      "        for ch in arg[1:]:\n",
      "            opt = normalize_opt(prefix + ch, self.ctx)\n",
      "            option = self._short_opt.get(opt)\n",
      "            i += 1\n",
      "            if not option:\n",
      "                if self.ignore_unknown_options:\n",
      "                    unknown_options.append(ch)\n",
      "                    continue\n",
      "                raise NoSuchOption(opt)\n",
      "            if option.takes_value:\n",
      "                if i < len(arg):\n",
      "                    state.rargs.insert(0, arg[i:])\n",
      "                    stop = True\n",
      "                nargs = option.nargs\n",
      "                if len(state.rargs) < nargs:\n",
      "                    _error_opt_args(nargs, opt)\n",
      "                elif nargs == 1:\n",
      "                    value = state.rargs.pop(0)\n",
      "                else:\n",
      "                    value = tuple(state.rargs[:nargs])\n",
      "                    del state.rargs[:nargs]\n",
      "            else:\n",
      "                value = None\n",
      "            option.process(value, state)\n",
      "            if stop:\n",
      "                break\n",
      "        if self.ignore_unknown_options and unknown_options:\n",
      "            state.largs.append(prefix + ''.join(unknown_options))\n",
      "_match_short_opt(self=<click.parser.OptionParser object at 0x7fe6e9563820>, arg='-m', state={opts={}, largs=[], rargs=['foo', '-mbar'], order=[]}, self._args=[], self._long_opt={'--message': <click.parser.Option object at 0x7fe6e95632b0>, '--help': <click.parser.Option object at 0x7fe6e9563940>}, self._opt_prefixes={'--', '-'}, self._short_opt={'-m': <click.parser.Option object at 0x7fe6e95632b0>}, self.allow_interspersed_args=True, self.ctx=<click.core.Context object at 0x7fe6e96d9130>, self.ignore_unknown_options=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "stop = False\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _match_short_opt(self, arg, state):\n",
      "        stop = False\n",
      "        i = 1\n",
      "        prefix = arg[0]\n",
      "        unknown_options = []\n",
      "        for ch in arg[1:]:\n",
      "            opt = normalize_opt(prefix + ch, self.ctx)\n",
      "            option = self._short_opt.get(opt)\n",
      "            i += 1\n",
      "            if not option:\n",
      "                if self.ignore_unknown_options:\n",
      "                    unknown_options.append(ch)\n",
      "                    continue\n",
      "                raise NoSuchOption(opt)\n",
      "            if option.takes_value:\n",
      "                if i < len(arg):\n",
      "                    state.rargs.insert(0, arg[i:])\n",
      "                    stop = True\n",
      "                nargs = option.nargs\n",
      "                if len(state.rargs) < nargs:\n",
      "                    _error_opt_args(nargs, opt)\n",
      "                elif nargs == 1:\n",
      "                    value = state.rargs.pop(0)\n",
      "                else:\n",
      "                    value = tuple(state.rargs[:nargs])\n",
      "                    del state.rargs[:nargs]\n",
      "            else:\n",
      "                value = None\n",
      "            option.process(value, state)\n",
      "            if stop:\n",
      "                break\n",
      "        if self.ignore_unknown_options and unknown_options:\n",
      "            state.largs.append(prefix + ''.join(unknown_options))\n",
      "_match_short_opt(self=<click.parser.OptionParser object at 0x7fe6e9563820>, arg='-m', state={opts={}, largs=[], rargs=['foo', '-mbar'], order=[]}, self._args=[], self._long_opt={'--message': <click.parser.Option object at 0x7fe6e95632b0>, '--help': <click.parser.Option object at 0x7fe6e9563940>}, self._opt_prefixes={'--', '-'}, self._short_opt={'-m': <click.parser.Option object at 0x7fe6e95632b0>}, self.allow_interspersed_args=True, self.ctx=<click.core.Context object at 0x7fe6e96d9130>, self.ignore_unknown_options=False)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "i = 1\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _match_short_opt(self, arg, state):\n",
      "        stop = False\n",
      "        i = 1\n",
      "        prefix = arg[0]\n",
      "        unknown_options = []\n",
      "        for ch in arg[1:]:\n",
      "            opt = normalize_opt(prefix + ch, self.ctx)\n",
      "            option = self._short_opt.get(opt)\n",
      "            i += 1\n",
      "            if not option:\n",
      "                if self.ignore_unknown_options:\n",
      "                    unknown_options.append(ch)\n",
      "                    continue\n",
      "                raise NoSuchOption(opt)\n",
      "            if option.takes_value:\n",
      "                if i < len(arg):\n",
      "                    state.rargs.insert(0, arg[i:])\n",
      "                    stop = True\n",
      "                nargs = option.nargs\n",
      "                if len(state.rargs) < nargs:\n",
      "                    _error_opt_args(nargs, opt)\n",
      "                elif nargs == 1:\n",
      "                    value = state.rargs.pop(0)\n",
      "                else:\n",
      "                    value = tuple(state.rargs[:nargs])\n",
      "                    del state.rargs[:nargs]\n",
      "            else:\n",
      "                value = None\n",
      "            option.process(value, state)\n",
      "            if stop:\n",
      "                break\n",
      "        if self.ignore_unknown_options and unknown_options:\n",
      "            state.largs.append(prefix + ''.join(unknown_options))\n",
      "_match_short_opt(self=<click.parser.OptionParser object at 0x7fe6e9563820>, arg='-m', state={opts={}, largs=[], rargs=['foo', '-mbar'], order=[]}, self._args=[], self._long_opt={'--message': <click.parser.Option object at 0x7fe6e95632b0>, '--help': <click.parser.Option object at 0x7fe6e9563940>}, self._opt_prefixes={'--', '-'}, self._short_opt={'-m': <click.parser.Option object at 0x7fe6e95632b0>}, self.allow_interspersed_args=True, self.ctx=<click.core.Context object at 0x7fe6e96d9130>, self.ignore_unknown_options=False)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "prefix = arg[0]\n",
      "State:\n",
      "'-'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _match_short_opt(self, arg, state):\n",
      "        stop = False\n",
      "        i = 1\n",
      "        prefix = arg[0]\n",
      "        unknown_options = []\n",
      "        for ch in arg[1:]:\n",
      "            opt = normalize_opt(prefix + ch, self.ctx)\n",
      "            option = self._short_opt.get(opt)\n",
      "            i += 1\n",
      "            if not option:\n",
      "                if self.ignore_unknown_options:\n",
      "                    unknown_options.append(ch)\n",
      "                    continue\n",
      "                raise NoSuchOption(opt)\n",
      "            if option.takes_value:\n",
      "                if i < len(arg):\n",
      "                    state.rargs.insert(0, arg[i:])\n",
      "                    stop = True\n",
      "                nargs = option.nargs\n",
      "                if len(state.rargs) < nargs:\n",
      "                    _error_opt_args(nargs, opt)\n",
      "                elif nargs == 1:\n",
      "                    value = state.rargs.pop(0)\n",
      "                else:\n",
      "                    value = tuple(state.rargs[:nargs])\n",
      "                    del state.rargs[:nargs]\n",
      "            else:\n",
      "                value = None\n",
      "            option.process(value, state)\n",
      "            if stop:\n",
      "                break\n",
      "        if self.ignore_unknown_options and unknown_options:\n",
      "            state.largs.append(prefix + ''.join(unknown_options))\n",
      "_match_short_opt(self=<click.parser.OptionParser object at 0x7fe6e9563820>, arg='-m', state={opts={}, largs=[], rargs=['foo', '-mbar'], order=[]}, self._args=[], self._long_opt={'--message': <click.parser.Option object at 0x7fe6e95632b0>, '--help': <click.parser.Option object at 0x7fe6e9563940>}, self._opt_prefixes={'--', '-'}, self._short_opt={'-m': <click.parser.Option object at 0x7fe6e95632b0>}, self.allow_interspersed_args=True, self.ctx=<click.core.Context object at 0x7fe6e96d9130>, self.ignore_unknown_options=False)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "unknown_options = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def render_progress(self):\n",
      "        from .termui import get_terminal_size\n",
      "        nl = False\n",
      "        if self.is_hidden:\n",
      "            buf = [self.label]\n",
      "            nl = True\n",
      "        else:\n",
      "            buf = []\n",
      "            if self.autowidth:\n",
      "                old_width = self.width\n",
      "                self.width = 0\n",
      "                clutter_length = term_len(self.format_progress_line())\n",
      "                new_width = max(0, get_terminal_size()[0] - clutter_length)\n",
      "                if new_width < old_width:\n",
      "                    buf.append(BEFORE_BAR)\n",
      "                    buf.append(' ' * self.max_width)\n",
      "                    self.max_width = new_width\n",
      "                self.width = new_width\n",
      "            clear_width = self.width\n",
      "            if self.max_width is not None:\n",
      "                clear_width = self.max_width\n",
      "            buf.append(BEFORE_BAR)\n",
      "            line = self.format_progress_line()\n",
      "            line_len = term_len(line)\n",
      "            if self.max_width is None or self.max_width < line_len:\n",
      "                self.max_width = line_len\n",
      "            buf.append(line)\n",
      "            buf.append(' ' * (clear_width - line_len))\n",
      "        line = ''.join(buf)\n",
      "        if line != self._last_line:\n",
      "            self._last_line = line\n",
      "            echo(line, file=self.file, color=self.color, nl=nl)\n",
      "            self.file.flush()\n",
      "render_progress(self=<click._termui_impl.ProgressBar object at 0x7fe6e973e940>, self._last_line=None, self.autowidth=False, self.avg=[], self.bar_template='%(label)s  [%(bar)s]  %(info)s', self.color=None, self.current_item=None, self.empty_char='-', self.entered=True, self.eta_known=False, self.file=<_io.TextIOWrapper encoding='utf-8'>, self.fill_char='\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "nl = False\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def render_progress(self):\n",
      "        from .termui import get_terminal_size\n",
      "        nl = False\n",
      "        if self.is_hidden:\n",
      "            buf = [self.label]\n",
      "            nl = True\n",
      "        else:\n",
      "            buf = []\n",
      "            if self.autowidth:\n",
      "                old_width = self.width\n",
      "                self.width = 0\n",
      "                clutter_length = term_len(self.format_progress_line())\n",
      "                new_width = max(0, get_terminal_size()[0] - clutter_length)\n",
      "                if new_width < old_width:\n",
      "                    buf.append(BEFORE_BAR)\n",
      "                    buf.append(' ' * self.max_width)\n",
      "                    self.max_width = new_width\n",
      "                self.width = new_width\n",
      "            clear_width = self.width\n",
      "            if self.max_width is not None:\n",
      "                clear_width = self.max_width\n",
      "            buf.append(BEFORE_BAR)\n",
      "            line = self.format_progress_line()\n",
      "            line_len = term_len(line)\n",
      "            if self.max_width is None or self.max_width < line_len:\n",
      "                self.max_width = line_len\n",
      "            buf.append(line)\n",
      "            buf.append(' ' * (clear_width - line_len))\n",
      "        line = ''.join(buf)\n",
      "        if line != self._last_line:\n",
      "            self._last_line = line\n",
      "            echo(line, file=self.file, color=self.color, nl=nl)\n",
      "            self.file.flush()\n",
      "render_progress(self=<click._termui_impl.ProgressBar object at 0x7fe6e973e940>, self._last_line=None, self.autowidth=False, self.avg=[], self.bar_template='%(label)s  [%(bar)s]  %(info)s', self.color=None, self.current_item=None, self.empty_char='-', self.entered=True, self.eta_known=False, self.file=<_io.TextIOWrapper encoding='utf-8'>, self.fill_char='\n",
      "\n",
      "n:\n",
      "25\n",
      "Statement:\n",
      "buf.append(BEFORE_BAR)\n",
      "State:\n",
      "['\\r\\x1b[?25l']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def format_progress_line(self):\n",
      "        show_percent = self.show_percent\n",
      "        info_bits = []\n",
      "        if self.length_known:\n",
      "            bar_length = int(self.pct * self.width)\n",
      "            bar = self.fill_char * bar_length\n",
      "            bar += self.empty_char * (self.width - bar_length)\n",
      "            if show_percent is None:\n",
      "                show_percent = not self.show_pos\n",
      "        else:\n",
      "            if self.finished:\n",
      "                bar = self.fill_char * self.width\n",
      "            else:\n",
      "                bar = list(self.empty_char * (self.width or 1))\n",
      "                if self.time_per_iteration != 0:\n",
      "                    bar[int((math.cos(self.pos * self.time_per_iteration)\n",
      "                        / 2.0 + 0.5) * self.width)] = self.fill_char\n",
      "                bar = ''.join(bar)\n",
      "        if self.show_pos:\n",
      "            info_bits.append(self.format_pos())\n",
      "        if show_percent:\n",
      "            info_bits.append(self.format_pct())\n",
      "        if self.show_eta and self.eta_known and not self.finished:\n",
      "            info_bits.append(self.format_eta())\n",
      "        if self.item_show_func is not None:\n",
      "            item_info = self.item_show_func(self.current_item)\n",
      "            if item_info is not None:\n",
      "                info_bits.append(item_info)\n",
      "        return (self.bar_template % {\n",
      "            'label': self.label,\n",
      "            'bar': bar,\n",
      "            'info': self.info_sep.join(info_bits)\n",
      "        }).rstrip()\n",
      "format_progress_line(self=<click._termui_impl.ProgressBar object at 0x7fe6e973e940>, self._last_line=None, self.autowidth=False, self.avg=[], self.bar_template='%(label)s  [%(bar)s]  %(info)s', self.color=None, self.current_item=None, self.empty_char='-', self.entered=True, self.eta_known=False, self.file=<_io.TextIOWrapper encoding='utf-8'>, self.fill_char='\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "show_percent = self.show_percent\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def format_progress_line(self):\n",
      "        show_percent = self.show_percent\n",
      "        info_bits = []\n",
      "        if self.length_known:\n",
      "            bar_length = int(self.pct * self.width)\n",
      "            bar = self.fill_char * bar_length\n",
      "            bar += self.empty_char * (self.width - bar_length)\n",
      "            if show_percent is None:\n",
      "                show_percent = not self.show_pos\n",
      "        else:\n",
      "            if self.finished:\n",
      "                bar = self.fill_char * self.width\n",
      "            else:\n",
      "                bar = list(self.empty_char * (self.width or 1))\n",
      "                if self.time_per_iteration != 0:\n",
      "                    bar[int((math.cos(self.pos * self.time_per_iteration)\n",
      "                        / 2.0 + 0.5) * self.width)] = self.fill_char\n",
      "                bar = ''.join(bar)\n",
      "        if self.show_pos:\n",
      "            info_bits.append(self.format_pos())\n",
      "        if show_percent:\n",
      "            info_bits.append(self.format_pct())\n",
      "        if self.show_eta and self.eta_known and not self.finished:\n",
      "            info_bits.append(self.format_eta())\n",
      "        if self.item_show_func is not None:\n",
      "            item_info = self.item_show_func(self.current_item)\n",
      "            if item_info is not None:\n",
      "                info_bits.append(item_info)\n",
      "        return (self.bar_template % {\n",
      "            'label': self.label,\n",
      "            'bar': bar,\n",
      "            'info': self.info_sep.join(info_bits)\n",
      "        }).rstrip()\n",
      "format_progress_line(self=<click._termui_impl.ProgressBar object at 0x7fe6e973e940>, self._last_line=None, self.autowidth=False, self.avg=[], self.bar_template='%(label)s  [%(bar)s]  %(info)s', self.color=None, self.current_item=None, self.empty_char='-', self.entered=True, self.eta_known=False, self.file=<_io.TextIOWrapper encoding='utf-8'>, self.fill_char='\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "bar_length = int(self.pct * self.width)\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def next(self):\n",
      "        if self.is_hidden:\n",
      "            return next(self.iter)\n",
      "        try:\n",
      "            rv = next(self.iter)\n",
      "            self.current_item = rv\n",
      "        except StopIteration:\n",
      "            self.finish()\n",
      "            self.render_progress()\n",
      "            raise StopIteration()\n",
      "        else:\n",
      "            self.update(1)\n",
      "            return rv\n",
      "next(self=<click._termui_impl.ProgressBar object at 0x7fe6e973e940>, self._last_line='\\r\\x1b[?25l    padded line  [------------------------------------]    0%', self.autowidth=False, self.avg=[], self.bar_template='%(label)s  [%(bar)s]  %(info)s', self.color=None, self.current_item=None, self.empty_char='-', self.entered=True, self.eta_known=False, self.file=<_io.TextIOWrapper encoding='utf-8'>, self.fill_char='\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "rv = next(self.iter)\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def next(self):\n",
      "        if self.is_hidden:\n",
      "            return next(self.iter)\n",
      "        try:\n",
      "            rv = next(self.iter)\n",
      "            self.current_item = rv\n",
      "        except StopIteration:\n",
      "            self.finish()\n",
      "            self.render_progress()\n",
      "            raise StopIteration()\n",
      "        else:\n",
      "            self.update(1)\n",
      "            return rv\n",
      "next(self=<click._termui_impl.ProgressBar object at 0x7fe6e973e940>, self._last_line='\\r\\x1b[?25l    padded line  [------------------------------------]    0%', self.autowidth=False, self.avg=[], self.bar_template='%(label)s  [%(bar)s]  %(info)s', self.color=None, self.current_item=None, self.empty_char='-', self.entered=True, self.eta_known=False, self.file=<_io.TextIOWrapper encoding='utf-8'>, self.fill_char='\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "self.current_item = rv\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def make_step(self, n_steps):\n",
      "        self.pos += n_steps\n",
      "        if self.length_known and self.pos >= self.length:\n",
      "            self.finished = True\n",
      "        if (time.time() - self.last_eta) < 1.0:\n",
      "            return\n",
      "        self.last_eta = time.time()\n",
      "        self.avg = self.avg[-6:] + [-(self.start - time.time()) / (self.pos)]\n",
      "        self.eta_known = self.length_known\n",
      "make_step(self=<click._termui_impl.ProgressBar object at 0x7fe6e973e940>, n_steps=1, self._last_line='\\r\\x1b[?25l    padded line  [------------------------------------]    0%', self.autowidth=False, self.avg=[], self.bar_template='%(label)s  [%(bar)s]  %(info)s', self.color=None, self.current_item=0, self.empty_char='-', self.entered=True, self.eta_known=False, self.file=<_io.TextIOWrapper encoding='utf-8'>, self.fill_char='\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.pos += n_steps\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test():\n",
      "        i = click.get_binary_stream('stdin')\n",
      "        o = click.get_binary_stream('stdout')\n",
      "        while 1:\n",
      "            chunk = i.read(4096)\n",
      "            if not chunk:\n",
      "                break\n",
      "            o.write(chunk)\n",
      "            o.flush()\n",
      "test()\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "i = click.get_binary_stream('stdin')\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test():\n",
      "        i = click.get_binary_stream('stdin')\n",
      "        o = click.get_binary_stream('stdout')\n",
      "        while 1:\n",
      "            chunk = i.read(4096)\n",
      "            if not chunk:\n",
      "                break\n",
      "            o.write(chunk)\n",
      "            o.flush()\n",
      "test()\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "o = click.get_binary_stream('stdout')\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _build_prompt(text, suffix, show_default=False, default=None):\n",
      "    prompt = text\n",
      "    if default is not None and show_default:\n",
      "        prompt = '%s [%s]' % (prompt, default)\n",
      "    return prompt + suffix\n",
      "_build_prompt(text='Foo', suffix=': ', show_default=True, default=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "prompt = text\n",
      "State:\n",
      "'Foo'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_echo_via_pager(monkeypatch, capfd, cat):\n",
      "    monkeypatch.setitem(os.environ, 'PAGER', cat)\n",
      "    monkeypatch.setattr(click._termui_impl, 'isatty', lambda x: True)\n",
      "    click.echo_via_pager('haha')\n",
      "    out, err = capfd.readouterr()\n",
      "    assert out == 'haha\\n'\n",
      "test_echo_via_pager(monkeypatch={_setattr=[], _setitem=[], _cwd=None, _savesyspath=None}, capfd={request=<SubRequest 'capfd' for <Function test_echo_via_pager[cat]>>, _capture=<MultiCapture out=<FDCapture 1 oldfd=6 _state='started' tmpfile=<_io.TextIOWrapper name=\"<_io.FileIO name=7 mode='rb+' closefd=True>\" mode='r+' encoding='utf-8'>> err=<FDCapture 2 oldfd=8 _state='started' tmpfile=<_io.TextIOWrapper name=\"<_io.FileIO name=9 mode='rb+' closefd=True>\" mode='r+' encoding='utf-8'>> in_=None _state='started' _in_suspended=False>, _captured_out='', _captured_err=''}, cat='cat')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "monkeypatch.setitem(os.environ, 'PAGER', cat)\n",
      "State:\n",
      "{_setattr=[], _setitem=[(environ({'SHELL': '/bin/bash', 'LSCOLORS': 'Gxfxcxdxbxegedabagacad', 'USER_ZDOTDIR': '/home/XXX', 'COLORTERM': 'truecolor', 'LESS': '-R', 'TERM_PROGRAM_VERSION': '3.2a', 'GVM_VERSION': '1.0.22', 'CONDA_EXE': '/local/rcs/XXX/miniforge3/bin/conda', '_CE_M': '', 'TMUX': '/tmp/tmux-19200/default,59951,3', 'PKG_CONFIG_PATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib/pkgconfig:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib/pkgconfig:', '_P9K_TTY': '/dev/pts/20', 'GVM_PATH_BACKUP': '/home/XXX/.gvm/bin:/local/rcs/XXX/miniforge3/envs/mal/bin:/local/rcs/XXX/miniforge3/condabin:/home/XXX/.gdrive-downloader:/local/arise/XXX/miniforge3/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/bin:/home/XXX/.gvm/gos/go1.19.1/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/bin:/home/XXX/.gvm/bin:/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/XXX/.local/bin:/home/XXX/.local/bin:/home/XXX/.local/bin', 'P9K_TTY': 'old', 'LC_FIG_SET_PARENT': '4c022497-5122-4b80-b325-c89bab32302a', 'PWD': '/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/MrTango+click/MrTango+click', 'LOGNAME': 'XXX', 'XDG_SESSION_TYPE': 'tty', 'CONDA_PREFIX': '/local/rcs/XXX/miniforge3/envs/MrTango+click', 'VSCODE_GIT_ASKPASS_NODE': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/node', 'MOTD_SHOWN': 'pam', 'VSCODE_INJECTION': '1', 'GVM_OVERLAY_PREFIX': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay', 'HOME': '/home/XXX', 'LANG': 'en_US.UTF-8', 'DYLD_LIBRARY_PATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'gvm_pkgset_name': 'global', 'SSL_CERT_DIR': '/usr/lib/ssl/certs', 'CONDA_PROMPT_MODIFIER': '(MrTango+click) ', 'GIT_ASKPASS': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/extensions/git/dist/askpass.sh', 'GVM_ROOT': '/home/XXX/.gvm', 'SSH_CONNECTION': '127.0.0.1 39996 127.0.0.1 22', 'GOROOT': '/home/XXX/.gvm/gos/go1.19.1', 'NVM_DIR': '/local/rcs/XXX/.nvm', 'VSCODE_GIT_ASKPASS_EXTRA_ARGS': '', 'XDG_SESSION_CLASS': 'user', 'PYTHONPATH': ':/local/rcs/XXX/code/pytrace-collector:/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/MrTango+click/MrTango+click', 'TERM': 'screen', 'ZSH': '/home/XXX/.oh-my-zsh', '_CE_CONDA': '', 'VSCODE_NONCE': 'd0bc7031-48a3-4719-8bb5-ef236ddd0016', 'ZDOTDIR': '/home/XXX', 'USER': 'XXX', 'TMUX_PANE': '%3', 'VSCODE_GIT_IPC_HANDLE': '/run/user/19200/vscode-git-13d67c6199.sock', 'CONDA_SHLVL': '3', 'SHLVL': '3', 'PAGER': 'cat', '_P9K_SSH_TTY': '/dev/pts/20', 'XDG_SESSION_ID': '43', 'CONDA_PYTHON_EXE': '/local/rcs/XXX/miniforge3/bin/python', 'LD_LIBRARY_PATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib', 'XDG_RUNTIME_DIR': '/run/user/19200', 'SSL_CERT_FILE': '/usr/lib/ssl/certs/ca-certificates.crt', 'SSH_CLIENT': '127.0.0.1 46946 22', 'CONDA_DEFAULT_ENV': 'MrTango+click', 'P9K_SSH': '1', 'LC_ALL': 'en_US.UTF-8', 'VSCODE_GIT_ASKPASS_MAIN': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/extensions/git/dist/askpass-main.js', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'BROWSER': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/helpers/browser.sh', 'PATH': '/home/XXX/.gdrive-downloader:/local/arise/XXX/miniforge3/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/bin:/home/XXX/.gvm/gos/go1.19.1/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/bin:/home/XXX/.gvm/bin:/local/rcs/XXX/miniforge3/envs/MrTango+click/bin:/local/rcs/XXX/miniforge3/condabin:/home/XXX/.gdrive-downloader:/local/arise/XXX/miniforge3/bin:/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/XXX/.local/bin:/home/XXX/.local/bin', 'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/19200/bus', 'gvm_go_name': 'go1.19.1', 'CONDA_PREFIX_1': '/local/rcs/XXX/miniforge3', 'CONDA_PREFIX_2': '/local/rcs/XXX/miniforge3/envs/mal', 'OLDPWD': '/local/rcs/XXX/code/pytrace-collector', 'GOPATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global', 'TERM_PROGRAM': 'tmux', 'VSCODE_IPC_HOOK_CLI': '/run/user/19200/vscode-ipc-518d6355-acaf-4714-a359-be3fe9f21e09.sock', '_': '/local/rcs/XXX/miniforge3/envs/MrTango+click/bin/python', 'PYTEST_CURRENT_TEST': 'tests/test_utils.py::test_echo_via_pager[cat] (call)'}), 'PAGER', 'less')], _cwd=None, _savesyspath=None}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_echo_via_pager(monkeypatch, capfd, cat):\n",
      "    monkeypatch.setitem(os.environ, 'PAGER', cat)\n",
      "    monkeypatch.setattr(click._termui_impl, 'isatty', lambda x: True)\n",
      "    click.echo_via_pager('haha')\n",
      "    out, err = capfd.readouterr()\n",
      "    assert out == 'haha\\n'\n",
      "test_echo_via_pager(monkeypatch={_setattr=[], _setitem=[], _cwd=None, _savesyspath=None}, capfd={request=<SubRequest 'capfd' for <Function test_echo_via_pager[cat]>>, _capture=<MultiCapture out=<FDCapture 1 oldfd=6 _state='started' tmpfile=<_io.TextIOWrapper name=\"<_io.FileIO name=7 mode='rb+' closefd=True>\" mode='r+' encoding='utf-8'>> err=<FDCapture 2 oldfd=8 _state='started' tmpfile=<_io.TextIOWrapper name=\"<_io.FileIO name=9 mode='rb+' closefd=True>\" mode='r+' encoding='utf-8'>> in_=None _state='started' _in_suspended=False>, _captured_out='', _captured_err=''}, cat='cat')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "out, err = capfd.readouterr()\n",
      "State:\n",
      "'haha\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_echo_via_pager(monkeypatch, capfd, cat):\n",
      "    monkeypatch.setitem(os.environ, 'PAGER', cat)\n",
      "    monkeypatch.setattr(click._termui_impl, 'isatty', lambda x: True)\n",
      "    click.echo_via_pager('haha')\n",
      "    out, err = capfd.readouterr()\n",
      "    assert out == 'haha\\n'\n",
      "test_echo_via_pager(monkeypatch={_setattr=[], _setitem=[], _cwd=None, _savesyspath=None}, capfd={request=<SubRequest 'capfd' for <Function test_echo_via_pager[cat]>>, _capture=<MultiCapture out=<FDCapture 1 oldfd=6 _state='started' tmpfile=<_io.TextIOWrapper name=\"<_io.FileIO name=7 mode='rb+' closefd=True>\" mode='r+' encoding='utf-8'>> err=<FDCapture 2 oldfd=8 _state='started' tmpfile=<_io.TextIOWrapper name=\"<_io.FileIO name=9 mode='rb+' closefd=True>\" mode='r+' encoding='utf-8'>> in_=None _state='started' _in_suspended=False>, _captured_out='', _captured_err=''}, cat='cat')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "assert out == 'haha\\n'\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _pipepager(text, cmd, color):\n",
      "    import subprocess\n",
      "    env = dict(os.environ)\n",
      "    cmd_detail = cmd.rsplit('/', 1)[-1].split()\n",
      "    if color is None and cmd_detail[0] == 'less':\n",
      "        less_flags = os.environ.get('LESS', '') + ' '.join(cmd_detail[1:])\n",
      "        if not less_flags:\n",
      "            env['LESS'] = '-R'\n",
      "            color = True\n",
      "        elif 'r' in less_flags or 'R' in less_flags:\n",
      "            color = True\n",
      "    if not color:\n",
      "        text = strip_ansi(text)\n",
      "    c = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE,\n",
      "                         env=env)\n",
      "    encoding = get_best_encoding(c.stdin)\n",
      "    try:\n",
      "        c.stdin.write(text.encode(encoding, 'replace'))\n",
      "        c.stdin.close()\n",
      "    except (IOError, KeyboardInterrupt):\n",
      "        pass\n",
      "    while True:\n",
      "        try:\n",
      "            c.wait()\n",
      "        except KeyboardInterrupt:\n",
      "            pass\n",
      "        else:\n",
      "            break\n",
      "_pipepager(text='haha\\n', cmd='cat', color=None)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "env = dict(os.environ)\n",
      "State:\n",
      "{'SHELL': '/bin/bash', 'LSCOLORS': 'Gxfxcxdxbxegedabagacad', 'USER_ZDOTDIR': '/home/XXX', 'COLORTERM': 'truecolor', 'LESS': '-R', 'TERM_PROGRAM_VERSION': '3.2a', 'GVM_VERSION': '1.0.22', 'CONDA_EXE': '/local/rcs/XXX/miniforge3/bin/conda', '_CE_M': '', 'TMUX': '/tmp/tmux-19200/default,59951,3', 'PKG_CONFIG_PATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib/pkgconfig:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib/pkgconfig:', '_P9K_TTY': '/dev/pts/20', 'GVM_PATH_BACKUP': '/home/XXX/.gvm/bin:/local/rcs/XXX/miniforge3/envs/mal/bin:/local/rcs/XXX/miniforge3/condabin:/home/XXX/.gdrive-downloader:/local/arise/XXX/miniforge3/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/bin:/home/XXX/.gvm/gos/go1.19.1/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/bin:/home/XXX/.gvm/bin:/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/XXX/.local/bin:/home/XXX/.local/bin:/home/XXX/.local/bin', 'P9K_TTY': 'old', 'LC_FIG_SET_PARENT': '4c022497-5122-4b80-b325-c89bab32302a', 'PWD': '/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/MrTango+click/MrTango+click', 'LOGNAME': 'XXX', 'XDG_SESSION_TYPE': 'tty', 'CONDA_PREFIX': '/local/rcs/XXX/miniforge3/envs/MrTango+click', 'VSCODE_GIT_ASKPASS_NODE': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/node', 'MOTD_SHOWN': 'pam', 'VSCODE_INJECTION': '1', 'GVM_OVERLAY_PREFIX': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay', 'HOME': '/home/XXX', 'LANG': 'en_US.UTF-8', 'DYLD_LIBRARY_PATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'gvm_pkgset_name': 'global', 'SSL_CERT_DIR': '/usr/lib/ssl/certs', 'CONDA_PROMPT_MODIFIER': '(MrTango+click) ', 'GIT_ASKPASS': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/extensions/git/dist/askpass.sh', 'GVM_ROOT': '/home/XXX/.gvm', 'SSH_CONNECTION': '127.0.0.1 39996 127.0.0.1 22', 'GOROOT': '/home/XXX/.gvm/gos/go1.19.1', 'NVM_DIR': '/local/rcs/XXX/.nvm', 'VSCODE_GIT_ASKPASS_EXTRA_ARGS': '', 'XDG_SESSION_CLASS': 'user', 'PYTHONPATH': ':/local/rcs/XXX/code/pytrace-collector:/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/MrTango+click/MrTango+click', 'TERM': 'screen', 'ZSH': '/home/XXX/.oh-my-zsh', '_CE_CONDA': '', 'VSCODE_NONCE': 'd0bc7031-48a3-4719-8bb5-ef236ddd0016', 'ZDOTDIR': '/home/XXX', 'USER': 'XXX', 'TMUX_PANE': '%3', 'VSCODE_GIT_IPC_HANDLE': '/run/user/19200/vscode-git-13d67c6199.sock', 'CONDA_SHLVL': '3', 'SHLVL': '3', 'PAGER': 'cat', '_P9K_SSH_TTY': '/dev/pts/20', 'XDG_SESSION_ID': '43', 'CONDA_PYTHON_EXE': '/local/rcs/XXX/miniforge3/bin/python', 'LD_LIBRARY_PATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib', 'XDG_RUNTIME_DIR': '/run/user/19200', 'SSL_CERT_FILE': '/usr/lib/ssl/certs/ca-certificates.crt', 'SSH_CLIENT': '127.0.0.1 46946 22', 'CONDA_DEFAULT_ENV': 'MrTango+click', 'P9K_SSH': '1', 'LC_ALL': 'en_US.UTF-8', 'VSCODE_GIT_ASKPASS_MAIN': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/extensions/git/dist/askpass-main.js', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'BROWSER': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/helpers/browser.sh', 'PATH': '/home/XXX/.gdrive-downloader:/local/arise/XXX/miniforge3/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/bin:/home/XXX/.gvm/gos/go1.19.1/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/bin:/home/XXX/.gvm/bin:/local/rcs/XXX/miniforge3/envs/MrTango+click/bin:/local/rcs/XXX/miniforge3/condabin:/home/XXX/.gdrive-downloader:/local/arise/XXX/miniforge3/bin:/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/XXX/.local/bin:/home/XXX/.local/bin', 'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/19200/bus', 'gvm_go_name': 'go1.19.1', 'CONDA_PREFIX_1': '/local/rcs/XXX/miniforge3', 'CONDA_PREFIX_2': '/local/rcs/XXX/miniforge3/envs/mal', 'OLDPWD': '/local/rcs/XXX/code/pytrace-collector', 'GOPATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global', 'TERM_PROGRAM': 'tmux', 'VSCODE_IPC_HOOK_CLI': '/run/user/19200/vscode-ipc-518d6355-acaf-4714-a359-be3fe9f21e09.sock', '_': '/local/rcs/XXX/miniforge3/envs/MrTango+click/bin/python', 'PYTEST_CURRENT_TEST': 'tests/test_utils.py::test_echo_via_pager[cat] (call)'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse_array(data, start=0):\n",
      "    endcnt = data.find(CRLF, start + 1)\n",
      "    if endcnt == -1:\n",
      "        raise ParseError(\"Unterminated array element count after pos {}.\".format(start + 1))\n",
      "    try:\n",
      "        count = int(data[start + 1:endcnt])\n",
      "    except (ValueError, TypeError):\n",
      "        raise ParseError(\"Invalid array element count at pos {} - {}.\".format(start + 1, endcnt))\n",
      "    start = endcnt + CRLFLEN\n",
      "    if count == -1:\n",
      "        return None, endcnt\n",
      "    result = []\n",
      "    for i in range(count):\n",
      "        if start + 4 < len(data):\n",
      "            obj, start = _decode(data, start)\n",
      "            result.append(obj)\n",
      "        else:\n",
      "            raise ParseError(\"Unterminated array element at pos {}\".format(start))\n",
      "    return result, start\n",
      "parse_array(data='*3\\r\\n$3\\r\\nSET\\r\\n$15\\r\\nmemtier-8232902\\r\\n$2\\r\\nxx\\r\\n*3\\r\\n$3\\r\\nSET\\r\\n$15\\r\\nmemtier-8232902\\r\\n$2\\r\\nxx\\r\\n*3\\r\\n$3\\r\\nSET\\r\\n$15\\r\\nmemtier-7630684\\r\\n$3\\r\\nAAA\\r\\n', start=0)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "endcnt = data.find(CRLF, start + 1)\n",
      "State:\n",
      "2\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _adjust_sell_amount(self, stock_code, amount):\n",
      "        stock_code = stock_code[-6:]\n",
      "        user = self._users[0]\n",
      "        position = user.position\n",
      "        try:\n",
      "            stock = next(s for s in position if s[\"\"] == stock_code)\n",
      "        except StopIteration:\n",
      "            logger.info(\" %s  %s, \", stock_code, stock_code)\n",
      "            return amount\n",
      "        available_amount = stock[\"\"]\n",
      "        if available_amount >= amount:\n",
      "            return amount\n",
      "        adjust_amount = available_amount // 100 * 100\n",
      "        logger.info(\n",
      "            \" %s  %s,  %s,  %s\",\n",
      "            stock_code,\n",
      "            available_amount,\n",
      "            amount,\n",
      "            adjust_amount,\n",
      "        )\n",
      "        return adjust_amount\n",
      "_adjust_sell_amount(self=<easytrader.xq_follower.XueQiuFollower object at 0x7f0fc209db20>, stock_code='169101', amount=600, self._adjust_sell=True, self._users=[<MagicMock id='139705656662000'>], self.expired_cmds=set(), self.s=<requests.sessions.Session object at 0x7f0fc209d2e0>, self.slippage=0.0, self.trade_queue=<queue.Queue object at 0x7f0fc209dfd0>)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "user = self._users[0]\n",
      "State:\n",
      "<MagicMock id='139705656662000'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def project_transactions(self, transactions, assets):\n",
      "        for transaction in transactions:\n",
      "            weight_diff = self.none_to_zero(transaction[\"weight\"]) - self.none_to_zero(\n",
      "                transaction[\"prev_weight\"]\n",
      "            )\n",
      "            initial_amount = abs(weight_diff) / 100 * assets / transaction[\"price\"]\n",
      "            transaction[\"datetime\"] = datetime.fromtimestamp(\n",
      "                transaction[\"created_at\"] // 1000\n",
      "            )\n",
      "            transaction[\"stock_code\"] = transaction[\"stock_symbol\"].lower()\n",
      "            transaction[\"action\"] = \"buy\" if weight_diff > 0 else \"sell\"\n",
      "            transaction[\"amount\"] = int(round(initial_amount, -2))\n",
      "            if transaction[\"action\"] == \"sell\" and self._adjust_sell:\n",
      "                transaction[\"amount\"] = self._adjust_sell_amount(\n",
      "                    transaction[\"stock_code\"], transaction[\"amount\"]\n",
      "                )\n",
      "project_transactions(self=<easytrader.xq_follower.XueQiuFollower object at 0x7f0fc209dd00>, transactions=[{'weight': 10, 'prev_weight': 0, 'price': 10, 'stock_symbol': '162411', 'created_at': 1712231485016}], assets=1000, self._adjust_sell=True, self._adjust_sell_amount=<MagicMock id='139705656663152'>, self._users=None, self.expired_cmds=set(), self.s=<requests.sessions.Session object at 0x7f0fc2030910>, self.slippage=0.0, self.trade_queue=<queue.Queue object at 0x7f0fc209d850>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "weight_diff = self.none_to_zero(transaction[\"weight\"]) - self.none_to_zero(\n",
      "State:\n",
      "10\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_headers(self) -> Dict:\n",
      "        headers = {\n",
      "            'Accept': 'application/json',\n",
      "            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
      "        }\n",
      "        if self.API_KEY:\n",
      "            assert self.API_KEY\n",
      "            headers['X-MBX-APIKEY'] = self.API_KEY\n",
      "        return headers\n",
      "_get_headers(self=<binance.client.Client object at 0x7f60a8df5880>, self.API_KEY='api_key', self.API_SECRET='api_secret', self.API_URL='https://api.binance.com/api', self.FUTURES_COIN_DATA_URL='https://dapi.binance.com/futures/data', self.FUTURES_COIN_URL='https://dapi.binance.com/dapi', self.FUTURES_DATA_URL='https://fapi.binance.com/futures/data', self.FUTURES_URL='https://fapi.binance.com/fapi', self.MARGIN_API_URL='https://api.binance.com/sapi', self.OPTIONS_TESTNET_URL='https://testnet.binanceops.com/eapi', self.OPTIONS_URL='https://eapi.binance.com/eapi', self.PRIVATE_KEY=None, self.WEBSITE_URL='https://www.binance.com', self.tld='com')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "headers = {\n",
      "State:\n",
      "{'Accept': 'application/json', 'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_request_kwargs(self, method, signed: bool, force_params: bool = False, **kwargs) -> Dict:\n",
      "        kwargs['timeout'] = self.REQUEST_TIMEOUT\n",
      "        if self._requests_params:\n",
      "            kwargs.update(self._requests_params)\n",
      "        data = kwargs.get('data', None)\n",
      "        if data and isinstance(data, dict):\n",
      "            kwargs['data'] = data\n",
      "            if 'requests_params' in kwargs['data']:\n",
      "                kwargs.update(kwargs['data']['requests_params'])\n",
      "                del kwargs['data']['requests_params']\n",
      "        if signed:\n",
      "            kwargs['data']['timestamp'] = int(time.time() * 1000 + self.timestamp_offset)\n",
      "            kwargs['data']['signature'] = self._generate_signature(kwargs['data'])\n",
      "        if data:\n",
      "            kwargs['data'] = self._order_params(kwargs['data'])\n",
      "            null_args = [i for i, (key, value) in enumerate(kwargs['data']) if value is None]\n",
      "            for i in reversed(null_args):\n",
      "                del kwargs['data'][i]\n",
      "        if data and (method == 'get' or force_params):\n",
      "            kwargs['params'] = '&'.join('%s=%s' % (data[0], data[1]) for data in kwargs['data'])\n",
      "            del kwargs['data']\n",
      "        return kwargs\n",
      "_get_request_kwargs(self=<binance.client.Client object at 0x7f60a8df5880>, method='get', signed=False, force_params=False, kwargs={}, self.API_KEY='api_key', self.API_SECRET='api_secret', self.API_URL='https://api.binance.com/api', self.FUTURES_COIN_DATA_URL='https://dapi.binance.com/futures/data', self.FUTURES_COIN_URL='https://dapi.binance.com/dapi', self.FUTURES_DATA_URL='https://fapi.binance.com/futures/data', self.FUTURES_URL='https://fapi.binance.com/fapi', self.MARGIN_API_URL='https://api.binance.com/sapi', self.OPTIONS_TESTNET_URL='https://testnet.binanceops.com/eapi', self.OPTIONS_URL='https://eapi.binance.com/eapi', self.PRIVATE_KEY=None, self.WEBSITE_URL='https://www.binance.com', self._requests_params=None, self.response=None, self.session=<requests.sessions.Session object at 0x7f60a8df5f10>, self.testnet=False, self.timestamp_offset=0, self.tld='com')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "kwargs['timeout'] = self.REQUEST_TIMEOUT\n",
      "State:\n",
      "{'timeout': 10}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def fetch(url: str, **kwargs) -> Selector:\n",
      "    kwargs.setdefault('headers', DEFAULT_HEADERS)\n",
      "    try:\n",
      "        res = requests.get(url, **kwargs)\n",
      "        res.encoding = kwargs.get('encoding', DEFAULT_ENCODING)\n",
      "        res.raise_for_status()\n",
      "    except requests.RequestException as e:\n",
      "        print(e)\n",
      "    else:\n",
      "        html = res.text\n",
      "        tree = Selector(text=html)\n",
      "        return tree\n",
      "fetch(url='https://konachan.com/post', kwargs={})\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "kwargs.setdefault('headers', DEFAULT_HEADERS)\n",
      "State:\n",
      "{'headers': {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36'}}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def fix_signature(func: Callable, source: Callable, skip_n_params: int):\n",
      "    original_doc = func.__doc__\n",
      "    update_wrapper(func, source)\n",
      "    if original_doc:\n",
      "        func.__doc__ = f'{original_doc}\\n{func.__doc__}'\n",
      "    sig = signature(func)\n",
      "    sig = sig.replace(parameters=tuple(sig.parameters.values())[skip_n_params:])\n",
      "    func.__signature__ = sig\n",
      "    return func\n",
      "fix_signature(func=<function spinner_controller.<locals>.inner_controller.<locals>.compile_and_check at 0x7f355614d4c0>, source=<function check at 0x7f3556385dc0>, skip_n_params=1)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "original_doc = func.__doc__\n",
      "State:\n",
      "'Compile this spinner factory at its natural length, and...'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def inner_schedule(*args, **kwargs):\n",
      "                signature(command).bind(1, *args, **kwargs)\n",
      "                extra, cmd_type = dict(extra_commands), EXTRA_COMMANDS[command]\n",
      "                extra[cmd_type] = extra.get(cmd_type, ()) + ((command, args, kwargs),)\n",
      "                return inner_controller(spinner_inner_factory, op_params, extra)\n",
      "inner_schedule(args=(1,), kwargs={}, command=<function reshape at 0x7f35563858b0>, extra_commands={}, inner_controller=<function spinner_controller.<locals>.inner_controller at 0x7f3555de31f0>, op_params={}, spinner_inner_factory=<function frame_spinner_factory.<locals>.inner_spinner_factory at 0x7f3555de34c0>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "extra, cmd_type = dict(extra_commands), EXTRA_COMMANDS[command]\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def draw_known(apply_state, percent):\n",
      "            virtual_fill = round(virtual_length * max(0., min(1., percent)))\n",
      "            fill = fill_style(*divmod(virtual_fill, num_graphemes))\n",
      "            border, texts = apply_state(fill)\n",
      "            border = overflow if percent > 1. else None if percent == 1. else border\n",
      "            return fix_cells(combine_cells(fill, tip, *texts)[len_tip:length + len_tip]), border\n",
      "draw_known(apply_state=<function bar_factory.<locals>.inner_bar_factory.<locals>.running at 0x7f35539a6040>, percent=-0.5, fill_style=<function bar_factory.<locals>.inner_bar_factory.<locals>.fill_style at 0x7f3553b38f70>, len_tip=5, length=10, num_graphemes=1, overflow=('x',), tip=('@', '\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "virtual_fill = round(virtual_length * max(0., min(1., percent)))\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def draw_known(apply_state, percent):\n",
      "            virtual_fill = round(virtual_length * max(0., min(1., percent)))\n",
      "            fill = fill_style(*divmod(virtual_fill, num_graphemes))\n",
      "            border, texts = apply_state(fill)\n",
      "            border = overflow if percent > 1. else None if percent == 1. else border\n",
      "            return fix_cells(combine_cells(fill, tip, *texts)[len_tip:length + len_tip]), border\n",
      "draw_known(apply_state=<function bar_factory.<locals>.inner_bar_factory.<locals>.running at 0x7f35539a6040>, percent=-0.5, fill_style=<function bar_factory.<locals>.inner_bar_factory.<locals>.fill_style at 0x7f3553b38f70>, len_tip=5, length=10, num_graphemes=1, overflow=('x',), tip=('@', '\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "fill = fill_style(*divmod(virtual_fill, num_graphemes))\n",
      "State:\n",
      "()\n",
      "==================================================\n",
      "Clean Code:\n",
      "def draw_known(apply_state, percent):\n",
      "            virtual_fill = round(virtual_length * max(0., min(1., percent)))\n",
      "            fill = fill_style(*divmod(virtual_fill, num_graphemes))\n",
      "            border, texts = apply_state(fill)\n",
      "            border = overflow if percent > 1. else None if percent == 1. else border\n",
      "            return fix_cells(combine_cells(fill, tip, *texts)[len_tip:length + len_tip]), border\n",
      "draw_known(apply_state=<function bar_factory.<locals>.inner_bar_factory.<locals>.running at 0x7f35539a6040>, percent=-0.5, fill_style=<function bar_factory.<locals>.inner_bar_factory.<locals>.fill_style at 0x7f3553b38f70>, len_tip=5, length=10, num_graphemes=1, overflow=('x',), tip=('@', '\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "border, texts = apply_state(fill)\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_split_options(param, expected):\n",
      "    if expected is SAME:\n",
      "        expected = param\n",
      "    assert split_options(param) == expected\n",
      "test_split_options(param=None, expected=(None, None))\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "assert split_options(param) == expected\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_term(file=None, force_tty=None, cols=None):\n",
      "    if file is None:\n",
      "        file = sys.stdout\n",
      "    base = tty.new(file, cols or 80)\n",
      "    if hasattr(file, 'isatty') and file.isatty() if force_tty is None else force_tty:\n",
      "        return _create(jupyter.get_from(base) if _is_notebook() else base, True)\n",
      "    return _create(non_tty.get_from(base), False)\n",
      "get_term(file=None, force_tty=None, cols=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "file = sys.stdout\n",
      "State:\n",
      "<_io.TextIOWrapper encoding='UTF-8'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def write(stream, part):\n",
      "        if isinstance(part, bytes):\n",
      "            part = part.decode(ENCODING)\n",
      "        buffer = buffers[stream]\n",
      "        if part != '\\n':\n",
      "            osc = part.find('\\x1b]')\n",
      "            if osc >= 0:\n",
      "                end, s = part.find('\\x07', osc + 2), 1\n",
      "                if end < 0:\n",
      "                    end, s = part.find('\\x1b\\\\', osc + 2), 2\n",
      "                    if end < 0:\n",
      "                        end, s = len(part), 0\n",
      "                stream.write(part[osc:end + s])\n",
      "                stream.flush()\n",
      "                part = part[:osc] + part[end + s:]\n",
      "                if not part:\n",
      "                    return\n",
      "            gen = chain.from_iterable(zip(repeat(None), part.splitlines(True)))\n",
      "            buffer.extend(islice(gen, 1, None))\n",
      "        else:\n",
      "            header = get_header()\n",
      "            spacer = ' ' * len(header)\n",
      "            nested = ''.join(line or spacer for line in buffer)\n",
      "            text = f'{header}{nested.rstrip()}\\n'\n",
      "            with cond_refresh:\n",
      "                if stream in base:\n",
      "                    term.clear_line()\n",
      "                    term.clear_end_screen()\n",
      "                stream.write(text)\n",
      "                stream.flush()\n",
      "                cond_refresh.notify()\n",
      "                buffer[:] = []\n",
      "write(stream=<_io.TextIOWrapper encoding='UTF-8'>, part='ok', base=(<_io.TextIOWrapper encoding='UTF-8'>, <_io.TextIOWrapper encoding='UTF-8'>), buffers=defaultdict(<class 'list'>, {}), cond_refresh=<Condition(<unlocked _thread.RLock object owner=0 count=0 at 0x7f354c4b5a50>, 0)>, get_header=<function gen_header.<locals>.inner at 0x7f354785ef70>, term=namespace(interactive=False, cursor_up_1=<function _ansi_escape_sequence.<locals>.inner at 0x7f354785ee50>, write=<built-in method write of CaptureIO object at 0x7f3548f50a00>, flush=<built-in method flush of CaptureIO object at 0x7f3548f50a00>, cols=<function get_from.<locals>.cols at 0x7f354785eb80>, carriage_return='', clear_line=<function _ansi_escape_sequence.<locals>.inner at 0x7f35564913a0>, clear_end_line=<function _ansi_escape_sequence.<locals>.inner at 0x7f3556491430>, clear_end_screen=<function _ansi_escape_sequence.<locals>.inner at 0x7f3556491550>, hide_cursor=<function _ansi_escape_sequence.<locals>.inner at 0x7f35564915e0>, show_cursor=<function _ansi_escape_sequence.<locals>.inner at 0x7f3556491670>, factory_cursor_up=<function factory_cursor_up at 0x7f3556491700>))\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "gen = chain.from_iterable(zip(repeat(None), part.splitlines(True)))\n",
      "State:\n",
      "REPR FAILED\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __init__(self, func, value, default):\n",
      "        self.func = func\n",
      "        if isinstance(value, str):\n",
      "            self.f = value\n",
      "        elif value:\n",
      "            self.f = default\n",
      "        else:\n",
      "            self.f = ''\n",
      "        if self.f:\n",
      "            self.f += ' '\n",
      "__init__(self=<alive_progress.core.progress._Widget object at 0x7f354a355520>, func=<function __alive_bar.<locals>.monitor_run at 0x7f35458a6280>, value=True, default='{percent:.0%} [{count}/{total}]')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "self.f = default\n",
      "State:\n",
      "'{percent:.0%} [{count}/{total}]'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __init__(self, func, value, default):\n",
      "        self.func = func\n",
      "        if isinstance(value, str):\n",
      "            self.f = value\n",
      "        elif value:\n",
      "            self.f = default\n",
      "        else:\n",
      "            self.f = ''\n",
      "        if self.f:\n",
      "            self.f += ' '\n",
      "__init__(self=<alive_progress.core.progress._Widget object at 0x7f354a355520>, func=<function __alive_bar.<locals>.monitor_run at 0x7f35458a6280>, value=True, default='{percent:.0%} [{count}/{total}]')\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "self.f += ' '\n",
      "State:\n",
      "'{percent:.0%} [{count}/{total}] '\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _render_title(config, title=None):\n",
      "    title, length = to_cells(title or config.title or ''), config.title_length\n",
      "    if not length:\n",
      "        return title\n",
      "    len_title = len(title)\n",
      "    if len_title <= length:\n",
      "        return combine_cells(title, (' ',) * (length - len_title))\n",
      "    if length == 1:\n",
      "        return '',\n",
      "    return combine_cells(fix_cells(title[:length - 1]), ('',))\n",
      "_render_title(config=Config(title=None, length=3, max_cols=80, spinner=<function spinner_controller.<locals>.inner_controller.<locals>.spinner_compiler_dispatcher_factory at 0x7f3555c001f0>, bar=<function bar_controller.<locals>.bar_assembler_factory at 0x7f3554094550>, unknown=<function spinner_controller.<locals>.inner_controller.<locals>.spinner_compiler_dispatcher_factory at 0x7f35557bdee0>, force_tty=False, disable=False, manual=True, enrich_print=True, receipt=True, receipt_text=False, monitor=True, elapsed=True, stats=True, title_length=0, spinner_length=0, refresh_secs=0.0, monitor_end=True, elapsed_end=True, stats_end=True, ctrl_c=True, dual_line=False, unit='U', scale='SI', precision=1, file=<_io.TextIOWrapper encoding='UTF-8'>), title=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "title, length = to_cells(title or config.title or ''), config.title_length\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def start_monitoring(offset=0.):\n",
      "        term.hide_cursor()\n",
      "        hook_manager.install()\n",
      "        bar_handle._handle = bar\n",
      "        run.init = time.perf_counter() - offset\n",
      "        event_renderer.set()\n",
      "start_monitoring(offset=0.0, bar=<function __alive_bar.<locals>.bar at 0x7f3549a930d0>, bar_handle={_handle=None}, event_renderer={_cond=<Condition(<unlocked _thread.lock object at 0x7f354eb01c60>, 0)>, _flag=False}, hook_manager=namespace(flush_buffers=<function buffered_hook_manager.<locals>.flush_buffers at 0x7f354a3b35e0>, install=<function buffered_hook_manager.<locals>.install at 0x7f3545926ca0>, uninstall=<function buffered_hook_manager.<locals>.uninstall at 0x7f3545926f70>), run=<function __alive_bar.<locals>.run at 0x7f35479bb4c0>, term=namespace(interactive=False, cursor_up_1=<function _ansi_escape_sequence.<locals>.inner at 0x7f3545926b80>, write=<built-in method write of CaptureIO object at 0x7f3545b2e450>, flush=<built-in method flush of CaptureIO object at 0x7f3545b2e450>, cols=<function get_from.<locals>.cols at 0x7f35459265e0>, carriage_return='', clear_line=<function _ansi_escape_sequence.<locals>.inner at 0x7f35564913a0>, clear_end_line=<function _ansi_escape_sequence.<locals>.inner at 0x7f3556491430>, clear_end_screen=<function _ansi_escape_sequence.<locals>.inner at 0x7f3556491550>, hide_cursor=<function _ansi_escape_sequence.<locals>.inner at 0x7f35564915e0>, show_cursor=<function _ansi_escape_sequence.<locals>.inner at 0x7f3556491670>, factory_cursor_up=<function factory_cursor_up at 0x7f3556491700>))\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "bar_handle._handle = bar\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def stop_monitoring():\n",
      "        term.show_cursor()\n",
      "        hook_manager.uninstall()\n",
      "        bar_handle._handle = None\n",
      "        return time.perf_counter() - run.init\n",
      "stop_monitoring(bar_handle={}, hook_manager=namespace(flush_buffers=<function buffered_hook_manager.<locals>.flush_buffers at 0x7f354a3b35e0>, install=<function buffered_hook_manager.<locals>.install at 0x7f3545926ca0>, uninstall=<function buffered_hook_manager.<locals>.uninstall at 0x7f3545926f70>), run=<function __alive_bar.<locals>.run at 0x7f35479bb4c0>, term=namespace(interactive=False, cursor_up_1=<function _ansi_escape_sequence.<locals>.inner at 0x7f3545926b80>, write=<built-in method write of CaptureIO object at 0x7f3545b2e450>, flush=<built-in method flush of CaptureIO object at 0x7f3545b2e450>, cols=<function get_from.<locals>.cols at 0x7f35459265e0>, carriage_return='', clear_line=<function _ansi_escape_sequence.<locals>.inner at 0x7f35564913a0>, clear_end_line=<function _ansi_escape_sequence.<locals>.inner at 0x7f3556491430>, clear_end_screen=<function _ansi_escape_sequence.<locals>.inner at 0x7f3556491550>, hide_cursor=<function _ansi_escape_sequence.<locals>.inner at 0x7f35564915e0>, show_cursor=<function _ansi_escape_sequence.<locals>.inner at 0x7f3556491670>, factory_cursor_up=<function factory_cursor_up at 0x7f3556491700>))\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "bar_handle._handle = None\n",
      "State:\n",
      "{_handle=None}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def elapsed_text(seconds, precise, prefix=''):\n",
      "    seconds = round(seconds, 1 if precise else 0)\n",
      "    if seconds < 60.:\n",
      "        return '{}{:{}f}s'.format(prefix, seconds, .1 if precise else .0)\n",
      "    minutes, seconds = divmod(seconds, 60.)\n",
      "    if minutes < 60.:\n",
      "        return '{}{:.0f}:{:0{}f}'.format(prefix, minutes, seconds, 4.1 if precise else 2.0)\n",
      "    hours, minutes = divmod(minutes, 60.)\n",
      "    return '{}{:.0f}:{:02.0f}:{:0{}f}'.format(prefix, hours, minutes, seconds,\n",
      "                                              4.1 if precise else 2.0)\n",
      "elapsed_text(seconds=1.23, precise=True, prefix='')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "seconds = round(seconds, 1 if precise else 0)\n",
      "State:\n",
      "1.2\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __getitem__(self, table_name):\n",
      "        return self.get_table(table_name)\n",
      "__getitem__(self=<Database(sqlite://)>, table_name='weather', self._tables={}, self.connections={}, self.engine=Engine(sqlite://), self.ensure_schema=True, self.is_postgres=False, self.is_sqlite=True, self.local=<_thread._local object at 0x7f5a15959d10>, self.lock=<unlocked _thread.RLock object owner=0 count=0 at 0x7f5a159edbd0>, self.row_type=<class 'collections.OrderedDict'>, self.schema=None, self.types=<dataset.types.Types object at 0x7f5a159edb50>, self.url='sqlite://')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "return self.get_table(table_name)\n",
      "State:\n",
      "REPR FAILED\n",
      "==================================================\n",
      "Clean Code:\n",
      "def insert_many(self, rows, chunk_size=1000, ensure=None, types=None):\n",
      "        sync_row = {}\n",
      "        for row in rows:\n",
      "            sync_keys = list(sync_row.keys())\n",
      "            for key in [k for k in row.keys() if k not in sync_keys]:\n",
      "                sync_row[key] = row[key]\n",
      "        self._sync_columns(sync_row, ensure, types=types)\n",
      "        columns = sync_row.keys()\n",
      "        chunk = []\n",
      "        for index, row in enumerate(rows):\n",
      "            chunk.append(row)\n",
      "            if len(chunk) == chunk_size or index == len(rows) - 1:\n",
      "                chunk = pad_chunk_columns(chunk, columns)\n",
      "                self.table.insert().execute(chunk)\n",
      "                chunk = []\n",
      "insert_many(self=REPR FAILED, rows=[{'date': datetime.datetime(2011, 1, 1, 0, 0), 'temperature': 1, 'place': 'Glway'}, {'date': datetime.datetime(2011, 1, 2, 0, 0), 'temperature': -1, 'place': 'Glway'}, {'date': datetime.datetime(2011, 1, 3, 0, 0), 'temperature': 0, 'place': 'Glway'}, {'date': datetime.datetime(2011, 1, 1, 0, 0), 'temperature': 6, 'place': 'Brkeley'}, {'date': datetime.datetime(2011, 1, 2, 0, 0), 'temperature': 8, 'place': 'Brkeley'}, {'date': datetime.datetime(2011, 1, 3, 0, 0), 'temperature': 5, 'place': 'Brkeley'}], chunk_size=1000, ensure=None, types=None, self._auto_create=True, self._columns=None, self._indexes=[], self._primary_id='id', self._primary_increment=True, self._primary_type=<class 'sqlalchemy.sql.sqltypes.Integer'>, self._table=None, self.db=<Database(sqlite://)>, self.name='weather')\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "sync_row = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def mknode(id=None, ip=None, port=None, intid=None):\n",
      "    if intid is not None:\n",
      "        id = pack('>l', intid)\n",
      "    id = id or hashlib.sha1(str(random.getrandbits(255))).digest()\n",
      "    return Node(id, ip, port)\n",
      "mknode(id=None, ip=None, port=None, intid=0)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "id = pack('>l', intid)\n",
      "State:\n",
      "b'\\x00\\x00\\x00\\x00'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def getblockimpl(lines, first, last, pilcrow):\n",
      "    max = len(lines) - 1\n",
      "    first -= 1\n",
      "    last -= 1\n",
      "    i = first\n",
      "    while i < max and not hastext(lines[i]):\n",
      "        if i >= last and istoplevel(lines[i + 1]):\n",
      "            return None, None, '\n",
      "        i += 1\n",
      "    while last < max and not istoplevel(lines[last + 1]):\n",
      "        last += 1\n",
      "    while first < last and not hastext(lines[first]):\n",
      "        first += 1\n",
      "    while first and not istoplevel(lines[first]):\n",
      "        first -= 1\n",
      "    lines[last]\n",
      "    return first, last, eol.join(l for l in lines[first:last + 1] if hastext(l)) + pilcrow + eol\n",
      "getblockimpl(lines=['', 'hello', 'function with', ' indented block', 'class with', '', '  block after blank', '  \\tand its own indented block', '\\t', '  and back again after a wrong blank', '', 'something else', '\\t', ' \\t', 'last'], first=10, last=11, pilcrow='')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "max = len(lines) - 1\n",
      "State:\n",
      "14\n",
      "==================================================\n",
      "Clean Code:\n",
      "def getblockimpl(lines, first, last, pilcrow):\n",
      "    max = len(lines) - 1\n",
      "    first -= 1\n",
      "    last -= 1\n",
      "    i = first\n",
      "    while i < max and not hastext(lines[i]):\n",
      "        if i >= last and istoplevel(lines[i + 1]):\n",
      "            return None, None, '\n",
      "        i += 1\n",
      "    while last < max and not istoplevel(lines[last + 1]):\n",
      "        last += 1\n",
      "    while first < last and not hastext(lines[first]):\n",
      "        first += 1\n",
      "    while first and not istoplevel(lines[first]):\n",
      "        first -= 1\n",
      "    lines[last]\n",
      "    return first, last, eol.join(l for l in lines[first:last + 1] if hastext(l)) + pilcrow + eol\n",
      "getblockimpl(lines=['', 'hello', 'function with', ' indented block', 'class with', '', '  block after blank', '  \\tand its own indented block', '\\t', '  and back again after a wrong blank', '', 'something else', '\\t', ' \\t', 'last'], first=10, last=11, pilcrow='')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "first -= 1\n",
      "State:\n",
      "9\n",
      "==================================================\n",
      "Clean Code:\n",
      "def getblockimpl(lines, first, last, pilcrow):\n",
      "    max = len(lines) - 1\n",
      "    first -= 1\n",
      "    last -= 1\n",
      "    i = first\n",
      "    while i < max and not hastext(lines[i]):\n",
      "        if i >= last and istoplevel(lines[i + 1]):\n",
      "            return None, None, '\n",
      "        i += 1\n",
      "    while last < max and not istoplevel(lines[last + 1]):\n",
      "        last += 1\n",
      "    while first < last and not hastext(lines[first]):\n",
      "        first += 1\n",
      "    while first and not istoplevel(lines[first]):\n",
      "        first -= 1\n",
      "    lines[last]\n",
      "    return first, last, eol.join(l for l in lines[first:last + 1] if hastext(l)) + pilcrow + eol\n",
      "getblockimpl(lines=['', 'hello', 'function with', ' indented block', 'class with', '', '  block after blank', '  \\tand its own indented block', '\\t', '  and back again after a wrong blank', '', 'something else', '\\t', ' \\t', 'last'], first=10, last=11, pilcrow='')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "last -= 1\n",
      "State:\n",
      "10\n",
      "==================================================\n",
      "Clean Code:\n",
      "def getblockimpl(lines, first, last, pilcrow):\n",
      "    max = len(lines) - 1\n",
      "    first -= 1\n",
      "    last -= 1\n",
      "    i = first\n",
      "    while i < max and not hastext(lines[i]):\n",
      "        if i >= last and istoplevel(lines[i + 1]):\n",
      "            return None, None, '\n",
      "        i += 1\n",
      "    while last < max and not istoplevel(lines[last + 1]):\n",
      "        last += 1\n",
      "    while first < last and not hastext(lines[first]):\n",
      "        first += 1\n",
      "    while first and not istoplevel(lines[first]):\n",
      "        first -= 1\n",
      "    lines[last]\n",
      "    return first, last, eol.join(l for l in lines[first:last + 1] if hastext(l)) + pilcrow + eol\n",
      "getblockimpl(lines=['', 'hello', 'function with', ' indented block', 'class with', '', '  block after blank', '  \\tand its own indented block', '\\t', '  and back again after a wrong blank', '', 'something else', '\\t', ' \\t', 'last'], first=10, last=11, pilcrow='')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "i = first\n",
      "State:\n",
      "9\n",
      "==================================================\n",
      "Clean Code:\n",
      "def strip(tokens):\n",
      "        output = \"\"\n",
      "        for type_, value in tokens:\n",
      "            if type_ == TokenType.TEXT:\n",
      "                output += value\n",
      "        return output\n",
      "strip(tokens=[(1, ''), (2, '\\x1b[32m'), (1, ''), (1, '{time:YYYY-MM-DD HH:mm:ss.SSS}'), (1, ''), (4, '\\x1b[0m'), (1, ' | '), (3, None), (1, ''), (1, '{level: <8}'), (1, ''), (4, '\\x1b[0m'), (1, ' | '), (2, '\\x1b[36m'), (1, ''), (1, '{name}'), (1, ''), (4, '\\x1b[0m'), (1, ':'), (2, '\\x1b[36m'), (1, ''), (1, '{function}'), (1, ''), (4, '\\x1b[0m'), (1, ':'), (2, '\\x1b[36m'), (1, ''), (1, '{line}'), (1, ''), (4, '\\x1b[0m'), (1, ' - '), (3, None), (1, ''), (1, '{message}'), (1, ''), (4, '\\x1b[0m'), (1, '\\n'), (1, '{exception}')])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "output = \"\"\n",
      "State:\n",
      "''\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse(text, *, strip=False, strict=True):\n",
      "    parser = loguru._colorizer.AnsiParser()\n",
      "    parser.feed(text)\n",
      "    tokens = parser.done(strict=strict)\n",
      "    if strip:\n",
      "        return parser.strip(tokens)\n",
      "    return parser.colorize(tokens, \"\")\n",
      "parse(text='<red>Foo</red>\\n', strip=False, strict=True)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "parser = loguru._colorizer.AnsiParser()\n",
      "State:\n",
      "{_tokens=[], _tags=[], _color_tokens=[]}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse(text, *, strip=False, strict=True):\n",
      "    parser = loguru._colorizer.AnsiParser()\n",
      "    parser.feed(text)\n",
      "    tokens = parser.done(strict=strict)\n",
      "    if strip:\n",
      "        return parser.strip(tokens)\n",
      "    return parser.colorize(tokens, \"\")\n",
      "parse(text='<red>Foo</red>\\n', strip=False, strict=True)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "parser.feed(text)\n",
      "State:\n",
      "{_tokens=[(1, ''), (2, '\\x1b[31m'), (1, 'Foo'), (4, '\\x1b[0m'), (1, '\\n')], _tags=[], _color_tokens=[]}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse(text, *, strip=False, strict=True):\n",
      "    parser = loguru._colorizer.AnsiParser()\n",
      "    parser.feed(text)\n",
      "    tokens = parser.done(strict=strict)\n",
      "    if strip:\n",
      "        return parser.strip(tokens)\n",
      "    return parser.colorize(tokens, \"\")\n",
      "parse(text='<red>Foo</red>\\n', strip=False, strict=True)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "tokens = parser.done(strict=strict)\n",
      "State:\n",
      "[(1, ''), (2, '\\x1b[31m'), (1, 'Foo'), (4, '\\x1b[0m'), (1, '\\n')]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def stop(self):\n",
      "        with self._protected_lock():\n",
      "            self._stopped = True\n",
      "            if self._enqueue:\n",
      "                if self._owner_process_pid != os.getpid():\n",
      "                    return\n",
      "                self._queue.put(None)\n",
      "                self._thread.join()\n",
      "                if hasattr(self._queue, \"close\"):\n",
      "                    self._queue.close()\n",
      "            self._sink.stop()\n",
      "stop(self=(id=0, level=10, sink=<stderr>), self._colorize=False, self._confirmation_event=None, self._confirmation_lock=None, self._decolorized_format='{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} - {message}\\n{exception}', self._enqueue=False, self._error_interceptor=<loguru._error_interceptor.ErrorInterceptor object at 0x7fbe3c13e400>, self._exception_formatter=<loguru._better_exceptions.ExceptionFormatter object at 0x7fbe3bf7c9a0>, self._filter=None, self._formatter=<loguru._colorizer.ColoredFormat object at 0x7fbe3c1a69d0>, self._id=0, self._is_formatter_dynamic=False, self._levelno=10, self._levels_ansi_codes={'TRACE': '\\x1b[36m\\x1b[1m', 'DEBUG': '\\x1b[34m\\x1b[1m', 'INFO': '\\x1b[1m', 'SUCCESS': '\\x1b[32m\\x1b[1m', 'WARNING': '\\x1b[33m\\x1b[1m', 'ERROR': '\\x1b[31m\\x1b[1m', 'CRITICAL': '\\x1b[41m\\x1b[1m', None: ''}, self._lock=<unlocked _thread.lock object at 0x7fbe3bf7cc90>, self._lock_acquired=<_thread._local object at 0x7fbe3bec8db0>, self._memoize_dynamic_format=None, self._multiprocessing_context=None, self._name='<stderr>', self._owner_process_pid=None, self._precolorized_formats={}, self._queue=None, self._queue_lock=None, self._serialize=False, self._sink=<loguru._simple_sinks.StreamSink object at 0x7fbe3c1effd0>, self._stopped=False, self._thread=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self._stopped = True\n",
      "State:\n",
      "True\n",
      "==================================================\n",
      "Clean Code:\n",
      "def emit(self, record, level_id, from_decorator, is_raw, colored_message):\n",
      "        try:\n",
      "            if self._levelno > record[\"level\"].no:\n",
      "                return\n",
      "            if self._filter is not None:\n",
      "                if not self._filter(record):\n",
      "                    return\n",
      "            if self._is_formatter_dynamic:\n",
      "                dynamic_format = self._formatter(record)\n",
      "            formatter_record = record.copy()\n",
      "            if not record[\"exception\"]:\n",
      "                formatter_record[\"exception\"] = \"\"\n",
      "            else:\n",
      "                type_, value, tb = record[\"exception\"]\n",
      "                formatter = self._exception_formatter\n",
      "                lines = formatter.format_exception(type_, value, tb, from_decorator=from_decorator)\n",
      "                formatter_record[\"exception\"] = \"\".join(lines)\n",
      "            if colored_message is not None and colored_message.stripped != record[\"message\"]:\n",
      "                colored_message = None\n",
      "            if is_raw:\n",
      "                if colored_message is None or not self._colorize:\n",
      "                    formatted = record[\"message\"]\n",
      "                else:\n",
      "                    ansi_level = self._levels_ansi_codes[level_id]\n",
      "                    formatted = colored_message.colorize(ansi_level)\n",
      "            elif self._is_formatter_dynamic:\n",
      "                if not self._colorize:\n",
      "                    precomputed_format = self._memoize_dynamic_format(dynamic_format)\n",
      "                    formatted = precomputed_format.format_map(formatter_record)\n",
      "                elif colored_message is None:\n",
      "                    ansi_level = self._levels_ansi_codes[level_id]\n",
      "                    _, precomputed_format = self._memoize_dynamic_format(dynamic_format, ansi_level)\n",
      "                    formatted = precomputed_format.format_map(formatter_record)\n",
      "                else:\n",
      "                    ansi_level = self._levels_ansi_codes[level_id]\n",
      "                    formatter, precomputed_format = self._memoize_dynamic_format(\n",
      "                        dynamic_format, ansi_level\n",
      "                    )\n",
      "                    coloring_message = formatter.make_coloring_message(\n",
      "                        record[\"message\"], ansi_level=ansi_level, colored_message=colored_message\n",
      "                    )\n",
      "                    formatter_record[\"message\"] = coloring_message\n",
      "                    formatted = precomputed_format.format_map(formatter_record)\n",
      "            else:\n",
      "                if not self._colorize:\n",
      "                    precomputed_format = self._decolorized_format\n",
      "                    formatted = precomputed_format.format_map(formatter_record)\n",
      "                elif colored_message is None:\n",
      "                    ansi_level = self._levels_ansi_codes[level_id]\n",
      "                    precomputed_format = self._precolorized_formats[level_id]\n",
      "                    formatted = precomputed_format.format_map(formatter_record)\n",
      "                else:\n",
      "                    ansi_level = self._levels_ansi_codes[level_id]\n",
      "                    precomputed_format = self._precolorized_formats[level_id]\n",
      "                    coloring_message = self._formatter.make_coloring_message(\n",
      "                        record[\"message\"], ansi_level=ansi_level, colored_message=colored_message\n",
      "                    )\n",
      "                    formatter_record[\"message\"] = coloring_message\n",
      "                    formatted = precomputed_format.format_map(formatter_record)\n",
      "            if self._serialize:\n",
      "                formatted = self._serialize_record(formatted, record)\n",
      "            str_record = Message(formatted)\n",
      "            str_record.record = record\n",
      "            with self._protected_lock():\n",
      "                if self._stopped:\n",
      "                    return\n",
      "                if self._enqueue:\n",
      "                    self._queue.put(str_record)\n",
      "                else:\n",
      "                    self._sink.write(str_record)\n",
      "        except Exception:\n",
      "            if not self._error_interceptor.should_catch():\n",
      "                raise\n",
      "            self._error_interceptor.print(record)\n",
      "emit(self=(id=0, level=10, sink=w), record={'elapsed': datetime.timedelta(seconds=7, microseconds=175743), 'exception': None, 'extra': {}, 'file': (name='test_activation.py', path='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/Delgan+loguru/Delgan+loguru/tests/test_activation.py'), 'function': 'test_disable', 'level': (name='DEBUG', no=10, icon=''), 'line': 23, 'message': 'message', 'module': 'test_activation', 'name': 'tests.test_activation', 'process': (id=1702659, name='MainProcess'), 'thread': (id=140455058487104, name='MainThread'), 'time': datetime(2024, 4, 3, 14, 56, 17, 119037, tzinfo=datetime.timezone(datetime.timedelta(days=-1, seconds=72000), 'EDT'))}, level_id='DEBUG', from_decorator=False, is_raw=False, colored_message=None, self._colorize=False, self._confirmation_event=None, self._confirmation_lock=None, self._decolorized_format='{message}\\n{exception}', self._enqueue=False, self._error_interceptor=<loguru._error_interceptor.ErrorInterceptor object at 0x7fbe3babefa0>, self._exception_formatter=<loguru._better_exceptions.ExceptionFormatter object at 0x7fbe3bb99550>, self._filter=None, self._formatter=<loguru._colorizer.ColoredFormat object at 0x7fbe3ba14ee0>, self._id=0, self._is_formatter_dynamic=False, self._levelno=10, self._levels_ansi_codes={'TRACE': '\\x1b[36m\\x1b[1m', 'DEBUG': '\\x1b[34m\\x1b[1m', 'INFO': '\\x1b[1m', 'SUCCESS': '\\x1b[32m\\x1b[1m', 'WARNING': '\\x1b[33m\\x1b[1m', 'ERROR': '\\x1b[31m\\x1b[1m', 'CRITICAL': '\\x1b[41m\\x1b[1m', None: ''}, self._lock=<unlocked _thread.lock object at 0x7fbe3bf3f090>, self._lock_acquired=<_thread._local object at 0x7fbe394ccdb0>, self._memoize_dynamic_format=None, self._multiprocessing_context=None, self._name='w', self._owner_process_pid=None, self._precolorized_formats={}, self._queue=None, self._queue_lock=None, self._serialize=False, self._sink=<loguru._simple_sinks.CallableSink object at 0x7fbe3c1a69d0>, self._stopped=False, self._thread=None)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "formatter_record[\"exception\"] = \"\"\n",
      "State:\n",
      "{'elapsed': datetime.timedelta(seconds=7, microseconds=175743), 'exception': '', 'extra': {}, 'file': (name='test_activation.py', path='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/Delgan+loguru/Delgan+loguru/tests/test_activation.py'), 'function': 'test_disable', 'level': (name='DEBUG', no=10, icon=''), 'line': 23, 'message': 'message', 'module': 'test_activation', 'name': 'tests.test_activation', 'process': (id=1702659, name='MainProcess'), 'thread': (id=140455058487104, name='MainThread'), 'time': datetime(2024, 4, 3, 14, 56, 17, 119037, tzinfo=datetime.timezone(datetime.timedelta(days=-1, seconds=72000), 'EDT'))}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_without_formatting(string, *, recursion_depth=2, recursive=False):\n",
      "        if recursion_depth < 0:\n",
      "            raise ValueError(\"Max string recursion exceeded\")\n",
      "        formatter = Formatter()\n",
      "        parser = AnsiParser()\n",
      "        messages_color_tokens = []\n",
      "        for literal_text, field_name, format_spec, conversion in formatter.parse(string):\n",
      "            if literal_text and literal_text[-1] in \"{}\":\n",
      "                literal_text += literal_text[-1]\n",
      "            parser.feed(literal_text, raw=recursive)\n",
      "            if field_name is not None:\n",
      "                if field_name == \"message\":\n",
      "                    if recursive:\n",
      "                        messages_color_tokens.append(None)\n",
      "                    else:\n",
      "                        color_tokens = parser.current_color_tokens()\n",
      "                        messages_color_tokens.append(color_tokens)\n",
      "                field = \"{%s\" % field_name\n",
      "                if conversion:\n",
      "                    field += \"!%s\" % conversion\n",
      "                if format_spec:\n",
      "                    field += \":%s\" % format_spec\n",
      "                field += \"}\"\n",
      "                parser.feed(field, raw=True)\n",
      "                _, color_tokens = Colorizer._parse_without_formatting(\n",
      "                    format_spec, recursion_depth=recursion_depth - 1, recursive=True\n",
      "                )\n",
      "                messages_color_tokens.extend(color_tokens)\n",
      "        return parser.done(), messages_color_tokens\n",
      "_parse_without_formatting(string='', recursion_depth=1, recursive=True)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "formatter = Formatter()\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_without_formatting(string, *, recursion_depth=2, recursive=False):\n",
      "        if recursion_depth < 0:\n",
      "            raise ValueError(\"Max string recursion exceeded\")\n",
      "        formatter = Formatter()\n",
      "        parser = AnsiParser()\n",
      "        messages_color_tokens = []\n",
      "        for literal_text, field_name, format_spec, conversion in formatter.parse(string):\n",
      "            if literal_text and literal_text[-1] in \"{}\":\n",
      "                literal_text += literal_text[-1]\n",
      "            parser.feed(literal_text, raw=recursive)\n",
      "            if field_name is not None:\n",
      "                if field_name == \"message\":\n",
      "                    if recursive:\n",
      "                        messages_color_tokens.append(None)\n",
      "                    else:\n",
      "                        color_tokens = parser.current_color_tokens()\n",
      "                        messages_color_tokens.append(color_tokens)\n",
      "                field = \"{%s\" % field_name\n",
      "                if conversion:\n",
      "                    field += \"!%s\" % conversion\n",
      "                if format_spec:\n",
      "                    field += \":%s\" % format_spec\n",
      "                field += \"}\"\n",
      "                parser.feed(field, raw=True)\n",
      "                _, color_tokens = Colorizer._parse_without_formatting(\n",
      "                    format_spec, recursion_depth=recursion_depth - 1, recursive=True\n",
      "                )\n",
      "                messages_color_tokens.extend(color_tokens)\n",
      "        return parser.done(), messages_color_tokens\n",
      "_parse_without_formatting(string='', recursion_depth=1, recursive=True)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "parser = AnsiParser()\n",
      "State:\n",
      "{_tokens=[], _tags=[], _color_tokens=[]}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _format_value(self, v):\n",
      "        try:\n",
      "            v = repr(v)\n",
      "        except Exception:\n",
      "            v = \"<unprintable %s object>\" % type(v).__name__\n",
      "        max_length = self._max_length\n",
      "        if max_length is not None and len(v) > max_length:\n",
      "            v = v[: max_length - 3] + \"...\"\n",
      "        return v\n",
      "_format_value(self=<loguru._better_exceptions.ExceptionFormatter object at 0x7fbe3bdd29a0>, v=<module 'runpy' from '/local/rcs/XXX/miniforge3/envs/Delgan+loguru/lib/python3.9/runpy.py'>, self._backtrace=True, self._cap_char='', self._catch_point_identifier=' <Loguru catch point here>', self._colorize=False, self._diagnose=True, self._encoding='utf8', self._hidden_frames_filename='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/Delgan+loguru/Delgan+loguru/loguru/_logger.py', self._lib_dirs=['/local/rcs/XXX/miniforge3/envs/delgan+loguru/lib/python3.9/', '/local/rcs/XXX/miniforge3/envs/delgan+loguru/lib/python3.9/site-packages/'], self._max_length=128, self._pipe_char='', self._prefix='', self._syntax_highlighter=<loguru._better_exceptions.SyntaxHighlighter object at 0x7fbe3bdd27f0>, self._theme={'introduction': '\\x1b[33m\\x1b[1m{}\\x1b[0m', 'cause': '\\x1b[1m{}\\x1b[0m', 'context': '\\x1b[1m{}\\x1b[0m', 'dirname': '\\x1b[32m{}\\x1b[0m', 'basename': '\\x1b[32m\\x1b[1m{}\\x1b[0m', 'line': '\\x1b[33m{}\\x1b[0m', 'function': '\\x1b[35m{}\\x1b[0m', 'exception_type': '\\x1b[31m\\x1b[1m{}\\x1b[0m', 'exception_value': '\\x1b[1m{}\\x1b[0m', 'arrows': '\\x1b[36m{}\\x1b[0m', 'value': '\\x1b[36m\\x1b[1m{}\\x1b[0m'})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "v = repr(v)\n",
      "State:\n",
      "\"<module 'runpy' from '/local/rcs/XXX/miniforge3/envs/Delgan+loguru/lib/python3.9/runpy.py'>\"\n",
      "==================================================\n",
      "Clean Code:\n",
      "def extract_api_info_for_service(service, path_prefix):\n",
      "    service_methods = service.DESCRIPTOR.methods\n",
      "    res = {}\n",
      "    for service_method in service_methods:\n",
      "        endpoints = service_method.GetOptions().Extensions[databricks_pb2.rpc].endpoints\n",
      "        endpoint = endpoints[0]\n",
      "        endpoint_path = _get_path(path_prefix, endpoint.path)\n",
      "        res[service().GetRequestClass(service_method)] = (endpoint_path, endpoint.method)\n",
      "    return res\n",
      "extract_api_info_for_service(service=<class 'model_registry_pb2.ModelRegistryService'>, path_prefix='/api/2.0')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "service_methods = service.DESCRIPTOR.methods\n",
      "State:\n",
      "<ServiceMethods sequence>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def extract_all_api_info_for_service(service, path_prefix):\n",
      "    service_methods = service.DESCRIPTOR.methods\n",
      "    res = {}\n",
      "    for service_method in service_methods:\n",
      "        endpoints = service_method.GetOptions().Extensions[databricks_pb2.rpc].endpoints\n",
      "        res[service().GetRequestClass(service_method)] = [\n",
      "            (_get_path(path_prefix, endpoint.path), endpoint.method) for endpoint in endpoints\n",
      "        ]\n",
      "    return res\n",
      "extract_all_api_info_for_service(service=<class 'model_registry_pb2.ModelRegistryService'>, path_prefix='/api/2.0')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "service_methods = service.DESCRIPTOR.methods\n",
      "State:\n",
      "<ServiceMethods sequence>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def deprecated_decorator(func):\n",
      "        since_str = f\" since {since}\" if since else \"\"\n",
      "        impact_str = impact if impact else \"This method will be removed in a future release.\"\n",
      "        notice = \"``{qual_function_name}`` is deprecated{since_string}. {impact}\".format(\n",
      "            qual_function_name=\".\".join([func.__module__, func.__qualname__]),\n",
      "            since_string=since_str,\n",
      "            impact=impact_str,\n",
      "        )\n",
      "        if alternative is not None and alternative.strip():\n",
      "            notice += f\" Use ``{alternative}`` instead.\"\n",
      "        @wraps(func)\n",
      "        def deprecated_func(*args, **kwargs):\n",
      "            warnings.warn(notice, category=FutureWarning, stacklevel=2)\n",
      "            return func(*args, **kwargs)\n",
      "        if func.__doc__ is not None:\n",
      "            deprecated_func.__doc__ = \".. Warning:: \" + notice + \"\\n\" + func.__doc__\n",
      "        return deprecated_func\n",
      "deprecated_decorator(func=<function MlflowClient.get_latest_versions at 0x7fdcd980e310>, alternative=None, impact='Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/2.10.1/model-registry.html\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "since_str = f\" since {since}\" if since else \"\"\n",
      "State:\n",
      "' since 2.9.0'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def deprecated_decorator(func):\n",
      "        since_str = f\" since {since}\" if since else \"\"\n",
      "        impact_str = impact if impact else \"This method will be removed in a future release.\"\n",
      "        notice = \"``{qual_function_name}`` is deprecated{since_string}. {impact}\".format(\n",
      "            qual_function_name=\".\".join([func.__module__, func.__qualname__]),\n",
      "            since_string=since_str,\n",
      "            impact=impact_str,\n",
      "        )\n",
      "        if alternative is not None and alternative.strip():\n",
      "            notice += f\" Use ``{alternative}`` instead.\"\n",
      "        @wraps(func)\n",
      "        def deprecated_func(*args, **kwargs):\n",
      "            warnings.warn(notice, category=FutureWarning, stacklevel=2)\n",
      "            return func(*args, **kwargs)\n",
      "        if func.__doc__ is not None:\n",
      "            deprecated_func.__doc__ = \".. Warning:: \" + notice + \"\\n\" + func.__doc__\n",
      "        return deprecated_func\n",
      "deprecated_decorator(func=<function MlflowClient.get_latest_versions at 0x7fdcd980e310>, alternative=None, impact='Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/2.10.1/model-registry.html\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "impact_str = impact if impact else \"This method will be removed in a future release.\"\n",
      "State:\n",
      "'Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/2.10.1/model-registry.html#migrating-from-stages'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def deprecated_decorator(func):\n",
      "        since_str = f\" since {since}\" if since else \"\"\n",
      "        impact_str = impact if impact else \"This method will be removed in a future release.\"\n",
      "        notice = \"``{qual_function_name}`` is deprecated{since_string}. {impact}\".format(\n",
      "            qual_function_name=\".\".join([func.__module__, func.__qualname__]),\n",
      "            since_string=since_str,\n",
      "            impact=impact_str,\n",
      "        )\n",
      "        if alternative is not None and alternative.strip():\n",
      "            notice += f\" Use ``{alternative}`` instead.\"\n",
      "        @wraps(func)\n",
      "        def deprecated_func(*args, **kwargs):\n",
      "            warnings.warn(notice, category=FutureWarning, stacklevel=2)\n",
      "            return func(*args, **kwargs)\n",
      "        if func.__doc__ is not None:\n",
      "            deprecated_func.__doc__ = \".. Warning:: \" + notice + \"\\n\" + func.__doc__\n",
      "        return deprecated_func\n",
      "deprecated_decorator(func=<function MlflowClient.get_latest_versions at 0x7fdcd980e310>, alternative=None, impact='Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/2.10.1/model-registry.html\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "notice = \"``{qual_function_name}`` is deprecated{since_string}. {impact}\".format(\n",
      "State:\n",
      "'``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/2.10.1/model-registry.html#migrating-from-stages'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def random_port():\n",
      "        port = helper.random_port()\n",
      "        while port in generated_ports:\n",
      "            port = helper.random_port()\n",
      "        generated_ports.add(port)\n",
      "        return port\n",
      "random_port(generated_ports=set())\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "port = helper.random_port()\n",
      "State:\n",
      "56663\n",
      "==================================================\n",
      "Clean Code:\n",
      "def random_port():\n",
      "        port = helper.random_port()\n",
      "        while port in generated_ports:\n",
      "            port = helper.random_port()\n",
      "        generated_ports.add(port)\n",
      "        return port\n",
      "random_port(generated_ports=set())\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "generated_ports.add(port)\n",
      "State:\n",
      "{56663}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_split_img_txt_da(inputs):\n",
      "    txt_da = DocumentArray()\n",
      "    img_da = DocumentArray()\n",
      "    for doc in inputs[0]:\n",
      "        split_img_txt_da(doc, img_da, txt_da)\n",
      "    assert len(txt_da) == inputs[1][0]\n",
      "    assert len(img_da) == inputs[1][1]\n",
      "test_split_img_txt_da(inputs=(<DocumentArray (length=4) at 140538627068688>, (3, 1)))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "txt_da = DocumentArray()\n",
      "State:\n",
      "<DocumentArray (length=0) at 140537876329136>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_split_img_txt_da(inputs):\n",
      "    txt_da = DocumentArray()\n",
      "    img_da = DocumentArray()\n",
      "    for doc in inputs[0]:\n",
      "        split_img_txt_da(doc, img_da, txt_da)\n",
      "    assert len(txt_da) == inputs[1][0]\n",
      "    assert len(img_da) == inputs[1][1]\n",
      "test_split_img_txt_da(inputs=(<DocumentArray (length=4) at 140538627068688>, (3, 1)))\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "img_da = DocumentArray()\n",
      "State:\n",
      "<DocumentArray (length=0) at 140537876330432>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_split_img_txt_da(inputs):\n",
      "    txt_da = DocumentArray()\n",
      "    img_da = DocumentArray()\n",
      "    for doc in inputs[0]:\n",
      "        split_img_txt_da(doc, img_da, txt_da)\n",
      "    assert len(txt_da) == inputs[1][0]\n",
      "    assert len(img_da) == inputs[1][1]\n",
      "test_split_img_txt_da(inputs=(<DocumentArray (length=4) at 140538627068688>, (3, 1)))\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "assert len(txt_da) == inputs[1][0]\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_server_preprocess_ndarray_image(image_uri, size):\n",
      "    d1 = Document(uri=image_uri)\n",
      "    d1.load_uri_to_blob()\n",
      "    d2 = Document(uri=image_uri)\n",
      "    d2.load_uri_to_image_tensor()\n",
      "    t1 = _transform_blob(size)(d1.blob).numpy()\n",
      "    t2 = _transform_ndarray(size)(d2.tensor).numpy()\n",
      "    assert t1.shape == t2.shape\n",
      "test_server_preprocess_ndarray_image(image_uri='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/jina-ai+clip-as-service/jina-ai+clip-as-service/tests/img/00000.jpg', size=224)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "d1 = Document(uri=image_uri)\n",
      "State:\n",
      "<Document ('id', 'mime_type', 'uri') at 3b1f02f040a677b8a7dfd83176b52d78>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_server_preprocess_ndarray_image(image_uri, size):\n",
      "    d1 = Document(uri=image_uri)\n",
      "    d1.load_uri_to_blob()\n",
      "    d2 = Document(uri=image_uri)\n",
      "    d2.load_uri_to_image_tensor()\n",
      "    t1 = _transform_blob(size)(d1.blob).numpy()\n",
      "    t2 = _transform_ndarray(size)(d2.tensor).numpy()\n",
      "    assert t1.shape == t2.shape\n",
      "test_server_preprocess_ndarray_image(image_uri='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/jina-ai+clip-as-service/jina-ai+clip-as-service/tests/img/00000.jpg', size=224)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "d1.load_uri_to_blob()\n",
      "State:\n",
      "<Document ('id', 'blob', 'mime_type', 'uri') at 3b1f02f040a677b8a7dfd83176b52d78>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_server_preprocess_ndarray_image(image_uri, size):\n",
      "    d1 = Document(uri=image_uri)\n",
      "    d1.load_uri_to_blob()\n",
      "    d2 = Document(uri=image_uri)\n",
      "    d2.load_uri_to_image_tensor()\n",
      "    t1 = _transform_blob(size)(d1.blob).numpy()\n",
      "    t2 = _transform_ndarray(size)(d2.tensor).numpy()\n",
      "    assert t1.shape == t2.shape\n",
      "test_server_preprocess_ndarray_image(image_uri='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/jina-ai+clip-as-service/jina-ai+clip-as-service/tests/img/00000.jpg', size=224)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "d2 = Document(uri=image_uri)\n",
      "State:\n",
      "<Document ('id', 'mime_type', 'uri') at eee52bb275edc026b2fb333d7656e3c6>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_server_preprocess_ndarray_image(image_uri, size):\n",
      "    d1 = Document(uri=image_uri)\n",
      "    d1.load_uri_to_blob()\n",
      "    d2 = Document(uri=image_uri)\n",
      "    d2.load_uri_to_image_tensor()\n",
      "    t1 = _transform_blob(size)(d1.blob).numpy()\n",
      "    t2 = _transform_ndarray(size)(d2.tensor).numpy()\n",
      "    assert t1.shape == t2.shape\n",
      "test_server_preprocess_ndarray_image(image_uri='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/jina-ai+clip-as-service/jina-ai+clip-as-service/tests/img/00000.jpg', size=224)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "d2.load_uri_to_image_tensor()\n",
      "State:\n",
      "<Document ('id', 'tensor', 'mime_type', 'uri') at eee52bb275edc026b2fb333d7656e3c6>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __setitem__(self, key, value):\n",
      "        setattr(self, self._LabelTrait[key], value)\n",
      "__setitem__(self=<QuantStudio.FactorDataBase.FactorDB.DataFactor object at 0x7fac1c488950>, key='', value='object', self.Name='Factor0', self._ArgOrder=    0    1dtype: int64, self._CacheData=None, self._ConfigFile=None, self._Data=            000000.SZ  000001.SZ  000002.SZ  ...  000007.SZ  000008.SZ  000009.SZ2018-01-01   1.764052   0.400157   0.978738  ...  -0.151357  -0.103219   0.4105992018-01-02   0.144044   1.454274   0.761038  ...  -0.205158   0.313068  -0.8540962018-01-03  -2.552990   0.653619   0.864436  ...  -0.187184   1.532779   1.4693592018-01-04   0.154947   0.378163  -0.887786  ...   1.202380  -0.387327  -0.3023032018-01-05  -1.048553  -1.420018  -1.706270  ...   0.777490  -1.613898  -0.2127402018-01-06  -0.895467   0.386902  -0.510805  ...   0.302472  -0.634322  -0.3627412018-01-07  -0.672460  -0.359553  -0.813146  ...   0.462782  -0.907298   0.0519452018-01-08   0.729091   0.128983   1.139401  ...  -0.578850  -0.311553   0.0561652018-01-09  -1.165150   0.900826   0.465662  ...  -0.179925  -1.070753   1.0544522018-01-10  -0.403177   1.222445   0.208275  ...   1.785870   0.126912   0.4019892018-01-11   1.883151  -1.347759  -1.270485  ...  -0.747455   1.922942   1.4805152018-01-12   1.867559   0.906045  -0.861226  ...  -0.155010   0.614079   0.9222072018-01-13   0.376426  -1.099401   0.298238  ...   1.849264   0.672295   0.4074622018-01-14  -0.769916   0.539249  -0.674333  ...  -0.208299   0.396007  -1.0930622018-01-15  -1.491258   0.439392   0.166673  ...   1.117016  -1.315907  -0.4615852018-01-16  -0.068242   1.713343  -0.744755  ...  -1.079932  -1.147469  -0.4378202018-01-17  -0.498032   1.929532   0.949421  ...  -1.544771   1.188030   0.3169432018-01-18   0.920859   0.318728   0.856831  ...  -0.689550  -0.455533   0.0174792018-01-19  -0.353994  -1.374951  -0.643618  ...   0.052165  -0.739563   1.5430152018-01-20  -1.292857   0.267051  -0.039283  ...   0.823504   2.163236   1.3365282018-01-21  -0.369182  -0.239379   1.099660  ...  -0.738031   0.279925  -0.0981502018-01-22   0.910179   0.317218   0.786328  ...   0.379152   2.259309  -0.0422572018-01-23  -0.955945  -0.345982  -0.463596  ...   0.232181  -0.597316  -0.2379222018-01-24  -1.424061  -0.493320  -0.542861  ...  -2.069985   0.426259   0.6769082018-01-25  -0.637437  -0.397272  -0.132881  ...   1.079619  -0.813364  -1.4664242018-01-26   0.521065  -0.575788   0.141953  ...  -1.383364  -1.582938   0.6103792018-01-27  -1.188859  -0.506816  -0.596314  ...   0.088422  -0.310886   0.0974002018-01-28   0.399046  -2.772593   1.955912  ...  -0.116104  -2.030684   2.0644932018-01-29  -0.110541   1.020173  -0.692050  ...   1.211145   0.689818   1.3018462018-01-30  -0.628088  -0.481027   2.303917  ...   0.582954  -0.399449   0.370056[30 rows x 10 columns], self._DataContent='Factor', self._FactorTable=None, self._LabelTrait={'': 'DataType', '': 'LookBack'}, self._NameInFT='Factor0', self._OperationMode=None, self._QS_Logger=<RootLogger root (WARNING)>, self._RawDataFile='', self._isCacheDataOK=False, self._isStarted=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "setattr(self, self._LabelTrait[key], value)\n",
      "State:\n",
      "'object'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def TestTimeOperationRollingNoniterativeMultiDTSingleIDFun(f, idt, iid, x, args):\n",
      "    N = args[\"N\"]\n",
      "    Rslt = np.full((x[0].shape[0]-N+1, ), fill_value=np.nan)\n",
      "    for i in range(4, x[0].shape[0]): Rslt[i-N+1] = np.mean(x[0][i-N+1:i+1])\n",
      "    return Rslt\n",
      "TestTimeOperationRollingNoniterativeMultiDTSingleIDFun(f={_Descriptors=[<QuantStudio.FactorDataBase.FactorDB.DataFactor object at 0x7fac1c488950>], UserData={}, _FactorTable=None, _NameInFT='TestFactor', Name='TestFactor', _isStarted=False, _CacheData=None, _OperationMode=None, _RawDataFile='', _isCacheDataOK=False, _QS_Logger=<RootLogger root (WARNING)>, _LabelTrait={'': 'DTMode', 'ID': 'IDMode', '': 'LookBack', '': 'LookBackMode', '': 'iLookBack', '': 'iLookBackMode', '': 'iInitData', '': 'Operator', '': 'ModelArgs', '': 'DataType'}, _ArgOrder=        0        1      2      3ID      4      5      6    7    8     9dtype: int64, LookBack=[4], LookBackMode=[''], _ConfigFile=None, ModelArgs={'N': 5}, DTMode='', IDMode='ID', iLookBackMode='', DataType='double', iLookBack=0}, idt=[None, None, None, None, datetime.datetime(2018, 1, 1, 0, 0), datetime.datetime(2018, 1, 2, 0, 0), datetime.datetime(2018, 1, 3, 0, 0), datetime.datetime(2018, 1, 4, 0, 0), datetime.datetime(2018, 1, 5, 0, 0), datetime.datetime(2018, 1, 6, 0, 0), datetime.datetime(2018, 1, 7, 0, 0), datetime.datetime(2018, 1, 8, 0, 0), datetime.datetime(2018, 1, 9, 0, 0), datetime.datetime(2018, 1, 10, 0, 0), datetime.datetime(2018, 1, 11, 0, 0), datetime.datetime(2018, 1, 12, 0, 0), datetime.datetime(2018, 1, 13, 0, 0), datetime.datetime(2018, 1, 14, 0, 0), datetime.datetime(2018, 1, 15, 0, 0), datetime.datetime(2018, 1, 16, 0, 0), datetime.datetime(2018, 1, 17, 0, 0), datetime.datetime(2018, 1, 18, 0, 0), datetime.datetime(2018, 1, 19, 0, 0), datetime.datetime(2018, 1, 20, 0, 0), datetime.datetime(2018, 1, 21, 0, 0), datetime.datetime(2018, 1, 22, 0, 0), datetime.datetime(2018, 1, 23, 0, 0), datetime.datetime(2018, 1, 24, 0, 0), datetime.datetime(2018, 1, 25, 0, 0), datetime.datetime(2018, 1, 26, 0, 0), datetime.datetime(2018, 1, 27, 0, 0), datetime.datetime(2018, 1, 28, 0, 0), datetime.datetime(2018, 1, 29, 0, 0), datetime.datetime(2018, 1, 30, 0, 0)], iid='000000.SZ', x=[array([        nan,         nan,         nan,         nan,  1.76405235,        0.14404357, -2.55298982,  0.15494743, -1.04855297, -0.89546656,       -0.67246045,  0.72909056, -1.16514984, -0.40317695,  1.8831507 ,        1.86755896,  0.37642553, -0.76991607, -1.49125759, -0.06824161,       -0.49803245,  0.92085882, -0.35399391, -1.29285691, -0.36918184,        0.91017891, -0.955945  , -1.42406091, -0.63743703,  0.52106488,       -1.18885926,  0.39904635, -0.11054066, -0.62808756])], args={'N': 5})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "N = args[\"N\"]\n",
      "State:\n",
      "5\n",
      "==================================================\n",
      "Clean Code:\n",
      "def TestTimeOperationRollingNoniterativeMultiDTSingleIDFun(f, idt, iid, x, args):\n",
      "    N = args[\"N\"]\n",
      "    Rslt = np.full((x[0].shape[0]-N+1, ), fill_value=np.nan)\n",
      "    for i in range(4, x[0].shape[0]): Rslt[i-N+1] = np.mean(x[0][i-N+1:i+1])\n",
      "    return Rslt\n",
      "TestTimeOperationRollingNoniterativeMultiDTSingleIDFun(f={_Descriptors=[<QuantStudio.FactorDataBase.FactorDB.DataFactor object at 0x7fac1c488950>], UserData={}, _FactorTable=None, _NameInFT='TestFactor', Name='TestFactor', _isStarted=False, _CacheData=None, _OperationMode=None, _RawDataFile='', _isCacheDataOK=False, _QS_Logger=<RootLogger root (WARNING)>, _LabelTrait={'': 'DTMode', 'ID': 'IDMode', '': 'LookBack', '': 'LookBackMode', '': 'iLookBack', '': 'iLookBackMode', '': 'iInitData', '': 'Operator', '': 'ModelArgs', '': 'DataType'}, _ArgOrder=        0        1      2      3ID      4      5      6    7    8     9dtype: int64, LookBack=[4], LookBackMode=[''], _ConfigFile=None, ModelArgs={'N': 5}, DTMode='', IDMode='ID', iLookBackMode='', DataType='double', iLookBack=0}, idt=[None, None, None, None, datetime.datetime(2018, 1, 1, 0, 0), datetime.datetime(2018, 1, 2, 0, 0), datetime.datetime(2018, 1, 3, 0, 0), datetime.datetime(2018, 1, 4, 0, 0), datetime.datetime(2018, 1, 5, 0, 0), datetime.datetime(2018, 1, 6, 0, 0), datetime.datetime(2018, 1, 7, 0, 0), datetime.datetime(2018, 1, 8, 0, 0), datetime.datetime(2018, 1, 9, 0, 0), datetime.datetime(2018, 1, 10, 0, 0), datetime.datetime(2018, 1, 11, 0, 0), datetime.datetime(2018, 1, 12, 0, 0), datetime.datetime(2018, 1, 13, 0, 0), datetime.datetime(2018, 1, 14, 0, 0), datetime.datetime(2018, 1, 15, 0, 0), datetime.datetime(2018, 1, 16, 0, 0), datetime.datetime(2018, 1, 17, 0, 0), datetime.datetime(2018, 1, 18, 0, 0), datetime.datetime(2018, 1, 19, 0, 0), datetime.datetime(2018, 1, 20, 0, 0), datetime.datetime(2018, 1, 21, 0, 0), datetime.datetime(2018, 1, 22, 0, 0), datetime.datetime(2018, 1, 23, 0, 0), datetime.datetime(2018, 1, 24, 0, 0), datetime.datetime(2018, 1, 25, 0, 0), datetime.datetime(2018, 1, 26, 0, 0), datetime.datetime(2018, 1, 27, 0, 0), datetime.datetime(2018, 1, 28, 0, 0), datetime.datetime(2018, 1, 29, 0, 0), datetime.datetime(2018, 1, 30, 0, 0)], iid='000000.SZ', x=[array([        nan,         nan,         nan,         nan,  1.76405235,        0.14404357, -2.55298982,  0.15494743, -1.04855297, -0.89546656,       -0.67246045,  0.72909056, -1.16514984, -0.40317695,  1.8831507 ,        1.86755896,  0.37642553, -0.76991607, -1.49125759, -0.06824161,       -0.49803245,  0.92085882, -0.35399391, -1.29285691, -0.36918184,        0.91017891, -0.955945  , -1.42406091, -0.63743703,  0.52106488,       -1.18885926,  0.39904635, -0.11054066, -0.62808756])], args={'N': 5})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "Rslt = np.full((x[0].shape[0]-N+1, ), fill_value=np.nan)\n",
      "State:\n",
      "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,       nan, nan, nan, nan])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def TestTimeOperationRollingIterativeMultiDTSingleIDFun(f, idt, iid, x, args):\n",
      "    N = args[\"N\"]\n",
      "    Rslt = np.copy(x[1])\n",
      "    for i in range(1, x[1].shape[0]): Rslt[i] = 2 / (N+1) * x[1][i] + (1 - 2 / (N+1)) * Rslt[i-1]\n",
      "    return Rslt\n",
      "TestTimeOperationRollingIterativeMultiDTSingleIDFun(f={_Descriptors=[<QuantStudio.FactorDataBase.FactorDB.DataFactor object at 0x7fac1c488950>], UserData={}, _FactorTable=None, _NameInFT='TestFactor', Name='TestFactor', _isStarted=False, _CacheData=None, _OperationMode=None, _RawDataFile='', _isCacheDataOK=False, _QS_Logger=<RootLogger root (WARNING)>, _LabelTrait={'': 'DTMode', 'ID': 'IDMode', '': 'LookBack', '': 'LookBackMode', '': 'iLookBack', '': 'iLookBackMode', '': 'iInitData', '': 'Operator', '': 'ModelArgs', '': 'DataType'}, _ArgOrder=        0        1      2      3ID      4      5      6    7    8     9dtype: int64, LookBack=[0], LookBackMode=[''], _ConfigFile=None, ModelArgs={'N': 5}, iLookBack=1, DTMode='', IDMode='ID', iLookBackMode='', DataType='double', iInitData=None}, idt=[None, datetime.datetime(2018, 1, 1, 0, 0), datetime.datetime(2018, 1, 2, 0, 0), datetime.datetime(2018, 1, 3, 0, 0), datetime.datetime(2018, 1, 4, 0, 0), datetime.datetime(2018, 1, 5, 0, 0), datetime.datetime(2018, 1, 6, 0, 0), datetime.datetime(2018, 1, 7, 0, 0), datetime.datetime(2018, 1, 8, 0, 0), datetime.datetime(2018, 1, 9, 0, 0), datetime.datetime(2018, 1, 10, 0, 0), datetime.datetime(2018, 1, 11, 0, 0), datetime.datetime(2018, 1, 12, 0, 0), datetime.datetime(2018, 1, 13, 0, 0), datetime.datetime(2018, 1, 14, 0, 0), datetime.datetime(2018, 1, 15, 0, 0), datetime.datetime(2018, 1, 16, 0, 0), datetime.datetime(2018, 1, 17, 0, 0), datetime.datetime(2018, 1, 18, 0, 0), datetime.datetime(2018, 1, 19, 0, 0), datetime.datetime(2018, 1, 20, 0, 0), datetime.datetime(2018, 1, 21, 0, 0), datetime.datetime(2018, 1, 22, 0, 0), datetime.datetime(2018, 1, 23, 0, 0), datetime.datetime(2018, 1, 24, 0, 0), datetime.datetime(2018, 1, 25, 0, 0), datetime.datetime(2018, 1, 26, 0, 0), datetime.datetime(2018, 1, 27, 0, 0), datetime.datetime(2018, 1, 28, 0, 0), datetime.datetime(2018, 1, 29, 0, 0), datetime.datetime(2018, 1, 30, 0, 0)], iid='000000.SZ', x=[array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,       nan, nan, nan, nan]), array([ 1.76405235,  0.14404357, -2.55298982,  0.15494743, -1.04855297,       -0.89546656, -0.67246045,  0.72909056, -1.16514984, -0.40317695,        1.8831507 ,  1.86755896,  0.37642553, -0.76991607, -1.49125759,       -0.06824161, -0.49803245,  0.92085882, -0.35399391, -1.29285691,       -0.36918184,  0.91017891, -0.955945  , -1.42406091, -0.63743703,        0.52106488, -1.18885926,  0.39904635, -0.11054066, -0.62808756])], args={'N': 5})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "N = args[\"N\"]\n",
      "State:\n",
      "5\n",
      "==================================================\n",
      "Clean Code:\n",
      "def TestTimeOperationRollingIterativeMultiDTSingleIDFun(f, idt, iid, x, args):\n",
      "    N = args[\"N\"]\n",
      "    Rslt = np.copy(x[1])\n",
      "    for i in range(1, x[1].shape[0]): Rslt[i] = 2 / (N+1) * x[1][i] + (1 - 2 / (N+1)) * Rslt[i-1]\n",
      "    return Rslt\n",
      "TestTimeOperationRollingIterativeMultiDTSingleIDFun(f={_Descriptors=[<QuantStudio.FactorDataBase.FactorDB.DataFactor object at 0x7fac1c488950>], UserData={}, _FactorTable=None, _NameInFT='TestFactor', Name='TestFactor', _isStarted=False, _CacheData=None, _OperationMode=None, _RawDataFile='', _isCacheDataOK=False, _QS_Logger=<RootLogger root (WARNING)>, _LabelTrait={'': 'DTMode', 'ID': 'IDMode', '': 'LookBack', '': 'LookBackMode', '': 'iLookBack', '': 'iLookBackMode', '': 'iInitData', '': 'Operator', '': 'ModelArgs', '': 'DataType'}, _ArgOrder=        0        1      2      3ID      4      5      6    7    8     9dtype: int64, LookBack=[0], LookBackMode=[''], _ConfigFile=None, ModelArgs={'N': 5}, iLookBack=1, DTMode='', IDMode='ID', iLookBackMode='', DataType='double', iInitData=None}, idt=[None, datetime.datetime(2018, 1, 1, 0, 0), datetime.datetime(2018, 1, 2, 0, 0), datetime.datetime(2018, 1, 3, 0, 0), datetime.datetime(2018, 1, 4, 0, 0), datetime.datetime(2018, 1, 5, 0, 0), datetime.datetime(2018, 1, 6, 0, 0), datetime.datetime(2018, 1, 7, 0, 0), datetime.datetime(2018, 1, 8, 0, 0), datetime.datetime(2018, 1, 9, 0, 0), datetime.datetime(2018, 1, 10, 0, 0), datetime.datetime(2018, 1, 11, 0, 0), datetime.datetime(2018, 1, 12, 0, 0), datetime.datetime(2018, 1, 13, 0, 0), datetime.datetime(2018, 1, 14, 0, 0), datetime.datetime(2018, 1, 15, 0, 0), datetime.datetime(2018, 1, 16, 0, 0), datetime.datetime(2018, 1, 17, 0, 0), datetime.datetime(2018, 1, 18, 0, 0), datetime.datetime(2018, 1, 19, 0, 0), datetime.datetime(2018, 1, 20, 0, 0), datetime.datetime(2018, 1, 21, 0, 0), datetime.datetime(2018, 1, 22, 0, 0), datetime.datetime(2018, 1, 23, 0, 0), datetime.datetime(2018, 1, 24, 0, 0), datetime.datetime(2018, 1, 25, 0, 0), datetime.datetime(2018, 1, 26, 0, 0), datetime.datetime(2018, 1, 27, 0, 0), datetime.datetime(2018, 1, 28, 0, 0), datetime.datetime(2018, 1, 29, 0, 0), datetime.datetime(2018, 1, 30, 0, 0)], iid='000000.SZ', x=[array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,       nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,       nan, nan, nan, nan]), array([ 1.76405235,  0.14404357, -2.55298982,  0.15494743, -1.04855297,       -0.89546656, -0.67246045,  0.72909056, -1.16514984, -0.40317695,        1.8831507 ,  1.86755896,  0.37642553, -0.76991607, -1.49125759,       -0.06824161, -0.49803245,  0.92085882, -0.35399391, -1.29285691,       -0.36918184,  0.91017891, -0.955945  , -1.42406091, -0.63743703,        0.52106488, -1.18885926,  0.39904635, -0.11054066, -0.62808756])], args={'N': 5})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "Rslt = np.copy(x[1])\n",
      "State:\n",
      "array([ 1.76405235,  0.14404357, -2.55298982,  0.15494743, -1.04855297,       -0.89546656, -0.67246045,  0.72909056, -1.16514984, -0.40317695,        1.8831507 ,  1.86755896,  0.37642553, -0.76991607, -1.49125759,       -0.06824161, -0.49803245,  0.92085882, -0.35399391, -1.29285691,       -0.36918184,  0.91017891, -0.955945  , -1.42406091, -0.63743703,        0.52106488, -1.18885926,  0.39904635, -0.11054066, -0.62808756])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _transform_column(col_name, col_obj):\n",
      "            column_dict[col_name] = col_obj\n",
      "            if col_obj.primary_key:\n",
      "                primary_keys[col_name] = col_obj\n",
      "            col_obj.set_column_name(col_name)\n",
      "            attrs[col_name] = ColumnDescriptor(col_obj)\n",
      "_transform_column(col_name='partition', col_obj={partition_key=True, primary_key=True, index=False, db_field=None, required=False, clustering_order=None, discriminator_column=False, column_name=None, static=False, value=None, position=0}, attrs={'__module__': 'tests.integration.cqlengine.columns.test_container_columns', '__qualname__': 'TestSetModel', 'partition': <cassandra.cqlengine.columns.UUID object at 0x7f81e38d64f0>, 'int_set': <cassandra.cqlengine.columns.Set object at 0x7f81e38d6670>, 'text_set': <cassandra.cqlengine.columns.Set object at 0x7f81e38d6130>, '__abstract__': False, '__discriminator_value__': None, '__default_ttl__': None}, column_dict=OrderedDict(), primary_keys=OrderedDict())\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "col_obj.set_column_name(col_name)\n",
      "State:\n",
      "{partition_key=True, primary_key=True, index=False, db_field=None, required=False, clustering_order=None, discriminator_column=False, column_name='partition', static=False, value=None, position=0}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _from_timetuple(self, t):\n",
      "        self.days_from_epoch = calendar.timegm(t) // Date.DAY\n",
      "_from_timetuple(self=Date(0), t=time.struct_time(tm_year=2024, tm_mon=4, tm_mday=3, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=2, tm_yday=94, tm_isdst=-1))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.days_from_epoch = calendar.timegm(t) // Date.DAY\n",
      "State:\n",
      "19816\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _insert(self, key, value):\n",
      "        flat_key = self._serialize_key(key)\n",
      "        i = self._index.get(flat_key, -1)\n",
      "        if i >= 0:\n",
      "            self._items[i] = (key, value)\n",
      "        else:\n",
      "            self._items.append((key, value))\n",
      "            self._index[flat_key] = len(self._items) - 1\n",
      "_insert(self=OrderedMapSerializedKey([]), key='bob', value=199)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "flat_key = self._serialize_key(key)\n",
      "State:\n",
      "b'\\xe3\\x81\\xbfbob'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _insert(self, key, value):\n",
      "        flat_key = self._serialize_key(key)\n",
      "        i = self._index.get(flat_key, -1)\n",
      "        if i >= 0:\n",
      "            self._items[i] = (key, value)\n",
      "        else:\n",
      "            self._items.append((key, value))\n",
      "            self._index[flat_key] = len(self._items) - 1\n",
      "_insert(self=OrderedMapSerializedKey([]), key='bob', value=199)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "i = self._index.get(flat_key, -1)\n",
      "State:\n",
      "-1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def setval(self, val):\n",
      "        self.value = val\n",
      "setval(self=<cassandra.cqlengine.columns.BaseValueManager object at 0x7f81e0241fa0>, val=UUID('5dcf740d-d39f-4449-ba95-9eeccd4c6fa4'), self.column=<cassandra.cqlengine.columns.UUID object at 0x7f81e3724b80>, self.explicit=False, self.instance=IntegerTest(test_id=None, value=None), self.previous_value=None, self.value=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.value = val\n",
      "State:\n",
      "IntegerTest(test_id=UUID('5dcf740d-d39f-4449-ba95-9eeccd4c6fa4'), value=None)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def filter(self, *args, **kwargs):\n",
      "        if len([x for x in kwargs.values() if x is None]):\n",
      "            raise CQLEngineException(\"None values on filter are not allowed\")\n",
      "        clone = copy.deepcopy(self)\n",
      "        for operator in args:\n",
      "            if not isinstance(operator, WhereClause):\n",
      "                raise QueryException('{0} is not a valid query operator'.format(operator))\n",
      "            clone._where.append(operator)\n",
      "        for arg, val in kwargs.items():\n",
      "            col_name, col_op = self._parse_filter_arg(arg)\n",
      "            quote_field = True\n",
      "            try:\n",
      "                column = self.model._get_column(col_name)\n",
      "            except KeyError:\n",
      "                if col_name == 'pk__token':\n",
      "                    if not isinstance(val, Token):\n",
      "                        raise QueryException(\"Virtual column 'pk__token' may only be compared to Token() values\")\n",
      "                    column = columns._PartitionKeysToken(self.model)\n",
      "                    quote_field = False\n",
      "                else:\n",
      "                    raise QueryException(\"Can't resolve column name: '{0}'\".format(col_name))\n",
      "            if isinstance(val, Token):\n",
      "                if col_name != 'pk__token':\n",
      "                    raise QueryException(\"Token() values may only be compared to the 'pk__token' virtual column\")\n",
      "                partition_columns = column.partition_columns\n",
      "                if len(partition_columns) != len(val.value):\n",
      "                    raise QueryException(\n",
      "                        'Token() received {0} arguments but model has {1} partition keys'.format(\n",
      "                            len(val.value), len(partition_columns)))\n",
      "                val.set_columns(partition_columns)\n",
      "            operator_class = BaseWhereOperator.get_operator(col_op or 'EQ')\n",
      "            operator = operator_class()\n",
      "            if isinstance(operator, InOperator):\n",
      "                if not isinstance(val, (list, tuple)):\n",
      "                    raise QueryException('IN queries must use a list/tuple value')\n",
      "                query_val = [column.to_database(v) for v in val]\n",
      "            elif isinstance(val, BaseQueryFunction):\n",
      "                query_val = val\n",
      "            else:\n",
      "                query_val = column.to_database(val)\n",
      "            clone._where.append(WhereClause(column.db_field_name, operator, query_val, quote_field=quote_field))\n",
      "        return clone\n",
      "filter(self=<cassandra.cqlengine.query.SimpleQuerySet object at 0x7f81ded333d0>, args=(), kwargs={'test_id': 5}, self._allow_filtering=False, self._batch=None, self._consistency=None, self._defer_fields=[], self._flat_values_list=False, self._if_not_exists=False, self._limit=10000, self._only_fields=[], self._order=[], self._result_cache=None, self._result_idx=None, self._timeout=<object object at 0x7f81e520eb80>, self._timestamp=None, self._transaction=[], self._ttl=None, self._values_list=False, self._where=[], self.model=<cassandra.cqlengine.named.NamedTable object at 0x7f81ded336a0>)\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "quote_field = True\n",
      "State:\n",
      "True\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_app(self, capp=None):\n",
      "        if not capp:\n",
      "            capp = self._get_celery_app()\n",
      "        events = Events(capp, IOLoop.current())\n",
      "        app = Flower(capp=capp, events=events,\n",
      "                     options=options, handlers=handlers, **settings)\n",
      "        return app\n",
      "get_app(self=<tests.unit.api.test_auth.BasicAuthTests testMethod=test_auth>, capp=None, self._AsyncHTTPTestCase__port=39617, self._AsyncTestCase__failure=None, self._AsyncTestCase__running=False, self._AsyncTestCase__stop_args=None, self._AsyncTestCase__stopped=False, self._AsyncTestCase__timeout=None, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fc8312b0a90>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_auth', self._test_generator=None, self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.http_client=<tornado.simple_httpclient.SimpleAsyncHTTPClient object at 0x7fc8313724c0>, self.io_loop=<tornado.platform.asyncio.AsyncIOLoop object at 0x7fc8312b0310>, self.test_auth=<tornado.testing._TestMethodWrapper object at 0x7fc8312e6160>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "events = Events(capp, IOLoop.current())\n",
      "State:\n",
      "<Events(Thread-1, initial daemon)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_current_user(self):\n",
      "        basic_auth = self.application.options.basic_auth\n",
      "        if basic_auth:\n",
      "            auth_header = self.request.headers.get(\"Authorization\", \"\")\n",
      "            try:\n",
      "                basic, credentials = auth_header.split()\n",
      "                credentials = b64decode(credentials.encode()).decode()\n",
      "                if basic != 'Basic':\n",
      "                    raise tornado.web.HTTPError(401)\n",
      "                for stored_credential in basic_auth:\n",
      "                    if hmac.compare_digest(stored_credential, credentials):\n",
      "                        break\n",
      "                else:\n",
      "                    raise tornado.web.HTTPError(401)\n",
      "            except ValueError as exc:\n",
      "                raise tornado.web.HTTPError(401) from exc\n",
      "        if not self.application.options.auth:\n",
      "            return True\n",
      "        user = self.get_secure_cookie('user')\n",
      "        if user:\n",
      "            if not isinstance(user, str):\n",
      "                user = user.decode()\n",
      "            if re.match(self.application.options.auth, user):\n",
      "                return user\n",
      "        return None\n",
      "get_current_user(self=<flower.api.workers.ListWorkers object at 0x7fc8311bb640>, self._auto_finish=True, self._finished=False, self._headers=<tornado.httputil.HTTPHeaders object at 0x7fc8311bb820>, self._headers_written=False, self._prepared_future=None, self._reason='OK', self._status_code=200, self._transforms=[], self._write_buffer=[], self.application=<flower.app.Flower object at 0x7fc831284250>, self.path_args=[], self.path_kwargs={}, self.request=HTTPServerRequest(protocol='http', host='127.0.0.1:39617', method='GET', uri='/api/workers', version='HTTP/1.1', remote_ip='127.0.0.1'), self.ui={'_tt_modules': <tornado.web._UIModuleNamespace object at 0x7fc8311bb910>, 'modules': <tornado.web._UIModuleNamespace object at 0x7fc8311bb910>})\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "except ValueError as exc:\n",
      "State:\n",
      "ValueError('not enough values to unpack (expected 2, got 0)')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def write_error(self, status_code, **kwargs):\n",
      "        exc_info = kwargs.get('exc_info')\n",
      "        log_message = exc_info[1].log_message\n",
      "        if log_message:\n",
      "            self.write(log_message)\n",
      "        self.set_status(status_code)\n",
      "        self.finish()\n",
      "write_error(self=<flower.api.workers.ListWorkers object at 0x7fc8311bb640>, status_code=401, kwargs={'exc_info': (<class 'tornado.web.HTTPError'>, HTTPError(), <traceback object at 0x7fc8311e2980>)}, self._auto_finish=True, self._finished=False, self._headers=<tornado.httputil.HTTPHeaders object at 0x7fc8311bb5b0>, self._headers_written=False, self._prepared_future=None, self._reason='Unauthorized', self._status_code=401, self._transforms=[], self._write_buffer=[], self.application=<flower.app.Flower object at 0x7fc831284250>, self.path_args=[], self.path_kwargs={}, self.request=HTTPServerRequest(protocol='http', host='127.0.0.1:39617', method='GET', uri='/api/workers', version='HTTP/1.1', remote_ip='127.0.0.1'), self.ui={'_tt_modules': <tornado.web._UIModuleNamespace object at 0x7fc8311bb910>, 'modules': <tornado.web._UIModuleNamespace object at 0x7fc8311bb910>})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "log_message = exc_info[1].log_message\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_argument(self, name, default=[], strip=True, type=None):\n",
      "        arg = super().get_argument(name, default, strip)\n",
      "        if arg and isinstance(arg, str):\n",
      "            arg = tornado.escape.xhtml_escape(arg)\n",
      "        if type is not None:\n",
      "            try:\n",
      "                if type is bool:\n",
      "                    arg = strtobool(str(arg))\n",
      "                else:\n",
      "                    arg = type(arg)\n",
      "            except (ValueError, TypeError) as exc:\n",
      "                if arg is None and default is None:\n",
      "                    return arg\n",
      "                raise tornado.web.HTTPError(\n",
      "                        400,\n",
      "                        f\"Invalid argument '{arg}' of type '{type.__name__}'\") from exc\n",
      "        return arg\n",
      "get_argument(self=<flower.api.workers.ListWorkers object at 0x7fc8311bb910>, name='refresh', default=False, strip=True, type=<class 'bool'>, __class__=<class 'flower.views.BaseHandler'>, self._auto_finish=True, self._current_user=True, self._finished=False, self._headers=<tornado.httputil.HTTPHeaders object at 0x7fc8311bb4f0>, self._headers_written=False, self._prepared_future=None, self._reason='OK', self._status_code=200, self._transforms=[], self._write_buffer=[], self.application=<flower.app.Flower object at 0x7fc831284250>, self.path_args=[], self.path_kwargs={}, self.request=HTTPServerRequest(protocol='http', host='127.0.0.1:39617', method='GET', uri='/api/workers', version='HTTP/1.1', remote_ip='127.0.0.1'), self.ui={'_tt_modules': <tornado.web._UIModuleNamespace object at 0x7fc8311bbac0>, 'modules': <tornado.web._UIModuleNamespace object at 0x7fc8311bbac0>})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "arg = super().get_argument(name, default, strip)\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def post(self, url, **kwargs):\n",
      "        if 'body' in kwargs and isinstance(kwargs['body'], dict):\n",
      "            kwargs['body'] = urlencode(kwargs['body'])\n",
      "        return self.fetch(url, method='POST', **kwargs)\n",
      "post(self=<tests.unit.api.test_control.UnknownWorkerControlTests testMethod=test_unknown_worker>, url='/api/worker/shutdown/test', kwargs={'body': {}}, self._AsyncHTTPTestCase__port=36523, self._AsyncTestCase__failure=None, self._AsyncTestCase__running=False, self._AsyncTestCase__stop_args=None, self._AsyncTestCase__stopped=False, self._AsyncTestCase__timeout=None, self._app=<flower.app.Flower object at 0x7fc83121d850>, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fc8312c44c0>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_unknown_worker', self._test_generator=None, self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.http_client=<tornado.simple_httpclient.SimpleAsyncHTTPClient object at 0x7fc83121da30>, self.http_server=<tornado.httpserver.HTTPServer object at 0x7fc8308e5f40>, self.io_loop=<tornado.platform.asyncio.AsyncIOLoop object at 0x7fc8313183d0>, self.test_unknown_worker=<tornado.testing._TestMethodWrapper object at 0x7fc831302640>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "kwargs['body'] = urlencode(kwargs['body'])\n",
      "State:\n",
      "{'body': ''}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def normalize_options(self, options):\n",
      "        if 'eta' in options:\n",
      "            options['eta'] = datetime.strptime(options['eta'],\n",
      "                                               self.DATE_FORMAT)\n",
      "        if 'countdown' in options:\n",
      "            options['countdown'] = float(options['countdown'])\n",
      "        if 'expires' in options:\n",
      "            expires = options['expires']\n",
      "            try:\n",
      "                expires = float(expires)\n",
      "            except ValueError:\n",
      "                expires = datetime.strptime(expires, self.DATE_FORMAT)\n",
      "            options['expires'] = expires\n",
      "normalize_options(self=<flower.api.tasks.TaskAsyncApply object at 0x7fc830544d00>, options={'countdown': '3'}, self._auto_finish=True, self._current_user=True, self._finished=False, self._headers=<tornado.httputil.HTTPHeaders object at 0x7fc8305442b0>, self._headers_written=False, self._prepared_future=None, self._reason='OK', self._status_code=200, self._transforms=[], self._write_buffer=[], self.application=<flower.app.Flower object at 0x7fc8307f5d60>, self.path_args=['foo'], self.path_kwargs={}, self.request=HTTPServerRequest(protocol='http', host='127.0.0.1:37233', method='POST', uri='/api/task/async-apply/foo', version='HTTP/1.1', remote_ip='127.0.0.1'), self.ui={'_tt_modules': <tornado.web._UIModuleNamespace object at 0x7fc8305446a0>, 'modules': <tornado.web._UIModuleNamespace object at 0x7fc8305446a0>})\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "options['countdown'] = float(options['countdown'])\n",
      "State:\n",
      "{'countdown': 3.0}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def task_succeeded_events(worker, id=None, name=None, runtime=0.1234, retries=0, eta=None):\n",
      "    id = id or uuid()\n",
      "    name = name or 'sometask'\n",
      "    return [Event('task-received', uuid=id, name=name,\n",
      "                  args='(2, 2)', kwargs=\"{'foo': 'bar'}\",\n",
      "                  retries=retries, eta=eta, hostname=worker),\n",
      "            Event('task-started', uuid=id, hostname=worker),\n",
      "            Event('task-succeeded', uuid=id, result='4',\n",
      "                  runtime=runtime, hostname=worker)]\n",
      "task_succeeded_events(worker='worker1', id=None, name=None, runtime=0.1234, retries=0, eta=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "id = id or uuid()\n",
      "State:\n",
      "'95b9f5c5-3693-4723-a64a-acb161eab235'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def task_succeeded_events(worker, id=None, name=None, runtime=0.1234, retries=0, eta=None):\n",
      "    id = id or uuid()\n",
      "    name = name or 'sometask'\n",
      "    return [Event('task-received', uuid=id, name=name,\n",
      "                  args='(2, 2)', kwargs=\"{'foo': 'bar'}\",\n",
      "                  retries=retries, eta=eta, hostname=worker),\n",
      "            Event('task-started', uuid=id, hostname=worker),\n",
      "            Event('task-succeeded', uuid=id, result='4',\n",
      "                  runtime=runtime, hostname=worker)]\n",
      "task_succeeded_events(worker='worker1', id=None, name=None, runtime=0.1234, retries=0, eta=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "name = name or 'sometask'\n",
      "State:\n",
      "'sometask'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def event(self, event):\n",
      "        super().event(event)\n",
      "        worker_name = event['hostname']\n",
      "        event_type = event['type']\n",
      "        self.counter[worker_name][event_type] += 1\n",
      "        if event_type.startswith('task-'):\n",
      "            task_id = event['uuid']\n",
      "            task = self.tasks.get(task_id)\n",
      "            task_name = event.get('name', '')\n",
      "            if not task_name and task_id in self.tasks:\n",
      "                task_name = task.name or ''\n",
      "            self.metrics.events.labels(worker_name, event_type, task_name).inc()\n",
      "            runtime = event.get('runtime', 0)\n",
      "            if runtime:\n",
      "                self.metrics.runtime.labels(worker_name, task_name).observe(runtime)\n",
      "            task_started = task.started\n",
      "            task_received = task.received\n",
      "            if event_type == 'task-received' and not task.eta and task_received:\n",
      "                self.metrics.number_of_prefetched_tasks.labels(worker_name, task_name).inc()\n",
      "            if event_type == 'task-started' and not task.eta and task_started and task_received:\n",
      "                self.metrics.prefetch_time.labels(worker_name, task_name).set(task_started - task_received)\n",
      "                self.metrics.number_of_prefetched_tasks.labels(worker_name, task_name).dec()\n",
      "            if event_type in ['task-succeeded', 'task-failed'] and not task.eta and task_started and task_received:\n",
      "                self.metrics.prefetch_time.labels(worker_name, task_name).set(0)\n",
      "        if event_type == 'worker-online':\n",
      "            self.metrics.worker_online.labels(worker_name).set(1)\n",
      "        if event_type == 'worker-heartbeat':\n",
      "            self.metrics.worker_online.labels(worker_name).set(1)\n",
      "            num_executing_tasks = event.get('active')\n",
      "            if num_executing_tasks is not None:\n",
      "                self.metrics.worker_number_of_currently_executing_tasks.labels(worker_name).set(num_executing_tasks)\n",
      "        if event_type == 'worker-offline':\n",
      "            self.metrics.worker_online.labels(worker_name).set(0)\n",
      "event(self=<State: events=0 tasks=0>, event={'hostname': 'worker1', 'timestamp': 1712215956.0920622, 'type': 'worker-online', 'clock': 0, 'local_received': 1712215956.1021538}, __class__=<class 'flower.events.EventsState'>, self._mutex=<unlocked _thread.lock object at 0x7fc8308cb900>, self._seen_types=set(), self._taskheap=[], self._tasks_to_resolve={}, self.counter=defaultdict(<class 'collections.Counter'>, {}), self.event_callback=None, self.handlers={}, self.max_tasks_in_memory=10000, self.max_workers_in_memory=5000, self.metrics=<flower.events.PrometheusMetrics object at 0x7fc831284520>, self.on_node_join=None, self.on_node_leave=None, self.tasks=OrderedDict(), self.tasks_by_type=CallableDefaultdict(<class '_weakrefset.WeakSet'>, {}), self.tasks_by_worker=CallableDefaultdict(<class '_weakrefset.WeakSet'>, {}), self.workers=OrderedDict([('worker1', <Worker: worker1 (OFFLINE clock:0))]))\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "worker_name = event['hostname']\n",
      "State:\n",
      "'worker1'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse_search_terms(raw_search_value):\n",
      "    search_regexp = r'(?:[^\\s,\"]|\"(?:\\\\.|[^\"])*\")+'\n",
      "    if not raw_search_value:\n",
      "        return {}\n",
      "    parsed_search = {}\n",
      "    for query_part in re.findall(search_regexp, raw_search_value):\n",
      "        if not query_part:\n",
      "            continue\n",
      "        if query_part.startswith('result:'):\n",
      "            parsed_search['result'] = preprocess_search_value(query_part[len('result:'):])\n",
      "        elif query_part.startswith('args:'):\n",
      "            if 'args' not in parsed_search:\n",
      "                parsed_search['args'] = []\n",
      "            parsed_search['args'].append(preprocess_search_value(query_part[len('args:'):]))\n",
      "        elif query_part.startswith('kwargs:'):\n",
      "            if 'kwargs'not in parsed_search:\n",
      "                parsed_search['kwargs'] = {}\n",
      "            try:\n",
      "                key, value = [p.strip() for p in query_part[len('kwargs:'):].split('=')]\n",
      "            except ValueError:\n",
      "                continue\n",
      "            parsed_search['kwargs'][key] = preprocess_search_value(value)\n",
      "        elif query_part.startswith('state'):\n",
      "            if 'state' not in parsed_search:\n",
      "                parsed_search['state'] = []\n",
      "            parsed_search['state'].append(preprocess_search_value(query_part[len('state:'):]))\n",
      "        else:\n",
      "            parsed_search['any'] = preprocess_search_value(query_part)\n",
      "    return parsed_search\n",
      "parse_search_terms(raw_search_value={})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "search_regexp = r'(?:[^\\s,\"]|\"(?:\\\\.|[^\"])*\")+'  # splits by space, ignores space in quotes\n",
      "State:\n",
      "'(?:[^\\\\s,\"]|\"(?:\\\\\\\\.|[^\"])*\")+'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def inspect(self, workername=None):\n",
      "        feutures = []\n",
      "        for method in self.methods:\n",
      "            feutures.append(self.io_loop.run_in_executor(None, partial(self._inspect, method, workername)))\n",
      "        return feutures\n",
      "inspect(self=<flower.inspector.Inspector object at 0x7fc8308ae070>, workername=None, self.capp=<Celery __main__ at 0x7fc831294f10>, self.io_loop=<tornado.platform.asyncio.AsyncIOLoop object at 0x7fc831372430>, self.timeout=1.0, self.workers=defaultdict(<class 'dict'>, {}))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "feutures = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def inspect(self, workername=None):\n",
      "        feutures = []\n",
      "        for method in self.methods:\n",
      "            feutures.append(self.io_loop.run_in_executor(None, partial(self._inspect, method, workername)))\n",
      "        return feutures\n",
      "inspect(self=<flower.inspector.Inspector object at 0x7fc8308ae070>, workername=None, self.capp=<Celery __main__ at 0x7fc831294f10>, self.io_loop=<tornado.platform.asyncio.AsyncIOLoop object at 0x7fc831372430>, self.timeout=1.0, self.workers=defaultdict(<class 'dict'>, {}))\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "feutures.append(self.io_loop.run_in_executor(None, partial(self._inspect, method, workername)))\n",
      "State:\n",
      "[<Future pending cb=[_chain_future.<locals>._call_check_cancel() at /local/rcs/XXX/miniforge3/envs/mher+flower/lib/python3.9/asyncio/futures.py:384]>]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _on_update(self, workername, method, response):\n",
      "        info = self.workers[workername]\n",
      "        info[method] = response\n",
      "        info['timestamp'] = time.time()\n",
      "_on_update(self=<flower.inspector.Inspector object at 0x7fc8308ae070>, workername='celery@worker1', method='inspect_method', response=['tasks.add', 'tasks.sleep'], self.capp=<Celery __main__ at 0x7fc831294f10>, self.io_loop=<tornado.platform.asyncio.AsyncIOLoop object at 0x7fc831372430>, self.timeout=1.0, self.workers=defaultdict(<class 'dict'>, {}))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "info = self.workers[workername]\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _on_update(self, workername, method, response):\n",
      "        info = self.workers[workername]\n",
      "        info[method] = response\n",
      "        info['timestamp'] = time.time()\n",
      "_on_update(self=<flower.inspector.Inspector object at 0x7fc8308ae070>, workername='celery@worker1', method='inspect_method', response=['tasks.add', 'tasks.sleep'], self.capp=<Celery __main__ at 0x7fc831294f10>, self.io_loop=<tornado.platform.asyncio.AsyncIOLoop object at 0x7fc831372430>, self.timeout=1.0, self.workers=defaultdict(<class 'dict'>, {}))\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "info[method] = response\n",
      "State:\n",
      "{'inspect_method': ['tasks.add', 'tasks.sleep']}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _on_update(self, workername, method, response):\n",
      "        info = self.workers[workername]\n",
      "        info[method] = response\n",
      "        info['timestamp'] = time.time()\n",
      "_on_update(self=<flower.inspector.Inspector object at 0x7fc8308ae070>, workername='celery@worker1', method='inspect_method', response=['tasks.add', 'tasks.sleep'], self.capp=<Celery __main__ at 0x7fc831294f10>, self.io_loop=<tornado.platform.asyncio.AsyncIOLoop object at 0x7fc831372430>, self.timeout=1.0, self.workers=defaultdict(<class 'dict'>, {}))\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "info['timestamp'] = time.time()\n",
      "State:\n",
      "{'inspect_method': ['tasks.add', 'tasks.sleep'], 'timestamp': 1712215956.9193895}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def apply_env_options():\n",
      "    \"apply options passed through environment variables\"\n",
      "    env_options = filter(is_flower_envvar, os.environ)\n",
      "    for env_var_name in env_options:\n",
      "        name = env_var_name.replace(ENV_VAR_PREFIX, '', 1).lower()\n",
      "        value = os.environ[env_var_name]\n",
      "        try:\n",
      "            option = options._options[name]\n",
      "        except KeyError:\n",
      "            option = options._options[name.replace('_', '-')]\n",
      "        if option.multiple:\n",
      "            value = [option.type(i) for i in value.split(',')]\n",
      "        else:\n",
      "            if option.type is bool:\n",
      "                value = bool(strtobool(value))\n",
      "            else:\n",
      "                value = option.type(value)\n",
      "        setattr(options, name, value)\n",
      "apply_env_options()\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "env_options = filter(is_flower_envvar, os.environ)\n",
      "State:\n",
      "REPR FAILED\n",
      "==================================================\n",
      "Clean Code:\n",
      "def apply_env_options():\n",
      "    \"apply options passed through environment variables\"\n",
      "    env_options = filter(is_flower_envvar, os.environ)\n",
      "    for env_var_name in env_options:\n",
      "        name = env_var_name.replace(ENV_VAR_PREFIX, '', 1).lower()\n",
      "        value = os.environ[env_var_name]\n",
      "        try:\n",
      "            option = options._options[name]\n",
      "        except KeyError:\n",
      "            option = options._options[name.replace('_', '-')]\n",
      "        if option.multiple:\n",
      "            value = [option.type(i) for i in value.split(',')]\n",
      "        else:\n",
      "            if option.type is bool:\n",
      "                value = bool(strtobool(value))\n",
      "            else:\n",
      "                value = option.type(value)\n",
      "        setattr(options, name, value)\n",
      "apply_env_options()\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "name = env_var_name.replace(ENV_VAR_PREFIX, '', 1).lower()\n",
      "State:\n",
      "'auto_refresh'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def apply_env_options():\n",
      "    \"apply options passed through environment variables\"\n",
      "    env_options = filter(is_flower_envvar, os.environ)\n",
      "    for env_var_name in env_options:\n",
      "        name = env_var_name.replace(ENV_VAR_PREFIX, '', 1).lower()\n",
      "        value = os.environ[env_var_name]\n",
      "        try:\n",
      "            option = options._options[name]\n",
      "        except KeyError:\n",
      "            option = options._options[name.replace('_', '-')]\n",
      "        if option.multiple:\n",
      "            value = [option.type(i) for i in value.split(',')]\n",
      "        else:\n",
      "            if option.type is bool:\n",
      "                value = bool(strtobool(value))\n",
      "            else:\n",
      "                value = option.type(value)\n",
      "        setattr(options, name, value)\n",
      "apply_env_options()\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "value = os.environ[env_var_name]\n",
      "State:\n",
      "'false'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _prepare_virtual_host(self, vhost):\n",
      "        if not isinstance(vhost, numbers.Integral):\n",
      "            if not vhost or vhost == '/':\n",
      "                vhost = 0\n",
      "            elif vhost.startswith('/'):\n",
      "                vhost = vhost[1:]\n",
      "            try:\n",
      "                vhost = int(vhost)\n",
      "            except ValueError as exc:\n",
      "                raise ValueError(f'Database is int between 0 and limit - 1, not {vhost}') from exc\n",
      "        return vhost\n",
      "_prepare_virtual_host(self=<flower.utils.broker.Redis object at 0x7fc8303c1b50>, vhost='', self.broker_prefix='', self.host='localhost', self.password=None, self.port=6379, self.priority_steps=[0, 3, 6, 9], self.redis=None, self.sep='\\x06\\x16', self.username=None, self.vhost='')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "vhost = 0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def bugreport(app=None):\n",
      "    try:\n",
      "        import celery\n",
      "        import humanize\n",
      "        import tornado\n",
      "        app = app or celery.Celery()\n",
      "        return 'flower   -> flower:%s tornado:%s humanize:%s%s' % (\n",
      "            __version__,\n",
      "            tornado.version,\n",
      "            getattr(humanize, '__version__', None) or getattr(humanize, 'VERSION'),\n",
      "            app.bugreport()\n",
      "        )\n",
      "    except (ImportError, AttributeError) as e:\n",
      "        return f\"Error when generating bug report: {e}. Have you installed correct versions of Flower's dependencies?\"\n",
      "bugreport(app=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "import celery\n",
      "State:\n",
      "<module 'celery' from '/local/rcs/XXX/miniforge3/envs/mher+flower/lib/python3.9/site-packages/celery/__init__.py'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def bugreport(app=None):\n",
      "    try:\n",
      "        import celery\n",
      "        import humanize\n",
      "        import tornado\n",
      "        app = app or celery.Celery()\n",
      "        return 'flower   -> flower:%s tornado:%s humanize:%s%s' % (\n",
      "            __version__,\n",
      "            tornado.version,\n",
      "            getattr(humanize, '__version__', None) or getattr(humanize, 'VERSION'),\n",
      "            app.bugreport()\n",
      "        )\n",
      "    except (ImportError, AttributeError) as e:\n",
      "        return f\"Error when generating bug report: {e}. Have you installed correct versions of Flower's dependencies?\"\n",
      "bugreport(app=None)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "import humanize\n",
      "State:\n",
      "<module 'humanize' from '/local/rcs/XXX/miniforge3/envs/mher+flower/lib/python3.9/site-packages/humanize/__init__.py'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def bugreport(app=None):\n",
      "    try:\n",
      "        import celery\n",
      "        import humanize\n",
      "        import tornado\n",
      "        app = app or celery.Celery()\n",
      "        return 'flower   -> flower:%s tornado:%s humanize:%s%s' % (\n",
      "            __version__,\n",
      "            tornado.version,\n",
      "            getattr(humanize, '__version__', None) or getattr(humanize, 'VERSION'),\n",
      "            app.bugreport()\n",
      "        )\n",
      "    except (ImportError, AttributeError) as e:\n",
      "        return f\"Error when generating bug report: {e}. Have you installed correct versions of Flower's dependencies?\"\n",
      "bugreport(app=None)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "import tornado\n",
      "State:\n",
      "<module 'tornado' from '/local/rcs/XXX/miniforge3/envs/mher+flower/lib/python3.9/site-packages/tornado/__init__.py'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def abs_path(path):\n",
      "    path = os.path.expanduser(path)\n",
      "    if not os.path.isabs(path):\n",
      "        cwd = os.environ.get('PWD') or os.getcwd()\n",
      "        path = os.path.join(cwd, path)\n",
      "    return path\n",
      "abs_path(path='~/file.txt')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "path = os.path.expanduser(path)\n",
      "State:\n",
      "'/home/XXX/file.txt'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def authenticate(pattern, email):\n",
      "    if '|' in pattern:\n",
      "        return email in pattern.split('|')\n",
      "    if '*' in pattern:\n",
      "        pattern = re.escape(pattern).replace(r'\\.\\*', r\"[A-Za-z0-9!\n",
      "        return re.fullmatch(pattern, email)\n",
      "    return pattern == email\n",
      "authenticate(pattern='.*@example.com', email='one@example.com')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "pattern = re.escape(pattern).replace(r'\\.\\*', r\"[A-Za-z0-9!#$%&'*+/=?^_`{|}~.\\-]*\")\n",
      "State:\n",
      "\"[A-Za-z0-9!#$%&'*+/=?^_`{|}~.\\\\-]*@example\\\\.com\"\n",
      "==================================================\n",
      "Clean Code:\n",
      "def task_failed_events(worker, id=None, name=None):\n",
      "    id = id or uuid()\n",
      "    name = name or 'sometask'\n",
      "    return [Event('task-received', uuid=id, name=name,\n",
      "                  args='(2, 2)', kwargs=\"{'foo': 'bar'}\",\n",
      "                  retries=0, eta=None, hostname=worker),\n",
      "            Event('task-started', uuid=id, hostname=worker),\n",
      "            Event('task-failed', uuid=id, exception=\"KeyError('foo')\",\n",
      "                  traceback='line 1 at main', hostname=worker)]\n",
      "task_failed_events(worker='worker3', id=None, name=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "id = id or uuid()\n",
      "State:\n",
      "'74ab0138-27c4-4571-9127-2f5241170ad8'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def task_failed_events(worker, id=None, name=None):\n",
      "    id = id or uuid()\n",
      "    name = name or 'sometask'\n",
      "    return [Event('task-received', uuid=id, name=name,\n",
      "                  args='(2, 2)', kwargs=\"{'foo': 'bar'}\",\n",
      "                  retries=0, eta=None, hostname=worker),\n",
      "            Event('task-started', uuid=id, hostname=worker),\n",
      "            Event('task-failed', uuid=id, exception=\"KeyError('foo')\",\n",
      "                  traceback='line 1 at main', hostname=worker)]\n",
      "task_failed_events(worker='worker3', id=None, name=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "name = name or 'sometask'\n",
      "State:\n",
      "'sometask'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def maybe_normalize_for_sort(cls, tasks, sort_by):\n",
      "        sort_keys = {'name': str, 'state': str, 'received': float, 'started': float, 'runtime': float}\n",
      "        if sort_by in sort_keys:\n",
      "            for _, task in tasks:\n",
      "                attr_value = getattr(task, sort_by, None)\n",
      "                if attr_value:\n",
      "                    try:\n",
      "                        setattr(task, sort_by, sort_keys[sort_by](attr_value))\n",
      "                    except TypeError:\n",
      "                        pass\n",
      "maybe_normalize_for_sort(cls=<class 'flower.views.tasks.TasksDataTable'>, tasks=<generator object State.tasks_by_time at 0x7fc82aeb49e0>, sort_by='name')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "attr_value = getattr(task, sort_by, None)\n",
      "State:\n",
      "'task1'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def handle_data(self, data):\n",
      "        if self.inTable:\n",
      "            self.table += data\n",
      "handle_data(self=<tests.unit.utils.HtmlTableParser object at 0x7fc82b31ec10>, data='\\\\n', self._HTMLParser__starttag_text='<table id=\"workers-table\" class=\"table table-bordered table-striped table-hover w-100\">', self.cdata_elem=None, self.convert_charrefs=True, self.inTable=True, self.interesting=re.compile('[&<]'), self.lasttag='table', self.lineno=1, self.offset=3435, self.rawdata='b\\'\\\\n<!doctype html>\\\\n<html lang=\"en\">\\\\n<head>\\\\n<meta charset=\"utf-8\">\\\\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\\\n<title>Flower</title>\\\\n<link rel=\"stylesheet\" href=\"/static/css/bootstrap.min.css?v=eea261d9b89e1738193c9f9b06196592b31a87daee9aaec45629e54645e460853a40ad27b3860a83b2c0b65c3ebb2b039371a4fb42a1aa592695d33e74be6dd1\">\\\\n<link rel=\"stylesheet\" href=\"/static/css/datatables-1.13.4.min.css?v=74c4a6b553604403588edd63769db7dfbaf4c22cb68aba292a93e02ac283a6136e686b2fd7502a7816f160fae558412a7c877b81fd557298f07b32e026739559\">\\\\n<link href=\"/static/css/flower.css?v=cb65558ddda9a029f1ef92d591aec21646e1c50225c1f94f6ba79686bd6690f029b56fd21aef9a8a7b9e2c069bf113decaae7ee1098f33132619f22f47124415\" rel=\"stylesheet\">\\\\n</head>\\\\n<body class=\"m-2\">\\\\n\\\\n<nav class=\"navbar navbar-expand-lg navbar-light bg-green mx-2\">\\\\n<a class=\"navbar-brand\" href=\"/\">\\\\n<img src=\"/static/favicon.ico?v=ff1ba46e61b7e034e9ce38326f398a2b86c222a137e2eb96a3ea16c77300d423d6ebf0cc8d4ac73d95087e6114ef8e13fa52fa5b6f9fadc0b5d1a9e3680015b8\" width=\"30\" height=\"30\" class=\"d-inline-block align-top\" alt=\"\">\\\\nFlower\\\\n</a>\\\\n<button class=\"navbar-toggler\" type=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.table += data\n",
      "State:\n",
      "'<table id=\"workers-table\" class=\"table table-bordered table-striped table-hover w-100\">\\\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def handle_starttag(self, tag, attrs):\n",
      "        if tag == 'table':\n",
      "            self.inTable = True\n",
      "        if self.inTable:\n",
      "            self.table += '<%s' % tag\n",
      "            for attr in attrs:\n",
      "                self.table += ' %s=\"%s\"' % attr\n",
      "            self.table += '>'\n",
      "handle_starttag(self=<tests.unit.utils.HtmlTableParser object at 0x7fc82b31ec10>, tag='table', attrs=[('id', 'workers-table'), ('class', 'table table-bordered table-striped table-hover w-100')], self._HTMLParser__starttag_text='<table id=\"workers-table\" class=\"table table-bordered table-striped table-hover w-100\">', self.cdata_elem=None, self.convert_charrefs=True, self.inTable=False, self.interesting=re.compile('[&<]'), self.lasttag='table', self.lineno=1, self.offset=3348, self.rawdata='b\\'\\\\n<!doctype html>\\\\n<html lang=\"en\">\\\\n<head>\\\\n<meta charset=\"utf-8\">\\\\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\\\n<title>Flower</title>\\\\n<link rel=\"stylesheet\" href=\"/static/css/bootstrap.min.css?v=eea261d9b89e1738193c9f9b06196592b31a87daee9aaec45629e54645e460853a40ad27b3860a83b2c0b65c3ebb2b039371a4fb42a1aa592695d33e74be6dd1\">\\\\n<link rel=\"stylesheet\" href=\"/static/css/datatables-1.13.4.min.css?v=74c4a6b553604403588edd63769db7dfbaf4c22cb68aba292a93e02ac283a6136e686b2fd7502a7816f160fae558412a7c877b81fd557298f07b32e026739559\">\\\\n<link href=\"/static/css/flower.css?v=cb65558ddda9a029f1ef92d591aec21646e1c50225c1f94f6ba79686bd6690f029b56fd21aef9a8a7b9e2c069bf113decaae7ee1098f33132619f22f47124415\" rel=\"stylesheet\">\\\\n</head>\\\\n<body class=\"m-2\">\\\\n\\\\n<nav class=\"navbar navbar-expand-lg navbar-light bg-green mx-2\">\\\\n<a class=\"navbar-brand\" href=\"/\">\\\\n<img src=\"/static/favicon.ico?v=ff1ba46e61b7e034e9ce38326f398a2b86c222a137e2eb96a3ea16c77300d423d6ebf0cc8d4ac73d95087e6114ef8e13fa52fa5b6f9fadc0b5d1a9e3680015b8\" width=\"30\" height=\"30\" class=\"d-inline-block align-top\" alt=\"\">\\\\nFlower\\\\n</a>\\\\n<button class=\"navbar-toggler\" type=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.inTable = True\n",
      "State:\n",
      "True\n",
      "==================================================\n",
      "Clean Code:\n",
      "def handle_starttag(self, tag, attrs):\n",
      "        if tag == 'table':\n",
      "            self.inTable = True\n",
      "        if self.inTable:\n",
      "            self.table += '<%s' % tag\n",
      "            for attr in attrs:\n",
      "                self.table += ' %s=\"%s\"' % attr\n",
      "            self.table += '>'\n",
      "handle_starttag(self=<tests.unit.utils.HtmlTableParser object at 0x7fc82b31ec10>, tag='table', attrs=[('id', 'workers-table'), ('class', 'table table-bordered table-striped table-hover w-100')], self._HTMLParser__starttag_text='<table id=\"workers-table\" class=\"table table-bordered table-striped table-hover w-100\">', self.cdata_elem=None, self.convert_charrefs=True, self.inTable=False, self.interesting=re.compile('[&<]'), self.lasttag='table', self.lineno=1, self.offset=3348, self.rawdata='b\\'\\\\n<!doctype html>\\\\n<html lang=\"en\">\\\\n<head>\\\\n<meta charset=\"utf-8\">\\\\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\\\n<title>Flower</title>\\\\n<link rel=\"stylesheet\" href=\"/static/css/bootstrap.min.css?v=eea261d9b89e1738193c9f9b06196592b31a87daee9aaec45629e54645e460853a40ad27b3860a83b2c0b65c3ebb2b039371a4fb42a1aa592695d33e74be6dd1\">\\\\n<link rel=\"stylesheet\" href=\"/static/css/datatables-1.13.4.min.css?v=74c4a6b553604403588edd63769db7dfbaf4c22cb68aba292a93e02ac283a6136e686b2fd7502a7816f160fae558412a7c877b81fd557298f07b32e026739559\">\\\\n<link href=\"/static/css/flower.css?v=cb65558ddda9a029f1ef92d591aec21646e1c50225c1f94f6ba79686bd6690f029b56fd21aef9a8a7b9e2c069bf113decaae7ee1098f33132619f22f47124415\" rel=\"stylesheet\">\\\\n</head>\\\\n<body class=\"m-2\">\\\\n\\\\n<nav class=\"navbar navbar-expand-lg navbar-light bg-green mx-2\">\\\\n<a class=\"navbar-brand\" href=\"/\">\\\\n<img src=\"/static/favicon.ico?v=ff1ba46e61b7e034e9ce38326f398a2b86c222a137e2eb96a3ea16c77300d423d6ebf0cc8d4ac73d95087e6114ef8e13fa52fa5b6f9fadc0b5d1a9e3680015b8\" width=\"30\" height=\"30\" class=\"d-inline-block align-top\" alt=\"\">\\\\nFlower\\\\n</a>\\\\n<button class=\"navbar-toggler\" type=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "self.table += '<%s' % tag\n",
      "State:\n",
      "'<table'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def handle_endtag(self, tag):\n",
      "        if self.inTable:\n",
      "            self.table += '</%s>' % tag\n",
      "            if tag == 'table':\n",
      "                self.inTable = False\n",
      "handle_endtag(self=<tests.unit.utils.HtmlTableParser object at 0x7fc82b31ec10>, tag='th', self._HTMLParser__starttag_text='<th>', self.cdata_elem=None, self.convert_charrefs=True, self.inTable=True, self.interesting=re.compile('[&<]'), self.lasttag='th', self.lineno=1, self.offset=3462, self.rawdata='b\\'\\\\n<!doctype html>\\\\n<html lang=\"en\">\\\\n<head>\\\\n<meta charset=\"utf-8\">\\\\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\\\n<title>Flower</title>\\\\n<link rel=\"stylesheet\" href=\"/static/css/bootstrap.min.css?v=eea261d9b89e1738193c9f9b06196592b31a87daee9aaec45629e54645e460853a40ad27b3860a83b2c0b65c3ebb2b039371a4fb42a1aa592695d33e74be6dd1\">\\\\n<link rel=\"stylesheet\" href=\"/static/css/datatables-1.13.4.min.css?v=74c4a6b553604403588edd63769db7dfbaf4c22cb68aba292a93e02ac283a6136e686b2fd7502a7816f160fae558412a7c877b81fd557298f07b32e026739559\">\\\\n<link href=\"/static/css/flower.css?v=cb65558ddda9a029f1ef92d591aec21646e1c50225c1f94f6ba79686bd6690f029b56fd21aef9a8a7b9e2c069bf113decaae7ee1098f33132619f22f47124415\" rel=\"stylesheet\">\\\\n</head>\\\\n<body class=\"m-2\">\\\\n\\\\n<nav class=\"navbar navbar-expand-lg navbar-light bg-green mx-2\">\\\\n<a class=\"navbar-brand\" href=\"/\">\\\\n<img src=\"/static/favicon.ico?v=ff1ba46e61b7e034e9ce38326f398a2b86c222a137e2eb96a3ea16c77300d423d6ebf0cc8d4ac73d95087e6114ef8e13fa52fa5b6f9fadc0b5d1a9e3680015b8\" width=\"30\" height=\"30\" class=\"d-inline-block align-top\" alt=\"\">\\\\nFlower\\\\n</a>\\\\n<button class=\"navbar-toggler\" type=\"button\" data-bs-toggle=\"collapse\" data-bs-target=\"\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.table += '</%s>' % tag\n",
      "State:\n",
      "'<table id=\"workers-table\" class=\"table table-bordered table-striped table-hover w-100\">\\\\n<thead>\\\\n<tr>\\\\n<th>Worker</th>'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_integration(install_vspec, path):\n",
      "    output = subprocess.check_output(\n",
      "        [VSPEC_RUNNER, '.', VSPEC_FOLDER, os.path.relpath(path, root)],\n",
      "        cwd=root,\n",
      "    )\n",
      "    had_ok = False\n",
      "    for line in output.splitlines():\n",
      "        if (line.startswith(b'not ok') or\n",
      "                line.startswith(b'Error') or\n",
      "                line.startswith(b'Bail out!')):\n",
      "            pytest.fail(u\"{0} failed:\\n{1}\".format(\n",
      "                path, output.decode('utf-8')), pytrace=False)\n",
      "        if not had_ok and line.startswith(b'ok'):\n",
      "            had_ok = True\n",
      "    if not had_ok:\n",
      "        pytest.fail(u\"{0} failed: no 'ok' found:\\n{1}\".format(\n",
      "            path, output.decode('utf-8')), pytrace=False)\n",
      "test_integration(install_vspec=None, path='test/vspec/signatures.vim')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "output = subprocess.check_output(\n",
      "State:\n",
      "b'not found in \\'runtimepath\\': \"ftdetect/*.vim\"\\nok 1 - signatures simple\\n4 buffers wiped out\\n--- Autocommands ---\\njedi_call_signatures  CursorHoldI\\n<buffer=6>\\ncall jedi#show_call_signatures()\\n\\tLast set from /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi-vim/davidhalter+jedi-vim/autoload/jedi.vim line 612\\njedi_call_signatures  InsertEnter\\n<buffer=6>\\nlet s:show_call_signatures_last = [0, 0, \\'\\']\\n\\tLast set from /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi-vim/davidhalter+jedi-vim/autoload/jedi.vim line 603\\nlet b:_jedi_orig_updatetime = &updatetime | let &updatetime = g:jedi#show_call_signatures_delay\\n\\tLast set from /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi-vim/davidhalter+jedi-vim/autoload/jedi.vim line 606\\njedi_call_signatures  InsertLeave\\n<buffer=6>\\ncall jedi#clear_call_signatures()\\n\\tLast set from /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi-vim/davidhalter+jedi-vim/autoload/jedi.vim line 604\\nif exists(\\'b:_jedi_orig_updatetime\\') |   let &updatetime = b:_jedi_orig_updatetime |   unlet b:_jedi_orig_updatetime | endif\\n\\tLast set from /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi-vim/davidhalter+jedi-vim/autoload/jedi.vim line 608\\n--- Autocommands ---\\njedi_call_signatures  CursorHoldI\\n<buffer=5>\\ncall jedi#show_call_signatures()\\n\\tLast set from /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi-vim/davidhalter+jedi-vim/autoload/jedi.vim line 612\\njedi_call_signatures  InsertEnter\\n<buffer=5>\\nlet s:show_call_signatures_last = [0, 0, \\'\\']\\n\\tLast set from /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi-vim/davidhalter+jedi-vim/autoload/jedi.vim line 603\\nlet b:_jedi_orig_updatetime = &updatetime | let &updatetime = g:jedi#show_call_signatures_delay\\n\\tLast set from /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi-vim/davidhalter+jedi-vim/autoload/jedi.vim line 606\\njedi_call_signatures  InsertLeave\\n<buffer=5>\\ncall jedi#clear_call_signatures()\\n\\tLast set from /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi-vim/davidhalter+jedi-vim/autoload/jedi.vim line 604\\nif exists(\\'b:_jedi_orig_updatetime\\') |   let &updatetime = b:_jedi_orig_updatetime |   unlet b:_jedi_orig_updatetime | endif\\n\\tLast set from /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi-vim/davidhalter+jedi-vim/autoload/jedi.vim line 608\\nok 2 - signatures multiple buffers\\n2 buffers wiped out\\nok 3 - signatures simple after CursorHoldI with only parenthesis\\nok 4 - signatures highlights correct argument\\nok 5 - signatures no signature\\nok 6 - signatures signatures disabled\\nstaticmethod(f: Callable[..., Any])\\nfoo(a, b)\\nok 7 - signatures command line simple\\x07\\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa(arg1, \\xe2\\x80\\xa6)\\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa(\\xe2\\x80\\xa6, arg2, \\xe2\\x80\\xa6)\\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa(\\xe2\\x80\\xa6, a, b, c)\\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa(\\xe2\\x80\\xa6, c)\\naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa(\\xe2\\x80\\xa6)\\nok 8 - signatures command line truncation\\nok 9 - signatures command line no signature\\n1..9\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_integration(install_vspec, path):\n",
      "    output = subprocess.check_output(\n",
      "        [VSPEC_RUNNER, '.', VSPEC_FOLDER, os.path.relpath(path, root)],\n",
      "        cwd=root,\n",
      "    )\n",
      "    had_ok = False\n",
      "    for line in output.splitlines():\n",
      "        if (line.startswith(b'not ok') or\n",
      "                line.startswith(b'Error') or\n",
      "                line.startswith(b'Bail out!')):\n",
      "            pytest.fail(u\"{0} failed:\\n{1}\".format(\n",
      "                path, output.decode('utf-8')), pytrace=False)\n",
      "        if not had_ok and line.startswith(b'ok'):\n",
      "            had_ok = True\n",
      "    if not had_ok:\n",
      "        pytest.fail(u\"{0} failed: no 'ok' found:\\n{1}\".format(\n",
      "            path, output.decode('utf-8')), pytrace=False)\n",
      "test_integration(install_vspec=None, path='test/vspec/signatures.vim')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "had_ok = False\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def guet_usage(command_builder_map: dict) -> str:\n",
      "    help_message = 'usage: guet <command>\\n'\n",
      "    for key in command_builder_map:\n",
      "        short_help_message = ''\n",
      "        if not isinstance(command_builder_map[key], CommandFactoryMethod):\n",
      "            short_help_message = command_builder_map[key].get_short_help_message()\n",
      "        else:\n",
      "            short_help_message = command_builder_map[key].short_help_message()\n",
      "        help_message += f'\\n   {key} -- {short_help_message}'\n",
      "    return help_message + '\\n'\n",
      "guet_usage(command_builder_map={'MockCommandB': <guet.commands.command_factory.CommandFactoryMethod object at 0x7fc0ae007fd0>})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "help_message = 'usage: guet <command>\\n'\n",
      "State:\n",
      "'usage: guet <command>\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def guet_usage(command_builder_map: dict) -> str:\n",
      "    help_message = 'usage: guet <command>\\n'\n",
      "    for key in command_builder_map:\n",
      "        short_help_message = ''\n",
      "        if not isinstance(command_builder_map[key], CommandFactoryMethod):\n",
      "            short_help_message = command_builder_map[key].get_short_help_message()\n",
      "        else:\n",
      "            short_help_message = command_builder_map[key].short_help_message()\n",
      "        help_message += f'\\n   {key} -- {short_help_message}'\n",
      "    return help_message + '\\n'\n",
      "guet_usage(command_builder_map={'MockCommandB': <guet.commands.command_factory.CommandFactoryMethod object at 0x7fc0ae007fd0>})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "short_help_message = ''\n",
      "State:\n",
      "''\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _separate_key_and_value(self) -> List:\n",
      "        key_and_value = []\n",
      "        for arg in _only_args_starting_with_double_dash(self._args):\n",
      "            key, value = arg.split('=')\n",
      "            key_and_value.append((_remove_double_dash(key), value))\n",
      "        return key_and_value\n",
      "_separate_key_and_value(self=<guet.commands.config.set_config_strategy.SetConfigStrategy object at 0x7fc0ad95a100>, self._args=['config', '--invalidKey=true'], self.context=<MagicMock name='Context.instance()' id='140465523546576'>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "key_and_value = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _separate_key_and_value(self) -> List:\n",
      "        key_and_value = []\n",
      "        for arg in _only_args_starting_with_double_dash(self._args):\n",
      "            key, value = arg.split('=')\n",
      "            key_and_value.append((_remove_double_dash(key), value))\n",
      "        return key_and_value\n",
      "_separate_key_and_value(self=<guet.commands.config.set_config_strategy.SetConfigStrategy object at 0x7fc0ad95a100>, self._args=['config', '--invalidKey=true'], self.context=<MagicMock name='Context.instance()' id='140465523546576'>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "key, value = arg.split('=')\n",
      "State:\n",
      "'--invalidKey'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _separate_key_and_value(self) -> List:\n",
      "        key_and_value = []\n",
      "        for arg in _only_args_starting_with_double_dash(self._args):\n",
      "            key, value = arg.split('=')\n",
      "            key_and_value.append((_remove_double_dash(key), value))\n",
      "        return key_and_value\n",
      "_separate_key_and_value(self=<guet.commands.config.set_config_strategy.SetConfigStrategy object at 0x7fc0ad95a100>, self._args=['config', '--invalidKey=true'], self.context=<MagicMock name='Context.instance()' id='140465523546576'>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "key_and_value.append((_remove_double_dash(key), value))\n",
      "State:\n",
      "[('invalidKey', 'true')]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _append_key_and_value_to_config(config: Settings, key_and_value: List):\n",
      "    for key_value in key_and_value:\n",
      "        try:\n",
      "            config.set(key_value[0], key_value[1])\n",
      "        except KeyError:\n",
      "            print(f'Cannot set \\\"{key_value[0]}\\\", not valid configuration.\\n')\n",
      "            exit(1)\n",
      "_append_key_and_value_to_config(config={version=None, _settings={'pairReset': <guet.settings.setting.Setting object at 0x7fc0adba7a60>, 'debug': <guet.settings.setting.Setting object at 0x7fc0adba7040>}}, key_and_value=[('invalidKey', 'true')])\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "config.set(key_value[0], key_value[1])\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _add_committer_to_committers(all_committers: List[str], initials: str, name: str, email: str):\n",
      "    committer_formatted = f'{initials},{name},{email}\\n'\n",
      "    committer_position = _position_of_committer_with_initials(all_committers, initials)\n",
      "    if committer_position is _COMMITTER_NOT_PRESENT:\n",
      "        all_committers.append(committer_formatted)\n",
      "    else:\n",
      "        all_committers[committer_position] = committer_formatted\n",
      "_add_committer_to_committers(all_committers=[], initials='initials3', name='name3', email='email3')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "committer_formatted = f'{initials},{name},{email}\\n'\n",
      "State:\n",
      "'initials3,name3,email3\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _add_committer_to_committers(all_committers: List[str], initials: str, name: str, email: str):\n",
      "    committer_formatted = f'{initials},{name},{email}\\n'\n",
      "    committer_position = _position_of_committer_with_initials(all_committers, initials)\n",
      "    if committer_position is _COMMITTER_NOT_PRESENT:\n",
      "        all_committers.append(committer_formatted)\n",
      "    else:\n",
      "        all_committers[committer_position] = committer_formatted\n",
      "_add_committer_to_committers(all_committers=[], initials='initials3', name='name3', email='email3')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "committer_position = _position_of_committer_with_initials(all_committers, initials)\n",
      "State:\n",
      "-1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _add_committer_to_committers(all_committers: List[str], initials: str, name: str, email: str):\n",
      "    committer_formatted = f'{initials},{name},{email}\\n'\n",
      "    committer_position = _position_of_committer_with_initials(all_committers, initials)\n",
      "    if committer_position is _COMMITTER_NOT_PRESENT:\n",
      "        all_committers.append(committer_formatted)\n",
      "    else:\n",
      "        all_committers[committer_position] = committer_formatted\n",
      "_add_committer_to_committers(all_committers=[], initials='initials3', name='name3', email='email3')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "all_committers.append(committer_formatted)\n",
      "State:\n",
      "['initials3,name3,email3\\n']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add_committer(initials: str, name: str, email: str, *, file_path: str = _GLOBAL) -> None:\n",
      "    all_committers = _read_all_committers_from_file(file_path)\n",
      "    _add_committer_to_committers(all_committers, initials, name, email)\n",
      "    _write_committers_to_file(all_committers, file_path)\n",
      "add_committer(initials='initials3', name='name3', email='email3', file_path='/home/XXX/.guet/committers')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "all_committers = _read_all_committers_from_file(file_path)\n",
      "State:\n",
      "['initials1,name1,email1\\n', 'initials2,name2,email2\\n']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add_committer(initials: str, name: str, email: str, *, file_path: str = _GLOBAL) -> None:\n",
      "    all_committers = _read_all_committers_from_file(file_path)\n",
      "    _add_committer_to_committers(all_committers, initials, name, email)\n",
      "    _write_committers_to_file(all_committers, file_path)\n",
      "add_committer(initials='initials3', name='name3', email='email3', file_path='/home/XXX/.guet/committers')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "_add_committer_to_committers(all_committers, initials, name, email)\n",
      "State:\n",
      "['initials1,name1,email1\\n', 'initials2,name2,email2\\n', 'initials3,name3,email3\\n']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _load_global_committers(path: str) -> List[Committer]:\n",
      "    lines = read_lines(path)\n",
      "    committers = []\n",
      "    for line in lines:\n",
      "        initials, name, email = line.rstrip().split(',')\n",
      "        committers.append(GlobalCommitter(initials=initials, name=name, email=email))\n",
      "    return committers\n",
      "_load_global_committers(path='/home/XXX/.guet/committers')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "lines = read_lines(path)\n",
      "State:\n",
      "['initials,name,email\\n']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _load_global_committers(path: str) -> List[Committer]:\n",
      "    lines = read_lines(path)\n",
      "    committers = []\n",
      "    for line in lines:\n",
      "        initials, name, email = line.rstrip().split(',')\n",
      "        committers.append(GlobalCommitter(initials=initials, name=name, email=email))\n",
      "    return committers\n",
      "_load_global_committers(path='/home/XXX/.guet/committers')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "committers = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _load_global_committers(path: str) -> List[Committer]:\n",
      "    lines = read_lines(path)\n",
      "    committers = []\n",
      "    for line in lines:\n",
      "        initials, name, email = line.rstrip().split(',')\n",
      "        committers.append(GlobalCommitter(initials=initials, name=name, email=email))\n",
      "    return committers\n",
      "_load_global_committers(path='/home/XXX/.guet/committers')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "initials, name, email = line.rstrip().split(',')\n",
      "State:\n",
      "'initials'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def remove(self, committer: Committer):\n",
      "        self._committers.remove(committer)\n",
      "        _write_committers(self._committers)\n",
      "remove(self=<guet.config.committers.Committers object at 0x7fc0ad301d60>, committer={name='name', email='email', initials='initials'}, self._committers=[<guet.committers.global_committer.GlobalCommitter object at 0x7fc0ad2899d0>])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self._committers.remove(committer)\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _load_local_committers(path: str, path_to_project_root: str) -> List[Committer]:\n",
      "    lines = read_lines(path)\n",
      "    committers = []\n",
      "    for line in lines:\n",
      "        initials, name, email = line.rstrip().split(',')\n",
      "        committers.append(LocalCommitter(initials=initials, name=name, email=email, project_root=path_to_project_root))\n",
      "    return committers\n",
      "_load_local_committers(path='/path/to/project/root/.guet/committers', path_to_project_root='/path/to/project/root')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "lines = read_lines(path)\n",
      "State:\n",
      "['initials1,othername1,otheremail1\\n']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _load_local_committers(path: str, path_to_project_root: str) -> List[Committer]:\n",
      "    lines = read_lines(path)\n",
      "    committers = []\n",
      "    for line in lines:\n",
      "        initials, name, email = line.rstrip().split(',')\n",
      "        committers.append(LocalCommitter(initials=initials, name=name, email=email, project_root=path_to_project_root))\n",
      "    return committers\n",
      "_load_local_committers(path='/path/to/project/root/.guet/committers', path_to_project_root='/path/to/project/root')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "committers = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _load_local_committers(path: str, path_to_project_root: str) -> List[Committer]:\n",
      "    lines = read_lines(path)\n",
      "    committers = []\n",
      "    for line in lines:\n",
      "        initials, name, email = line.rstrip().split(',')\n",
      "        committers.append(LocalCommitter(initials=initials, name=name, email=email, project_root=path_to_project_root))\n",
      "    return committers\n",
      "_load_local_committers(path='/path/to/project/root/.guet/committers', path_to_project_root='/path/to/project/root')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "initials, name, email = line.rstrip().split(',')\n",
      "State:\n",
      "'initials1'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def load(self, configuration_file_lines: List[str]):\n",
      "        lines_without_version = configuration_file_lines[2:]\n",
      "        self.version = configuration_file_lines[0].rstrip()\n",
      "        for attribute in lines_without_version:\n",
      "            self._load_attribute(attribute)\n",
      "load(self=<guet.settings.settings.Settings object at 0x7fc0ad21ab20>, configuration_file_lines=['2.0.0\\n', '\\n', 'pairReset=False\\n'], self._settings={'pairReset': <guet.settings.setting.Setting object at 0x7fc0ad21ab80>, 'debug': <guet.settings.setting.Setting object at 0x7fc0ad21a850>}, self.version=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "lines_without_version = configuration_file_lines[2:]\n",
      "State:\n",
      "['pairReset=False\\n']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def load(self, configuration_file_lines: List[str]):\n",
      "        lines_without_version = configuration_file_lines[2:]\n",
      "        self.version = configuration_file_lines[0].rstrip()\n",
      "        for attribute in lines_without_version:\n",
      "            self._load_attribute(attribute)\n",
      "load(self=<guet.settings.settings.Settings object at 0x7fc0ad21ab20>, configuration_file_lines=['2.0.0\\n', '\\n', 'pairReset=False\\n'], self._settings={'pairReset': <guet.settings.setting.Setting object at 0x7fc0ad21ab80>, 'debug': <guet.settings.setting.Setting object at 0x7fc0ad21a850>}, self.version=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.version = configuration_file_lines[0].rstrip()\n",
      "State:\n",
      "'2.0.0'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def write(self) -> List[str]:\n",
      "        version = [f'{self.version}\\n', '\\n']\n",
      "        settings = []\n",
      "        for key in self._settings:\n",
      "            if not self._settings[key].is_default_value():\n",
      "                settings.append(self._convert_to_line(key))\n",
      "        return version + settings\n",
      "write(self=<guet.settings.settings.Settings object at 0x7fc0ad11ed90>, self._settings={'pairReset': <guet.settings.setting.Setting object at 0x7fc0ad1fde50>, 'debug': <guet.settings.setting.Setting object at 0x7fc0ad1fd9d0>}, self.version='2.0.0')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "version = [f'{self.version}\\n', '\\n']\n",
      "State:\n",
      "['2.0.0\\n', '\\n']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def write(self) -> List[str]:\n",
      "        version = [f'{self.version}\\n', '\\n']\n",
      "        settings = []\n",
      "        for key in self._settings:\n",
      "            if not self._settings[key].is_default_value():\n",
      "                settings.append(self._convert_to_line(key))\n",
      "        return version + settings\n",
      "write(self=<guet.settings.settings.Settings object at 0x7fc0ad11ed90>, self._settings={'pairReset': <guet.settings.setting.Setting object at 0x7fc0ad1fde50>, 'debug': <guet.settings.setting.Setting object at 0x7fc0ad1fd9d0>}, self.version='2.0.0')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "settings = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def write(self) -> List[str]:\n",
      "        version = [f'{self.version}\\n', '\\n']\n",
      "        settings = []\n",
      "        for key in self._settings:\n",
      "            if not self._settings[key].is_default_value():\n",
      "                settings.append(self._convert_to_line(key))\n",
      "        return version + settings\n",
      "write(self=<guet.settings.settings.Settings object at 0x7fc0ad11ed90>, self._settings={'pairReset': <guet.settings.setting.Setting object at 0x7fc0ad1fde50>, 'debug': <guet.settings.setting.Setting object at 0x7fc0ad1fd9d0>}, self.version='2.0.0')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "settings.append(self._convert_to_line(key))\n",
      "State:\n",
      "['pairReset=False\\n']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_current_committers(committers: List[Committer]) -> None:\n",
      "    path = join(CONFIGURATION_DIRECTORY, constants.COMMITTERS_SET)\n",
      "    current_set = _all_committers_set_from_file()\n",
      "    formatted_set_committers_information = _format_committers_to_committers_set_format(committers)\n",
      "    _add_to_current_set_lines(current_set, formatted_set_committers_information)\n",
      "    write_lines(path, current_set)\n",
      "set_current_committers(committers=[<guet.config.committer.Committer object at 0x7fc0ad1fdca0>, <guet.config.committer.Committer object at 0x7fc0ad089250>])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "path = join(CONFIGURATION_DIRECTORY, constants.COMMITTERS_SET)\n",
      "State:\n",
      "'/home/XXX/.guet/committersset'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_current_committers(committers: List[Committer]) -> None:\n",
      "    path = join(CONFIGURATION_DIRECTORY, constants.COMMITTERS_SET)\n",
      "    current_set = _all_committers_set_from_file()\n",
      "    formatted_set_committers_information = _format_committers_to_committers_set_format(committers)\n",
      "    _add_to_current_set_lines(current_set, formatted_set_committers_information)\n",
      "    write_lines(path, current_set)\n",
      "set_current_committers(committers=[<guet.config.committer.Committer object at 0x7fc0ad1fdca0>, <guet.config.committer.Committer object at 0x7fc0ad089250>])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "current_set = _all_committers_set_from_file()\n",
      "State:\n",
      "['initials3,initials4,1000000000000,/absolute/path/to/other/.git']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_current_committers(committers: List[Committer]) -> None:\n",
      "    path = join(CONFIGURATION_DIRECTORY, constants.COMMITTERS_SET)\n",
      "    current_set = _all_committers_set_from_file()\n",
      "    formatted_set_committers_information = _format_committers_to_committers_set_format(committers)\n",
      "    _add_to_current_set_lines(current_set, formatted_set_committers_information)\n",
      "    write_lines(path, current_set)\n",
      "set_current_committers(committers=[<guet.config.committer.Committer object at 0x7fc0ad1fdca0>, <guet.config.committer.Committer object at 0x7fc0ad089250>])\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "formatted_set_committers_information = _format_committers_to_committers_set_format(committers)\n",
      "State:\n",
      "'initials1,initials2,1000000000000,/absolute/path/to/.git'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_current_committers(committers: List[Committer]) -> None:\n",
      "    path = join(CONFIGURATION_DIRECTORY, constants.COMMITTERS_SET)\n",
      "    current_set = _all_committers_set_from_file()\n",
      "    formatted_set_committers_information = _format_committers_to_committers_set_format(committers)\n",
      "    _add_to_current_set_lines(current_set, formatted_set_committers_information)\n",
      "    write_lines(path, current_set)\n",
      "set_current_committers(committers=[<guet.config.committer.Committer object at 0x7fc0ad1fdca0>, <guet.config.committer.Committer object at 0x7fc0ad089250>])\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "_add_to_current_set_lines(current_set, formatted_set_committers_information)\n",
      "State:\n",
      "['initials3,initials4,1000000000000,/absolute/path/to/other/.git', 'initials1,initials2,1000000000000,/absolute/path/to/.git']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _add_to_current_set_lines(current_set, formatted_set_committers_information):\n",
      "    git_path = git_path_from_cwd()\n",
      "    line_with_git_path = next((line for line in current_set if line.endswith(git_path)), None)\n",
      "    if line_with_git_path:\n",
      "        index = current_set.index(line_with_git_path)\n",
      "        current_set[index] = formatted_set_committers_information\n",
      "    else:\n",
      "        current_set.append(formatted_set_committers_information)\n",
      "_add_to_current_set_lines(current_set=['initials3,initials4,1000000000000,/absolute/path/to/other/.git'], formatted_set_committers_information='initials1,initials2,1000000000000,/absolute/path/to/.git')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "git_path = git_path_from_cwd()\n",
      "State:\n",
      "'/absolute/path/to/.git'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def committers(self):\n",
      "        if self._committers is None:\n",
      "            self._committers = Committers(path_to_project_root=self.project_root_directory)\n",
      "            self.add_set_committer_observer(self._committers)\n",
      "        return self._committers\n",
      "committers(self=<guet.context.context.Context object at 0x7fc0ad0599a0>, self._committers=None, self._git=None, self._project_root_directory='path/to/project/root/', self.current_committers_observer=[])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self._committers = Committers(path_to_project_root=self.project_root_directory)\n",
      "State:\n",
      "<MagicMock name='Committers()' id='140465513712128'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def committers(self):\n",
      "        if self._committers is None:\n",
      "            self._committers = Committers(path_to_project_root=self.project_root_directory)\n",
      "            self.add_set_committer_observer(self._committers)\n",
      "        return self._committers\n",
      "committers(self=<guet.context.context.Context object at 0x7fc0ad0599a0>, self._committers=None, self._git=None, self._project_root_directory='path/to/project/root/', self.current_committers_observer=[])\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.add_set_committer_observer(self._committers)\n",
      "State:\n",
      "[<MagicMock name='Committers()' id='140465513712128'>]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def project_root_directory(self) -> str:\n",
      "        if self._project_root_directory is None:\n",
      "            root = project_root()\n",
      "            self._project_root_directory = root\n",
      "        return self._project_root_directory\n",
      "project_root_directory(self=<guet.context.context.Context object at 0x7fc0ad0bc8e0>, self._committers=None, self._git=None, self._project_root_directory=None, self.current_committers_observer=[])\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "root = project_root()\n",
      "State:\n",
      "'/path/to/cwd'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def project_root_directory(self) -> str:\n",
      "        if self._project_root_directory is None:\n",
      "            root = project_root()\n",
      "            self._project_root_directory = root\n",
      "        return self._project_root_directory\n",
      "project_root_directory(self=<guet.context.context.Context object at 0x7fc0ad0bc8e0>, self._committers=None, self._git=None, self._project_root_directory=None, self.current_committers_observer=[])\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "self._project_root_directory = root\n",
      "State:\n",
      "'/path/to/cwd'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add_set_committer_observer(self, observer: SetCommitterObserver):\n",
      "        self.current_committers_observer.append(observer)\n",
      "add_set_committer_observer(self=<guet.context.context.Context object at 0x7fc0ad0599a0>, observer=<MagicMock name='Committers()' id='140465513712128'>, self._committers=<MagicMock name='Committers()' id='140465513712128'>, self._git=None, self._project_root_directory='path/to/project/root/', self.current_committers_observer=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.current_committers_observer.append(observer)\n",
      "State:\n",
      "[<MagicMock name='Committers()' id='140465513712128'>]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def git(self):\n",
      "        if self._git is None:\n",
      "            self._git = Git(join(self.project_root_directory, '.git'))\n",
      "            self.add_set_committer_observer(self._git)\n",
      "        return self._git\n",
      "git(self=<guet.context.context.Context object at 0x7fc0ad050550>, self._committers=None, self._git=None, self._project_root_directory='path/to/project/root/', self.current_committers_observer=[])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self._git = Git(join(self.project_root_directory, '.git'))\n",
      "State:\n",
      "<MagicMock name='Git()' id='140465513411776'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def git(self):\n",
      "        if self._git is None:\n",
      "            self._git = Git(join(self.project_root_directory, '.git'))\n",
      "            self.add_set_committer_observer(self._git)\n",
      "        return self._git\n",
      "git(self=<guet.context.context.Context object at 0x7fc0ad050550>, self._committers=None, self._git=None, self._project_root_directory='path/to/project/root/', self.current_committers_observer=[])\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.add_set_committer_observer(self._git)\n",
      "State:\n",
      "[<MagicMock name='Git()' id='140465513411776'>]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _load_hooks(hook_strategy: HookLoader) -> List[Hook]:\n",
      "    hooks = []\n",
      "    for hook_name in GUET_HOOKS:\n",
      "        hook_strategy.apply(hook_name, hooks)\n",
      "    return hooks\n",
      "_load_hooks(hook_strategy={create=<guet.git._create_strategy.DontCreateStrategy object at 0x7fc0acf09460>, file_name=<guet.git._file_name_strategy.BaseFileNameStrategy object at 0x7fc0acf09370>, path_to_repository='/path/to/.git'})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "hooks = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def create_hooks(self, alongside: bool = False) -> None:\n",
      "        if alongside:\n",
      "            hook_loader = HookLoader(self.path_to_repository, AlongsideFileNameStrategy(), DoCreateStrategy())\n",
      "        else:\n",
      "            hook_loader = HookLoader(self.path_to_repository, BaseFileNameStrategy(), DoCreateStrategy())\n",
      "        self.hooks = _load_hooks(hook_loader)\n",
      "        for hook in self.hooks:\n",
      "            hook.save()\n",
      "create_hooks(self=<guet.git.git.Git object at 0x7fc0adbff8e0>, alongside=True, self._author=Author(name='name', email='name@localhost'), self._commit_msg=['First line of commit message', 'Second line of commit message'], self._config_lines=['[core]', '\\trepositoryformatversion = 0', '\\tfilemode = true', '\\tbare = false', '\\tlogallrefupdates = true', '\\tignorecase = true', '\\tprecomposeunicode = true', '[user]', '\\tname = name', '\\temail = name@localhost'], self.hooks=[], self.path_to_repository='/path/to/.git')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "self.hooks = _load_hooks(hook_loader)\n",
      "State:\n",
      "[<Mock id='140465511896064'>, <Mock id='140465511895152'>, <Mock id='140465511896784'>]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def all_valid_hooks(hooks: List[Hook]) -> bool:\n",
      "    hook_names = [_name(hook) for hook in _normal_hooks(hooks)]\n",
      "    valid_names = GUET_HOOKS == hook_names\n",
      "    valid_content = all([hook.is_guet_hook() for hook in _normal_hooks(hooks)])\n",
      "    if not (valid_names and valid_content):\n",
      "        hook_names = [_name(hook).replace('-guet', '') for hook in _dash_guet_normal_hooks(hooks)]\n",
      "        valid_names = GUET_HOOKS == hook_names\n",
      "        valid_content = all([hook.is_guet_hook() for hook in _dash_guet_normal_hooks(hooks)])\n",
      "        return valid_names and valid_content\n",
      "    return True\n",
      "all_valid_hooks(hooks=[<Mock id='140465523545424'>, <Mock id='140465523545520'>, <Mock id='140465523546288'>])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "valid_names = GUET_HOOKS == hook_names\n",
      "State:\n",
      "True\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _dash_guet_normal_hooks(hooks: List[Hook]) -> List[Hook]:\n",
      "    final = []\n",
      "    for hook in hooks:\n",
      "        name = _name(hook)\n",
      "        if name.endswith('-guet') and name.replace('-guet', '') in GUET_HOOKS:\n",
      "            final.append(hook)\n",
      "    return final\n",
      "_dash_guet_normal_hooks(hooks=[<Mock id='140465523545424'>, <Mock id='140465523545520'>, <Mock id='140465523546288'>])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "final = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def apply(self, hook_name: str, hooks: List[Hook]):\n",
      "        hook_path = join(self.path_to_repository, 'hooks', self.file_name.apply(hook_name))\n",
      "        try:\n",
      "            hooks.append(Hook(hook_path, create=self.create.apply()))\n",
      "        except FileNotFoundError:\n",
      "            pass\n",
      "        except NotGuetHookError:\n",
      "            pass\n",
      "apply(self=<guet.git._hook_loader.HookLoader object at 0x7fc0acc70a30>, hook_name='commit-msg', hooks=[], self.create=<guet.git._create_strategy.DontCreateStrategy object at 0x7fc0acc70d60>, self.file_name=<guet.git._file_name_strategy.AlongsideFileNameStrategy object at 0x7fc0acc70b50>, self.path_to_repository='/path/to/.git')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "hook_path = join(self.path_to_repository, 'hooks', self.file_name.apply(hook_name))\n",
      "State:\n",
      "'/path/to/.git/hooks/commit-msg-guet'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def apply(self, hook_name: str, hooks: List[Hook]):\n",
      "        hook_path = join(self.path_to_repository, 'hooks', self.file_name.apply(hook_name))\n",
      "        try:\n",
      "            hooks.append(Hook(hook_path, create=self.create.apply()))\n",
      "        except FileNotFoundError:\n",
      "            pass\n",
      "        except NotGuetHookError:\n",
      "            pass\n",
      "apply(self=<guet.git._hook_loader.HookLoader object at 0x7fc0acc70a30>, hook_name='commit-msg', hooks=[], self.create=<guet.git._create_strategy.DontCreateStrategy object at 0x7fc0acc70d60>, self.file_name=<guet.git._file_name_strategy.AlongsideFileNameStrategy object at 0x7fc0acc70b50>, self.path_to_repository='/path/to/.git')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "hooks.append(Hook(hook_path, create=self.create.apply()))\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def non_guet_hooks_present(self) -> bool:\n",
      "        non_guet_hook_found = False\n",
      "        for hook in self.hooks:\n",
      "            if not hook.is_guet_hook():\n",
      "                non_guet_hook_found = True\n",
      "        return non_guet_hook_found\n",
      "non_guet_hooks_present(self=<guet.git.git.Git object at 0x7fc0acc0d1f0>, self._author=Author(name='name', email='name@localhost'), self._commit_msg=['First line of commit message', 'Second line of commit message'], self._config_lines=['[core]', '\\trepositoryformatversion = 0', '\\tfilemode = true', '\\tbare = false', '\\tlogallrefupdates = true', '\\tignorecase = true', '\\tprecomposeunicode = true', '[user]', '\\tname = name', '\\temail = name@localhost'], self.hooks=[], self.path_to_repository='/path/to/.git')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "non_guet_hook_found = False\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def overwrite_current_author(current_config: List[str], new_author: Author) -> None:\n",
      "    for i, line in enumerate(current_config):\n",
      "        if 'name =' in line:\n",
      "            current_config[i] = f'\\tname = {new_author.name}'\n",
      "        elif 'email =' in line:\n",
      "            current_config[i] = f'\\temail = {new_author.email}'\n",
      "overwrite_current_author(current_config=['[core]', '\\trepositoryformatversion = 0', '\\tfilemode = true', '\\tbare = false', '\\tlogallrefupdates = true', '\\tignorecase = true', '\\tprecomposeunicode = true', '[user]', '\\tname = name', '\\temail = name@localhost'], new_author=Author(name='new_name', email='new_email'))\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "current_config[i] = f'\\tname = {new_author.name}'\n",
      "State:\n",
      "['[core]', '\\trepositoryformatversion = 0', '\\tfilemode = true', '\\tbare = false', '\\tlogallrefupdates = true', '\\tignorecase = true', '\\tprecomposeunicode = true', '[user]', '\\tname = new_name', '\\temail = name@localhost']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def overwrite_current_author(current_config: List[str], new_author: Author) -> None:\n",
      "    for i, line in enumerate(current_config):\n",
      "        if 'name =' in line:\n",
      "            current_config[i] = f'\\tname = {new_author.name}'\n",
      "        elif 'email =' in line:\n",
      "            current_config[i] = f'\\temail = {new_author.email}'\n",
      "overwrite_current_author(current_config=['[core]', '\\trepositoryformatversion = 0', '\\tfilemode = true', '\\tbare = false', '\\tlogallrefupdates = true', '\\tignorecase = true', '\\tprecomposeunicode = true', '[user]', '\\tname = name', '\\temail = name@localhost'], new_author=Author(name='new_name', email='new_email'))\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "current_config[i] = f'\\temail = {new_author.email}'\n",
      "State:\n",
      "['[core]', '\\trepositoryformatversion = 0', '\\tfilemode = true', '\\tbare = false', '\\tlogallrefupdates = true', '\\tignorecase = true', '\\tprecomposeunicode = true', '[user]', '\\tname = new_name', '\\temail = new_email']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_file_content(create, path_to_hook):\n",
      "        _content = Hook._get_file_content(path_to_hook, create)\n",
      "        if _content != GUET_HOOK_FILE:\n",
      "            _content = Hook._handle_mismatched_content(_content, create)\n",
      "        return _content\n",
      "_parse_file_content(create=True, path_to_hook='/path/to/.git/hooks/name')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "_content = Hook._get_file_content(path_to_hook, create)\n",
      "State:\n",
      "['Other', 'Content']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_file_content(create, path_to_hook):\n",
      "        _content = Hook._get_file_content(path_to_hook, create)\n",
      "        if _content != GUET_HOOK_FILE:\n",
      "            _content = Hook._handle_mismatched_content(_content, create)\n",
      "        return _content\n",
      "_parse_file_content(create=True, path_to_hook='/path/to/.git/hooks/name')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "_content = Hook._handle_mismatched_content(_content, create)\n",
      "State:\n",
      "['#! /usr/bin/env python3', 'from guet.hooks import manage', 'import sys', 'manage(sys.argv[0])']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _handle_mismatched_content(_content, create):\n",
      "        if create:\n",
      "            _content = GUET_HOOK_FILE\n",
      "        return _content\n",
      "_handle_mismatched_content(_content=['Other', 'Content'], create=True)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "_content = GUET_HOOK_FILE\n",
      "State:\n",
      "['#! /usr/bin/env python3', 'from guet.hooks import manage', 'import sys', 'manage(sys.argv[0])']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _update_mtime(self):\n",
      "        self.last_mtime = self._get_mtime()\n",
      "_update_mtime(self=<tests.ranger.container.test_bookmarks.NotValidatedBookmarks object at 0x7f3791a14400>, self.autosave=False, self.bookmarktype=<class 'str'>, self.dct={}, self.nonpersistent_bookmarks=set(), self.original_dict={}, self.path='/tmp/pytest-of-XXX/pytest-223/testbookmarks0/bookmarkfile')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.last_mtime = self._get_mtime()\n",
      "State:\n",
      "1712228562.9172025\n",
      "==================================================\n",
      "Clean Code:\n",
      "def save(self):\n",
      "        self.update()\n",
      "        if self.path is None:\n",
      "            return\n",
      "        path_new = self.path + '.new'\n",
      "        try:\n",
      "            with open(path_new, 'w', encoding=\"utf-8\") as fobj:\n",
      "                for key, value in self.dct.items():\n",
      "                    if key in ALLOWED_KEYS \\\n",
      "                            and key not in self.nonpersistent_bookmarks:\n",
      "                        key_value = \"{0}:{1}\\n\".format(key, value)\n",
      "                        if not PY3 and isinstance(key_value, str):\n",
      "                            key_value = key_value.decode(\"utf-8\")\n",
      "                        fobj.write(key_value)\n",
      "        except OSError as ex:\n",
      "            self.fm.notify('Bookmarks error: {0}'.format(str(ex)), bad=True)\n",
      "            return\n",
      "        try:\n",
      "            old_perms = os.stat(self.path)\n",
      "            os.chown(path_new, old_perms.st_uid, old_perms.st_gid)\n",
      "            os.chmod(path_new, old_perms.st_mode)\n",
      "            if os.path.islink(self.path):\n",
      "                target_path = os.path.realpath(self.path)\n",
      "                os.rename(path_new, target_path)\n",
      "            else:\n",
      "                os.rename(path_new, self.path)\n",
      "        except OSError as ex:\n",
      "            self.fm.notify('Bookmarks error: {0}'.format(str(ex)), bad=True)\n",
      "            return\n",
      "        self._update_mtime()\n",
      "save(self=<tests.ranger.container.test_bookmarks.NotValidatedBookmarks object at 0x7f3791a14400>, self.autosave=False, self.bookmarktype=<class 'str'>, self.dct={\"'\": 'the milk'}, self.last_mtime=1712228562.9172025, self.nonpersistent_bookmarks=set(), self.original_dict={}, self.path='/tmp/pytest-of-XXX/pytest-223/testbookmarks0/bookmarkfile')\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "key_value = \"{0}:{1}\\n\".format(key, value)\n",
      "State:\n",
      "\"':the milk\\n\"\n",
      "==================================================\n",
      "Clean Code:\n",
      "def update(self):\n",
      "        real_dict = self._load_dict()\n",
      "        if real_dict is None:\n",
      "            return\n",
      "        real_dict_copy = real_dict.copy()\n",
      "        for key in set(self.dct) | set(real_dict):\n",
      "            if key in self.dct:\n",
      "                current = self.dct[key]\n",
      "            else:\n",
      "                current = None\n",
      "            if key in self.original_dict:\n",
      "                original = self.original_dict[key]\n",
      "            else:\n",
      "                original = None\n",
      "            if key in real_dict:\n",
      "                real = real_dict[key]\n",
      "            else:\n",
      "                real = None\n",
      "            if current == original and current != real:\n",
      "                continue\n",
      "            if key not in self.dct:\n",
      "                del real_dict[key]\n",
      "            else:\n",
      "                real_dict[key] = current\n",
      "        self._set_dict(real_dict, original=real_dict_copy)\n",
      "update(self=<tests.ranger.container.test_bookmarks.NotValidatedBookmarks object at 0x7f3791a14400>, self.autosave=False, self.bookmarktype=<class 'str'>, self.dct={\"'\": 'the milk'}, self.last_mtime=1712228562.9172025, self.nonpersistent_bookmarks=set(), self.original_dict={}, self.path='/tmp/pytest-of-XXX/pytest-223/testbookmarks0/bookmarkfile')\n",
      "\n",
      "n:\n",
      "25\n",
      "Statement:\n",
      "real = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def update_if_outdated(self):\n",
      "        if self.last_mtime != self._get_mtime():\n",
      "            self.update()\n",
      "update_if_outdated(self=<tests.ranger.container.test_bookmarks.NotValidatedBookmarks object at 0x7f37919f6160>, self.autosave=False, self.bookmarktype=<class 'str'>, self.dct={\"'\": 'the milk'}, self.last_mtime=1712228562.9452024, self.nonpersistent_bookmarks=set(), self.original_dict={\"'\": 'the milk'}, self.path='/tmp/pytest-of-XXX/pytest-223/testbookmarks0/bookmarkfile', self.update=<function testbookmarks.<locals>.crash at 0x7f3791a5b430>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.update()\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def back(self):\n",
      "        self.index = max(0, self.index - 1)\n",
      "        return self.current()\n",
      "back(self=<ranger.container.history.History object at 0x7f37919f6fd0>, self.history=['10', '11', '12', '13', '14', '15', '16', '17', '18', '19'], self.index=9, self.maxlen=10, self.unique=True)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.index = max(0, self.index - 1)\n",
      "State:\n",
      "8\n",
      "==================================================\n",
      "Clean Code:\n",
      "def search(self, string, n):\n",
      "        if n != 0 and string:\n",
      "            step = 1 if n > 0 else -1\n",
      "            i = self.index\n",
      "            steps_left = steps_left_at_start = int(abs(n))\n",
      "            while steps_left:\n",
      "                i += step\n",
      "                if i >= len(self.history) or i < 0:\n",
      "                    break\n",
      "                if self.history[i].startswith(string):\n",
      "                    steps_left -= 1\n",
      "            if steps_left != steps_left_at_start:\n",
      "                self.index = i\n",
      "        return self.current()\n",
      "search(self=<ranger.container.history.History object at 0x7f37919f6fd0>, string='45', n=-9, self.history=['10', '11', '12', '13', '14', '15', '16', '17', '18', '19'], self.index=8, self.maxlen=10, self.unique=True)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "step = 1 if n > 0 else -1\n",
      "State:\n",
      "-1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def search(self, string, n):\n",
      "        if n != 0 and string:\n",
      "            step = 1 if n > 0 else -1\n",
      "            i = self.index\n",
      "            steps_left = steps_left_at_start = int(abs(n))\n",
      "            while steps_left:\n",
      "                i += step\n",
      "                if i >= len(self.history) or i < 0:\n",
      "                    break\n",
      "                if self.history[i].startswith(string):\n",
      "                    steps_left -= 1\n",
      "            if steps_left != steps_left_at_start:\n",
      "                self.index = i\n",
      "        return self.current()\n",
      "search(self=<ranger.container.history.History object at 0x7f37919f6fd0>, string='45', n=-9, self.history=['10', '11', '12', '13', '14', '15', '16', '17', '18', '19'], self.index=8, self.maxlen=10, self.unique=True)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "i = self.index\n",
      "State:\n",
      "8\n",
      "==================================================\n",
      "Clean Code:\n",
      "def search(self, string, n):\n",
      "        if n != 0 and string:\n",
      "            step = 1 if n > 0 else -1\n",
      "            i = self.index\n",
      "            steps_left = steps_left_at_start = int(abs(n))\n",
      "            while steps_left:\n",
      "                i += step\n",
      "                if i >= len(self.history) or i < 0:\n",
      "                    break\n",
      "                if self.history[i].startswith(string):\n",
      "                    steps_left -= 1\n",
      "            if steps_left != steps_left_at_start:\n",
      "                self.index = i\n",
      "        return self.current()\n",
      "search(self=<ranger.container.history.History object at 0x7f37919f6fd0>, string='45', n=-9, self.history=['10', '11', '12', '13', '14', '15', '16', '17', '18', '19'], self.index=8, self.maxlen=10, self.unique=True)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "steps_left = steps_left_at_start = int(abs(n))\n",
      "State:\n",
      "9\n",
      "==================================================\n",
      "Clean Code:\n",
      "def forward(self):\n",
      "        if self.history:\n",
      "            self.index += 1\n",
      "            if self.index > len(self.history) - 1:\n",
      "                self.index = len(self.history) - 1\n",
      "        else:\n",
      "            self.index = 0\n",
      "        return self.current()\n",
      "forward(self=<ranger.container.history.History object at 0x7f37919f6fd0>, self.history=['10', '11', '12', '13', '14', '15', '16', '17', '18', '19'], self.index=8, self.maxlen=10, self.unique=True)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.index += 1\n",
      "State:\n",
      "9\n",
      "==================================================\n",
      "Clean Code:\n",
      "def move(self, n):\n",
      "        self.index = max(0, min(len(self.history) - 1, self.index + n))\n",
      "        return self.current()\n",
      "move(self=<ranger.container.history.History object at 0x7f37919f6fd0>, n=-3, self.history=['10', '11', '12', '13', '14', '15', '16', '17', '18', '19'], self.index=9, self.maxlen=10, self.unique=True)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.index = max(0, min(len(self.history) - 1, self.index + n))\n",
      "State:\n",
      "6\n",
      "==================================================\n",
      "Clean Code:\n",
      "def basename_natural(self):\n",
      "        basename_list = []\n",
      "        for string in _EXTRACT_NUMBER_RE.split(self.relative_path):\n",
      "            try:\n",
      "                basename_list += [('0', int(string))]\n",
      "            except ValueError:\n",
      "                basename_list += [(string, 0)]\n",
      "        return basename_list\n",
      "basename_natural(self=<FileSystemObject /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/ranger+ranger/ranger+ranger/hello112>, self.basename='hello112', self.basename_natural__reset=<function lazy_property.__get__.<locals>.reset_function at 0x7f3791a795e0>, self.display_data={}, self.path='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/ranger+ranger/ranger+ranger/hello112', self.preload=None, self.relative_path='hello112')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "basename_list = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def basename_natural_lower(self):\n",
      "        basename_list = []\n",
      "        for string in _EXTRACT_NUMBER_RE.split(self.relative_path_lower):\n",
      "            try:\n",
      "                basename_list += [('0', int(string))]\n",
      "            except ValueError:\n",
      "                basename_list += [(string, 0)]\n",
      "        return basename_list\n",
      "basename_natural_lower(self=<FileSystemObject /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/ranger+ranger/ranger+ranger/hello112>, self.basename='hello112', self.basename_natural=[('', 0), ('h', 0), ('', 0), ('e', 0), ('', 0), ('l', 0), ('', 0), ('l', 0), ('', 0), ('o', 0), ('', 0), ('0', 112), ('', 0)], self.basename_natural__reset=<function lazy_property.__get__.<locals>.reset_function at 0x7f3791a795e0>, self.basename_natural_lower__reset=<function lazy_property.__get__.<locals>.reset_function at 0x7f3791590f70>, self.display_data={}, self.path='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/ranger+ranger/ranger+ranger/hello112', self.preload=None, self.relative_path='hello112')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "basename_list = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _multilabel_data():\n",
      "  targets = tf.constant([1.0, 1.0, 1.0, 0.0, 0.0, 0.0], shape=[6, 1])\n",
      "  targets = tf.concat([targets, targets], 1)\n",
      "  logits_positives = tf.constant([[0.0, 15],\n",
      "                                  [16, 0.0],\n",
      "                                  [14, 0.0]], shape=[3, 2])\n",
      "  logits_negatives = tf.constant([[-17, 0.0],\n",
      "                                  [-15, 0.0],\n",
      "                                  [0.0, -101]], shape=[3, 2])\n",
      "  logits = tf.concat([logits_positives, logits_negatives], 0)\n",
      "  priors = tf.constant(0.5, shape=[2])\n",
      "  return targets, logits, priors\n",
      "_multilabel_data()\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "targets = tf.constant([1.0, 1.0, 1.0, 0.0, 0.0, 0.0], shape=[6, 1])\n",
      "State:\n",
      "<tf.Tensor: shape=(6, 1), dtype=float32, numpy=array([[1.],       [1.],       [1.],       [0.],       [0.],       [0.]], dtype=float32)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _multilabel_data():\n",
      "  targets = tf.constant([1.0, 1.0, 1.0, 0.0, 0.0, 0.0], shape=[6, 1])\n",
      "  targets = tf.concat([targets, targets], 1)\n",
      "  logits_positives = tf.constant([[0.0, 15],\n",
      "                                  [16, 0.0],\n",
      "                                  [14, 0.0]], shape=[3, 2])\n",
      "  logits_negatives = tf.constant([[-17, 0.0],\n",
      "                                  [-15, 0.0],\n",
      "                                  [0.0, -101]], shape=[3, 2])\n",
      "  logits = tf.concat([logits_positives, logits_negatives], 0)\n",
      "  priors = tf.constant(0.5, shape=[2])\n",
      "  return targets, logits, priors\n",
      "_multilabel_data()\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "targets = tf.concat([targets, targets], 1)\n",
      "State:\n",
      "<tf.Tensor: shape=(6, 2), dtype=float32, numpy=array([[1., 1.],       [1., 1.],       [1., 1.],       [0., 0.],       [0., 0.],       [0., 0.]], dtype=float32)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _multilabel_data():\n",
      "  targets = tf.constant([1.0, 1.0, 1.0, 0.0, 0.0, 0.0], shape=[6, 1])\n",
      "  targets = tf.concat([targets, targets], 1)\n",
      "  logits_positives = tf.constant([[0.0, 15],\n",
      "                                  [16, 0.0],\n",
      "                                  [14, 0.0]], shape=[3, 2])\n",
      "  logits_negatives = tf.constant([[-17, 0.0],\n",
      "                                  [-15, 0.0],\n",
      "                                  [0.0, -101]], shape=[3, 2])\n",
      "  logits = tf.concat([logits_positives, logits_negatives], 0)\n",
      "  priors = tf.constant(0.5, shape=[2])\n",
      "  return targets, logits, priors\n",
      "_multilabel_data()\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "logits_positives = tf.constant([[0.0, 15],\n",
      "State:\n",
      "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=array([[ 0., 15.],       [16.,  0.],       [14.,  0.]], dtype=float32)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _multilabel_data():\n",
      "  targets = tf.constant([1.0, 1.0, 1.0, 0.0, 0.0, 0.0], shape=[6, 1])\n",
      "  targets = tf.concat([targets, targets], 1)\n",
      "  logits_positives = tf.constant([[0.0, 15],\n",
      "                                  [16, 0.0],\n",
      "                                  [14, 0.0]], shape=[3, 2])\n",
      "  logits_negatives = tf.constant([[-17, 0.0],\n",
      "                                  [-15, 0.0],\n",
      "                                  [0.0, -101]], shape=[3, 2])\n",
      "  logits = tf.concat([logits_positives, logits_negatives], 0)\n",
      "  priors = tf.constant(0.5, shape=[2])\n",
      "  return targets, logits, priors\n",
      "_multilabel_data()\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "priors = tf.constant(0.5, shape=[2])\n",
      "State:\n",
      "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.5, 0.5], dtype=float32)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _other_multilabel_data(surrogate_type):\n",
      "  targets = tf.constant(\n",
      "      [1.0] * 6 + [0.0] * 6, shape=[12, 1])\n",
      "  targets = tf.concat([targets, targets], 1)\n",
      "  logits_positives = tf.constant([[0.0, 13],\n",
      "                                  [12, 0.0],\n",
      "                                  [15, 0.0],\n",
      "                                  [0.0, 30],\n",
      "                                  [13, 0.0],\n",
      "                                  [18, 0.0]], shape=[6, 2])\n",
      "  cost_2 = 1.0 if surrogate_type == 'hinge' else 1.09861229\n",
      "  logits_negatives = tf.constant([[-16, cost_2],\n",
      "                                  [-15, cost_2],\n",
      "                                  [cost_2, -111],\n",
      "                                  [-133, -14,],\n",
      "                                  [-14.0100101, -16,],\n",
      "                                  [-19.888828882, -101]], shape=[6, 2])\n",
      "  logits = tf.concat([logits_positives, logits_negatives], 0)\n",
      "  priors = tf.constant(0.5, shape=[2])\n",
      "  def builder():\n",
      "    return targets, logits, priors\n",
      "  return builder\n",
      "_other_multilabel_data(surrogate_type='hinge')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "targets = tf.constant(\n",
      "State:\n",
      "<tf.Tensor: shape=(12, 1), dtype=float32, numpy=array([[1.],       [1.],       [1.],       [1.],       [1.],       [1.],       [0.],       [0.],       [0.],       [0.],       [0.],       [0.]], dtype=float32)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _other_multilabel_data(surrogate_type):\n",
      "  targets = tf.constant(\n",
      "      [1.0] * 6 + [0.0] * 6, shape=[12, 1])\n",
      "  targets = tf.concat([targets, targets], 1)\n",
      "  logits_positives = tf.constant([[0.0, 13],\n",
      "                                  [12, 0.0],\n",
      "                                  [15, 0.0],\n",
      "                                  [0.0, 30],\n",
      "                                  [13, 0.0],\n",
      "                                  [18, 0.0]], shape=[6, 2])\n",
      "  cost_2 = 1.0 if surrogate_type == 'hinge' else 1.09861229\n",
      "  logits_negatives = tf.constant([[-16, cost_2],\n",
      "                                  [-15, cost_2],\n",
      "                                  [cost_2, -111],\n",
      "                                  [-133, -14,],\n",
      "                                  [-14.0100101, -16,],\n",
      "                                  [-19.888828882, -101]], shape=[6, 2])\n",
      "  logits = tf.concat([logits_positives, logits_negatives], 0)\n",
      "  priors = tf.constant(0.5, shape=[2])\n",
      "  def builder():\n",
      "    return targets, logits, priors\n",
      "  return builder\n",
      "_other_multilabel_data(surrogate_type='hinge')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "targets = tf.concat([targets, targets], 1)\n",
      "State:\n",
      "<tf.Tensor: shape=(12, 2), dtype=float32, numpy=array([[1., 1.],       [1., 1.],       [1., 1.],       [1., 1.],       [1., 1.],       [1., 1.],       [0., 0.],       [0., 0.],       [0., 0.],       [0., 0.],       [0., 0.],       [0., 0.]], dtype=float32)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _other_multilabel_data(surrogate_type):\n",
      "  targets = tf.constant(\n",
      "      [1.0] * 6 + [0.0] * 6, shape=[12, 1])\n",
      "  targets = tf.concat([targets, targets], 1)\n",
      "  logits_positives = tf.constant([[0.0, 13],\n",
      "                                  [12, 0.0],\n",
      "                                  [15, 0.0],\n",
      "                                  [0.0, 30],\n",
      "                                  [13, 0.0],\n",
      "                                  [18, 0.0]], shape=[6, 2])\n",
      "  cost_2 = 1.0 if surrogate_type == 'hinge' else 1.09861229\n",
      "  logits_negatives = tf.constant([[-16, cost_2],\n",
      "                                  [-15, cost_2],\n",
      "                                  [cost_2, -111],\n",
      "                                  [-133, -14,],\n",
      "                                  [-14.0100101, -16,],\n",
      "                                  [-19.888828882, -101]], shape=[6, 2])\n",
      "  logits = tf.concat([logits_positives, logits_negatives], 0)\n",
      "  priors = tf.constant(0.5, shape=[2])\n",
      "  def builder():\n",
      "    return targets, logits, priors\n",
      "  return builder\n",
      "_other_multilabel_data(surrogate_type='hinge')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "logits_positives = tf.constant([[0.0, 13],\n",
      "State:\n",
      "<tf.Tensor: shape=(6, 2), dtype=float32, numpy=array([[ 0., 13.],       [12.,  0.],       [15.,  0.],       [ 0., 30.],       [13.,  0.],       [18.,  0.]], dtype=float32)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def testManualROCLoss(self, surrogate_type, labels, logits, expected_value):\n",
      "    labels = tf.constant(labels)\n",
      "    logits = tf.constant(logits)\n",
      "    loss, _ = loss_layers.roc_auc_loss(\n",
      "        labels=labels, logits=logits, surrogate_type=surrogate_type)\n",
      "    with self.test_session():\n",
      "      self.assertAllClose(expected_value, tf.reduce_sum(loss).eval())\n",
      "testManualROCLoss(self=<loss_layers_test.ROCAUCLossTest testMethod=testManualROCLoss_hinge>, surrogate_type='hinge', labels=[0.0, 0.0, 0.0, 1.0, 1.0, 1.0], logits=[-0.2, -0.05, 0.0, 0.95, 0.8, 1.0], expected_value=0.06666666666666667, self._cached_session=None, self._cleanups=[(<bound method ExitStack.close of <contextlib.ExitStack object at 0x7fb7605299d0>>, (), {})], self._exit_stack=<contextlib.ExitStack object at 0x7fb7605299d0>, self._outcome=<unittest.case._Outcome object at 0x7fb76ea14b50>, self._set_default_seed=True, self._subtest=None, self._tempdir=None, self._testMethodDoc=\"testManualROCLoss_hinge('hinge', [0.0, 0.0, 0.0, 1.0, 1.0, 1.0], [-0.2, -0.05, 0.0, 0.95, 0.8, 1.0], 0.06666666666666667)\", self._testMethodName='testManualROCLoss_hinge', self._test_start_time=1712202882.4752297, self._threads=[], self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "labels = tf.constant(labels)\n",
      "State:\n",
      "<tf.Tensor: shape=(6,), dtype=float32, numpy=array([0., 0., 0., 1., 1., 1.], dtype=float32)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def testManualROCLoss(self, surrogate_type, labels, logits, expected_value):\n",
      "    labels = tf.constant(labels)\n",
      "    logits = tf.constant(logits)\n",
      "    loss, _ = loss_layers.roc_auc_loss(\n",
      "        labels=labels, logits=logits, surrogate_type=surrogate_type)\n",
      "    with self.test_session():\n",
      "      self.assertAllClose(expected_value, tf.reduce_sum(loss).eval())\n",
      "testManualROCLoss(self=<loss_layers_test.ROCAUCLossTest testMethod=testManualROCLoss_hinge>, surrogate_type='hinge', labels=[0.0, 0.0, 0.0, 1.0, 1.0, 1.0], logits=[-0.2, -0.05, 0.0, 0.95, 0.8, 1.0], expected_value=0.06666666666666667, self._cached_session=None, self._cleanups=[(<bound method ExitStack.close of <contextlib.ExitStack object at 0x7fb7605299d0>>, (), {})], self._exit_stack=<contextlib.ExitStack object at 0x7fb7605299d0>, self._outcome=<unittest.case._Outcome object at 0x7fb76ea14b50>, self._set_default_seed=True, self._subtest=None, self._tempdir=None, self._testMethodDoc=\"testManualROCLoss_hinge('hinge', [0.0, 0.0, 0.0, 1.0, 1.0, 1.0], [-0.2, -0.05, 0.0, 0.95, 0.8, 1.0], 0.06666666666666667)\", self._testMethodName='testManualROCLoss_hinge', self._test_start_time=1712202882.4752297, self._threads=[], self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "logits = tf.constant(logits)\n",
      "State:\n",
      "<tf.Tensor: shape=(6,), dtype=float32, numpy=array([-0.2 , -0.05,  0.  ,  0.95,  0.8 ,  1.  ], dtype=float32)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def testManualROCLoss(self, surrogate_type, labels, logits, expected_value):\n",
      "    labels = tf.constant(labels)\n",
      "    logits = tf.constant(logits)\n",
      "    loss, _ = loss_layers.roc_auc_loss(\n",
      "        labels=labels, logits=logits, surrogate_type=surrogate_type)\n",
      "    with self.test_session():\n",
      "      self.assertAllClose(expected_value, tf.reduce_sum(loss).eval())\n",
      "testManualROCLoss(self=<loss_layers_test.ROCAUCLossTest testMethod=testManualROCLoss_hinge>, surrogate_type='hinge', labels=[0.0, 0.0, 0.0, 1.0, 1.0, 1.0], logits=[-0.2, -0.05, 0.0, 0.95, 0.8, 1.0], expected_value=0.06666666666666667, self._cached_session=None, self._cleanups=[(<bound method ExitStack.close of <contextlib.ExitStack object at 0x7fb7605299d0>>, (), {})], self._exit_stack=<contextlib.ExitStack object at 0x7fb7605299d0>, self._outcome=<unittest.case._Outcome object at 0x7fb76ea14b50>, self._set_default_seed=True, self._subtest=None, self._tempdir=None, self._testMethodDoc=\"testManualROCLoss_hinge('hinge', [0.0, 0.0, 0.0, 1.0, 1.0, 1.0], [-0.2, -0.05, 0.0, 0.95, 0.8, 1.0], 0.06666666666666667)\", self._testMethodName='testManualROCLoss_hinge', self._test_start_time=1712202882.4752297, self._threads=[], self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "loss, _ = loss_layers.roc_auc_loss(\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def file(input_file, light=False):\n",
      "    util.create_dir(os.path.join(CONF_DIR, \"colorschemes/light/\"))\n",
      "    util.create_dir(os.path.join(CONF_DIR, \"colorschemes/dark/\"))\n",
      "    theme_name = \".\".join((input_file, \"json\"))\n",
      "    bri = \"light\" if light else \"dark\"\n",
      "    user_theme_file = os.path.join(CONF_DIR, \"colorschemes\", bri, theme_name)\n",
      "    theme_file = os.path.join(MODULE_DIR, \"colorschemes\", bri, theme_name)\n",
      "    if input_file in (\"random\", \"random_dark\"):\n",
      "        theme_file = get_random_theme()\n",
      "    elif input_file == \"random_light\":\n",
      "        theme_file = get_random_theme(light)\n",
      "    elif input_file == \"random_user\":\n",
      "        theme_file = get_random_theme_user()\n",
      "    elif os.path.isfile(user_theme_file):\n",
      "        theme_file = user_theme_file\n",
      "    elif os.path.isfile(input_file):\n",
      "        theme_file = input_file\n",
      "    if os.path.isfile(theme_file):\n",
      "        logging.info(\"Set theme to \\033[1;37m%s\\033[0m.\",\n",
      "                     os.path.basename(theme_file))\n",
      "        util.save_file(os.path.basename(theme_file),\n",
      "                       os.path.join(CACHE_DIR, \"last_used_theme\"))\n",
      "        return parse(theme_file)\n",
      "    logging.error(\"No %s colorscheme file found.\", bri)\n",
      "    logging.error(\"Try adding   '-l' to set light themes.\")\n",
      "    logging.error(\"Try removing '-l' to set dark themes.\")\n",
      "    sys.exit(1)\n",
      "file(input_file='tests/test_files/test_file.json', light=False)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "theme_name = \".\".join((input_file, \"json\"))\n",
      "State:\n",
      "'tests/test_files/test_file.json.json'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def file(input_file, light=False):\n",
      "    util.create_dir(os.path.join(CONF_DIR, \"colorschemes/light/\"))\n",
      "    util.create_dir(os.path.join(CONF_DIR, \"colorschemes/dark/\"))\n",
      "    theme_name = \".\".join((input_file, \"json\"))\n",
      "    bri = \"light\" if light else \"dark\"\n",
      "    user_theme_file = os.path.join(CONF_DIR, \"colorschemes\", bri, theme_name)\n",
      "    theme_file = os.path.join(MODULE_DIR, \"colorschemes\", bri, theme_name)\n",
      "    if input_file in (\"random\", \"random_dark\"):\n",
      "        theme_file = get_random_theme()\n",
      "    elif input_file == \"random_light\":\n",
      "        theme_file = get_random_theme(light)\n",
      "    elif input_file == \"random_user\":\n",
      "        theme_file = get_random_theme_user()\n",
      "    elif os.path.isfile(user_theme_file):\n",
      "        theme_file = user_theme_file\n",
      "    elif os.path.isfile(input_file):\n",
      "        theme_file = input_file\n",
      "    if os.path.isfile(theme_file):\n",
      "        logging.info(\"Set theme to \\033[1;37m%s\\033[0m.\",\n",
      "                     os.path.basename(theme_file))\n",
      "        util.save_file(os.path.basename(theme_file),\n",
      "                       os.path.join(CACHE_DIR, \"last_used_theme\"))\n",
      "        return parse(theme_file)\n",
      "    logging.error(\"No %s colorscheme file found.\", bri)\n",
      "    logging.error(\"Try adding   '-l' to set light themes.\")\n",
      "    logging.error(\"Try removing '-l' to set dark themes.\")\n",
      "    sys.exit(1)\n",
      "file(input_file='tests/test_files/test_file.json', light=False)\n",
      "\n",
      "n:\n",
      "25\n",
      "Statement:\n",
      "theme_file = input_file\n",
      "State:\n",
      "'tests/test_files/test_file.json'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_image_dir(img_dir):\n",
      "    current_wall = wallpaper.get()\n",
      "    current_wall = os.path.basename(current_wall)\n",
      "    file_types = (\".png\", \".jpg\", \".jpeg\", \".jpe\", \".gif\")\n",
      "    return [img.name for img in os.scandir(img_dir)\n",
      "            if img.name.lower().endswith(file_types)], current_wall\n",
      "get_image_dir(img_dir='tests/test_files')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "current_wall = wallpaper.get()\n",
      "State:\n",
      "'/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/dylanaraps+pywal/dylanaraps+pywal/tests/test_files/test.jpg'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_image_dir(img_dir):\n",
      "    current_wall = wallpaper.get()\n",
      "    current_wall = os.path.basename(current_wall)\n",
      "    file_types = (\".png\", \".jpg\", \".jpeg\", \".jpe\", \".gif\")\n",
      "    return [img.name for img in os.scandir(img_dir)\n",
      "            if img.name.lower().endswith(file_types)], current_wall\n",
      "get_image_dir(img_dir='tests/test_files')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "current_wall = os.path.basename(current_wall)\n",
      "State:\n",
      "'test.jpg'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def merge_args_into_config(args, config_params):\n",
      "  final_args = copy.deepcopy(config_params)\n",
      "  config_keys = config_params.keys()\n",
      "  for key, value in args.items():\n",
      "    if key in config_keys:\n",
      "      if any([isinstance(value, t) for t in [str, bool, float, int]]):\n",
      "        final_args[key] = value\n",
      "      elif isinstance(value, list):\n",
      "        final_args[key] = value + config_params[key]\n",
      "      elif isinstance(value, dict):\n",
      "        final_args[key].update(**value)\n",
      "    else:\n",
      "      final_args[key] = value\n",
      "  return final_args\n",
      "merge_args_into_config(args={'input_folder': 'foo/bar', 'resize_images': False, 'im_size': 500, 'compress_pdf': False, 'pdf_im_resolution': 500, 'images_allowlist': {'path1/': 1000}, 'commands_to_delete': ['\\\\todo1'], 'use_external_tikz': 'foo/bar/tikz'}, config_params={'input_folder': 'foo_/bar_', 'resize_images': True, 'im_size': 1000, 'compress_pdf': True, 'pdf_im_resolution': 1000, 'images_allowlist': {'path2/': 1000}, 'commands_to_delete': ['\\\\todo2'], 'use_external_tikz': 'foo_/bar_/tikz_'})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "final_args = copy.deepcopy(config_params)\n",
      "State:\n",
      "{'input_folder': 'foo_/bar_', 'resize_images': True, 'im_size': 1000, 'compress_pdf': True, 'pdf_im_resolution': 1000, 'images_allowlist': {'path2/': 1000}, 'commands_to_delete': ['\\\\todo2'], 'use_external_tikz': 'foo_/bar_/tikz_'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def merge_args_into_config(args, config_params):\n",
      "  final_args = copy.deepcopy(config_params)\n",
      "  config_keys = config_params.keys()\n",
      "  for key, value in args.items():\n",
      "    if key in config_keys:\n",
      "      if any([isinstance(value, t) for t in [str, bool, float, int]]):\n",
      "        final_args[key] = value\n",
      "      elif isinstance(value, list):\n",
      "        final_args[key] = value + config_params[key]\n",
      "      elif isinstance(value, dict):\n",
      "        final_args[key].update(**value)\n",
      "    else:\n",
      "      final_args[key] = value\n",
      "  return final_args\n",
      "merge_args_into_config(args={'input_folder': 'foo/bar', 'resize_images': False, 'im_size': 500, 'compress_pdf': False, 'pdf_im_resolution': 500, 'images_allowlist': {'path1/': 1000}, 'commands_to_delete': ['\\\\todo1'], 'use_external_tikz': 'foo/bar/tikz'}, config_params={'input_folder': 'foo_/bar_', 'resize_images': True, 'im_size': 1000, 'compress_pdf': True, 'pdf_im_resolution': 1000, 'images_allowlist': {'path2/': 1000}, 'commands_to_delete': ['\\\\todo2'], 'use_external_tikz': 'foo_/bar_/tikz_'})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "config_keys = config_params.keys()\n",
      "State:\n",
      "dict_keys(['input_folder', 'resize_images', 'im_size', 'compress_pdf', 'pdf_im_resolution', 'images_allowlist', 'commands_to_delete', 'use_external_tikz'])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def merge_args_into_config(args, config_params):\n",
      "  final_args = copy.deepcopy(config_params)\n",
      "  config_keys = config_params.keys()\n",
      "  for key, value in args.items():\n",
      "    if key in config_keys:\n",
      "      if any([isinstance(value, t) for t in [str, bool, float, int]]):\n",
      "        final_args[key] = value\n",
      "      elif isinstance(value, list):\n",
      "        final_args[key] = value + config_params[key]\n",
      "      elif isinstance(value, dict):\n",
      "        final_args[key].update(**value)\n",
      "    else:\n",
      "      final_args[key] = value\n",
      "  return final_args\n",
      "merge_args_into_config(args={'input_folder': 'foo/bar', 'resize_images': False, 'im_size': 500, 'compress_pdf': False, 'pdf_im_resolution': 500, 'images_allowlist': {'path1/': 1000}, 'commands_to_delete': ['\\\\todo1'], 'use_external_tikz': 'foo/bar/tikz'}, config_params={'input_folder': 'foo_/bar_', 'resize_images': True, 'im_size': 1000, 'compress_pdf': True, 'pdf_im_resolution': 1000, 'images_allowlist': {'path2/': 1000}, 'commands_to_delete': ['\\\\todo2'], 'use_external_tikz': 'foo_/bar_/tikz_'})\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "final_args[key] = value + config_params[key]\n",
      "State:\n",
      "{'input_folder': 'foo/bar', 'resize_images': False, 'im_size': 500, 'compress_pdf': False, 'pdf_im_resolution': 500, 'images_allowlist': {'path2/': 1000, 'path1/': 1000}, 'commands_to_delete': ['\\\\todo1', '\\\\todo2'], 'use_external_tikz': 'foo_/bar_/tikz_'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _remove_command(text, command, keep_text=False):\n",
      "  base_pattern = r'\\\\' + command + r'\\{((?:[^{}]+|\\{(?1)\\})*)\\}'\n",
      "  while True:\n",
      "    all_substitutions = []\n",
      "    has_match = False\n",
      "    for match in regex.finditer(base_pattern, text):\n",
      "      has_match = True\n",
      "      new_substring = (\n",
      "          ''\n",
      "          if not keep_text\n",
      "          else text[match.span()[0] + len(command) + 2 : match.span()[1] - 1]\n",
      "      )\n",
      "      if match.span()[1] < len(text):\n",
      "        next_newline = text[match.span()[1] :].find('\\n')\n",
      "        if next_newline != -1:\n",
      "          text_until_newline = text[\n",
      "              match.span()[1] : match.span()[1] + next_newline\n",
      "          ]\n",
      "          if (\n",
      "              not text_until_newline or text_until_newline.isspace()\n",
      "          ) and not keep_text:\n",
      "            new_substring = '%'\n",
      "      all_substitutions.append(\n",
      "          (match.span()[0], match.span()[1], new_substring)\n",
      "      )\n",
      "    for start, end, new_substring in reversed(all_substitutions):\n",
      "      text = text[:start] + new_substring + text[end:]\n",
      "    if not keep_text or not has_match:\n",
      "      break\n",
      "  return text\n",
      "_remove_command(text='A\\\\todo{B\\nC}D\\nE\\n\\\\end{document}', command='todo', keep_text=False)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "all_substitutions = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _remove_command(text, command, keep_text=False):\n",
      "  base_pattern = r'\\\\' + command + r'\\{((?:[^{}]+|\\{(?1)\\})*)\\}'\n",
      "  while True:\n",
      "    all_substitutions = []\n",
      "    has_match = False\n",
      "    for match in regex.finditer(base_pattern, text):\n",
      "      has_match = True\n",
      "      new_substring = (\n",
      "          ''\n",
      "          if not keep_text\n",
      "          else text[match.span()[0] + len(command) + 2 : match.span()[1] - 1]\n",
      "      )\n",
      "      if match.span()[1] < len(text):\n",
      "        next_newline = text[match.span()[1] :].find('\\n')\n",
      "        if next_newline != -1:\n",
      "          text_until_newline = text[\n",
      "              match.span()[1] : match.span()[1] + next_newline\n",
      "          ]\n",
      "          if (\n",
      "              not text_until_newline or text_until_newline.isspace()\n",
      "          ) and not keep_text:\n",
      "            new_substring = '%'\n",
      "      all_substitutions.append(\n",
      "          (match.span()[0], match.span()[1], new_substring)\n",
      "      )\n",
      "    for start, end, new_substring in reversed(all_substitutions):\n",
      "      text = text[:start] + new_substring + text[end:]\n",
      "    if not keep_text or not has_match:\n",
      "      break\n",
      "  return text\n",
      "_remove_command(text='A\\\\todo{B\\nC}D\\nE\\n\\\\end{document}', command='todo', keep_text=False)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "has_match = True\n",
      "State:\n",
      "True\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _remove_comments_inline(text):\n",
      "  if 'auto-ignore' in text:\n",
      "    return text\n",
      "  if text.lstrip(' ').lstrip('\\t').startswith('%'):\n",
      "    return ''\n",
      "  url_pattern = r'\\\\url\\{(?>[^{}]|(?R))*\\}'\n",
      "  def remove_comments(segment):\n",
      "    if segment.lstrip().startswith('%'):\n",
      "      return ''\n",
      "    match = regex.search(r'(?<!\\\\)%', segment)\n",
      "    if match:\n",
      "      return segment[: match.end()] + '\\n'\n",
      "    else:\n",
      "      return segment\n",
      "  segments = regex.split(f'({url_pattern})', text)\n",
      "  for i in range(len(segments)):\n",
      "    if not regex.match(url_pattern, segments[i]):\n",
      "      segments[i] = remove_comments(segments[i])\n",
      "  final_text = ''.join(segments)\n",
      "  return (\n",
      "      final_text\n",
      "      if final_text.endswith('\\n') or final_text.endswith('\\\\n')\n",
      "      else final_text + '\\n'\n",
      "  )\n",
      "_remove_comments_inline(text='Foo %Comment\\n')\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "segments = regex.split(f'({url_pattern})', text)\n",
      "State:\n",
      "['Foo %Comment\\n']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _remove_iffalse_block(text):\n",
      "  p = regex.compile(r'\\\\if\\s*(\\w+)|\\\\fi(?!\\w)')\n",
      "  level = -1\n",
      "  positions_to_delete = []\n",
      "  start, end = 0, 0\n",
      "  for m in p.finditer(text):\n",
      "    if (\n",
      "        m.group().replace(' ', '') == r'\\iffalse'\n",
      "        or m.group().replace(' ', '') == r'\\if0'\n",
      "    ) and level == -1:\n",
      "      level += 1\n",
      "      start = m.start()\n",
      "    elif m.group().startswith(r'\\if') and level >= 0:\n",
      "      level += 1\n",
      "    elif m.group() == r'\\fi' and level >= 0:\n",
      "      if level == 0:\n",
      "        end = m.end()\n",
      "        positions_to_delete.append((start, end))\n",
      "      level -= 1\n",
      "    else:\n",
      "      pass\n",
      "  for start, end in reversed(positions_to_delete):\n",
      "    if end < len(text) and text[end].isspace():\n",
      "      end_to_del = end + 1\n",
      "    else:\n",
      "      end_to_del = end\n",
      "    text = text[:start] + text[end_to_del:]\n",
      "  return text\n",
      "_remove_iffalse_block(text='\\\\newcommand\\\\figref[1]{Figure~\\\\ref{fig:\\\\\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "p = regex.compile(r'\\\\if\\s*(\\w+)|\\\\fi(?!\\w)')\n",
      "State:\n",
      "regex.Regex('\\\\\\\\if\\\\s*(\\\\w+)|\\\\\\\\fi(?!\\\\w)', flags=regex.V0)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _remove_iffalse_block(text):\n",
      "  p = regex.compile(r'\\\\if\\s*(\\w+)|\\\\fi(?!\\w)')\n",
      "  level = -1\n",
      "  positions_to_delete = []\n",
      "  start, end = 0, 0\n",
      "  for m in p.finditer(text):\n",
      "    if (\n",
      "        m.group().replace(' ', '') == r'\\iffalse'\n",
      "        or m.group().replace(' ', '') == r'\\if0'\n",
      "    ) and level == -1:\n",
      "      level += 1\n",
      "      start = m.start()\n",
      "    elif m.group().startswith(r'\\if') and level >= 0:\n",
      "      level += 1\n",
      "    elif m.group() == r'\\fi' and level >= 0:\n",
      "      if level == 0:\n",
      "        end = m.end()\n",
      "        positions_to_delete.append((start, end))\n",
      "      level -= 1\n",
      "    else:\n",
      "      pass\n",
      "  for start, end in reversed(positions_to_delete):\n",
      "    if end < len(text) and text[end].isspace():\n",
      "      end_to_del = end + 1\n",
      "    else:\n",
      "      end_to_del = end\n",
      "    text = text[:start] + text[end_to_del:]\n",
      "  return text\n",
      "_remove_iffalse_block(text='\\\\newcommand\\\\figref[1]{Figure~\\\\ref{fig:\\\\\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "level = -1\n",
      "State:\n",
      "-1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _remove_iffalse_block(text):\n",
      "  p = regex.compile(r'\\\\if\\s*(\\w+)|\\\\fi(?!\\w)')\n",
      "  level = -1\n",
      "  positions_to_delete = []\n",
      "  start, end = 0, 0\n",
      "  for m in p.finditer(text):\n",
      "    if (\n",
      "        m.group().replace(' ', '') == r'\\iffalse'\n",
      "        or m.group().replace(' ', '') == r'\\if0'\n",
      "    ) and level == -1:\n",
      "      level += 1\n",
      "      start = m.start()\n",
      "    elif m.group().startswith(r'\\if') and level >= 0:\n",
      "      level += 1\n",
      "    elif m.group() == r'\\fi' and level >= 0:\n",
      "      if level == 0:\n",
      "        end = m.end()\n",
      "        positions_to_delete.append((start, end))\n",
      "      level -= 1\n",
      "    else:\n",
      "      pass\n",
      "  for start, end in reversed(positions_to_delete):\n",
      "    if end < len(text) and text[end].isspace():\n",
      "      end_to_del = end + 1\n",
      "    else:\n",
      "      end_to_del = end\n",
      "    text = text[:start] + text[end_to_del:]\n",
      "  return text\n",
      "_remove_iffalse_block(text='\\\\newcommand\\\\figref[1]{Figure~\\\\ref{fig:\\\\\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "positions_to_delete = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def setUp(self):\n",
      "    super(IntegrationTests, self).setUp()\n",
      "    self.out_path = 'tex_arXiv'\n",
      "setUp(self=<arxiv_latex_cleaner_test.IntegrationTests testMethod=test_complete_from_dir>, __class__=<class 'arxiv_latex_cleaner_test.IntegrationTests'>, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7f1855667a90>, self._subtest=None, self._testMethodDoc=\"test_complete_from_dir(input_dir='tex')\", self._testMethodName='test_complete_from_dir', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_complete_from_dir=<bound method IntegrationTests.test_complete of <arxiv_latex_cleaner_test.IntegrationTests testMethod=test_complete_from_dir>>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.out_path = 'tex_arXiv'\n",
      "State:\n",
      "'tex_arXiv'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_complete(self, input_dir):\n",
      "    out_path_true = 'tex_arXiv_true'\n",
      "    if path.isdir(self.out_path):\n",
      "      raise RuntimeError(\n",
      "          'The folder {:s} should not exist.'.format(self.out_path)\n",
      "      )\n",
      "    arxiv_latex_cleaner.run_arxiv_cleaner({\n",
      "        'input_folder': input_dir,\n",
      "        'images_allowlist': {\n",
      "            'images/im2_included.jpg': 200,\n",
      "            'images/im3_included.png': 400,\n",
      "        },\n",
      "        'resize_images': True,\n",
      "        'im_size': 100,\n",
      "        'compress_pdf': False,\n",
      "        'pdf_im_resolution': 500,\n",
      "        'commands_to_delete': ['mytodo'],\n",
      "        'commands_only_to_delete': ['red'],\n",
      "        'environments_to_delete': ['mynote'],\n",
      "        'use_external_tikz': 'ext_tikz',\n",
      "        'keep_bib': False,\n",
      "    })\n",
      "    out_files = set(arxiv_latex_cleaner._list_all_files(self.out_path))\n",
      "    out_files_true = set(arxiv_latex_cleaner._list_all_files(out_path_true))\n",
      "    self.assertEqual(out_files, out_files_true)\n",
      "    for f1 in out_files:\n",
      "      self._compare_files(\n",
      "          path.join(self.out_path, f1), path.join(out_path_true, f1)\n",
      "      )\n",
      "test_complete(self=<arxiv_latex_cleaner_test.IntegrationTests testMethod=test_complete_from_dir>, input_dir='tex', self._cleanups=[(<bound method ExitStack.close of <contextlib.ExitStack object at 0x7f1855667d00>>, (), {})], self._exit_stack=<contextlib.ExitStack object at 0x7f1855667d00>, self._outcome=<unittest.case._Outcome object at 0x7f1855667a90>, self._subtest=None, self._testMethodDoc=\"test_complete_from_dir(input_dir='tex')\", self._testMethodName='test_complete_from_dir', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.out_path='tex_arXiv', self.test_complete_from_dir=<bound method IntegrationTests.test_complete of <arxiv_latex_cleaner_test.IntegrationTests testMethod=test_complete_from_dir>>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "out_path_true = 'tex_arXiv_true'\n",
      "State:\n",
      "'tex_arXiv_true'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def run_arxiv_cleaner(parameters):\n",
      "  files_to_delete = [\n",
      "      r'\\.aux$',\n",
      "      r'\\.sh$',\n",
      "      r'\\.blg$',\n",
      "      r'\\.brf$',\n",
      "      r'\\.log$',\n",
      "      r'\\.out$',\n",
      "      r'\\.ps$',\n",
      "      r'\\.dvi$',\n",
      "      r'\\.synctex.gz$',\n",
      "      '~$',\n",
      "      r'\\.backup$',\n",
      "      r'\\.gitignore$',\n",
      "      r'\\.DS_Store$',\n",
      "      r'\\.svg$',\n",
      "      r'^\\.idea',\n",
      "      r'\\.dpth$',\n",
      "      r'\\.md5$',\n",
      "      r'\\.dep$',\n",
      "      r'\\.auxlock$',\n",
      "      r'\\.fls$',\n",
      "      r'\\.fdb_latexmk$',\n",
      "  ]\n",
      "  if not parameters['keep_bib']:\n",
      "    files_to_delete.append(r'\\.bib$')\n",
      "  parameters.update({\n",
      "      'to_delete': files_to_delete,\n",
      "      'figures_to_copy_if_referenced': [\n",
      "          r'\\.png$',\n",
      "          r'\\.jpg$',\n",
      "          r'\\.jpeg$',\n",
      "          r'\\.pdf$',\n",
      "      ],\n",
      "  })\n",
      "  logging.info('Collecting file structure.')\n",
      "  parameters['output_folder'] = _create_out_folder(parameters['input_folder'])\n",
      "  from_zip = parameters['input_folder'].endswith('.zip')\n",
      "  tempdir_context = (\n",
      "      tempfile.TemporaryDirectory() if from_zip else contextlib.suppress()\n",
      "  )\n",
      "  with tempdir_context as tempdir:\n",
      "    if from_zip:\n",
      "      logging.info('Unzipping input folder.')\n",
      "      shutil.unpack_archive(parameters['input_folder'], tempdir)\n",
      "      parameters['input_folder'] = tempdir\n",
      "    splits = _split_all_files(parameters)\n",
      "    logging.info('Reading all tex files')\n",
      "    tex_contents = _read_all_tex_contents(\n",
      "        splits['tex_in_root'] + splits['tex_not_in_root'], parameters\n",
      "    )\n",
      "    for tex_file in tex_contents:\n",
      "      logging.info('Removing comments in file %s.', tex_file)\n",
      "      tex_contents[tex_file] = _remove_comments_and_commands_to_delete(\n",
      "          tex_contents[tex_file], parameters\n",
      "      )\n",
      "    for tex_file in tex_contents:\n",
      "      logging.info('Replacing \\\\includesvg calls in file %s.', tex_file)\n",
      "      tex_contents[tex_file] = _replace_includesvg(\n",
      "          tex_contents[tex_file], splits['svg_inkscape']\n",
      "      )\n",
      "    for tex_file in tex_contents:\n",
      "      logging.info('Replacing Tikz Pictures in file %s.', tex_file)\n",
      "      content = _replace_tikzpictures(\n",
      "          tex_contents[tex_file], splits['external_tikz_figures']\n",
      "      )\n",
      "      tex_contents[tex_file] = content.split('\\n')\n",
      "    _keep_only_referenced_tex(tex_contents, splits)\n",
      "    _add_root_tex_files(splits)\n",
      "    for tex_file in splits['tex_to_copy']:\n",
      "      logging.info('Replacing patterns in file %s.', tex_file)\n",
      "      content = '\\n'.join(tex_contents[tex_file])\n",
      "      content = _find_and_replace_patterns(\n",
      "          content, parameters.get('patterns_and_insertions', list())\n",
      "      )\n",
      "      tex_contents[tex_file] = content\n",
      "      new_path = os.path.join(parameters['output_folder'], tex_file)\n",
      "      logging.info('Writing modified contents to %s.', new_path)\n",
      "      _write_file_content(\n",
      "          content,\n",
      "          new_path,\n",
      "      )\n",
      "    full_content = '\\n'.join(\n",
      "        ''.join(tex_contents[fn]) for fn in splits['tex_to_copy']\n",
      "    )\n",
      "    _copy_only_referenced_non_tex_not_in_root(parameters, full_content, splits)\n",
      "    for non_tex_file in splits['non_tex_in_root']:\n",
      "      logging.info('Copying non-tex file %s.', non_tex_file)\n",
      "      _copy_file(non_tex_file, parameters)\n",
      "    _resize_and_copy_figures_if_referenced(parameters, full_content, splits)\n",
      "    logging.info('Outputs written to %s', parameters['output_folder'])\n",
      "run_arxiv_cleaner(parameters={'input_folder': 'tex', 'images_allowlist': {'images/im2_included.jpg': 200, 'images/im3_included.png': 400}, 'resize_images': True, 'im_size': 100, 'compress_pdf': False, 'pdf_im_resolution': 500, 'commands_to_delete': ['mytodo'], 'commands_only_to_delete': ['red'], 'environments_to_delete': ['mynote'], 'use_external_tikz': 'ext_tikz', 'keep_bib': False})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "files_to_delete = [\n",
      "State:\n",
      "['\\\\.aux$', '\\\\.sh$', '\\\\.blg$', '\\\\.brf$', '\\\\.log$', '\\\\.out$', '\\\\.ps$', '\\\\.dvi$', '\\\\.synctex.gz$', '~$', '\\\\.backup$', '\\\\.gitignore$', '\\\\.DS_Store$', '\\\\.svg$', '^\\\\.idea', '\\\\.dpth$', '\\\\.md5$', '\\\\.dep$', '\\\\.auxlock$', '\\\\.fls$', '\\\\.fdb_latexmk$']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _split_all_files(parameters):\n",
      "  file_splits = {\n",
      "      'all': _list_all_files(\n",
      "          parameters['input_folder'], ignore_dirs=['.git' + os.sep]\n",
      "      ),\n",
      "      'in_root': [\n",
      "          f\n",
      "          for f in os.listdir(parameters['input_folder'])\n",
      "          if os.path.isfile(os.path.join(parameters['input_folder'], f))\n",
      "      ],\n",
      "  }\n",
      "  file_splits['not_in_root'] = [\n",
      "      f for f in file_splits['all'] if f not in file_splits['in_root']\n",
      "  ]\n",
      "  file_splits['to_copy_in_root'] = _remove_pattern(\n",
      "      file_splits['in_root'],\n",
      "      parameters['to_delete'] + parameters['figures_to_copy_if_referenced'],\n",
      "  )\n",
      "  file_splits['to_copy_not_in_root'] = _remove_pattern(\n",
      "      file_splits['not_in_root'],\n",
      "      parameters['to_delete'] + parameters['figures_to_copy_if_referenced'],\n",
      "  )\n",
      "  file_splits['figures'] = _keep_pattern(\n",
      "      file_splits['all'], parameters['figures_to_copy_if_referenced']\n",
      "  )\n",
      "  file_splits['tex_in_root'] = _keep_pattern(\n",
      "      file_splits['to_copy_in_root'], ['.tex$', '.tikz$']\n",
      "  )\n",
      "  file_splits['tex_not_in_root'] = _keep_pattern(\n",
      "      file_splits['to_copy_not_in_root'], ['.tex$', '.tikz$']\n",
      "  )\n",
      "  file_splits['non_tex_in_root'] = _remove_pattern(\n",
      "      file_splits['to_copy_in_root'], ['.tex$', '.tikz$']\n",
      "  )\n",
      "  file_splits['non_tex_not_in_root'] = _remove_pattern(\n",
      "      file_splits['to_copy_not_in_root'], ['.tex$', '.tikz$']\n",
      "  )\n",
      "  if parameters.get('use_external_tikz', None) is not None:\n",
      "    file_splits['external_tikz_figures'] = _keep_pattern(\n",
      "        file_splits['all'], [parameters['use_external_tikz']]\n",
      "    )\n",
      "  else:\n",
      "    file_splits['external_tikz_figures'] = []\n",
      "  if parameters.get('svg_inkscape', None) is not None:\n",
      "    file_splits['svg_inkscape'] = _keep_pattern(\n",
      "        file_splits['all'], [parameters['svg_inkscape']]\n",
      "    )\n",
      "  else:\n",
      "    file_splits['svg_inkscape'] = []\n",
      "  return file_splits\n",
      "_split_all_files(parameters={'input_folder': 'tex', 'images_allowlist': {'images/im2_included.jpg': 200, 'images/im3_included.png': 400}, 'resize_images': True, 'im_size': 100, 'compress_pdf': False, 'pdf_im_resolution': 500, 'commands_to_delete': ['mytodo'], 'commands_only_to_delete': ['red'], 'environments_to_delete': ['mynote'], 'use_external_tikz': 'ext_tikz', 'keep_bib': False, 'to_delete': ['\\\\.aux$', '\\\\.sh$', '\\\\.blg$', '\\\\.brf$', '\\\\.log$', '\\\\.out$', '\\\\.ps$', '\\\\.dvi$', '\\\\.synctex.gz$', '~$', '\\\\.backup$', '\\\\.gitignore$', '\\\\.DS_Store$', '\\\\.svg$', '^\\\\.idea', '\\\\.dpth$', '\\\\.md5$', '\\\\.dep$', '\\\\.auxlock$', '\\\\.fls$', '\\\\.fdb_latexmk$', '\\\\.bib$'], 'figures_to_copy_if_referenced': ['\\\\.png$', '\\\\.jpg$', '\\\\.jpeg$', '\\\\.pdf$'], 'output_folder': '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/google-research+arxiv-latex-cleaner/google-research+arxiv-latex-cleaner/tex_arXiv'})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "file_splits = {\n",
      "State:\n",
      "{'all': ['main.bib', 'main.bbl', 'main.tex', 'main.aux', 'ext_tikz/test2.pdf', 'ext_tikz/test1.pdf', 'ext_tikz/figure_not_included.pdf', 'figures/data_not_included.txt', 'figures/figure_not_included.tex', 'figures/figure_not_included_2.tex', 'figures/figure_included.tikz', 'figures/figure_included.tex', 'figures/data_included.txt', 'not_included/figures/data_included.txt', 'images/im4_included.png', 'images/im1.png', 'images/im4_not_included.png', 'images/im3_included.png', 'images/im2_included.jpg', 'images/im5_not_included.jpg', 'images/im5_included.jpg', 'images/im1_included.png', 'images/im_not_included.png', 'images/include/images/im3_included.png'], 'in_root': ['main.bib', 'main.bbl', 'main.tex', 'main.aux']}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _split_all_files(parameters):\n",
      "  file_splits = {\n",
      "      'all': _list_all_files(\n",
      "          parameters['input_folder'], ignore_dirs=['.git' + os.sep]\n",
      "      ),\n",
      "      'in_root': [\n",
      "          f\n",
      "          for f in os.listdir(parameters['input_folder'])\n",
      "          if os.path.isfile(os.path.join(parameters['input_folder'], f))\n",
      "      ],\n",
      "  }\n",
      "  file_splits['not_in_root'] = [\n",
      "      f for f in file_splits['all'] if f not in file_splits['in_root']\n",
      "  ]\n",
      "  file_splits['to_copy_in_root'] = _remove_pattern(\n",
      "      file_splits['in_root'],\n",
      "      parameters['to_delete'] + parameters['figures_to_copy_if_referenced'],\n",
      "  )\n",
      "  file_splits['to_copy_not_in_root'] = _remove_pattern(\n",
      "      file_splits['not_in_root'],\n",
      "      parameters['to_delete'] + parameters['figures_to_copy_if_referenced'],\n",
      "  )\n",
      "  file_splits['figures'] = _keep_pattern(\n",
      "      file_splits['all'], parameters['figures_to_copy_if_referenced']\n",
      "  )\n",
      "  file_splits['tex_in_root'] = _keep_pattern(\n",
      "      file_splits['to_copy_in_root'], ['.tex$', '.tikz$']\n",
      "  )\n",
      "  file_splits['tex_not_in_root'] = _keep_pattern(\n",
      "      file_splits['to_copy_not_in_root'], ['.tex$', '.tikz$']\n",
      "  )\n",
      "  file_splits['non_tex_in_root'] = _remove_pattern(\n",
      "      file_splits['to_copy_in_root'], ['.tex$', '.tikz$']\n",
      "  )\n",
      "  file_splits['non_tex_not_in_root'] = _remove_pattern(\n",
      "      file_splits['to_copy_not_in_root'], ['.tex$', '.tikz$']\n",
      "  )\n",
      "  if parameters.get('use_external_tikz', None) is not None:\n",
      "    file_splits['external_tikz_figures'] = _keep_pattern(\n",
      "        file_splits['all'], [parameters['use_external_tikz']]\n",
      "    )\n",
      "  else:\n",
      "    file_splits['external_tikz_figures'] = []\n",
      "  if parameters.get('svg_inkscape', None) is not None:\n",
      "    file_splits['svg_inkscape'] = _keep_pattern(\n",
      "        file_splits['all'], [parameters['svg_inkscape']]\n",
      "    )\n",
      "  else:\n",
      "    file_splits['svg_inkscape'] = []\n",
      "  return file_splits\n",
      "_split_all_files(parameters={'input_folder': 'tex', 'images_allowlist': {'images/im2_included.jpg': 200, 'images/im3_included.png': 400}, 'resize_images': True, 'im_size': 100, 'compress_pdf': False, 'pdf_im_resolution': 500, 'commands_to_delete': ['mytodo'], 'commands_only_to_delete': ['red'], 'environments_to_delete': ['mynote'], 'use_external_tikz': 'ext_tikz', 'keep_bib': False, 'to_delete': ['\\\\.aux$', '\\\\.sh$', '\\\\.blg$', '\\\\.brf$', '\\\\.log$', '\\\\.out$', '\\\\.ps$', '\\\\.dvi$', '\\\\.synctex.gz$', '~$', '\\\\.backup$', '\\\\.gitignore$', '\\\\.DS_Store$', '\\\\.svg$', '^\\\\.idea', '\\\\.dpth$', '\\\\.md5$', '\\\\.dep$', '\\\\.auxlock$', '\\\\.fls$', '\\\\.fdb_latexmk$', '\\\\.bib$'], 'figures_to_copy_if_referenced': ['\\\\.png$', '\\\\.jpg$', '\\\\.jpeg$', '\\\\.pdf$'], 'output_folder': '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/google-research+arxiv-latex-cleaner/google-research+arxiv-latex-cleaner/tex_arXiv'})\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "file_splits['to_copy_not_in_root'] = _remove_pattern(\n",
      "State:\n",
      "{'all': ['main.bib', 'main.bbl', 'main.tex', 'main.aux', 'ext_tikz/test2.pdf', 'ext_tikz/test1.pdf', 'ext_tikz/figure_not_included.pdf', 'figures/data_not_included.txt', 'figures/figure_not_included.tex', 'figures/figure_not_included_2.tex', 'figures/figure_included.tikz', 'figures/figure_included.tex', 'figures/data_included.txt', 'not_included/figures/data_included.txt', 'images/im4_included.png', 'images/im1.png', 'images/im4_not_included.png', 'images/im3_included.png', 'images/im2_included.jpg', 'images/im5_not_included.jpg', 'images/im5_included.jpg', 'images/im1_included.png', 'images/im_not_included.png', 'images/include/images/im3_included.png'], 'in_root': ['main.bib', 'main.bbl', 'main.tex', 'main.aux'], 'not_in_root': ['ext_tikz/test2.pdf', 'ext_tikz/test1.pdf', 'ext_tikz/figure_not_included.pdf', 'figures/data_not_included.txt', 'figures/figure_not_included.tex', 'figures/figure_not_included_2.tex', 'figures/figure_included.tikz', 'figures/figure_included.tex', 'figures/data_included.txt', 'not_included/figures/data_included.txt', 'images/im4_included.png', 'images/im1.png', 'images/im4_not_included.png', 'images/im3_included.png', 'images/im2_included.jpg', 'images/im5_not_included.jpg', 'images/im5_included.jpg', 'images/im1_included.png', 'images/im_not_included.png', 'images/include/images/im3_included.png'], 'to_copy_in_root': ['main.bbl', 'main.tex'], 'to_copy_not_in_root': ['figures/data_not_included.txt', 'figures/figure_not_included.tex', 'figures/figure_not_included_2.tex', 'figures/figure_included.tikz', 'figures/figure_included.tex', 'figures/data_included.txt', 'not_included/figures/data_included.txt']}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _list_all_files(in_folder, ignore_dirs=None):\n",
      "  if ignore_dirs is None:\n",
      "    ignore_dirs = []\n",
      "  to_consider = [\n",
      "      os.path.join(os.path.relpath(path, in_folder), name)\n",
      "      if path != in_folder\n",
      "      else name\n",
      "      for path, _, files in os.walk(in_folder)\n",
      "      for name in files\n",
      "  ]\n",
      "  return _remove_pattern(to_consider, ignore_dirs)\n",
      "_list_all_files(in_folder='tex', ignore_dirs=['.git/'])\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "to_consider = [\n",
      "State:\n",
      "['main.bib', 'main.bbl', 'main.tex', 'main.aux', 'ext_tikz/test2.pdf', 'ext_tikz/test1.pdf', 'ext_tikz/figure_not_included.pdf', 'figures/data_not_included.txt', 'figures/figure_not_included.tex', 'figures/figure_not_included_2.tex', 'figures/figure_included.tikz', 'figures/figure_included.tex', 'figures/data_included.txt', 'not_included/figures/data_included.txt', 'images/im4_included.png', 'images/im1.png', 'images/im4_not_included.png', 'images/im3_included.png', 'images/im2_included.jpg', 'images/im5_not_included.jpg', 'images/im5_included.jpg', 'images/im1_included.png', 'images/im_not_included.png', 'images/include/images/im3_included.png']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _read_all_tex_contents(tex_files, parameters):\n",
      "  contents = {}\n",
      "  for fn in tex_files:\n",
      "    contents[fn] = _read_file_content(\n",
      "        os.path.join(parameters['input_folder'], fn)\n",
      "    )\n",
      "  return contents\n",
      "_read_all_tex_contents(tex_files=['main.tex', 'figures/figure_not_included.tex', 'figures/figure_not_included_2.tex', 'figures/figure_included.tikz', 'figures/figure_included.tex'], parameters={'input_folder': 'tex', 'images_allowlist': {'images/im2_included.jpg': 200, 'images/im3_included.png': 400}, 'resize_images': True, 'im_size': 100, 'compress_pdf': False, 'pdf_im_resolution': 500, 'commands_to_delete': ['mytodo'], 'commands_only_to_delete': ['red'], 'environments_to_delete': ['mynote'], 'use_external_tikz': 'ext_tikz', 'keep_bib': False, 'to_delete': ['\\\\.aux$', '\\\\.sh$', '\\\\.blg$', '\\\\.brf$', '\\\\.log$', '\\\\.out$', '\\\\.ps$', '\\\\.dvi$', '\\\\.synctex.gz$', '~$', '\\\\.backup$', '\\\\.gitignore$', '\\\\.DS_Store$', '\\\\.svg$', '^\\\\.idea', '\\\\.dpth$', '\\\\.md5$', '\\\\.dep$', '\\\\.auxlock$', '\\\\.fls$', '\\\\.fdb_latexmk$', '\\\\.bib$'], 'figures_to_copy_if_referenced': ['\\\\.png$', '\\\\.jpg$', '\\\\.jpeg$', '\\\\.pdf$'], 'output_folder': '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/google-research+arxiv-latex-cleaner/google-research+arxiv-latex-cleaner/tex_arXiv'})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "contents = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _read_file_content(filename):\n",
      "  with open(filename, 'r', encoding='utf-8') as fp:\n",
      "    lines = fp.readlines()\n",
      "    lines = _strip_tex_contents(lines, '\\\\end{document}')\n",
      "    return lines\n",
      "_read_file_content(filename='tex/main.tex')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "lines = fp.readlines()\n",
      "State:\n",
      "['\\\\begin{document}\\n', 'Text\\n', '% Whole line comment\\n', '\\n', 'Text% Inline comment\\n', '\\\\begin{comment}\\n', 'This is an environment comment.\\n', '\\\\end{comment}\\n', '\\n', 'This is a percent \\\\%.\\n', '% Whole line comment without newline\\n', '\\\\includegraphics{images/im1_included.png}\\n', '%\\\\includegraphics{images/im_not_included}\\n', '\\\\includegraphics{images/im3_included.png}\\n', '\\\\includegraphics{%\\n', '  images/im4_included.png%\\n', '  }\\n', '\\\\includegraphics[width=.5\\\\linewidth]{%\\n', '  images/im5_included.jpg}\\n', '%\\\\includegraphics{%\\n', '%  images/im4_not_included.png\\n', '%  }\\n', '%\\\\includegraphics[width=.5\\\\linewidth]{%\\n', '%  images/im5_not_included.jpg}\\n', '\\n', '% test whatever the path satrting with dot works when include graphics\\n', '\\\\includegraphics{./images/im3_included.png}\\n', '\\n', 'This line should\\\\mytodo{Do this later} not be separated\\n', '\\\\mytodo{This is a todo command with a nested \\\\textit{command}.\\n', 'Please remember that up to \\\\texttt{2 levels} of \\\\textit{nesting} are supported.}\\n', 'from this one.\\n', '\\n', '\\\\begin{mynote}\\n', '  This is a custom environment that could be excluded.\\n', '\\\\end{mynote}\\n', '\\n', '\\\\newif\\\\ifvar\\n', '\\n', '\\\\ifvar\\n', '\\\\if    false\\n', '\\\\if false\\n', '\\\\if 0\\n', '\\\\iffalse\\n', '\\\\ifvar\\n', 'Text\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\n', '\\\\newcommand{\\\\red}[1]{{\\\\color{red} #1}}\\n', 'hello test \\\\red{hello\\n', 'test \\\\red{hello}}\\n', 'test\\n', '\\n', '% content after this line should not be cleaned if \\\\end{document} is in a comment\\n', '\\n', '\\\\input{figures/figure_included.tex}\\n', '% \\\\input{figures/figure_not_included.tex}\\n', '\\n', '% Test for tikzpicture feature\\n', '% should be replaced\\n', '\\\\tikzsetnextfilename{test1}\\n', '\\\\begin{tikzpicture}\\n', '    \\\\node (test) at (0,0) {Test1};\\n', '\\\\end{tikzpicture}\\n', '\\n', '% should be replaced in included file\\n', '\\\\input{figures/figure_included.tikz}\\n', '\\n', '% should not be be replaced - no preceding tikzsetnextfilename command\\n', '\\\\begin{tikzpicture}\\n', '    \\\\node (test) at (0,0) {Test3};\\n', '\\\\end{tikzpicture}\\n', '\\n', '\\\\tikzsetnextfilename{test_no_match}\\n', '\\\\begin{tikzpicture}\\n', '    \\\\node (test) at (0,0) {Test4};\\n', '\\\\end{tikzpicture}\\n', '\\n', '\\\\end{document}\\n', '\\n', 'This should be ignored.\\n']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _read_file_content(filename):\n",
      "  with open(filename, 'r', encoding='utf-8') as fp:\n",
      "    lines = fp.readlines()\n",
      "    lines = _strip_tex_contents(lines, '\\\\end{document}')\n",
      "    return lines\n",
      "_read_file_content(filename='tex/main.tex')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "lines = _strip_tex_contents(lines, '\\\\end{document}')\n",
      "State:\n",
      "['\\\\begin{document}\\n', 'Text\\n', '% Whole line comment\\n', '\\n', 'Text% Inline comment\\n', '\\\\begin{comment}\\n', 'This is an environment comment.\\n', '\\\\end{comment}\\n', '\\n', 'This is a percent \\\\%.\\n', '% Whole line comment without newline\\n', '\\\\includegraphics{images/im1_included.png}\\n', '%\\\\includegraphics{images/im_not_included}\\n', '\\\\includegraphics{images/im3_included.png}\\n', '\\\\includegraphics{%\\n', '  images/im4_included.png%\\n', '  }\\n', '\\\\includegraphics[width=.5\\\\linewidth]{%\\n', '  images/im5_included.jpg}\\n', '%\\\\includegraphics{%\\n', '%  images/im4_not_included.png\\n', '%  }\\n', '%\\\\includegraphics[width=.5\\\\linewidth]{%\\n', '%  images/im5_not_included.jpg}\\n', '\\n', '% test whatever the path satrting with dot works when include graphics\\n', '\\\\includegraphics{./images/im3_included.png}\\n', '\\n', 'This line should\\\\mytodo{Do this later} not be separated\\n', '\\\\mytodo{This is a todo command with a nested \\\\textit{command}.\\n', 'Please remember that up to \\\\texttt{2 levels} of \\\\textit{nesting} are supported.}\\n', 'from this one.\\n', '\\n', '\\\\begin{mynote}\\n', '  This is a custom environment that could be excluded.\\n', '\\\\end{mynote}\\n', '\\n', '\\\\newif\\\\ifvar\\n', '\\n', '\\\\ifvar\\n', '\\\\if    false\\n', '\\\\if false\\n', '\\\\if 0\\n', '\\\\iffalse\\n', '\\\\ifvar\\n', 'Text\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\n', '\\\\newcommand{\\\\red}[1]{{\\\\color{red} #1}}\\n', 'hello test \\\\red{hello\\n', 'test \\\\red{hello}}\\n', 'test\\n', '\\n', '% content after this line should not be cleaned if \\\\end{document} is in a comment\\n', '\\n', '\\\\input{figures/figure_included.tex}\\n', '% \\\\input{figures/figure_not_included.tex}\\n', '\\n', '% Test for tikzpicture feature\\n', '% should be replaced\\n', '\\\\tikzsetnextfilename{test1}\\n', '\\\\begin{tikzpicture}\\n', '    \\\\node (test) at (0,0) {Test1};\\n', '\\\\end{tikzpicture}\\n', '\\n', '% should be replaced in included file\\n', '\\\\input{figures/figure_included.tikz}\\n', '\\n', '% should not be be replaced - no preceding tikzsetnextfilename command\\n', '\\\\begin{tikzpicture}\\n', '    \\\\node (test) at (0,0) {Test3};\\n', '\\\\end{tikzpicture}\\n', '\\n', '\\\\tikzsetnextfilename{test_no_match}\\n', '\\\\begin{tikzpicture}\\n', '    \\\\node (test) at (0,0) {Test4};\\n', '\\\\end{tikzpicture}\\n', '\\n', '\\\\end{document}\\n']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _remove_comments_and_commands_to_delete(content, parameters):\n",
      "  content = [_remove_comments_inline(line) for line in content]\n",
      "  content = _remove_environment(''.join(content), 'comment')\n",
      "  content = _remove_iffalse_block(content)\n",
      "  for environment in parameters.get('environments_to_delete', []):\n",
      "    content = _remove_environment(content, environment)\n",
      "  for command in parameters.get('commands_only_to_delete', []):\n",
      "    content = _remove_command(content, command, True)\n",
      "  for command in parameters['commands_to_delete']:\n",
      "    content = _remove_command(content, command, False)\n",
      "  return content\n",
      "_remove_comments_and_commands_to_delete(content=['\\\\begin{document}\\n', 'Text\\n', '% Whole line comment\\n', '\\n', 'Text% Inline comment\\n', '\\\\begin{comment}\\n', 'This is an environment comment.\\n', '\\\\end{comment}\\n', '\\n', 'This is a percent \\\\%.\\n', '% Whole line comment without newline\\n', '\\\\includegraphics{images/im1_included.png}\\n', '%\\\\includegraphics{images/im_not_included}\\n', '\\\\includegraphics{images/im3_included.png}\\n', '\\\\includegraphics{%\\n', '  images/im4_included.png%\\n', '  }\\n', '\\\\includegraphics[width=.5\\\\linewidth]{%\\n', '  images/im5_included.jpg}\\n', '%\\\\includegraphics{%\\n', '%  images/im4_not_included.png\\n', '%  }\\n', '%\\\\includegraphics[width=.5\\\\linewidth]{%\\n', '%  images/im5_not_included.jpg}\\n', '\\n', '% test whatever the path satrting with dot works when include graphics\\n', '\\\\includegraphics{./images/im3_included.png}\\n', '\\n', 'This line should\\\\mytodo{Do this later} not be separated\\n', '\\\\mytodo{This is a todo command with a nested \\\\textit{command}.\\n', 'Please remember that up to \\\\texttt{2 levels} of \\\\textit{nesting} are supported.}\\n', 'from this one.\\n', '\\n', '\\\\begin{mynote}\\n', '  This is a custom environment that could be excluded.\\n', '\\\\end{mynote}\\n', '\\n', '\\\\newif\\\\ifvar\\n', '\\n', '\\\\ifvar\\n', '\\\\if    false\\n', '\\\\if false\\n', '\\\\if 0\\n', '\\\\iffalse\\n', '\\\\ifvar\\n', 'Text\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\n', '\\\\newcommand{\\\\red}[1]{{\\\\color{red}\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "content = [_remove_comments_inline(line) for line in content]\n",
      "State:\n",
      "['\\\\begin{document}\\n', 'Text\\n', '', '\\n', 'Text%\\n', '\\\\begin{comment}\\n', 'This is an environment comment.\\n', '\\\\end{comment}\\n', '\\n', 'This is a percent \\\\%.\\n', '', '\\\\includegraphics{images/im1_included.png}\\n', '', '\\\\includegraphics{images/im3_included.png}\\n', '\\\\includegraphics{%\\n', '  images/im4_included.png%\\n', '  }\\n', '\\\\includegraphics[width=.5\\\\linewidth]{%\\n', '  images/im5_included.jpg}\\n', '', '', '', '', '', '\\n', '', '\\\\includegraphics{./images/im3_included.png}\\n', '\\n', 'This line should\\\\mytodo{Do this later} not be separated\\n', '\\\\mytodo{This is a todo command with a nested \\\\textit{command}.\\n', 'Please remember that up to \\\\texttt{2 levels} of \\\\textit{nesting} are supported.}\\n', 'from this one.\\n', '\\n', '\\\\begin{mynote}\\n', '  This is a custom environment that could be excluded.\\n', '\\\\end{mynote}\\n', '\\n', '\\\\newif\\\\ifvar\\n', '\\n', '\\\\ifvar\\n', '\\\\if    false\\n', '\\\\if false\\n', '\\\\if 0\\n', '\\\\iffalse\\n', '\\\\ifvar\\n', 'Text\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\n', '\\\\newcommand{\\\\red}[1]{{\\\\color{red} #1}}\\n', 'hello test \\\\red{hello\\n', 'test \\\\red{hello}}\\n', 'test\\n', '\\n', '', '\\n', '\\\\input{figures/figure_included.tex}\\n', '', '\\n', '', '', '\\\\tikzsetnextfilename{test1}\\n', '\\\\begin{tikzpicture}\\n', '    \\\\node (test) at (0,0) {Test1};\\n', '\\\\end{tikzpicture}\\n', '\\n', '', '\\\\input{figures/figure_included.tikz}\\n', '\\n', '', '\\\\begin{tikzpicture}\\n', '    \\\\node (test) at (0,0) {Test3};\\n', '\\\\end{tikzpicture}\\n', '\\n', '\\\\tikzsetnextfilename{test_no_match}\\n', '\\\\begin{tikzpicture}\\n', '    \\\\node (test) at (0,0) {Test4};\\n', '\\\\end{tikzpicture}\\n', '\\n', '\\\\end{document}\\n']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _remove_comments_and_commands_to_delete(content, parameters):\n",
      "  content = [_remove_comments_inline(line) for line in content]\n",
      "  content = _remove_environment(''.join(content), 'comment')\n",
      "  content = _remove_iffalse_block(content)\n",
      "  for environment in parameters.get('environments_to_delete', []):\n",
      "    content = _remove_environment(content, environment)\n",
      "  for command in parameters.get('commands_only_to_delete', []):\n",
      "    content = _remove_command(content, command, True)\n",
      "  for command in parameters['commands_to_delete']:\n",
      "    content = _remove_command(content, command, False)\n",
      "  return content\n",
      "_remove_comments_and_commands_to_delete(content=['\\\\begin{document}\\n', 'Text\\n', '% Whole line comment\\n', '\\n', 'Text% Inline comment\\n', '\\\\begin{comment}\\n', 'This is an environment comment.\\n', '\\\\end{comment}\\n', '\\n', 'This is a percent \\\\%.\\n', '% Whole line comment without newline\\n', '\\\\includegraphics{images/im1_included.png}\\n', '%\\\\includegraphics{images/im_not_included}\\n', '\\\\includegraphics{images/im3_included.png}\\n', '\\\\includegraphics{%\\n', '  images/im4_included.png%\\n', '  }\\n', '\\\\includegraphics[width=.5\\\\linewidth]{%\\n', '  images/im5_included.jpg}\\n', '%\\\\includegraphics{%\\n', '%  images/im4_not_included.png\\n', '%  }\\n', '%\\\\includegraphics[width=.5\\\\linewidth]{%\\n', '%  images/im5_not_included.jpg}\\n', '\\n', '% test whatever the path satrting with dot works when include graphics\\n', '\\\\includegraphics{./images/im3_included.png}\\n', '\\n', 'This line should\\\\mytodo{Do this later} not be separated\\n', '\\\\mytodo{This is a todo command with a nested \\\\textit{command}.\\n', 'Please remember that up to \\\\texttt{2 levels} of \\\\textit{nesting} are supported.}\\n', 'from this one.\\n', '\\n', '\\\\begin{mynote}\\n', '  This is a custom environment that could be excluded.\\n', '\\\\end{mynote}\\n', '\\n', '\\\\newif\\\\ifvar\\n', '\\n', '\\\\ifvar\\n', '\\\\if    false\\n', '\\\\if false\\n', '\\\\if 0\\n', '\\\\iffalse\\n', '\\\\ifvar\\n', 'Text\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\n', '\\\\newcommand{\\\\red}[1]{{\\\\color{red}\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "content = _remove_environment(''.join(content), 'comment')\n",
      "State:\n",
      "'\\\\begin{document}\\nText\\n\\nText%\\n\\n\\nThis is a percent \\\\%.\\n\\\\includegraphics{images/im1_included.png}\\n\\\\includegraphics{images/im3_included.png}\\n\\\\includegraphics{%\\n  images/im4_included.png%\\n  }\\n\\\\includegraphics[width=.5\\\\linewidth]{%\\n  images/im5_included.jpg}\\n\\n\\\\includegraphics{./images/im3_included.png}\\n\\nThis line should\\\\mytodo{Do this later} not be separated\\n\\\\mytodo{This is a todo command with a nested \\\\textit{command}.\\nPlease remember that up to \\\\texttt{2 levels} of \\\\textit{nesting} are supported.}\\nfrom this one.\\n\\n\\\\begin{mynote}\\n  This is a custom environment that could be excluded.\\n\\\\end{mynote}\\n\\n\\\\newif\\\\ifvar\\n\\n\\\\ifvar\\n\\\\if    false\\n\\\\if false\\n\\\\if 0\\n\\\\iffalse\\n\\\\ifvar\\nText\\n\\\\fi\\n\\\\fi\\n\\\\fi\\n\\\\fi\\n\\\\fi\\n\\\\fi\\n\\n\\\\newcommand{\\\\red}[1]{{\\\\color{red} #1}}\\nhello test \\\\red{hello\\ntest \\\\red{hello}}\\ntest\\n\\n\\n\\\\input{figures/figure_included.tex}\\n\\n\\\\tikzsetnextfilename{test1}\\n\\\\begin{tikzpicture}\\n    \\\\node (test) at (0,0) {Test1};\\n\\\\end{tikzpicture}\\n\\n\\\\input{figures/figure_included.tikz}\\n\\n\\\\begin{tikzpicture}\\n    \\\\node (test) at (0,0) {Test3};\\n\\\\end{tikzpicture}\\n\\n\\\\tikzsetnextfilename{test_no_match}\\n\\\\begin{tikzpicture}\\n    \\\\node (test) at (0,0) {Test4};\\n\\\\end{tikzpicture}\\n\\n\\\\end{document}\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _remove_comments_and_commands_to_delete(content, parameters):\n",
      "  content = [_remove_comments_inline(line) for line in content]\n",
      "  content = _remove_environment(''.join(content), 'comment')\n",
      "  content = _remove_iffalse_block(content)\n",
      "  for environment in parameters.get('environments_to_delete', []):\n",
      "    content = _remove_environment(content, environment)\n",
      "  for command in parameters.get('commands_only_to_delete', []):\n",
      "    content = _remove_command(content, command, True)\n",
      "  for command in parameters['commands_to_delete']:\n",
      "    content = _remove_command(content, command, False)\n",
      "  return content\n",
      "_remove_comments_and_commands_to_delete(content=['\\\\begin{document}\\n', 'Text\\n', '% Whole line comment\\n', '\\n', 'Text% Inline comment\\n', '\\\\begin{comment}\\n', 'This is an environment comment.\\n', '\\\\end{comment}\\n', '\\n', 'This is a percent \\\\%.\\n', '% Whole line comment without newline\\n', '\\\\includegraphics{images/im1_included.png}\\n', '%\\\\includegraphics{images/im_not_included}\\n', '\\\\includegraphics{images/im3_included.png}\\n', '\\\\includegraphics{%\\n', '  images/im4_included.png%\\n', '  }\\n', '\\\\includegraphics[width=.5\\\\linewidth]{%\\n', '  images/im5_included.jpg}\\n', '%\\\\includegraphics{%\\n', '%  images/im4_not_included.png\\n', '%  }\\n', '%\\\\includegraphics[width=.5\\\\linewidth]{%\\n', '%  images/im5_not_included.jpg}\\n', '\\n', '% test whatever the path satrting with dot works when include graphics\\n', '\\\\includegraphics{./images/im3_included.png}\\n', '\\n', 'This line should\\\\mytodo{Do this later} not be separated\\n', '\\\\mytodo{This is a todo command with a nested \\\\textit{command}.\\n', 'Please remember that up to \\\\texttt{2 levels} of \\\\textit{nesting} are supported.}\\n', 'from this one.\\n', '\\n', '\\\\begin{mynote}\\n', '  This is a custom environment that could be excluded.\\n', '\\\\end{mynote}\\n', '\\n', '\\\\newif\\\\ifvar\\n', '\\n', '\\\\ifvar\\n', '\\\\if    false\\n', '\\\\if false\\n', '\\\\if 0\\n', '\\\\iffalse\\n', '\\\\ifvar\\n', 'Text\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\\\fi\\n', '\\n', '\\\\newcommand{\\\\red}[1]{{\\\\color{red}\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "content = _remove_command(content, command, False)\n",
      "State:\n",
      "'\\\\begin{document}\\nText\\n\\nText%\\n\\n\\nThis is a percent \\\\%.\\n\\\\includegraphics{images/im1_included.png}\\n\\\\includegraphics{images/im3_included.png}\\n\\\\includegraphics{%\\n  images/im4_included.png%\\n  }\\n\\\\includegraphics[width=.5\\\\linewidth]{%\\n  images/im5_included.jpg}\\n\\n\\\\includegraphics{./images/im3_included.png}\\n\\nThis line should not be separated\\n%\\nfrom this one.\\n\\n\\n\\n\\\\newif\\\\ifvar\\n\\n\\\\ifvar\\n\\\\fi\\n\\n\\\\newcommand{\\\\red}[1]{{\\\\color{red} #1}}\\nhello test hello\\ntest hello\\ntest\\n\\n\\n\\\\input{figures/figure_included.tex}\\n\\n\\\\tikzsetnextfilename{test1}\\n\\\\begin{tikzpicture}\\n    \\\\node (test) at (0,0) {Test1};\\n\\\\end{tikzpicture}\\n\\n\\\\input{figures/figure_included.tikz}\\n\\n\\\\begin{tikzpicture}\\n    \\\\node (test) at (0,0) {Test3};\\n\\\\end{tikzpicture}\\n\\n\\\\tikzsetnextfilename{test_no_match}\\n\\\\begin{tikzpicture}\\n    \\\\node (test) at (0,0) {Test4};\\n\\\\end{tikzpicture}\\n\\n\\\\end{document}\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _compare_files(self, filename, filename_true):\n",
      "    if path.splitext(filename)[1].lower() in ['.jpg', '.jpeg', '.png']:\n",
      "      with Image.open(filename) as im, Image.open(filename_true) as im_true:\n",
      "        self.assertEqual(\n",
      "            im.size,\n",
      "            im_true.size,\n",
      "            'Images {:s} was not resized properly.'.format(filename),\n",
      "        )\n",
      "    else:\n",
      "      with open(filename, 'rb') as f:\n",
      "        processed_content = f.read().splitlines()\n",
      "      with open(filename_true, 'rb') as f:\n",
      "        groundtruth_content = f.read().splitlines()\n",
      "      self.assertEqual(\n",
      "          processed_content,\n",
      "          groundtruth_content,\n",
      "          '{:s} and {:s} are not equal.'.format(filename, filename_true),\n",
      "      )\n",
      "_compare_files(self=<arxiv_latex_cleaner_test.IntegrationTests testMethod=test_complete_from_dir>, filename='tex_arXiv/ext_tikz/test1.pdf', filename_true='tex_arXiv_true/ext_tikz/test1.pdf', self._cleanups=[(<bound method ExitStack.close of <contextlib.ExitStack object at 0x7f1855667d00>>, (), {})], self._exit_stack=<contextlib.ExitStack object at 0x7f1855667d00>, self._outcome=<unittest.case._Outcome object at 0x7f1855667a90>, self._subtest=None, self._testMethodDoc=\"test_complete_from_dir(input_dir='tex')\", self._testMethodName='test_complete_from_dir', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.out_path='tex_arXiv', self.test_complete_from_dir=<bound method IntegrationTests.test_complete of <arxiv_latex_cleaner_test.IntegrationTests testMethod=test_complete_from_dir>>)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "with open(filename_true, 'rb') as f:\n",
      "State:\n",
      "<_io.BufferedReader name='tex_arXiv_true/ext_tikz/test1.pdf'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def apply_changes(file_path: str, changes: List, confirm: bool = False):\n",
      "    with open(file_path) as f:\n",
      "        original_file_lines = f.readlines()\n",
      "    operation_changes = [change for change in changes if \"operation\" in change]\n",
      "    explanations = [\n",
      "        change[\"explanation\"] for change in changes if \"explanation\" in change\n",
      "    ]\n",
      "    operation_changes.sort(key=lambda x: x[\"line\"], reverse=True)\n",
      "    file_lines = original_file_lines.copy()\n",
      "    for change in operation_changes:\n",
      "        operation = change[\"operation\"]\n",
      "        line = change[\"line\"]\n",
      "        content = change[\"content\"]\n",
      "        if operation == \"Replace\":\n",
      "            file_lines[line - 1] = content + \"\\n\"\n",
      "        elif operation == \"Delete\":\n",
      "            del file_lines[line - 1]\n",
      "        elif operation == \"InsertAfter\":\n",
      "            file_lines.insert(line, content + \"\\n\")\n",
      "    cprint(\"Explanations:\", \"blue\")\n",
      "    for explanation in explanations:\n",
      "        cprint(f\"- {explanation}\", \"blue\")\n",
      "    print(\"\\nChanges to be made:\")\n",
      "    diff = difflib.unified_diff(original_file_lines, file_lines, lineterm=\"\")\n",
      "    for line in diff:\n",
      "        if line.startswith(\"+\"):\n",
      "            cprint(line, \"green\", end=\"\")\n",
      "        elif line.startswith(\"-\"):\n",
      "            cprint(line, \"red\", end=\"\")\n",
      "        else:\n",
      "            print(line, end=\"\")\n",
      "    if confirm:\n",
      "        confirmation = input(\"Do you want to apply these changes? (y/n): \")\n",
      "        if confirmation.lower() != \"y\":\n",
      "            print(\"Changes not applied\")\n",
      "            sys.exit(0)\n",
      "    with open(file_path, \"w\") as f:\n",
      "        f.writelines(file_lines)\n",
      "    print(\"Changes applied.\")\n",
      "apply_changes(file_path='/tmp/tmp6qrrn_j9', changes=[{'operation': 'Replace', 'line': 2, 'content': 'new second line'}], confirm=False)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "with open(file_path) as f:\n",
      "State:\n",
      "<_io.TextIOWrapper name='/tmp/tmp6qrrn_j9' mode='r' encoding='UTF-8'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_json_validated_response(mocker, chat_completion_response, nb_retry, fail):\n",
      "    with open(chat_completion_response, 'r') as file:\n",
      "        response = file.read()\n",
      "    mocker.patch(\n",
      "        \"openai.ChatCompletion.create\",\n",
      "        return_value=mock_open_ai_response_object(mocker=mocker, content=response))\n",
      "    if fail:\n",
      "        with pytest.raises(Exception) as err:\n",
      "            json_response = json_validated_response(\"gpt-4\", [\n",
      "                    {\n",
      "                        \"role\": \"user\",\n",
      "                        \"content\": \"prompt\"\n",
      "                    }\n",
      "                ],\n",
      "                nb_retry=nb_retry\n",
      "            )\n",
      "            assert err.value == \"No valid json response found after 3 tries. Exiting.\"\n",
      "    else:\n",
      "        json_response = json_validated_response(\"gpt-4\", [\n",
      "                {\n",
      "                    \"role\": \"user\",\n",
      "                    \"content\": \"prompt\"\n",
      "                }\n",
      "            ],\n",
      "            nb_retry=nb_retry\n",
      "        )\n",
      "        assert json_response\n",
      "test_json_validated_response(mocker={_patches_and_mocks=[], patch=<pytest_mock.plugin.MockerFixture._Patcher object at 0x7f5b71b3ebb0>, call=call, ANY=<ANY>, DEFAULT=sentinel.DEFAULT, sentinel=<unittest.mock._Sentinel object at 0x7f5b72380a00>}, chat_completion_response='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/biobootloader+wolverine/biobootloader+wolverine/tests/test_files/cc_resp.txt', nb_retry=3, fail=False)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "with open(chat_completion_response, 'r') as file:\n",
      "State:\n",
      "<_io.TextIOWrapper name='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/biobootloader+wolverine/biobootloader+wolverine/tests/test_files/cc_resp.txt' mode='r' encoding='UTF-8'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_json_validated_response(mocker, chat_completion_response, nb_retry, fail):\n",
      "    with open(chat_completion_response, 'r') as file:\n",
      "        response = file.read()\n",
      "    mocker.patch(\n",
      "        \"openai.ChatCompletion.create\",\n",
      "        return_value=mock_open_ai_response_object(mocker=mocker, content=response))\n",
      "    if fail:\n",
      "        with pytest.raises(Exception) as err:\n",
      "            json_response = json_validated_response(\"gpt-4\", [\n",
      "                    {\n",
      "                        \"role\": \"user\",\n",
      "                        \"content\": \"prompt\"\n",
      "                    }\n",
      "                ],\n",
      "                nb_retry=nb_retry\n",
      "            )\n",
      "            assert err.value == \"No valid json response found after 3 tries. Exiting.\"\n",
      "    else:\n",
      "        json_response = json_validated_response(\"gpt-4\", [\n",
      "                {\n",
      "                    \"role\": \"user\",\n",
      "                    \"content\": \"prompt\"\n",
      "                }\n",
      "            ],\n",
      "            nb_retry=nb_retry\n",
      "        )\n",
      "        assert json_response\n",
      "test_json_validated_response(mocker={_patches_and_mocks=[], patch=<pytest_mock.plugin.MockerFixture._Patcher object at 0x7f5b71b3ebb0>, call=call, ANY=<ANY>, DEFAULT=sentinel.DEFAULT, sentinel=<unittest.mock._Sentinel object at 0x7f5b72380a00>}, chat_completion_response='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/biobootloader+wolverine/biobootloader+wolverine/tests/test_files/cc_resp.txt', nb_retry=3, fail=False)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "response = file.read()\n",
      "State:\n",
      "'Explanation: The function `subtract_numbers` is never defined in the script, causing a `NameError` when it is called in the `calculate` function.\\n\\n[\\n  {\"explanation\": \"The \\'subtract_numbers\\' function is never defined in the script.\"},\\n  {\"operation\": \"InsertAfter\", \"line\": 12, \"content\": \"\\\\n# Define subtract_numbers function\\\\ndef subtract_numbers(a, b):\\\\n    return a - b\\\\n\"},\\n  {\"operation\": \"Replace\", \"line\": 18, \"content\": \"    if operation == \\\\\"add\\\\\":\\\\n        result = add_numbers(num1, num2)\\\\n    elif operation == \\\\\"subtract\\\\\":\\\\n        result = subtract_numbers(num1, num2)\\\\n    elif operation == \\\\\"multiply\\\\\":\\\\n        result = multiply_numbers(num1, num2)\\\\n    elif operation == \\\\\"divide\\\\\":\\\\n        result = divide_numbers(num1, num2)\\\\n    else:\\\\n        print(\\\\\"Invalid operation\\\\\")\\\\n\"},\\n  {\"operation\": \"Replace\", \"line\": 30, \"content\": \"    return result\\\\n\"}\\n]'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def return_logger(name: str) -> logging.Logger:\n",
      "    logger = logging.getLogger(name)\n",
      "    if not len(logger.handlers):\n",
      "        log_formatter = logging.Formatter('%(asctime)-s: %(levelname)-s %(message)s')\n",
      "        logger.setLevel(logging.DEBUG)\n",
      "        console_handler = logging.StreamHandler()\n",
      "        console_handler.setFormatter(log_formatter)\n",
      "        logger.addHandler(console_handler)\n",
      "    return logger\n",
      "return_logger(name='imagededup.handlers.metrics.classification')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "logger = logging.getLogger(name)\n",
      "State:\n",
      "<Logger imagededup.handlers.metrics.classification (WARNING)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def decorator(callback):\n",
      "            @wraps(callback)\n",
      "            def wrapper(*args, **kwargs):\n",
      "                return built_functions[public_name](*args, **kwargs)\n",
      "            public_name = name or callback.__name__\n",
      "            assert public_name not in self._built_functions\n",
      "            built_functions = self._built_functions\n",
      "            built_functions[public_name] = callback\n",
      "            self._cached_base_callbacks[public_name] = callback\n",
      "            return wrapper\n",
      "decorator(callback=<function AnonymousParamName.goto at 0x7f58666fe4c0>, name='goto_anonymous_param', self=<jedi.plugins._PluginManager object at 0x7f586687bfa0>, self._built_functions={}, self._cached_base_callbacks={}, self._registered_plugins=[])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "public_name = name or callback.__name__\n",
      "State:\n",
      "'goto_anonymous_param'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __call__(self, func):\n",
      "        self.func = func\n",
      "        if self.check_name is None:\n",
      "            self.check_name = func.__name__[2:]\n",
      "        return self\n",
      "__call__(self=<jedi.inference.compiled.value.CheckAttribute object at 0x7f58667d5430>, func=<function CompiledValue.py__class__ at 0x7f586661d5e0>, self.check_name=None)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.check_name = func.__name__[2:]\n",
      "State:\n",
      "'__class__'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def pytest_generate_tests(metafunc):\n",
      "    test_files = dict(map(parse_test_files_option,\n",
      "                          metafunc.config.option.test_files))\n",
      "    if 'case' in metafunc.fixturenames:\n",
      "        base_dir = metafunc.config.option.integration_case_dir\n",
      "        thirdparty = metafunc.config.option.thirdparty\n",
      "        cases = list(run.collect_dir_tests(base_dir, test_files))\n",
      "        if thirdparty:\n",
      "            cases.extend(run.collect_dir_tests(\n",
      "                os.path.join(base_dir, 'thirdparty'), test_files, True))\n",
      "        ids = [\"%s:%s\" % (c.module_name, c.line_nr_test) for c in cases]\n",
      "        metafunc.parametrize('case', cases, ids=ids)\n",
      "    if 'refactor_case' in metafunc.fixturenames:\n",
      "        base_dir = metafunc.config.option.refactor_case_dir\n",
      "        cases = list(refactor.collect_dir_tests(base_dir, test_files))\n",
      "        metafunc.parametrize(\n",
      "            'refactor_case', cases,\n",
      "            ids=[c.refactor_type + '-' + c.name for c in cases]\n",
      "        )\n",
      "    if 'static_analysis_case' in metafunc.fixturenames:\n",
      "        base_dir = os.path.join(os.path.dirname(__file__), 'static_analysis')\n",
      "        cases = list(collect_static_analysis_tests(base_dir, test_files))\n",
      "        metafunc.parametrize(\n",
      "            'static_analysis_case',\n",
      "            cases,\n",
      "            ids=[c.name for c in cases]\n",
      "        )\n",
      "pytest_generate_tests(metafunc={definition=<FunctionDefinition test_issue436>, config=<_pytest.config.Config object at 0x7f58673c03a0>, fixturenames=['clean_jedi_cache', 'Script', 'environment', 'request'], cls=None, _arg2fixturedefs={'clean_jedi_cache': (<FixtureDef argname='clean_jedi_cache' scope='session' baseid=''>,), 'Script': (<FixtureDef argname='Script' scope='session' baseid=''>,), 'environment': (<FixtureDef argname='environment' scope='session' baseid=''>,)}, _calls=[]})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "test_files = dict(map(parse_test_files_option,\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_subprocess(self):\n",
      "        if self._subprocess is not None and not self._subprocess.is_crashed:\n",
      "            return self._subprocess\n",
      "        try:\n",
      "            self._subprocess = CompiledSubprocess(self._start_executable,\n",
      "                                                  env_vars=self._env_vars)\n",
      "            info = self._subprocess._send(None, _get_info)\n",
      "        except Exception as exc:\n",
      "            raise InvalidPythonEnvironment(\n",
      "                \"Could not get version information for %r: %r\" % (\n",
      "                    self._start_executable,\n",
      "                    exc))\n",
      "        self.executable = info[0]\n",
      "        self.path = info[1]\n",
      "        self.version_info = _VersionInfo(*info[2])\n",
      "        return self._subprocess\n",
      "_get_subprocess(self=<SameEnvironment: 3.9.19 in /local/rcs/XXX/miniforge3/envs/davidhalter+jedi>, self._env_vars=None, self._start_executable='/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/bin/python', self.executable='/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/bin/python', self.path='/local/rcs/XXX/miniforge3/envs/davidhalter+jedi', self.version_info=VersionInfo(major=3, minor=9, micro=19))\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "self._subprocess = CompiledSubprocess(self._start_executable,\n",
      "State:\n",
      "<CompiledSubprocess _executable='/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/bin/python', is_crashed=False, pid=2345289>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_process(self):\n",
      "        debug.dbg('Start environment subprocess %s', self._executable)\n",
      "        parso_path = sys.modules['parso'].__file__\n",
      "        args = (\n",
      "            self._executable,\n",
      "            _MAIN_PATH,\n",
      "            os.path.dirname(os.path.dirname(parso_path)),\n",
      "            '.'.join(str(x) for x in sys.version_info[:3]),\n",
      "        )\n",
      "        process = _GeneralizedPopen(\n",
      "            args,\n",
      "            stdin=subprocess.PIPE,\n",
      "            stdout=subprocess.PIPE,\n",
      "            stderr=subprocess.PIPE,\n",
      "            env=self._env_vars\n",
      "        )\n",
      "        self._stderr_queue = queue.Queue()\n",
      "        self._stderr_thread = t = Thread(\n",
      "            target=_enqueue_output,\n",
      "            args=(process.stderr, self._stderr_queue)\n",
      "        )\n",
      "        t.daemon = True\n",
      "        t.start()\n",
      "        self._cleanup_callable = weakref.finalize(self,\n",
      "                                                  _cleanup_process,\n",
      "                                                  process,\n",
      "                                                  t)\n",
      "        return process\n",
      "_get_process(self=<CompiledSubprocess _executable='/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/bin/python', is_crashed=False, pid=2345289>, self._cleanup_callable=<function CompiledSubprocess.__init__.<locals>.<lambda> at 0x7f5856bb9040>, self._env_vars=None, self._executable='/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/bin/python', self._inference_state_deletion_queue=deque([]), self._memoize_method_dct={<function CompiledSubprocess._get_process at 0x7f5866382310>: {}})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "parso_path = sys.modules['parso'].__file__\n",
      "State:\n",
      "'/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages/parso/__init__.py'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_process(self):\n",
      "        debug.dbg('Start environment subprocess %s', self._executable)\n",
      "        parso_path = sys.modules['parso'].__file__\n",
      "        args = (\n",
      "            self._executable,\n",
      "            _MAIN_PATH,\n",
      "            os.path.dirname(os.path.dirname(parso_path)),\n",
      "            '.'.join(str(x) for x in sys.version_info[:3]),\n",
      "        )\n",
      "        process = _GeneralizedPopen(\n",
      "            args,\n",
      "            stdin=subprocess.PIPE,\n",
      "            stdout=subprocess.PIPE,\n",
      "            stderr=subprocess.PIPE,\n",
      "            env=self._env_vars\n",
      "        )\n",
      "        self._stderr_queue = queue.Queue()\n",
      "        self._stderr_thread = t = Thread(\n",
      "            target=_enqueue_output,\n",
      "            args=(process.stderr, self._stderr_queue)\n",
      "        )\n",
      "        t.daemon = True\n",
      "        t.start()\n",
      "        self._cleanup_callable = weakref.finalize(self,\n",
      "                                                  _cleanup_process,\n",
      "                                                  process,\n",
      "                                                  t)\n",
      "        return process\n",
      "_get_process(self=<CompiledSubprocess _executable='/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/bin/python', is_crashed=False, pid=2345289>, self._cleanup_callable=<function CompiledSubprocess.__init__.<locals>.<lambda> at 0x7f5856bb9040>, self._env_vars=None, self._executable='/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/bin/python', self._inference_state_deletion_queue=deque([]), self._memoize_method_dct={<function CompiledSubprocess._get_process at 0x7f5866382310>: {}})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "args = (\n",
      "State:\n",
      "('/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/bin/python', '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi/jedi/inference/compiled/subprocess/__main__.py', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages', '3.9.19')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_module(self):\n",
      "        names = None\n",
      "        is_package = False\n",
      "        if self.path is not None:\n",
      "            import_names, is_p = transform_path_to_dotted(\n",
      "                self._inference_state.get_sys_path(add_parent_paths=False),\n",
      "                self.path\n",
      "            )\n",
      "            if import_names is not None:\n",
      "                names = import_names\n",
      "                is_package = is_p\n",
      "        if self.path is None:\n",
      "            file_io = None\n",
      "        else:\n",
      "            file_io = KnownContentFileIO(self.path, self._code)\n",
      "        if self.path is not None and self.path.suffix == '.pyi':\n",
      "            stub_module = load_proper_stub_module(\n",
      "                self._inference_state,\n",
      "                self._inference_state.latest_grammar,\n",
      "                file_io,\n",
      "                names,\n",
      "                self._module_node\n",
      "            )\n",
      "            if stub_module is not None:\n",
      "                return stub_module\n",
      "        if names is None:\n",
      "            names = ('__main__',)\n",
      "        module = ModuleValue(\n",
      "            self._inference_state, self._module_node,\n",
      "            file_io=file_io,\n",
      "            string_names=names,\n",
      "            code_lines=self._code_lines,\n",
      "            is_package=is_package,\n",
      "        )\n",
      "        if names[0] not in ('builtins', 'typing'):\n",
      "            self._inference_state.module_cache.add(names, ValueSet([module]))\n",
      "        return module\n",
      "_get_module(self=<Script: 'example.py' <SameEnvironment: 3.9.19 in /local/rcs/XXX/miniforge3/envs/davidhalter+jedi>>, self._code='\\nimport json\\njson.lo', self._code_lines=['\\n', 'import json\\n', 'json.lo'], self._inference_state=<jedi.inference.InferenceState object at 0x7f58651a5b80>, self._memoize_method_dct={<function Script._get_module at 0x7f586623a040>: {}}, self._module_node=<Module: @2-3>, self._orig_path='example.py', self.path=PosixPath('/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi/example.py'))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "names = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_module(self):\n",
      "        names = None\n",
      "        is_package = False\n",
      "        if self.path is not None:\n",
      "            import_names, is_p = transform_path_to_dotted(\n",
      "                self._inference_state.get_sys_path(add_parent_paths=False),\n",
      "                self.path\n",
      "            )\n",
      "            if import_names is not None:\n",
      "                names = import_names\n",
      "                is_package = is_p\n",
      "        if self.path is None:\n",
      "            file_io = None\n",
      "        else:\n",
      "            file_io = KnownContentFileIO(self.path, self._code)\n",
      "        if self.path is not None and self.path.suffix == '.pyi':\n",
      "            stub_module = load_proper_stub_module(\n",
      "                self._inference_state,\n",
      "                self._inference_state.latest_grammar,\n",
      "                file_io,\n",
      "                names,\n",
      "                self._module_node\n",
      "            )\n",
      "            if stub_module is not None:\n",
      "                return stub_module\n",
      "        if names is None:\n",
      "            names = ('__main__',)\n",
      "        module = ModuleValue(\n",
      "            self._inference_state, self._module_node,\n",
      "            file_io=file_io,\n",
      "            string_names=names,\n",
      "            code_lines=self._code_lines,\n",
      "            is_package=is_package,\n",
      "        )\n",
      "        if names[0] not in ('builtins', 'typing'):\n",
      "            self._inference_state.module_cache.add(names, ValueSet([module]))\n",
      "        return module\n",
      "_get_module(self=<Script: 'example.py' <SameEnvironment: 3.9.19 in /local/rcs/XXX/miniforge3/envs/davidhalter+jedi>>, self._code='\\nimport json\\njson.lo', self._code_lines=['\\n', 'import json\\n', 'json.lo'], self._inference_state=<jedi.inference.InferenceState object at 0x7f58651a5b80>, self._memoize_method_dct={<function Script._get_module at 0x7f586623a040>: {}}, self._module_node=<Module: @2-3>, self._orig_path='example.py', self.path=PosixPath('/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi/example.py'))\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "is_package = False\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_module(self):\n",
      "        names = None\n",
      "        is_package = False\n",
      "        if self.path is not None:\n",
      "            import_names, is_p = transform_path_to_dotted(\n",
      "                self._inference_state.get_sys_path(add_parent_paths=False),\n",
      "                self.path\n",
      "            )\n",
      "            if import_names is not None:\n",
      "                names = import_names\n",
      "                is_package = is_p\n",
      "        if self.path is None:\n",
      "            file_io = None\n",
      "        else:\n",
      "            file_io = KnownContentFileIO(self.path, self._code)\n",
      "        if self.path is not None and self.path.suffix == '.pyi':\n",
      "            stub_module = load_proper_stub_module(\n",
      "                self._inference_state,\n",
      "                self._inference_state.latest_grammar,\n",
      "                file_io,\n",
      "                names,\n",
      "                self._module_node\n",
      "            )\n",
      "            if stub_module is not None:\n",
      "                return stub_module\n",
      "        if names is None:\n",
      "            names = ('__main__',)\n",
      "        module = ModuleValue(\n",
      "            self._inference_state, self._module_node,\n",
      "            file_io=file_io,\n",
      "            string_names=names,\n",
      "            code_lines=self._code_lines,\n",
      "            is_package=is_package,\n",
      "        )\n",
      "        if names[0] not in ('builtins', 'typing'):\n",
      "            self._inference_state.module_cache.add(names, ValueSet([module]))\n",
      "        return module\n",
      "_get_module(self=<Script: 'example.py' <SameEnvironment: 3.9.19 in /local/rcs/XXX/miniforge3/envs/davidhalter+jedi>>, self._code='\\nimport json\\njson.lo', self._code_lines=['\\n', 'import json\\n', 'json.lo'], self._inference_state=<jedi.inference.InferenceState object at 0x7f58651a5b80>, self._memoize_method_dct={<function Script._get_module at 0x7f586623a040>: {}}, self._module_node=<Module: @2-3>, self._orig_path='example.py', self.path=PosixPath('/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi/example.py'))\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "import_names, is_p = transform_path_to_dotted(\n",
      "State:\n",
      "('example',)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def remove_python_path_suffix(path):\n",
      "    for suffix in all_suffixes() + ['.pyi']:\n",
      "        if path.suffix == suffix:\n",
      "            path = path.with_name(path.stem)\n",
      "            break\n",
      "    return path\n",
      "remove_python_path_suffix(path=PosixPath('/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi/example.py'))\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "path = path.with_name(path.stem)\n",
      "State:\n",
      "PosixPath('/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi/example')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def complete_dict(module_context, code_lines, leaf, position, string, fuzzy):\n",
      "    bracket_leaf = leaf\n",
      "    if bracket_leaf != '[':\n",
      "        bracket_leaf = leaf.get_previous_leaf()\n",
      "    cut_end_quote = ''\n",
      "    if string:\n",
      "        cut_end_quote = get_quote_ending(string, code_lines, position, invert_result=True)\n",
      "    if bracket_leaf == '[':\n",
      "        if string is None and leaf is not bracket_leaf:\n",
      "            string = cut_value_at_position(leaf, position)\n",
      "        context = module_context.create_context(bracket_leaf)\n",
      "        before_node = before_bracket_leaf = bracket_leaf.get_previous_leaf()\n",
      "        if before_node in (')', ']', '}'):\n",
      "            before_node = before_node.parent\n",
      "        if before_node.type in ('atom', 'trailer', 'name'):\n",
      "            values = infer_call_of_leaf(context, before_bracket_leaf)\n",
      "            return list(_completions_for_dicts(\n",
      "                module_context.inference_state,\n",
      "                values,\n",
      "                '' if string is None else string,\n",
      "                cut_end_quote,\n",
      "                fuzzy=fuzzy,\n",
      "            ))\n",
      "    return []\n",
      "complete_dict(module_context=ModuleContext(<ModuleValue: example@2-3 is_stub=False>), code_lines=['\\n', 'import json\\n', 'json.lo'], leaf=<Name: lo@3,5>, position=(3, 7), string=None, fuzzy=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "bracket_leaf = leaf\n",
      "State:\n",
      "<Name: lo@3,5>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def complete_dict(module_context, code_lines, leaf, position, string, fuzzy):\n",
      "    bracket_leaf = leaf\n",
      "    if bracket_leaf != '[':\n",
      "        bracket_leaf = leaf.get_previous_leaf()\n",
      "    cut_end_quote = ''\n",
      "    if string:\n",
      "        cut_end_quote = get_quote_ending(string, code_lines, position, invert_result=True)\n",
      "    if bracket_leaf == '[':\n",
      "        if string is None and leaf is not bracket_leaf:\n",
      "            string = cut_value_at_position(leaf, position)\n",
      "        context = module_context.create_context(bracket_leaf)\n",
      "        before_node = before_bracket_leaf = bracket_leaf.get_previous_leaf()\n",
      "        if before_node in (')', ']', '}'):\n",
      "            before_node = before_node.parent\n",
      "        if before_node.type in ('atom', 'trailer', 'name'):\n",
      "            values = infer_call_of_leaf(context, before_bracket_leaf)\n",
      "            return list(_completions_for_dicts(\n",
      "                module_context.inference_state,\n",
      "                values,\n",
      "                '' if string is None else string,\n",
      "                cut_end_quote,\n",
      "                fuzzy=fuzzy,\n",
      "            ))\n",
      "    return []\n",
      "complete_dict(module_context=ModuleContext(<ModuleValue: example@2-3 is_stub=False>), code_lines=['\\n', 'import json\\n', 'json.lo'], leaf=<Name: lo@3,5>, position=(3, 7), string=None, fuzzy=False)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "bracket_leaf = leaf.get_previous_leaf()\n",
      "State:\n",
      "<Operator: .>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_code_for_stack(code_lines, leaf, position):\n",
      "    if leaf.start_pos >= position:\n",
      "        leaf = leaf.get_previous_leaf()\n",
      "        if leaf is None:\n",
      "            return ''\n",
      "    is_after_newline = leaf.type == 'newline'\n",
      "    while leaf.type == 'newline':\n",
      "        leaf = leaf.get_previous_leaf()\n",
      "        if leaf is None:\n",
      "            return ''\n",
      "    if leaf.type == 'error_leaf' or leaf.type == 'string':\n",
      "        if leaf.start_pos[0] < position[0]:\n",
      "            return ''\n",
      "        raise OnErrorLeaf(leaf)\n",
      "    else:\n",
      "        user_stmt = leaf\n",
      "        while True:\n",
      "            if user_stmt.parent.type in ('file_input', 'suite', 'simple_stmt'):\n",
      "                break\n",
      "            user_stmt = user_stmt.parent\n",
      "        if is_after_newline:\n",
      "            if user_stmt.start_pos[1] > position[1]:\n",
      "                return ''\n",
      "        return _get_code(code_lines, user_stmt.get_start_pos_of_prefix(), position)\n",
      "_get_code_for_stack(code_lines=['\\n', 'import json\\n', 'json.lo'], leaf=<Name: lo@3,5>, position=(3, 5))\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "leaf = leaf.get_previous_leaf()\n",
      "State:\n",
      "<Operator: .>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_code(code_lines, start_pos, end_pos):\n",
      "    lines = code_lines[start_pos[0] - 1:end_pos[0]]\n",
      "    lines[-1] = lines[-1][:end_pos[1]]\n",
      "    lines[0] = lines[0][start_pos[1]:]\n",
      "    return ''.join(lines)\n",
      "_get_code(code_lines=['\\n', 'import json\\n', 'json.lo'], start_pos=(3, 0), end_pos=(3, 5))\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "lines = code_lines[start_pos[0] - 1:end_pos[0]]\n",
      "State:\n",
      "['json.lo']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_code(code_lines, start_pos, end_pos):\n",
      "    lines = code_lines[start_pos[0] - 1:end_pos[0]]\n",
      "    lines[-1] = lines[-1][:end_pos[1]]\n",
      "    lines[0] = lines[0][start_pos[1]:]\n",
      "    return ''.join(lines)\n",
      "_get_code(code_lines=['\\n', 'import json\\n', 'json.lo'], start_pos=(3, 0), end_pos=(3, 5))\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "lines[-1] = lines[-1][:end_pos[1]]\n",
      "State:\n",
      "['json.']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_access_handle(self, handle):\n",
      "        self._handles[handle.id] = handle\n",
      "set_access_handle(self=<jedi.inference.compiled.subprocess.InferenceStateSameProcess object at 0x7f5855e8a5b0>, handle=<AccessHandle of DirectObjectAccess(<jedi.api.interpreter.NamespaceObject object at 0x..)>, self._handles={}, self._inference_state_id=140017375159920, self._inference_state_weakref=<weakref at 0x7f5855e1fd60; to 'InferenceState' at 0x7f5855e8a670>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self._handles[handle.id] = handle\n",
      "State:\n",
      "{140017375160592: <AccessHandle of DirectObjectAccess(<jedi.api.interpreter.NamespaceObject object at 0x..)>}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def create_from_access_path(inference_state, access_path):\n",
      "    value = None\n",
      "    for name, access in access_path.accesses:\n",
      "        value = create_cached_compiled_value(\n",
      "            inference_state,\n",
      "            access,\n",
      "            parent_context=None if value is None else value.as_context()\n",
      "        )\n",
      "    return value\n",
      "create_from_access_path(inference_state={environment=<jedi.api.environment.InterpreterEnvironment object at 0x7f5855e8a400>, script_path=None, compiled_subprocess=<jedi.inference.compiled.subprocess.InferenceStateSameProcess object at 0x7f5855e8a5b0>, grammar=<PythonGrammar:single_input file_input eval_input ...>, latest_grammar=<PythonGrammar:single_input file_input eval_input ...>, memoize_cache={}, module_cache=<jedi.inference.imports.ModuleCache object at 0x7f5855e8a3d0>, stub_module_cache={}, compiled_cache={}, inferred_element_counts={}, mixed_cache={}, analysis=[], dynamic_params_depth=0, do_dynamic_params_search=False, is_analysis=False, project=<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, access_cache={}, allow_unsafe_executions=True, flow_analysis_enabled=True, recursion_detector=<jedi.inference.recursion.RecursionDetector object at 0x7f5855e8a9a0>, execution_recursion_detector=<jedi.inference.recursion.ExecutionRecursionDetector object at 0x7f5855e8a520>}, access_path={accesses=[('jedi.api.interpreter', <AccessHandle of DirectObjectAccess(<module 'jedi.api.interpreter' from '/local/rcs/ji..)>), ('NamespaceObject', <AccessHandle of DirectObjectAccess(<jedi.api.interpreter.NamespaceObject object at 0x..)>)]})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "value = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def infer_node(context, element):\n",
      "    if isinstance(context, CompForContext):\n",
      "        return _infer_node(context, element)\n",
      "    if_stmt = element\n",
      "    while if_stmt is not None:\n",
      "        if_stmt = if_stmt.parent\n",
      "        if if_stmt.type in ('if_stmt', 'for_stmt'):\n",
      "            break\n",
      "        if parser_utils.is_scope(if_stmt):\n",
      "            if_stmt = None\n",
      "            break\n",
      "    predefined_if_name_dict = context.predefined_names.get(if_stmt)\n",
      "    if predefined_if_name_dict is None and if_stmt \\\n",
      "            and if_stmt.type == 'if_stmt' and context.inference_state.is_analysis:\n",
      "        if_stmt_test = if_stmt.children[1]\n",
      "        name_dicts = [{}]\n",
      "        if element.start_pos > if_stmt_test.end_pos:\n",
      "            if_names = get_names_of_node(if_stmt_test)\n",
      "            element_names = get_names_of_node(element)\n",
      "            str_element_names = [e.value for e in element_names]\n",
      "            if any(i.value in str_element_names for i in if_names):\n",
      "                for if_name in if_names:\n",
      "                    definitions = context.inference_state.infer(context, if_name)\n",
      "                    if len(definitions) > 1:\n",
      "                        if len(name_dicts) * len(definitions) > 16:\n",
      "                            debug.dbg('Too many options for if branch inference %s.', if_stmt)\n",
      "                            name_dicts = [{}]\n",
      "                            break\n",
      "                        original_name_dicts = list(name_dicts)\n",
      "                        name_dicts = []\n",
      "                        for definition in definitions:\n",
      "                            new_name_dicts = list(original_name_dicts)\n",
      "                            for i, name_dict in enumerate(new_name_dicts):\n",
      "                                new_name_dicts[i] = name_dict.copy()\n",
      "                                new_name_dicts[i][if_name.value] = ValueSet([definition])\n",
      "                            name_dicts += new_name_dicts\n",
      "                    else:\n",
      "                        for name_dict in name_dicts:\n",
      "                            name_dict[if_name.value] = definitions\n",
      "        if len(name_dicts) > 1:\n",
      "            result = NO_VALUES\n",
      "            for name_dict in name_dicts:\n",
      "                with context.predefine_names(if_stmt, name_dict):\n",
      "                    result |= _infer_node(context, element)\n",
      "            return result\n",
      "        else:\n",
      "            return _infer_node_if_inferred(context, element)\n",
      "    else:\n",
      "        if predefined_if_name_dict:\n",
      "            return _infer_node(context, element)\n",
      "        else:\n",
      "            return _infer_node_if_inferred(context, element)\n",
      "infer_node(context=ModuleContext(<ModuleValue: __main__@2-8 is_stub=False>), element=PythonNode(test, [<Name: f@8,11>, <Keyword: if>, PythonNode(atom_expr, [<Name: random@8,16>, PythonNode(trailer, [<Operator: .>, <Name: choice@8,23>]), PythonNode(trailer, [<Operator: (>, PythonNode(atom, [<Operator: [>, PythonNode(testlist_comp, [<Number: 0>, <Operator: ,>, <Number: 1>]), <Operator: ]>]), <Operator: )>])]), <Keyword: else>, <Name: C@8,42>]))\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "if_stmt = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def builtins_module(self):\n",
      "        module_name = 'builtins'\n",
      "        builtins_module, = self.import_module((module_name,), sys_path=[])\n",
      "        return builtins_module\n",
      "builtins_module(self=<jedi.inference.InferenceState object at 0x7f5855d8ac40>, self.access_cache={}, self.allow_unsafe_executions=False, self.analysis=[], self.compiled_cache={}, self.compiled_subprocess=<jedi.inference.compiled.subprocess.InferenceStateSubprocess object at 0x7f5855d8a730>, self.do_dynamic_params_search=True, self.dynamic_params_depth=0, self.environment=<SameEnvironment: 3.9.19 in /local/rcs/XXX/miniforge3/envs/davidhalter+jedi>, self.execution_recursion_detector=<jedi.inference.recursion.ExecutionRecursionDetector object at 0x7f5855d8abb0>, self.flow_analysis_enabled=True, self.grammar=<PythonGrammar:single_input file_input eval_input ...>, self.inferred_element_counts={<Module: @2-8>: 2}, self.is_analysis=False, self.latest_grammar=<PythonGrammar:single_input file_input eval_input ...>, self.memoize_cache={<function _infer_node_cached at 0x7f58663563a0>: {(ModuleContext(<ModuleValue: __main__@2-8 is_stub=False>), (PythonNode(test, [<Name: f@8,11>, <Keyword: if>, PythonNode(atom_expr, [<Name: random@8,16>, PythonNode(trailer, [<Operator: .>, <Name: choice@8,23>]), PythonNode(trailer, [<Operator: (>, PythonNode(atom, [<Operator: [>, PythonNode(testlist_comp, [<Number: 0>, <Operator: ,>, <Number: 1>]), <Operator: ]>]), <Operator: )>])]), <Keyword: else>, <Name: C@8,42>]),), frozenset()): S{}}, <function InferenceState.builtins_module at 0x7f586630b4c0>: {}}, self.mixed_cache={}, self.module_cache=<jedi.inference.imports.ModuleCache object at 0x7f5855d8a430>, self.project=<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, self.recursion_detector=<jedi.inference.recursion.RecursionDetector object at 0x7f5855e8a970>, self.script_path=None, self.stub_module_cache={})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "module_name = 'builtins'\n",
      "State:\n",
      "'builtins'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def from_sets(cls, sets):\n",
      "        aggregated = set()\n",
      "        for set_ in sets:\n",
      "            if isinstance(set_, ValueSet):\n",
      "                aggregated |= set_._set\n",
      "            else:\n",
      "                aggregated |= frozenset(set_)\n",
      "        return cls._from_frozen_set(frozenset(aggregated))\n",
      "from_sets(cls=<class 'jedi.inference.base_value.ValueSet'>, sets=<generator object import_module_decorator.<locals>.wrapper.<locals>.<genexpr> at 0x7f5855c90190>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "aggregated = set()\n",
      "State:\n",
      "set()\n",
      "==================================================\n",
      "Clean Code:\n",
      "def load_module(inference_state, dotted_name, **kwargs):\n",
      "    if dotted_name.startswith('tensorflow.'):\n",
      "        return None\n",
      "    access_path = inference_state.compiled_subprocess.load_module(dotted_name=dotted_name, **kwargs)\n",
      "    if access_path is None:\n",
      "        return None\n",
      "    return create_from_access_path(inference_state, access_path)\n",
      "load_module(inference_state={environment=<SameEnvironment: 3.9.19 in /local/rcs/XXX/miniforge3/envs/davidhalter+jedi>, script_path=None, compiled_subprocess=<jedi.inference.compiled.subprocess.InferenceStateSubprocess object at 0x7f5855d8a730>, grammar=<PythonGrammar:single_input file_input eval_input ...>, latest_grammar=<PythonGrammar:single_input file_input eval_input ...>, memoize_cache={<function _infer_node_cached at 0x7f58663563a0>: {(ModuleContext(<ModuleValue: __main__@2-8 is_stub=False>), (PythonNode(test, [<Name: f@8,11>, <Keyword: if>, PythonNode(atom_expr, [<Name: random@8,16>, PythonNode(trailer, [<Operator: .>, <Name: choice@8,23>]), PythonNode(trailer, [<Operator: (>, PythonNode(atom, [<Operator: [>, PythonNode(testlist_comp, [<Number: 0>, <Operator: ,>, <Number: 1>]), <Operator: ]>]), <Operator: )>])]), <Keyword: else>, <Name: C@8,42>]),), frozenset()): S{}}, <function InferenceState.builtins_module at 0x7f586630b4c0>: {}, <function Project._get_base_sys_path at 0x7f58661fc430>: {(<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, (<jedi.inference.InferenceState object at 0x7f5855d8ac40>,), frozenset()): ['/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi', '/local/rcs/XXX/code/pytrace-collector', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python39.zip', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/lib-dynload', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages/PySnooper-1.2.0-py3.9.egg']}}, module_cache=<jedi.inference.imports.ModuleCache object at 0x7f5855d8a430>, stub_module_cache={}, compiled_cache={}, inferred_element_counts={<Module: @2-8>: 2}, mixed_cache={}, analysis=[], dynamic_params_depth=0, do_dynamic_params_search=True, is_analysis=False, project=<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, access_cache={}, allow_unsafe_executions=False, flow_analysis_enabled=True, recursion_detector=<jedi.inference.recursion.RecursionDetector object at 0x7f5855e8a970>, execution_recursion_detector=<jedi.inference.recursion.ExecutionRecursionDetector object at 0x7f5855d8abb0>}, dotted_name='builtins', kwargs={'sys_path': []})\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "access_path = inference_state.compiled_subprocess.load_module(dotted_name=dotted_name, **kwargs)\n",
      "State:\n",
      "{accesses=[('builtins', <AccessHandle of #140274588125920>)]}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def infer(self, context, name):\n",
      "        def_ = name.get_definition(import_name_always=True)\n",
      "        if def_ is not None:\n",
      "            type_ = def_.type\n",
      "            is_classdef = type_ == 'classdef'\n",
      "            if is_classdef or type_ == 'funcdef':\n",
      "                if is_classdef:\n",
      "                    c = ClassValue(self, context, name.parent)\n",
      "                else:\n",
      "                    c = FunctionValue.from_context(context, name.parent)\n",
      "                return ValueSet([c])\n",
      "            if type_ == 'expr_stmt':\n",
      "                is_simple_name = name.parent.type not in ('power', 'trailer')\n",
      "                if is_simple_name:\n",
      "                    return infer_expr_stmt(context, def_, name)\n",
      "            if type_ == 'for_stmt':\n",
      "                container_types = context.infer_node(def_.children[3])\n",
      "                cn = ContextualizedNode(context, def_.children[3])\n",
      "                for_types = iterate_values(container_types, cn)\n",
      "                n = TreeNameDefinition(context, name)\n",
      "                return check_tuple_assignments(n, for_types)\n",
      "            if type_ in ('import_from', 'import_name'):\n",
      "                return imports.infer_import(context, name)\n",
      "            if type_ == 'with_stmt':\n",
      "                return tree_name_to_values(self, context, name)\n",
      "            elif type_ == 'param':\n",
      "                return context.py__getattribute__(name.value, position=name.end_pos)\n",
      "            elif type_ == 'namedexpr_test':\n",
      "                return context.infer_node(def_)\n",
      "        else:\n",
      "            result = follow_error_node_imports_if_possible(context, name)\n",
      "            if result is not None:\n",
      "                return result\n",
      "        return helpers.infer_call_of_leaf(context, name)\n",
      "infer(self=<jedi.inference.InferenceState object at 0x7f5855c938e0>, context=ModuleContext(<ModuleValue: example@1-3 is_stub=False>), name=<Name: f@1,4>, self.access_cache={}, self.allow_unsafe_executions=False, self.analysis=[], self.compiled_cache={}, self.compiled_subprocess=<jedi.inference.compiled.subprocess.InferenceStateSubprocess object at 0x7f5855c93580>, self.do_dynamic_params_search=True, self.dynamic_params_depth=0, self.environment=<SameEnvironment: 3.9.19 in /local/rcs/XXX/miniforge3/envs/davidhalter+jedi>, self.execution_recursion_detector=<jedi.inference.recursion.ExecutionRecursionDetector object at 0x7f5855c93f40>, self.flow_analysis_enabled=True, self.grammar=<PythonGrammar:single_input file_input eval_input ...>, self.inferred_element_counts={}, self.is_analysis=False, self.latest_grammar=<PythonGrammar:single_input file_input eval_input ...>, self.memoize_cache={<function Project._get_sys_path at 0x7f58661fc310>: {(<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, (<jedi.inference.InferenceState object at 0x7f5855c938e0>,), frozenset({('add_parent_paths', False)})): ['/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi', '/local/rcs/XXX/code/pytrace-collector', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python39.zip', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/lib-dynload', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages/PySnooper-1.2.0-py3.9.egg']}, <function Project._get_base_sys_path at 0x7f58661fc430>: {(<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, (<jedi.inference.InferenceState object at 0x7f5855c938e0>,), frozenset()): ['/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi', '/local/rcs/XXX/code/pytrace-collector', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python39.zip', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/lib-dynload', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages/PySnooper-1.2.0-py3.9.egg']}}, self.mixed_cache={}, self.module_cache=<jedi.inference.imports.ModuleCache object at 0x7f5855c93910>, self.project=<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, self.recursion_detector=<jedi.inference.recursion.RecursionDetector object at 0x7f5865296ca0>, self.script_path=PosixPath('/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi/example.py'), self.stub_module_cache={})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "def_ = name.get_definition(import_name_always=True)\n",
      "State:\n",
      "<Function: f@1-3>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def infer(self, context, name):\n",
      "        def_ = name.get_definition(import_name_always=True)\n",
      "        if def_ is not None:\n",
      "            type_ = def_.type\n",
      "            is_classdef = type_ == 'classdef'\n",
      "            if is_classdef or type_ == 'funcdef':\n",
      "                if is_classdef:\n",
      "                    c = ClassValue(self, context, name.parent)\n",
      "                else:\n",
      "                    c = FunctionValue.from_context(context, name.parent)\n",
      "                return ValueSet([c])\n",
      "            if type_ == 'expr_stmt':\n",
      "                is_simple_name = name.parent.type not in ('power', 'trailer')\n",
      "                if is_simple_name:\n",
      "                    return infer_expr_stmt(context, def_, name)\n",
      "            if type_ == 'for_stmt':\n",
      "                container_types = context.infer_node(def_.children[3])\n",
      "                cn = ContextualizedNode(context, def_.children[3])\n",
      "                for_types = iterate_values(container_types, cn)\n",
      "                n = TreeNameDefinition(context, name)\n",
      "                return check_tuple_assignments(n, for_types)\n",
      "            if type_ in ('import_from', 'import_name'):\n",
      "                return imports.infer_import(context, name)\n",
      "            if type_ == 'with_stmt':\n",
      "                return tree_name_to_values(self, context, name)\n",
      "            elif type_ == 'param':\n",
      "                return context.py__getattribute__(name.value, position=name.end_pos)\n",
      "            elif type_ == 'namedexpr_test':\n",
      "                return context.infer_node(def_)\n",
      "        else:\n",
      "            result = follow_error_node_imports_if_possible(context, name)\n",
      "            if result is not None:\n",
      "                return result\n",
      "        return helpers.infer_call_of_leaf(context, name)\n",
      "infer(self=<jedi.inference.InferenceState object at 0x7f5855c938e0>, context=ModuleContext(<ModuleValue: example@1-3 is_stub=False>), name=<Name: f@1,4>, self.access_cache={}, self.allow_unsafe_executions=False, self.analysis=[], self.compiled_cache={}, self.compiled_subprocess=<jedi.inference.compiled.subprocess.InferenceStateSubprocess object at 0x7f5855c93580>, self.do_dynamic_params_search=True, self.dynamic_params_depth=0, self.environment=<SameEnvironment: 3.9.19 in /local/rcs/XXX/miniforge3/envs/davidhalter+jedi>, self.execution_recursion_detector=<jedi.inference.recursion.ExecutionRecursionDetector object at 0x7f5855c93f40>, self.flow_analysis_enabled=True, self.grammar=<PythonGrammar:single_input file_input eval_input ...>, self.inferred_element_counts={}, self.is_analysis=False, self.latest_grammar=<PythonGrammar:single_input file_input eval_input ...>, self.memoize_cache={<function Project._get_sys_path at 0x7f58661fc310>: {(<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, (<jedi.inference.InferenceState object at 0x7f5855c938e0>,), frozenset({('add_parent_paths', False)})): ['/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi', '/local/rcs/XXX/code/pytrace-collector', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python39.zip', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/lib-dynload', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages/PySnooper-1.2.0-py3.9.egg']}, <function Project._get_base_sys_path at 0x7f58661fc430>: {(<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, (<jedi.inference.InferenceState object at 0x7f5855c938e0>,), frozenset()): ['/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi', '/local/rcs/XXX/code/pytrace-collector', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python39.zip', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/lib-dynload', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages/PySnooper-1.2.0-py3.9.egg']}}, self.mixed_cache={}, self.module_cache=<jedi.inference.imports.ModuleCache object at 0x7f5855c93910>, self.project=<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, self.recursion_detector=<jedi.inference.recursion.RecursionDetector object at 0x7f5865296ca0>, self.script_path=PosixPath('/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi/example.py'), self.stub_module_cache={})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "type_ = def_.type\n",
      "State:\n",
      "'funcdef'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def infer(self, context, name):\n",
      "        def_ = name.get_definition(import_name_always=True)\n",
      "        if def_ is not None:\n",
      "            type_ = def_.type\n",
      "            is_classdef = type_ == 'classdef'\n",
      "            if is_classdef or type_ == 'funcdef':\n",
      "                if is_classdef:\n",
      "                    c = ClassValue(self, context, name.parent)\n",
      "                else:\n",
      "                    c = FunctionValue.from_context(context, name.parent)\n",
      "                return ValueSet([c])\n",
      "            if type_ == 'expr_stmt':\n",
      "                is_simple_name = name.parent.type not in ('power', 'trailer')\n",
      "                if is_simple_name:\n",
      "                    return infer_expr_stmt(context, def_, name)\n",
      "            if type_ == 'for_stmt':\n",
      "                container_types = context.infer_node(def_.children[3])\n",
      "                cn = ContextualizedNode(context, def_.children[3])\n",
      "                for_types = iterate_values(container_types, cn)\n",
      "                n = TreeNameDefinition(context, name)\n",
      "                return check_tuple_assignments(n, for_types)\n",
      "            if type_ in ('import_from', 'import_name'):\n",
      "                return imports.infer_import(context, name)\n",
      "            if type_ == 'with_stmt':\n",
      "                return tree_name_to_values(self, context, name)\n",
      "            elif type_ == 'param':\n",
      "                return context.py__getattribute__(name.value, position=name.end_pos)\n",
      "            elif type_ == 'namedexpr_test':\n",
      "                return context.infer_node(def_)\n",
      "        else:\n",
      "            result = follow_error_node_imports_if_possible(context, name)\n",
      "            if result is not None:\n",
      "                return result\n",
      "        return helpers.infer_call_of_leaf(context, name)\n",
      "infer(self=<jedi.inference.InferenceState object at 0x7f5855c938e0>, context=ModuleContext(<ModuleValue: example@1-3 is_stub=False>), name=<Name: f@1,4>, self.access_cache={}, self.allow_unsafe_executions=False, self.analysis=[], self.compiled_cache={}, self.compiled_subprocess=<jedi.inference.compiled.subprocess.InferenceStateSubprocess object at 0x7f5855c93580>, self.do_dynamic_params_search=True, self.dynamic_params_depth=0, self.environment=<SameEnvironment: 3.9.19 in /local/rcs/XXX/miniforge3/envs/davidhalter+jedi>, self.execution_recursion_detector=<jedi.inference.recursion.ExecutionRecursionDetector object at 0x7f5855c93f40>, self.flow_analysis_enabled=True, self.grammar=<PythonGrammar:single_input file_input eval_input ...>, self.inferred_element_counts={}, self.is_analysis=False, self.latest_grammar=<PythonGrammar:single_input file_input eval_input ...>, self.memoize_cache={<function Project._get_sys_path at 0x7f58661fc310>: {(<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, (<jedi.inference.InferenceState object at 0x7f5855c938e0>,), frozenset({('add_parent_paths', False)})): ['/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi', '/local/rcs/XXX/code/pytrace-collector', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python39.zip', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/lib-dynload', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages/PySnooper-1.2.0-py3.9.egg']}, <function Project._get_base_sys_path at 0x7f58661fc430>: {(<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, (<jedi.inference.InferenceState object at 0x7f5855c938e0>,), frozenset()): ['/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi', '/local/rcs/XXX/code/pytrace-collector', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python39.zip', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/lib-dynload', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages/PySnooper-1.2.0-py3.9.egg']}}, self.mixed_cache={}, self.module_cache=<jedi.inference.imports.ModuleCache object at 0x7f5855c93910>, self.project=<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, self.recursion_detector=<jedi.inference.recursion.RecursionDetector object at 0x7f5865296ca0>, self.script_path=PosixPath('/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi/example.py'), self.stub_module_cache={})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "is_classdef = type_ == 'classdef'\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_root_context(self):\n",
      "        value = self\n",
      "        if value.parent_context is None:\n",
      "            return value.as_context()\n",
      "        while True:\n",
      "            if value.parent_context is None:\n",
      "                return value\n",
      "            value = value.parent_context\n",
      "get_root_context(self=<FunctionValue: <Function: f@1-3>>, self.inference_state=<jedi.inference.InferenceState object at 0x7f5855c938e0>, self.parent_context=ModuleContext(<ModuleValue: example@1-3 is_stub=False>), self.tree_node=<Function: f@1-3>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "value = self\n",
      "State:\n",
      "<FunctionValue: <Function: f@1-3>>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def infer_atom(context, atom):\n",
      "    state = context.inference_state\n",
      "    if atom.type == 'name':\n",
      "        stmt = tree.search_ancestor(atom, 'expr_stmt', 'lambdef', 'if_stmt') or atom\n",
      "        if stmt.type == 'if_stmt':\n",
      "            if not any(n.start_pos <= atom.start_pos < n.end_pos for n in stmt.get_test_nodes()):\n",
      "                stmt = atom\n",
      "        elif stmt.type == 'lambdef':\n",
      "            stmt = atom\n",
      "        position = stmt.start_pos\n",
      "        if _is_annotation_name(atom):\n",
      "            position = None\n",
      "        return context.py__getattribute__(atom, position=position)\n",
      "    elif atom.type == 'keyword':\n",
      "        if atom.value in ('False', 'True', 'None'):\n",
      "            return ValueSet([compiled.builtin_from_name(state, atom.value)])\n",
      "        elif atom.value == 'yield':\n",
      "            return NO_VALUES\n",
      "        assert False, 'Cannot infer the keyword %s' % atom\n",
      "    elif isinstance(atom, tree.Literal):\n",
      "        string = state.compiled_subprocess.safe_literal_eval(atom.value)\n",
      "        return ValueSet([compiled.create_simple_object(state, string)])\n",
      "    elif atom.type == 'strings':\n",
      "        value_set = infer_atom(context, atom.children[0])\n",
      "        for string in atom.children[1:]:\n",
      "            right = infer_atom(context, string)\n",
      "            value_set = _infer_comparison(context, value_set, '+', right)\n",
      "        return value_set\n",
      "    elif atom.type == 'fstring':\n",
      "        return compiled.get_string_value_set(state)\n",
      "    else:\n",
      "        c = atom.children\n",
      "        if c[0] == '(' and not len(c) == 2 \\\n",
      "                and not (c[1].type == 'testlist_comp'\n",
      "                         and len(c[1].children) > 1):\n",
      "            return context.infer_node(c[1])\n",
      "        try:\n",
      "            comp_for = c[1].children[1]\n",
      "        except (IndexError, AttributeError):\n",
      "            pass\n",
      "        else:\n",
      "            if comp_for == ':':\n",
      "                try:\n",
      "                    comp_for = c[1].children[3]\n",
      "                except IndexError:\n",
      "                    pass\n",
      "            if comp_for.type in ('comp_for', 'sync_comp_for'):\n",
      "                return ValueSet([iterable.comprehension_from_atom(\n",
      "                    state, context, atom\n",
      "                )])\n",
      "        array_node = c[1]\n",
      "        try:\n",
      "            array_node_c = array_node.children\n",
      "        except AttributeError:\n",
      "            array_node_c = []\n",
      "        if c[0] == '{' and (array_node == '}' or ':' in array_node_c\n",
      "                            or '**' in array_node_c):\n",
      "            new_value = iterable.DictLiteralValue(state, context, atom)\n",
      "        else:\n",
      "            new_value = iterable.SequenceLiteralValue(state, context, atom)\n",
      "        return ValueSet([new_value])\n",
      "infer_atom(context=ModuleContext(<ModuleValue: example@2-3 is_stub=False>), atom=<Name: os@3,0>)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "position = stmt.start_pos\n",
      "State:\n",
      "(3, 0)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_flow_scopes(node):\n",
      "    while True:\n",
      "        node = get_parent_scope(node, include_flows=True)\n",
      "        if node is None or is_scope(node):\n",
      "            return\n",
      "        yield node\n",
      "_get_flow_scopes(node=<Name: os@3,0>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "node = get_parent_scope(node, include_flows=True)\n",
      "State:\n",
      "<Module: @2-3>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def tree_name_to_values(inference_state, context, tree_name):\n",
      "    value_set = NO_VALUES\n",
      "    module_node = context.get_root_context().tree_node\n",
      "    if module_node is not None:\n",
      "        names = module_node.get_used_names().get(tree_name.value, [])\n",
      "        found_annotation = False\n",
      "        for name in names:\n",
      "            expr_stmt = name.parent\n",
      "            if expr_stmt.type == \"expr_stmt\" and expr_stmt.children[1].type == \"annassign\":\n",
      "                correct_scope = parser_utils.get_parent_scope(name) == context.tree_node\n",
      "                if correct_scope:\n",
      "                    found_annotation = True\n",
      "                    value_set |= annotation.infer_annotation(\n",
      "                        context, expr_stmt.children[1].children[1]\n",
      "                    ).execute_annotation()\n",
      "        if found_annotation:\n",
      "            return value_set\n",
      "    types = []\n",
      "    node = tree_name.get_definition(import_name_always=True, include_setitem=True)\n",
      "    if node is None:\n",
      "        node = tree_name.parent\n",
      "        if node.type == 'global_stmt':\n",
      "            c = context.create_context(tree_name)\n",
      "            if c.is_module():\n",
      "                return NO_VALUES\n",
      "            filter = next(c.get_filters())\n",
      "            names = filter.get(tree_name.value)\n",
      "            return ValueSet.from_sets(name.infer() for name in names)\n",
      "        elif node.type not in ('import_from', 'import_name'):\n",
      "            c = context.create_context(tree_name)\n",
      "            return infer_atom(c, tree_name)\n",
      "    typ = node.type\n",
      "    if typ == 'for_stmt':\n",
      "        types = annotation.find_type_from_comment_hint_for(context, node, tree_name)\n",
      "        if types:\n",
      "            return types\n",
      "    if typ == 'with_stmt':\n",
      "        types = annotation.find_type_from_comment_hint_with(context, node, tree_name)\n",
      "        if types:\n",
      "            return types\n",
      "    if typ in ('for_stmt', 'comp_for', 'sync_comp_for'):\n",
      "        try:\n",
      "            types = context.predefined_names[node][tree_name.value]\n",
      "        except KeyError:\n",
      "            cn = ContextualizedNode(context, node.children[3])\n",
      "            for_types = iterate_values(\n",
      "                cn.infer(),\n",
      "                contextualized_node=cn,\n",
      "                is_async=node.parent.type == 'async_stmt',\n",
      "            )\n",
      "            n = TreeNameDefinition(context, tree_name)\n",
      "            types = check_tuple_assignments(n, for_types)\n",
      "    elif typ == 'expr_stmt':\n",
      "        types = infer_expr_stmt(context, node, tree_name)\n",
      "    elif typ == 'with_stmt':\n",
      "        value_managers = context.infer_node(node.get_test_node_from_name(tree_name))\n",
      "        if node.parent.type == 'async_stmt':\n",
      "            enter_methods = value_managers.py__getattribute__('__aenter__')\n",
      "            coro = enter_methods.execute_with_values()\n",
      "            return coro.py__await__().py__stop_iteration_returns()\n",
      "        enter_methods = value_managers.py__getattribute__('__enter__')\n",
      "        return enter_methods.execute_with_values()\n",
      "    elif typ in ('import_from', 'import_name'):\n",
      "        types = imports.infer_import(context, tree_name)\n",
      "    elif typ in ('funcdef', 'classdef'):\n",
      "        types = _apply_decorators(context, node)\n",
      "    elif typ == 'try_stmt':\n",
      "        exceptions = context.infer_node(tree_name.get_previous_sibling().get_previous_sibling())\n",
      "        types = exceptions.execute_with_values()\n",
      "    elif typ == 'param':\n",
      "        types = NO_VALUES\n",
      "    elif typ == 'del_stmt':\n",
      "        types = NO_VALUES\n",
      "    elif typ == 'namedexpr_test':\n",
      "        types = infer_node(context, node)\n",
      "    else:\n",
      "        raise ValueError(\"Should not happen. type: %s\" % typ)\n",
      "    return types\n",
      "tree_name_to_values(inference_state={environment=<SameEnvironment: 3.9.19 in /local/rcs/XXX/miniforge3/envs/davidhalter+jedi>, script_path=PosixPath('/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi/example.py'), compiled_subprocess=<jedi.inference.compiled.subprocess.InferenceStateSubprocess object at 0x7f5855c93d90>, grammar=<PythonGrammar:single_input file_input eval_input ...>, latest_grammar=<PythonGrammar:single_input file_input eval_input ...>, memoize_cache={<function Project._get_sys_path at 0x7f58661fc310>: {(<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, (<jedi.inference.InferenceState object at 0x7f5855c93e80>,), frozenset({('add_parent_paths', False)})): ['/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi', '/local/rcs/XXX/code/pytrace-collector', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python39.zip', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/lib-dynload', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages/PySnooper-1.2.0-py3.9.egg']}, <function Project._get_base_sys_path at 0x7f58661fc430>: {(<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, (<jedi.inference.InferenceState object at 0x7f5855c93e80>,), frozenset()): ['/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi', '/local/rcs/XXX/code/pytrace-collector', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python39.zip', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/lib-dynload', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages/PySnooper-1.2.0-py3.9.egg']}, <function _infer_node_cached at 0x7f58663563a0>: {(ModuleContext(<ModuleValue: example@2-3 is_stub=False>), (<Name: os@3,0>,), frozenset()): S{}}}, module_cache=<jedi.inference.imports.ModuleCache object at 0x7f5855c930a0>, stub_module_cache={}, compiled_cache={}, inferred_element_counts={<Module: @2-3>: 1}, mixed_cache={}, analysis=[], dynamic_params_depth=0, do_dynamic_params_search=True, is_analysis=False, project=<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, access_cache={}, allow_unsafe_executions=False, flow_analysis_enabled=True, recursion_detector=<jedi.inference.recursion.RecursionDetector object at 0x7f5855c93a60>, execution_recursion_detector=<jedi.inference.recursion.ExecutionRecursionDetector object at 0x7f5855c93970>}, context=ModuleContext(<ModuleValue: example@2-3 is_stub=False>), tree_name=<Name: os@2,7>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "value_set = NO_VALUES\n",
      "State:\n",
      "S{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def tree_name_to_values(inference_state, context, tree_name):\n",
      "    value_set = NO_VALUES\n",
      "    module_node = context.get_root_context().tree_node\n",
      "    if module_node is not None:\n",
      "        names = module_node.get_used_names().get(tree_name.value, [])\n",
      "        found_annotation = False\n",
      "        for name in names:\n",
      "            expr_stmt = name.parent\n",
      "            if expr_stmt.type == \"expr_stmt\" and expr_stmt.children[1].type == \"annassign\":\n",
      "                correct_scope = parser_utils.get_parent_scope(name) == context.tree_node\n",
      "                if correct_scope:\n",
      "                    found_annotation = True\n",
      "                    value_set |= annotation.infer_annotation(\n",
      "                        context, expr_stmt.children[1].children[1]\n",
      "                    ).execute_annotation()\n",
      "        if found_annotation:\n",
      "            return value_set\n",
      "    types = []\n",
      "    node = tree_name.get_definition(import_name_always=True, include_setitem=True)\n",
      "    if node is None:\n",
      "        node = tree_name.parent\n",
      "        if node.type == 'global_stmt':\n",
      "            c = context.create_context(tree_name)\n",
      "            if c.is_module():\n",
      "                return NO_VALUES\n",
      "            filter = next(c.get_filters())\n",
      "            names = filter.get(tree_name.value)\n",
      "            return ValueSet.from_sets(name.infer() for name in names)\n",
      "        elif node.type not in ('import_from', 'import_name'):\n",
      "            c = context.create_context(tree_name)\n",
      "            return infer_atom(c, tree_name)\n",
      "    typ = node.type\n",
      "    if typ == 'for_stmt':\n",
      "        types = annotation.find_type_from_comment_hint_for(context, node, tree_name)\n",
      "        if types:\n",
      "            return types\n",
      "    if typ == 'with_stmt':\n",
      "        types = annotation.find_type_from_comment_hint_with(context, node, tree_name)\n",
      "        if types:\n",
      "            return types\n",
      "    if typ in ('for_stmt', 'comp_for', 'sync_comp_for'):\n",
      "        try:\n",
      "            types = context.predefined_names[node][tree_name.value]\n",
      "        except KeyError:\n",
      "            cn = ContextualizedNode(context, node.children[3])\n",
      "            for_types = iterate_values(\n",
      "                cn.infer(),\n",
      "                contextualized_node=cn,\n",
      "                is_async=node.parent.type == 'async_stmt',\n",
      "            )\n",
      "            n = TreeNameDefinition(context, tree_name)\n",
      "            types = check_tuple_assignments(n, for_types)\n",
      "    elif typ == 'expr_stmt':\n",
      "        types = infer_expr_stmt(context, node, tree_name)\n",
      "    elif typ == 'with_stmt':\n",
      "        value_managers = context.infer_node(node.get_test_node_from_name(tree_name))\n",
      "        if node.parent.type == 'async_stmt':\n",
      "            enter_methods = value_managers.py__getattribute__('__aenter__')\n",
      "            coro = enter_methods.execute_with_values()\n",
      "            return coro.py__await__().py__stop_iteration_returns()\n",
      "        enter_methods = value_managers.py__getattribute__('__enter__')\n",
      "        return enter_methods.execute_with_values()\n",
      "    elif typ in ('import_from', 'import_name'):\n",
      "        types = imports.infer_import(context, tree_name)\n",
      "    elif typ in ('funcdef', 'classdef'):\n",
      "        types = _apply_decorators(context, node)\n",
      "    elif typ == 'try_stmt':\n",
      "        exceptions = context.infer_node(tree_name.get_previous_sibling().get_previous_sibling())\n",
      "        types = exceptions.execute_with_values()\n",
      "    elif typ == 'param':\n",
      "        types = NO_VALUES\n",
      "    elif typ == 'del_stmt':\n",
      "        types = NO_VALUES\n",
      "    elif typ == 'namedexpr_test':\n",
      "        types = infer_node(context, node)\n",
      "    else:\n",
      "        raise ValueError(\"Should not happen. type: %s\" % typ)\n",
      "    return types\n",
      "tree_name_to_values(inference_state={environment=<SameEnvironment: 3.9.19 in /local/rcs/XXX/miniforge3/envs/davidhalter+jedi>, script_path=PosixPath('/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi/example.py'), compiled_subprocess=<jedi.inference.compiled.subprocess.InferenceStateSubprocess object at 0x7f5855c93d90>, grammar=<PythonGrammar:single_input file_input eval_input ...>, latest_grammar=<PythonGrammar:single_input file_input eval_input ...>, memoize_cache={<function Project._get_sys_path at 0x7f58661fc310>: {(<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, (<jedi.inference.InferenceState object at 0x7f5855c93e80>,), frozenset({('add_parent_paths', False)})): ['/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi', '/local/rcs/XXX/code/pytrace-collector', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python39.zip', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/lib-dynload', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages/PySnooper-1.2.0-py3.9.egg']}, <function Project._get_base_sys_path at 0x7f58661fc430>: {(<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, (<jedi.inference.InferenceState object at 0x7f5855c93e80>,), frozenset()): ['/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi', '/local/rcs/XXX/code/pytrace-collector', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python39.zip', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/lib-dynload', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages/PySnooper-1.2.0-py3.9.egg']}, <function _infer_node_cached at 0x7f58663563a0>: {(ModuleContext(<ModuleValue: example@2-3 is_stub=False>), (<Name: os@3,0>,), frozenset()): S{}}}, module_cache=<jedi.inference.imports.ModuleCache object at 0x7f5855c930a0>, stub_module_cache={}, compiled_cache={}, inferred_element_counts={<Module: @2-3>: 1}, mixed_cache={}, analysis=[], dynamic_params_depth=0, do_dynamic_params_search=True, is_analysis=False, project=<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, access_cache={}, allow_unsafe_executions=False, flow_analysis_enabled=True, recursion_detector=<jedi.inference.recursion.RecursionDetector object at 0x7f5855c93a60>, execution_recursion_detector=<jedi.inference.recursion.ExecutionRecursionDetector object at 0x7f5855c93970>}, context=ModuleContext(<ModuleValue: example@2-3 is_stub=False>), tree_name=<Name: os@2,7>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "module_node = context.get_root_context().tree_node\n",
      "State:\n",
      "<Module: @2-3>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def tree_name_to_values(inference_state, context, tree_name):\n",
      "    value_set = NO_VALUES\n",
      "    module_node = context.get_root_context().tree_node\n",
      "    if module_node is not None:\n",
      "        names = module_node.get_used_names().get(tree_name.value, [])\n",
      "        found_annotation = False\n",
      "        for name in names:\n",
      "            expr_stmt = name.parent\n",
      "            if expr_stmt.type == \"expr_stmt\" and expr_stmt.children[1].type == \"annassign\":\n",
      "                correct_scope = parser_utils.get_parent_scope(name) == context.tree_node\n",
      "                if correct_scope:\n",
      "                    found_annotation = True\n",
      "                    value_set |= annotation.infer_annotation(\n",
      "                        context, expr_stmt.children[1].children[1]\n",
      "                    ).execute_annotation()\n",
      "        if found_annotation:\n",
      "            return value_set\n",
      "    types = []\n",
      "    node = tree_name.get_definition(import_name_always=True, include_setitem=True)\n",
      "    if node is None:\n",
      "        node = tree_name.parent\n",
      "        if node.type == 'global_stmt':\n",
      "            c = context.create_context(tree_name)\n",
      "            if c.is_module():\n",
      "                return NO_VALUES\n",
      "            filter = next(c.get_filters())\n",
      "            names = filter.get(tree_name.value)\n",
      "            return ValueSet.from_sets(name.infer() for name in names)\n",
      "        elif node.type not in ('import_from', 'import_name'):\n",
      "            c = context.create_context(tree_name)\n",
      "            return infer_atom(c, tree_name)\n",
      "    typ = node.type\n",
      "    if typ == 'for_stmt':\n",
      "        types = annotation.find_type_from_comment_hint_for(context, node, tree_name)\n",
      "        if types:\n",
      "            return types\n",
      "    if typ == 'with_stmt':\n",
      "        types = annotation.find_type_from_comment_hint_with(context, node, tree_name)\n",
      "        if types:\n",
      "            return types\n",
      "    if typ in ('for_stmt', 'comp_for', 'sync_comp_for'):\n",
      "        try:\n",
      "            types = context.predefined_names[node][tree_name.value]\n",
      "        except KeyError:\n",
      "            cn = ContextualizedNode(context, node.children[3])\n",
      "            for_types = iterate_values(\n",
      "                cn.infer(),\n",
      "                contextualized_node=cn,\n",
      "                is_async=node.parent.type == 'async_stmt',\n",
      "            )\n",
      "            n = TreeNameDefinition(context, tree_name)\n",
      "            types = check_tuple_assignments(n, for_types)\n",
      "    elif typ == 'expr_stmt':\n",
      "        types = infer_expr_stmt(context, node, tree_name)\n",
      "    elif typ == 'with_stmt':\n",
      "        value_managers = context.infer_node(node.get_test_node_from_name(tree_name))\n",
      "        if node.parent.type == 'async_stmt':\n",
      "            enter_methods = value_managers.py__getattribute__('__aenter__')\n",
      "            coro = enter_methods.execute_with_values()\n",
      "            return coro.py__await__().py__stop_iteration_returns()\n",
      "        enter_methods = value_managers.py__getattribute__('__enter__')\n",
      "        return enter_methods.execute_with_values()\n",
      "    elif typ in ('import_from', 'import_name'):\n",
      "        types = imports.infer_import(context, tree_name)\n",
      "    elif typ in ('funcdef', 'classdef'):\n",
      "        types = _apply_decorators(context, node)\n",
      "    elif typ == 'try_stmt':\n",
      "        exceptions = context.infer_node(tree_name.get_previous_sibling().get_previous_sibling())\n",
      "        types = exceptions.execute_with_values()\n",
      "    elif typ == 'param':\n",
      "        types = NO_VALUES\n",
      "    elif typ == 'del_stmt':\n",
      "        types = NO_VALUES\n",
      "    elif typ == 'namedexpr_test':\n",
      "        types = infer_node(context, node)\n",
      "    else:\n",
      "        raise ValueError(\"Should not happen. type: %s\" % typ)\n",
      "    return types\n",
      "tree_name_to_values(inference_state={environment=<SameEnvironment: 3.9.19 in /local/rcs/XXX/miniforge3/envs/davidhalter+jedi>, script_path=PosixPath('/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi/example.py'), compiled_subprocess=<jedi.inference.compiled.subprocess.InferenceStateSubprocess object at 0x7f5855c93d90>, grammar=<PythonGrammar:single_input file_input eval_input ...>, latest_grammar=<PythonGrammar:single_input file_input eval_input ...>, memoize_cache={<function Project._get_sys_path at 0x7f58661fc310>: {(<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, (<jedi.inference.InferenceState object at 0x7f5855c93e80>,), frozenset({('add_parent_paths', False)})): ['/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi', '/local/rcs/XXX/code/pytrace-collector', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python39.zip', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/lib-dynload', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages/PySnooper-1.2.0-py3.9.egg']}, <function Project._get_base_sys_path at 0x7f58661fc430>: {(<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, (<jedi.inference.InferenceState object at 0x7f5855c93e80>,), frozenset()): ['/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi', '/local/rcs/XXX/code/pytrace-collector', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python39.zip', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/lib-dynload', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages/PySnooper-1.2.0-py3.9.egg']}, <function _infer_node_cached at 0x7f58663563a0>: {(ModuleContext(<ModuleValue: example@2-3 is_stub=False>), (<Name: os@3,0>,), frozenset()): S{}}}, module_cache=<jedi.inference.imports.ModuleCache object at 0x7f5855c930a0>, stub_module_cache={}, compiled_cache={}, inferred_element_counts={<Module: @2-3>: 1}, mixed_cache={}, analysis=[], dynamic_params_depth=0, do_dynamic_params_search=True, is_analysis=False, project=<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, access_cache={}, allow_unsafe_executions=False, flow_analysis_enabled=True, recursion_detector=<jedi.inference.recursion.RecursionDetector object at 0x7f5855c93a60>, execution_recursion_detector=<jedi.inference.recursion.ExecutionRecursionDetector object at 0x7f5855c93970>}, context=ModuleContext(<ModuleValue: example@2-3 is_stub=False>), tree_name=<Name: os@2,7>)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "names = module_node.get_used_names().get(tree_name.value, [])\n",
      "State:\n",
      "[<Name: os@2,7>, <Name: os@3,0>]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def tree_name_to_values(inference_state, context, tree_name):\n",
      "    value_set = NO_VALUES\n",
      "    module_node = context.get_root_context().tree_node\n",
      "    if module_node is not None:\n",
      "        names = module_node.get_used_names().get(tree_name.value, [])\n",
      "        found_annotation = False\n",
      "        for name in names:\n",
      "            expr_stmt = name.parent\n",
      "            if expr_stmt.type == \"expr_stmt\" and expr_stmt.children[1].type == \"annassign\":\n",
      "                correct_scope = parser_utils.get_parent_scope(name) == context.tree_node\n",
      "                if correct_scope:\n",
      "                    found_annotation = True\n",
      "                    value_set |= annotation.infer_annotation(\n",
      "                        context, expr_stmt.children[1].children[1]\n",
      "                    ).execute_annotation()\n",
      "        if found_annotation:\n",
      "            return value_set\n",
      "    types = []\n",
      "    node = tree_name.get_definition(import_name_always=True, include_setitem=True)\n",
      "    if node is None:\n",
      "        node = tree_name.parent\n",
      "        if node.type == 'global_stmt':\n",
      "            c = context.create_context(tree_name)\n",
      "            if c.is_module():\n",
      "                return NO_VALUES\n",
      "            filter = next(c.get_filters())\n",
      "            names = filter.get(tree_name.value)\n",
      "            return ValueSet.from_sets(name.infer() for name in names)\n",
      "        elif node.type not in ('import_from', 'import_name'):\n",
      "            c = context.create_context(tree_name)\n",
      "            return infer_atom(c, tree_name)\n",
      "    typ = node.type\n",
      "    if typ == 'for_stmt':\n",
      "        types = annotation.find_type_from_comment_hint_for(context, node, tree_name)\n",
      "        if types:\n",
      "            return types\n",
      "    if typ == 'with_stmt':\n",
      "        types = annotation.find_type_from_comment_hint_with(context, node, tree_name)\n",
      "        if types:\n",
      "            return types\n",
      "    if typ in ('for_stmt', 'comp_for', 'sync_comp_for'):\n",
      "        try:\n",
      "            types = context.predefined_names[node][tree_name.value]\n",
      "        except KeyError:\n",
      "            cn = ContextualizedNode(context, node.children[3])\n",
      "            for_types = iterate_values(\n",
      "                cn.infer(),\n",
      "                contextualized_node=cn,\n",
      "                is_async=node.parent.type == 'async_stmt',\n",
      "            )\n",
      "            n = TreeNameDefinition(context, tree_name)\n",
      "            types = check_tuple_assignments(n, for_types)\n",
      "    elif typ == 'expr_stmt':\n",
      "        types = infer_expr_stmt(context, node, tree_name)\n",
      "    elif typ == 'with_stmt':\n",
      "        value_managers = context.infer_node(node.get_test_node_from_name(tree_name))\n",
      "        if node.parent.type == 'async_stmt':\n",
      "            enter_methods = value_managers.py__getattribute__('__aenter__')\n",
      "            coro = enter_methods.execute_with_values()\n",
      "            return coro.py__await__().py__stop_iteration_returns()\n",
      "        enter_methods = value_managers.py__getattribute__('__enter__')\n",
      "        return enter_methods.execute_with_values()\n",
      "    elif typ in ('import_from', 'import_name'):\n",
      "        types = imports.infer_import(context, tree_name)\n",
      "    elif typ in ('funcdef', 'classdef'):\n",
      "        types = _apply_decorators(context, node)\n",
      "    elif typ == 'try_stmt':\n",
      "        exceptions = context.infer_node(tree_name.get_previous_sibling().get_previous_sibling())\n",
      "        types = exceptions.execute_with_values()\n",
      "    elif typ == 'param':\n",
      "        types = NO_VALUES\n",
      "    elif typ == 'del_stmt':\n",
      "        types = NO_VALUES\n",
      "    elif typ == 'namedexpr_test':\n",
      "        types = infer_node(context, node)\n",
      "    else:\n",
      "        raise ValueError(\"Should not happen. type: %s\" % typ)\n",
      "    return types\n",
      "tree_name_to_values(inference_state={environment=<SameEnvironment: 3.9.19 in /local/rcs/XXX/miniforge3/envs/davidhalter+jedi>, script_path=PosixPath('/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi/example.py'), compiled_subprocess=<jedi.inference.compiled.subprocess.InferenceStateSubprocess object at 0x7f5855c93d90>, grammar=<PythonGrammar:single_input file_input eval_input ...>, latest_grammar=<PythonGrammar:single_input file_input eval_input ...>, memoize_cache={<function Project._get_sys_path at 0x7f58661fc310>: {(<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, (<jedi.inference.InferenceState object at 0x7f5855c93e80>,), frozenset({('add_parent_paths', False)})): ['/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi', '/local/rcs/XXX/code/pytrace-collector', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python39.zip', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/lib-dynload', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages/PySnooper-1.2.0-py3.9.egg']}, <function Project._get_base_sys_path at 0x7f58661fc430>: {(<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, (<jedi.inference.InferenceState object at 0x7f5855c93e80>,), frozenset()): ['/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi', '/local/rcs/XXX/code/pytrace-collector', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python39.zip', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/lib-dynload', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages', '/local/rcs/XXX/miniforge3/envs/davidhalter+jedi/lib/python3.9/site-packages/PySnooper-1.2.0-py3.9.egg']}, <function _infer_node_cached at 0x7f58663563a0>: {(ModuleContext(<ModuleValue: example@2-3 is_stub=False>), (<Name: os@3,0>,), frozenset()): S{}}}, module_cache=<jedi.inference.imports.ModuleCache object at 0x7f5855c930a0>, stub_module_cache={}, compiled_cache={}, inferred_element_counts={<Module: @2-3>: 1}, mixed_cache={}, analysis=[], dynamic_params_depth=0, do_dynamic_params_search=True, is_analysis=False, project=<Project: /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/davidhalter+jedi/davidhalter+jedi>, access_cache={}, allow_unsafe_executions=False, flow_analysis_enabled=True, recursion_detector=<jedi.inference.recursion.RecursionDetector object at 0x7f5855c93a60>, execution_recursion_detector=<jedi.inference.recursion.ExecutionRecursionDetector object at 0x7f5855c93970>}, context=ModuleContext(<ModuleValue: example@2-3 is_stub=False>), tree_name=<Name: os@2,7>)\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "types = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_flow_branch_keyword(flow_node, node):\n",
      "    start_pos = node.start_pos\n",
      "    if not (flow_node.start_pos < start_pos <= flow_node.end_pos):\n",
      "        raise ValueError('The node is not part of the flow.')\n",
      "    keyword = None\n",
      "    for i, child in enumerate(flow_node.children):\n",
      "        if start_pos < child.start_pos:\n",
      "            return keyword\n",
      "        first_leaf = child.get_first_leaf()\n",
      "        if first_leaf in _FLOW_KEYWORDS:\n",
      "            keyword = first_leaf\n",
      "    return None\n",
      "get_flow_branch_keyword(flow_node=<ForStmt: for variable in [keyword, f, C, x]:     variable@15,0>, node=<Name: variable@15,4>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "start_pos = node.start_pos\n",
      "State:\n",
      "(15, 4)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _infer_node_if_inferred(context, element):\n",
      "    parent = element\n",
      "    while parent is not None:\n",
      "        parent = parent.parent\n",
      "        predefined_if_name_dict = context.predefined_names.get(parent)\n",
      "        if predefined_if_name_dict is not None:\n",
      "            return _infer_node(context, element)\n",
      "    return _infer_node_cached(context, element)\n",
      "_infer_node_if_inferred(context=ModuleContext(<ModuleValue: __main__@2-16 is_stub=False>), element=PythonNode(atom, [<Operator: [>, PythonNode(testlist_comp, [<Name: keyword@15,17>, <Operator: ,>, <Name: f@15,26>, <Operator: ,>, <Name: C@15,29>, <Operator: ,>, <Name: x@15,32>]), <Operator: ]>]))\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "parent = element\n",
      "State:\n",
      "PythonNode(atom, [<Operator: [>, PythonNode(testlist_comp, [<Name: keyword@15,17>, <Operator: ,>, <Name: f@15,26>, <Operator: ,>, <Name: C@15,29>, <Operator: ,>, <Name: x@15,32>]), <Operator: ]>])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_executable_nodes(node, last_added=False):\n",
      "    result = []\n",
      "    typ = node.type\n",
      "    if typ == 'name':\n",
      "        next_leaf = node.get_next_leaf()\n",
      "        if last_added is False and node.parent.type != 'param' and next_leaf != '=':\n",
      "            result.append(node)\n",
      "    elif typ == 'expr_stmt':\n",
      "        result.append(node)\n",
      "        for child in node.children:\n",
      "            result += get_executable_nodes(child, last_added=True)\n",
      "    elif typ == 'decorator':\n",
      "        if node.children[-2] == ')':\n",
      "            node = node.children[-3]\n",
      "            if node != '(':\n",
      "                result += get_executable_nodes(node)\n",
      "    else:\n",
      "        try:\n",
      "            children = node.children\n",
      "        except AttributeError:\n",
      "            pass\n",
      "        else:\n",
      "            if node.type in _EXECUTE_NODES and not last_added:\n",
      "                result.append(node)\n",
      "            for child in children:\n",
      "                result += get_executable_nodes(child, last_added)\n",
      "    return result\n",
      "get_executable_nodes(node=<Name: bar@1,0>, last_added=True)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "result = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def generate(resource_types=()):\n",
      "    resource_defs = {}\n",
      "    definitions = {\n",
      "        'resources': resource_defs,\n",
      "        'string_dict': {\n",
      "            \"type\": \"object\",\n",
      "            \"patternProperties\": {\n",
      "                \"\": {\"type\": \"string\"},\n",
      "            },\n",
      "        },\n",
      "        'basic_dict': {\n",
      "            \"type\": \"object\",\n",
      "            \"patternProperties\": {\n",
      "                \"\": {\n",
      "                    'oneOf': [\n",
      "                        {\"type\": \"string\"},\n",
      "                        {\"type\": \"boolean\"},\n",
      "                        {\"type\": \"number\"},\n",
      "                    ],\n",
      "                }\n",
      "            },\n",
      "        },\n",
      "        'iam-statement': {\n",
      "            'additionalProperties': False,\n",
      "            'type': 'object',\n",
      "            'properties': {\n",
      "                'Sid': {'type': 'string'},\n",
      "                'Effect': {'type': 'string', 'enum': ['Allow', 'Deny']},\n",
      "                'Principal': {'anyOf': [\n",
      "                    {'type': 'string'},\n",
      "                    {'type': 'object'}, {'type': 'array'}]},\n",
      "                'NotPrincipal': {'anyOf': [{'type': 'object'}, {'type': 'array'}]},\n",
      "                'Action': {'anyOf': [{'type': 'string'}, {'type': 'array'}]},\n",
      "                'NotAction': {'anyOf': [{'type': 'string'}, {'type': 'array'}]},\n",
      "                'Resource': {'anyOf': [{'type': 'string'}, {'type': 'array'}]},\n",
      "                'NotResource': {'anyOf': [{'type': 'string'}, {'type': 'array'}]},\n",
      "                'Condition': {'type': 'object'}\n",
      "            },\n",
      "            'required': ['Sid', 'Effect'],\n",
      "            'oneOf': [\n",
      "                {'required': ['Principal', 'Action', 'Resource']},\n",
      "                {'required': ['NotPrincipal', 'Action', 'Resource']},\n",
      "                {'required': ['Principal', 'NotAction', 'Resource']},\n",
      "                {'required': ['NotPrincipal', 'NotAction', 'Resource']},\n",
      "                {'required': ['Principal', 'Action', 'NotResource']},\n",
      "                {'required': ['NotPrincipal', 'Action', 'NotResource']},\n",
      "                {'required': ['Principal', 'NotAction', 'NotResource']},\n",
      "                {'required': ['NotPrincipal', 'NotAction', 'NotResource']}\n",
      "            ]\n",
      "        },\n",
      "        'actions': {},\n",
      "        'filters': {\n",
      "            'value': ValueFilter.schema,\n",
      "            'event': EventFilter.schema,\n",
      "            'age': AgeFilter.schema,\n",
      "            'reduce': ReduceFilter.schema,\n",
      "            'valuekv': {\n",
      "                'type': 'object',\n",
      "                'additionalProperties': {'oneOf': [{'type': 'number'}, {'type': 'null'},\n",
      "                    {'type': 'array', 'maxItems': 0}, {'type': 'string'}, {'type': 'boolean'}]},\n",
      "                'minProperties': 1,\n",
      "                'maxProperties': 1},\n",
      "        },\n",
      "        'filters_common': {\n",
      "            'list_item_attrs': _get_attr_schema(),\n",
      "            'comparison_operators': {\n",
      "                'enum': list(OPERATORS.keys())},\n",
      "            'value_types': {'enum': VALUE_TYPES},\n",
      "            'value_from': ValuesFrom.schema,\n",
      "            'value': {'oneOf': [\n",
      "                {'type': 'array'},\n",
      "                {'type': 'string'},\n",
      "                {'type': 'boolean'},\n",
      "                {'type': 'number'},\n",
      "                {'type': 'null'}]},\n",
      "        },\n",
      "        'policy': {\n",
      "            'type': 'object',\n",
      "            'required': ['name', 'resource'],\n",
      "            'additionalProperties': False,\n",
      "            'properties': {\n",
      "                'name': {\n",
      "                    'type': 'string',\n",
      "                    'pattern': \"^[A-z][A-z0-9]*(-[A-z0-9]+)*$\"},\n",
      "                'conditions': {\n",
      "                    'type': 'array',\n",
      "                    'items': {'anyOf': [\n",
      "                        {'type': 'object', 'additionalProperties': False,\n",
      "                         'properties': {'or': {\n",
      "                             '$ref': '\n",
      "                        {'type': 'object', 'additionalProperties': False,\n",
      "                         'properties': {'not': {\n",
      "                             '$ref': '\n",
      "                        {'type': 'object', 'additionalProperties': False,\n",
      "                         'properties': {'and': {\n",
      "                             '$ref': '\n",
      "                        {'$ref': '\n",
      "                        {'$ref': '\n",
      "                        {'$ref': '\n",
      "                'region': {'type': 'string'},\n",
      "                'tz': {'type': 'string'},\n",
      "                'start': {'format': 'date-time'},\n",
      "                'end': {'format': 'date-time'},\n",
      "                'resource': {'oneOf': [\n",
      "                    {'type': 'string'},\n",
      "                    {'type': 'array', 'items': {'type': 'string'}}]},\n",
      "                'max-resources': {'anyOf': [\n",
      "                    {'type': 'integer', 'minimum': 1},\n",
      "                    {'$ref': '\n",
      "                ]},\n",
      "                'max-resources-percent': {'type': 'number', 'minimum': 0, 'maximum': 100},\n",
      "                'comment': {'type': 'string'},\n",
      "                'comments': {'type': 'string'},\n",
      "                'description': {'type': 'string'},\n",
      "                'tags': {'type': 'array', 'items': {'type': 'string'}},\n",
      "                'metadata': {'type': 'object'},\n",
      "                'mode': {'$ref': '\n",
      "                'source': {'enum': list(sources.keys())},\n",
      "                'actions': {\n",
      "                    'type': 'array',\n",
      "                },\n",
      "                'filters': {\n",
      "                    'type': 'array'\n",
      "                },\n",
      "                'query': {\n",
      "                    'type': 'array', 'items': {'type': 'object'}}\n",
      "            },\n",
      "        },\n",
      "        'policy-mode': {\n",
      "            'anyOf': [e.schema for _, e in execution.items()],\n",
      "        },\n",
      "        'max-resources-properties': {\n",
      "            'type': 'object',\n",
      "            'additionalProperties': False,\n",
      "            'properties': {\n",
      "                'amount': {\"type\": 'integer', 'minimum': 1},\n",
      "                'op': {'enum': ['or', 'and']},\n",
      "                'percent': {'type': 'number', 'minimum': 0, 'maximum': 100}\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "    resource_refs = []\n",
      "    for cloud_name, cloud_type in sorted(clouds.items()):\n",
      "        for type_name, resource_type in sorted(cloud_type.resources.items()):\n",
      "            r_type_name = \"%s.%s\" % (cloud_name, type_name)\n",
      "            if resource_types and r_type_name not in resource_types:\n",
      "                if not resource_type.type_aliases:\n",
      "                    continue\n",
      "                elif not {\"%s.%s\" % (cloud_name, ralias) for ralias\n",
      "                        in resource_type.type_aliases}.intersection(\n",
      "                        resource_types):\n",
      "                    continue\n",
      "            aliases = []\n",
      "            if resource_type.type_aliases:\n",
      "                aliases.extend([\"%s.%s\" % (cloud_name, a) for a in resource_type.type_aliases])\n",
      "                if cloud_name == 'aws':\n",
      "                    aliases.extend(resource_type.type_aliases)\n",
      "            if cloud_name == 'aws':\n",
      "                aliases.append(type_name)\n",
      "            resource_refs.append(\n",
      "                process_resource(\n",
      "                    r_type_name,\n",
      "                    resource_type,\n",
      "                    resource_defs,\n",
      "                    aliases,\n",
      "                    definitions,\n",
      "                    cloud_name\n",
      "                ))\n",
      "    schema = {\n",
      "        \"$schema\": \"http://json-schema.org/draft-07/schema\n",
      "        'id': 'http://schema.cloudcustodian.io/v0/custodian.json',\n",
      "        'definitions': definitions,\n",
      "        'type': 'object',\n",
      "        'required': ['policies'],\n",
      "        'additionalProperties': False,\n",
      "        'properties': {\n",
      "            'vars': {'type': 'object'},\n",
      "            'policies': {\n",
      "                'type': 'array',\n",
      "                'additionalItems': False,\n",
      "                'items': {'anyOf': resource_refs}\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "    if not resource_refs:\n",
      "        schema['properties']['policies']['items'] = {'type': 'object'}\n",
      "    return schema\n",
      "generate(resource_types=())\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "resource_defs = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def generate(resource_types=()):\n",
      "    resource_defs = {}\n",
      "    definitions = {\n",
      "        'resources': resource_defs,\n",
      "        'string_dict': {\n",
      "            \"type\": \"object\",\n",
      "            \"patternProperties\": {\n",
      "                \"\": {\"type\": \"string\"},\n",
      "            },\n",
      "        },\n",
      "        'basic_dict': {\n",
      "            \"type\": \"object\",\n",
      "            \"patternProperties\": {\n",
      "                \"\": {\n",
      "                    'oneOf': [\n",
      "                        {\"type\": \"string\"},\n",
      "                        {\"type\": \"boolean\"},\n",
      "                        {\"type\": \"number\"},\n",
      "                    ],\n",
      "                }\n",
      "            },\n",
      "        },\n",
      "        'iam-statement': {\n",
      "            'additionalProperties': False,\n",
      "            'type': 'object',\n",
      "            'properties': {\n",
      "                'Sid': {'type': 'string'},\n",
      "                'Effect': {'type': 'string', 'enum': ['Allow', 'Deny']},\n",
      "                'Principal': {'anyOf': [\n",
      "                    {'type': 'string'},\n",
      "                    {'type': 'object'}, {'type': 'array'}]},\n",
      "                'NotPrincipal': {'anyOf': [{'type': 'object'}, {'type': 'array'}]},\n",
      "                'Action': {'anyOf': [{'type': 'string'}, {'type': 'array'}]},\n",
      "                'NotAction': {'anyOf': [{'type': 'string'}, {'type': 'array'}]},\n",
      "                'Resource': {'anyOf': [{'type': 'string'}, {'type': 'array'}]},\n",
      "                'NotResource': {'anyOf': [{'type': 'string'}, {'type': 'array'}]},\n",
      "                'Condition': {'type': 'object'}\n",
      "            },\n",
      "            'required': ['Sid', 'Effect'],\n",
      "            'oneOf': [\n",
      "                {'required': ['Principal', 'Action', 'Resource']},\n",
      "                {'required': ['NotPrincipal', 'Action', 'Resource']},\n",
      "                {'required': ['Principal', 'NotAction', 'Resource']},\n",
      "                {'required': ['NotPrincipal', 'NotAction', 'Resource']},\n",
      "                {'required': ['Principal', 'Action', 'NotResource']},\n",
      "                {'required': ['NotPrincipal', 'Action', 'NotResource']},\n",
      "                {'required': ['Principal', 'NotAction', 'NotResource']},\n",
      "                {'required': ['NotPrincipal', 'NotAction', 'NotResource']}\n",
      "            ]\n",
      "        },\n",
      "        'actions': {},\n",
      "        'filters': {\n",
      "            'value': ValueFilter.schema,\n",
      "            'event': EventFilter.schema,\n",
      "            'age': AgeFilter.schema,\n",
      "            'reduce': ReduceFilter.schema,\n",
      "            'valuekv': {\n",
      "                'type': 'object',\n",
      "                'additionalProperties': {'oneOf': [{'type': 'number'}, {'type': 'null'},\n",
      "                    {'type': 'array', 'maxItems': 0}, {'type': 'string'}, {'type': 'boolean'}]},\n",
      "                'minProperties': 1,\n",
      "                'maxProperties': 1},\n",
      "        },\n",
      "        'filters_common': {\n",
      "            'list_item_attrs': _get_attr_schema(),\n",
      "            'comparison_operators': {\n",
      "                'enum': list(OPERATORS.keys())},\n",
      "            'value_types': {'enum': VALUE_TYPES},\n",
      "            'value_from': ValuesFrom.schema,\n",
      "            'value': {'oneOf': [\n",
      "                {'type': 'array'},\n",
      "                {'type': 'string'},\n",
      "                {'type': 'boolean'},\n",
      "                {'type': 'number'},\n",
      "                {'type': 'null'}]},\n",
      "        },\n",
      "        'policy': {\n",
      "            'type': 'object',\n",
      "            'required': ['name', 'resource'],\n",
      "            'additionalProperties': False,\n",
      "            'properties': {\n",
      "                'name': {\n",
      "                    'type': 'string',\n",
      "                    'pattern': \"^[A-z][A-z0-9]*(-[A-z0-9]+)*$\"},\n",
      "                'conditions': {\n",
      "                    'type': 'array',\n",
      "                    'items': {'anyOf': [\n",
      "                        {'type': 'object', 'additionalProperties': False,\n",
      "                         'properties': {'or': {\n",
      "                             '$ref': '\n",
      "                        {'type': 'object', 'additionalProperties': False,\n",
      "                         'properties': {'not': {\n",
      "                             '$ref': '\n",
      "                        {'type': 'object', 'additionalProperties': False,\n",
      "                         'properties': {'and': {\n",
      "                             '$ref': '\n",
      "                        {'$ref': '\n",
      "                        {'$ref': '\n",
      "                        {'$ref': '\n",
      "                'region': {'type': 'string'},\n",
      "                'tz': {'type': 'string'},\n",
      "                'start': {'format': 'date-time'},\n",
      "                'end': {'format': 'date-time'},\n",
      "                'resource': {'oneOf': [\n",
      "                    {'type': 'string'},\n",
      "                    {'type': 'array', 'items': {'type': 'string'}}]},\n",
      "                'max-resources': {'anyOf': [\n",
      "                    {'type': 'integer', 'minimum': 1},\n",
      "                    {'$ref': '\n",
      "                ]},\n",
      "                'max-resources-percent': {'type': 'number', 'minimum': 0, 'maximum': 100},\n",
      "                'comment': {'type': 'string'},\n",
      "                'comments': {'type': 'string'},\n",
      "                'description': {'type': 'string'},\n",
      "                'tags': {'type': 'array', 'items': {'type': 'string'}},\n",
      "                'metadata': {'type': 'object'},\n",
      "                'mode': {'$ref': '\n",
      "                'source': {'enum': list(sources.keys())},\n",
      "                'actions': {\n",
      "                    'type': 'array',\n",
      "                },\n",
      "                'filters': {\n",
      "                    'type': 'array'\n",
      "                },\n",
      "                'query': {\n",
      "                    'type': 'array', 'items': {'type': 'object'}}\n",
      "            },\n",
      "        },\n",
      "        'policy-mode': {\n",
      "            'anyOf': [e.schema for _, e in execution.items()],\n",
      "        },\n",
      "        'max-resources-properties': {\n",
      "            'type': 'object',\n",
      "            'additionalProperties': False,\n",
      "            'properties': {\n",
      "                'amount': {\"type\": 'integer', 'minimum': 1},\n",
      "                'op': {'enum': ['or', 'and']},\n",
      "                'percent': {'type': 'number', 'minimum': 0, 'maximum': 100}\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "    resource_refs = []\n",
      "    for cloud_name, cloud_type in sorted(clouds.items()):\n",
      "        for type_name, resource_type in sorted(cloud_type.resources.items()):\n",
      "            r_type_name = \"%s.%s\" % (cloud_name, type_name)\n",
      "            if resource_types and r_type_name not in resource_types:\n",
      "                if not resource_type.type_aliases:\n",
      "                    continue\n",
      "                elif not {\"%s.%s\" % (cloud_name, ralias) for ralias\n",
      "                        in resource_type.type_aliases}.intersection(\n",
      "                        resource_types):\n",
      "                    continue\n",
      "            aliases = []\n",
      "            if resource_type.type_aliases:\n",
      "                aliases.extend([\"%s.%s\" % (cloud_name, a) for a in resource_type.type_aliases])\n",
      "                if cloud_name == 'aws':\n",
      "                    aliases.extend(resource_type.type_aliases)\n",
      "            if cloud_name == 'aws':\n",
      "                aliases.append(type_name)\n",
      "            resource_refs.append(\n",
      "                process_resource(\n",
      "                    r_type_name,\n",
      "                    resource_type,\n",
      "                    resource_defs,\n",
      "                    aliases,\n",
      "                    definitions,\n",
      "                    cloud_name\n",
      "                ))\n",
      "    schema = {\n",
      "        \"$schema\": \"http://json-schema.org/draft-07/schema\n",
      "        'id': 'http://schema.cloudcustodian.io/v0/custodian.json',\n",
      "        'definitions': definitions,\n",
      "        'type': 'object',\n",
      "        'required': ['policies'],\n",
      "        'additionalProperties': False,\n",
      "        'properties': {\n",
      "            'vars': {'type': 'object'},\n",
      "            'policies': {\n",
      "                'type': 'array',\n",
      "                'additionalItems': False,\n",
      "                'items': {'anyOf': resource_refs}\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "    if not resource_refs:\n",
      "        schema['properties']['policies']['items'] = {'type': 'object'}\n",
      "    return schema\n",
      "generate(resource_types=())\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "definitions = {\n",
      "State:\n",
      "{'resources': {}, 'string_dict': {'type': 'object', 'patternProperties': {'': {'type': 'string'}}}, 'basic_dict': {'type': 'object', 'patternProperties': {'': {'oneOf': [{'type': 'string'}, {'type': 'boolean'}, {'type': 'number'}]}}}, 'iam-statement': {'additionalProperties': False, 'type': 'object', 'properties': {'Sid': {'type': 'string'}, 'Effect': {'type': 'string', 'enum': ['Allow', 'Deny']}, 'Principal': {'anyOf': [{'type': 'string'}, {'type': 'object'}, {'type': 'array'}]}, 'NotPrincipal': {'anyOf': [{'type': 'object'}, {'type': 'array'}]}, 'Action': {'anyOf': [{'type': 'string'}, {'type': 'array'}]}, 'NotAction': {'anyOf': [{'type': 'string'}, {'type': 'array'}]}, 'Resource': {'anyOf': [{'type': 'string'}, {'type': 'array'}]}, 'NotResource': {'anyOf': [{'type': 'string'}, {'type': 'array'}]}, 'Condition': {'type': 'object'}}, 'required': ['Sid', 'Effect'], 'oneOf': [{'required': ['Principal', 'Action', 'Resource']}, {'required': ['NotPrincipal', 'Action', 'Resource']}, {'required': ['Principal', 'NotAction', 'Resource']}, {'required': ['NotPrincipal', 'NotAction', 'Resource']}, {'required': ['Principal', 'Action', 'NotResource']}, {'required': ['NotPrincipal', 'Action', 'NotResource']}, {'required': ['Principal', 'NotAction', 'NotResource']}, {'required': ['NotPrincipal', 'NotAction', 'NotResource']}]}, 'actions': {}, 'filters': {'value': {'type': 'object', 'additionalProperties': False, 'required': ['type'], 'properties': {'type': {'enum': ['value']}, 'key': {'type': 'string'}, 'value_type': {'$ref': '#/definitions/filters_common/value_types'}, 'default': {'type': 'object'}, 'value_regex': {'type': 'string'}, 'value_from': {'$ref': '#/definitions/filters_common/value_from'}, 'value': {'$ref': '#/definitions/filters_common/value'}, 'op': {'$ref': '#/definitions/filters_common/comparison_operators'}, 'value_path': {'type': 'string'}}}, 'event': {'type': 'object', 'additionalProperties': False, 'required': ['type'], 'properties': {'type': {'enum': ['event']}, 'key': {'type': 'string'}, 'value_type': {'$ref': '#/definitions/filters_common/value_types'}, 'default': {'type': 'object'}, 'value_regex': {'type': 'string'}, 'value_from': {'$ref': '#/definitions/filters_common/value_from'}, 'value': {'$ref': '#/definitions/filters_common/value'}, 'op': {'$ref': '#/definitions/filters_common/comparison_operators'}, 'value_path': {'type': 'string'}}}, 'age': None, 'reduce': {'type': 'object', 'additionalProperties': False, 'required': ['type'], 'properties': {'type': {'enum': ['reduce']}, 'group-by': {'oneOf': [{'type': 'string'}, {'type': 'object', 'key': {'type': 'string'}, 'value_type': {'enum': ['string', 'number', 'date']}, 'value_regex': 'string'}]}, 'sort-by': {'oneOf': [{'type': 'string'}, {'type': 'object', 'key': {'type': 'string'}, 'value_type': {'enum': ['string', 'number', 'date']}, 'value_regex': 'string'}]}, 'order': {'enum': ['asc', 'desc', 'reverse', 'randomize']}, 'null-order': {'enum': ['first', 'last']}, 'limit': {'type': 'number', 'minimum': 0}, 'limit-percent': {'type': 'number', 'minimum': 0, 'maximum': 100}, 'discard': {'type': 'number', 'minimum': 0}, 'discard-percent': {'type': 'number', 'minimum': 0, 'maximum': 100}}}, 'valuekv': {'type': 'object', 'additionalProperties': {'oneOf': [{'type': 'number'}, {'type': 'null'}, {'type': 'array', 'maxItems': 0}, {'type': 'string'}, {'type': 'boolean'}]}, 'minProperties': 1, 'maxProperties': 1}}, 'filters_common': {'list_item_attrs': {'items': {'anyOf': [{'$ref': '#/definitions/filters/value'}, {'$ref': '#/definitions/filters/valuekv'}, {'additional_properties': False, 'properties': {'and': {'type': 'array', 'items': {'anyOf': [{'$ref': '#/definitions/filters/value'}, {'$ref': '#/definitions/filters/valuekv'}]}}}, 'type': 'object'}, {'additional_properties': False, 'properties': {'or': {'type': 'array', 'items': {'anyOf': [{'$ref': '#/definitions/filters/value'}, {'$ref': '#/definitions/filters/valuekv'}]}}}, 'type': 'object'}, {'additional_properties': False, 'properties': {'not': {'type': 'array', 'items': {'anyOf': [{'$ref': '#/definitions/filters/value...onalProperties': False, 'properties': {'execution-options': {'type': 'object'}, 'function-prefix': {'type': 'string'}, 'member-role': {'type': 'string'}, 'packages': {'type': 'array', 'items': {'type': 'string'}}, 'layers': {'type': 'array', 'items': {'type': 'string'}}, 'concurrency': {'type': 'integer'}, 'runtime': {'enum': ['python3.8', 'python3.9', 'python3.10', 'python3.11', 'python3.12']}, 'role': {'type': 'string'}, 'handler': {'type': 'string'}, 'pattern': {'type': 'object', 'minProperties': 1}, 'timeout': {'type': 'number'}, 'memory': {'type': 'number'}, 'environment': {'type': 'object'}, 'tags': {'type': 'object'}, 'dead_letter_config': {'type': 'object'}, 'kms_key_arn': {'type': 'string'}, 'tracing_config': {'type': 'object'}, 'security_groups': {'type': 'array'}, 'subnets': {'type': 'array'}, 'type': {'enum': ['asg-instance-state']}, 'events': {'type': 'array', 'items': {'enum': ['launch-success', 'launch-failure', 'terminate-success', 'terminate-failure']}}}, 'required': ['type']}, {'type': 'object', 'additionalProperties': False, 'properties': {'execution-options': {'type': 'object'}, 'function-prefix': {'type': 'string'}, 'member-role': {'type': 'string'}, 'packages': {'type': 'array', 'items': {'type': 'string'}}, 'layers': {'type': 'array', 'items': {'type': 'string'}}, 'concurrency': {'type': 'integer'}, 'runtime': {'enum': ['python3.8', 'python3.9', 'python3.10', 'python3.11', 'python3.12']}, 'role': {'type': 'string'}, 'handler': {'type': 'string'}, 'pattern': {'type': 'object', 'minProperties': 1}, 'timeout': {'type': 'number'}, 'memory': {'type': 'number'}, 'environment': {'type': 'object'}, 'tags': {'type': 'object'}, 'dead_letter_config': {'type': 'object'}, 'kms_key_arn': {'type': 'string'}, 'tracing_config': {'type': 'object'}, 'security_groups': {'type': 'array'}, 'subnets': {'type': 'array'}, 'type': {'enum': ['guard-duty']}}, 'required': ['type']}, {'type': 'object', 'additionalProperties': False, 'properties': {'execution-options': {'type': 'object'}, 'function-prefix': {'type': 'string'}, 'member-role': {'type': 'string'}, 'packages': {'type': 'array', 'items': {'type': 'string'}}, 'layers': {'type': 'array', 'items': {'type': 'string'}}, 'concurrency': {'type': 'integer'}, 'runtime': {'enum': ['python3.8', 'python3.9', 'python3.10', 'python3.11', 'python3.12']}, 'role': {'type': 'string'}, 'handler': {'type': 'string'}, 'pattern': {'type': 'object', 'minProperties': 1}, 'timeout': {'type': 'number'}, 'memory': {'type': 'number'}, 'environment': {'type': 'object'}, 'tags': {'type': 'object'}, 'dead_letter_config': {'type': 'object'}, 'kms_key_arn': {'type': 'string'}, 'tracing_config': {'type': 'object'}, 'security_groups': {'type': 'array'}, 'subnets': {'type': 'array'}, 'type': {'enum': ['config-poll-rule']}, 'schedule': {'enum': ['One_Hour', 'Three_Hours', 'Six_Hours', 'Twelve_Hours', 'TwentyFour_Hours']}, 'ignore-support-check': {'type': 'boolean'}}, 'required': ['type']}, {'type': 'object', 'additionalProperties': False, 'properties': {'execution-options': {'type': 'object'}, 'function-prefix': {'type': 'string'}, 'member-role': {'type': 'string'}, 'packages': {'type': 'array', 'items': {'type': 'string'}}, 'layers': {'type': 'array', 'items': {'type': 'string'}}, 'concurrency': {'type': 'integer'}, 'runtime': {'enum': ['python3.8', 'python3.9', 'python3.10', 'python3.11', 'python3.12']}, 'role': {'type': 'string'}, 'handler': {'type': 'string'}, 'pattern': {'type': 'object', 'minProperties': 1}, 'timeout': {'type': 'number'}, 'memory': {'type': 'number'}, 'environment': {'type': 'object'}, 'tags': {'type': 'object'}, 'dead_letter_config': {'type': 'object'}, 'kms_key_arn': {'type': 'string'}, 'tracing_config': {'type': 'object'}, 'security_groups': {'type': 'array'}, 'subnets': {'type': 'array'}, 'type': {'enum': ['config-rule']}}, 'required': ['type']}]}, 'max-resources-properties': {'type': 'object', 'additionalProperties': False, 'properties': {'amount': {'type': 'integer', 'minimum': 1}, 'op': {'enum': ['or', 'and']}, 'percent': {'type': 'number', 'minimum': 0, 'maximum': 100}}}}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_attr_schema():\n",
      "    base_filters = [\n",
      "        {'$ref': '\n",
      "        {'$ref': '\n",
      "    ]\n",
      "    any_of = []\n",
      "    any_of.extend(base_filters)\n",
      "    for op in ('and', 'or', 'not',):\n",
      "        any_of.append(\n",
      "            {\n",
      "                'additional_properties': False,\n",
      "                'properties': {\n",
      "                    op: {\n",
      "                        'type': 'array',\n",
      "                        'items': {\n",
      "                            'anyOf': base_filters\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                'type': 'object'\n",
      "            }\n",
      "        )\n",
      "    attr_schema = {\n",
      "        'items': {\n",
      "            'anyOf': any_of\n",
      "        },\n",
      "        'type': 'array',\n",
      "    }\n",
      "    return attr_schema\n",
      "_get_attr_schema()\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "base_filters = [\n",
      "State:\n",
      "[{'$ref': '#/definitions/filters/value'}, {'$ref': '#/definitions/filters/valuekv'}]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_attr_schema():\n",
      "    base_filters = [\n",
      "        {'$ref': '\n",
      "        {'$ref': '\n",
      "    ]\n",
      "    any_of = []\n",
      "    any_of.extend(base_filters)\n",
      "    for op in ('and', 'or', 'not',):\n",
      "        any_of.append(\n",
      "            {\n",
      "                'additional_properties': False,\n",
      "                'properties': {\n",
      "                    op: {\n",
      "                        'type': 'array',\n",
      "                        'items': {\n",
      "                            'anyOf': base_filters\n",
      "                        }\n",
      "                    }\n",
      "                },\n",
      "                'type': 'object'\n",
      "            }\n",
      "        )\n",
      "    attr_schema = {\n",
      "        'items': {\n",
      "            'anyOf': any_of\n",
      "        },\n",
      "        'type': 'array',\n",
      "    }\n",
      "    return attr_schema\n",
      "_get_attr_schema()\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "any_of = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def import_ext(name, optional=False):\n",
      "  components = name.split(\".\")\n",
      "  if (len(components) > 1):\n",
      "    __import__(\".\".join(components[:-1]))\n",
      "  previous_dlopenflags = None\n",
      "  if (sys.platform.startswith(\"linux\")) :\n",
      "    previous_dlopenflags = sys.getdlopenflags()\n",
      "    sys.setdlopenflags(0x100|0x2)\n",
      "  try: mod = __import__(name)\n",
      "  except ImportError as e:\n",
      "    if (optional): return None\n",
      "    error_msg = str(e)\n",
      "    m = symbol_not_found_pat.search(error_msg)\n",
      "    if m:\n",
      "      error_msg = (  error_msg[:m.start(1)]\n",
      "                   + cpp_function_name.demangle(m.group(1))\n",
      "                   + error_msg[m.end(1):])\n",
      "    raise ImportError(\n",
      "      \"\\n  \".join(['__import__(\"%s\"): %s' % (name, error_msg), \"sys.path:\"]\n",
      "      + [\"  \"+p for p in sys.path]))\n",
      "  for comp in components[1:]:\n",
      "    mod = getattr(mod, comp)\n",
      "  if (previous_dlopenflags is not None):\n",
      "    sys.setdlopenflags(previous_dlopenflags)\n",
      "  if (python_libstdcxx_so is not None):\n",
      "    mod_file = getattr(mod, \"__file__\", None)\n",
      "    if (mod_file is not None):\n",
      "      for line in easy_run.fully_buffered(\n",
      "                    command='/usr/bin/ldd \"%s\"' % mod_file).stdout_lines:\n",
      "        if (line.strip().startswith(\"libstdc++.so\")):\n",
      "          mod_libstdcxx_so = line.split()[0]\n",
      "          if (mod_libstdcxx_so != python_libstdcxx_so):\n",
      "            raise SystemError(\"\"\"\\\n",
      "FATAL: libstdc++.so mismatch:\n",
      "  %s: %s\n",
      "  %s: %s\"\"\" % (sys.executable, python_libstdcxx_so,\n",
      "               mod_file, mod_libstdcxx_so))\n",
      "          break\n",
      "  return mod\n",
      "import_ext(name='boost_python_meta_ext', optional=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "components = name.split(\".\")\n",
      "State:\n",
      "['boost_python_meta_ext']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def import_ext(name, optional=False):\n",
      "  components = name.split(\".\")\n",
      "  if (len(components) > 1):\n",
      "    __import__(\".\".join(components[:-1]))\n",
      "  previous_dlopenflags = None\n",
      "  if (sys.platform.startswith(\"linux\")) :\n",
      "    previous_dlopenflags = sys.getdlopenflags()\n",
      "    sys.setdlopenflags(0x100|0x2)\n",
      "  try: mod = __import__(name)\n",
      "  except ImportError as e:\n",
      "    if (optional): return None\n",
      "    error_msg = str(e)\n",
      "    m = symbol_not_found_pat.search(error_msg)\n",
      "    if m:\n",
      "      error_msg = (  error_msg[:m.start(1)]\n",
      "                   + cpp_function_name.demangle(m.group(1))\n",
      "                   + error_msg[m.end(1):])\n",
      "    raise ImportError(\n",
      "      \"\\n  \".join(['__import__(\"%s\"): %s' % (name, error_msg), \"sys.path:\"]\n",
      "      + [\"  \"+p for p in sys.path]))\n",
      "  for comp in components[1:]:\n",
      "    mod = getattr(mod, comp)\n",
      "  if (previous_dlopenflags is not None):\n",
      "    sys.setdlopenflags(previous_dlopenflags)\n",
      "  if (python_libstdcxx_so is not None):\n",
      "    mod_file = getattr(mod, \"__file__\", None)\n",
      "    if (mod_file is not None):\n",
      "      for line in easy_run.fully_buffered(\n",
      "                    command='/usr/bin/ldd \"%s\"' % mod_file).stdout_lines:\n",
      "        if (line.strip().startswith(\"libstdc++.so\")):\n",
      "          mod_libstdcxx_so = line.split()[0]\n",
      "          if (mod_libstdcxx_so != python_libstdcxx_so):\n",
      "            raise SystemError(\"\"\"\\\n",
      "FATAL: libstdc++.so mismatch:\n",
      "  %s: %s\n",
      "  %s: %s\"\"\" % (sys.executable, python_libstdcxx_so,\n",
      "               mod_file, mod_libstdcxx_so))\n",
      "          break\n",
      "  return mod\n",
      "import_ext(name='boost_python_meta_ext', optional=False)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "previous_dlopenflags = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __enter__(self):\n",
      "            self.capmanager.suspend_global_capture(in_=True)\n",
      "            pass\n",
      "__enter__(self=<tests.conftest.suspend_capture.<locals>.suspend_guard object at 0x7f61e866d6a0>, self.capmanager=<CaptureManager _method='no' _global_capturing=<MultiCapture out=None err=None in_=None _state='started' _in_suspended=False> _capture_fixture=None>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.capmanager.suspend_global_capture(in_=True)\n",
      "State:\n",
      "<CaptureManager _method='no' _global_capturing=<MultiCapture out=None err=None in_=None _state='suspended' _in_suspended=False> _capture_fixture=None>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def host(self) -> str:\n",
      "        h = self.addr\n",
      "        if '@' in self.addr:\n",
      "            h = h.split(\"@\", maxsplit=1)[1]\n",
      "        if ':' in self.addr:\n",
      "            h = h.split(\":\", maxsplit=1)[0]\n",
      "        return h\n",
      "host(self=<carnival.host.Host object at 0x7f61e866d8b0>, self.addr='127.0.0.1', self.context={}, self.ssh_connect_timeout=10, self.ssh_password='secret', self.ssh_port=22222, self.ssh_user='root')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "h = self.addr\n",
      "State:\n",
      "'127.0.0.1'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __exit__(self, _1, _2, _3):\n",
      "            self.capmanager.resume_global_capture()\n",
      "__exit__(self=<tests.conftest.suspend_capture.<locals>.suspend_guard object at 0x7f61e866d6a0>, _1=<class 'paramiko.ssh_exception.NoValidConnectionsError'>, _2=NoValidConnectionsError(None, 'Unable to connect to port 22222 on 127.0.0.1'), _3=REPR FAILED, self.capmanager=<CaptureManager _method='no' _global_capturing=<MultiCapture out=None err=None in_=None _state='suspended' _in_suspended=False> _capture_fixture=None>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.capmanager.resume_global_capture()\n",
      "State:\n",
      "<CaptureManager _method='no' _global_capturing=<MultiCapture out=None err=None in_=None _state='started' _in_suspended=False> _capture_fixture=None>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_run(suspend_capture, host):\n",
      "    with suspend_capture:\n",
      "        with global_context.SetContext(host):\n",
      "            result = cmd.cli.run(\"ls -1 /\", hide=True)\n",
      "            assert result.ok is True\n",
      "            root_files = result.stdout.split(\"\\n\")\n",
      "            assert 'bin' in root_files\n",
      "            assert 'etc' in root_files\n",
      "            assert 'usr' in root_files\n",
      "            assert 'tmp' in root_files\n",
      "            assert 'sbin' in root_files\n",
      "test_run(suspend_capture={capmanager=<CaptureManager _method='no' _global_capturing=<MultiCapture out=None err=None in_=None _state='started' _in_suspended=False> _capture_fixture=None>}, host={addr='local', ssh_port=22, context={}, ssh_user=None, ssh_password=None, ssh_connect_timeout=10})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "result = cmd.cli.run(\"ls -1 /\", hide=True)\n",
      "State:\n",
      "<Result cmd='ls -1 /' exited=0>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_run(suspend_capture, host):\n",
      "    with suspend_capture:\n",
      "        with global_context.SetContext(host):\n",
      "            result = cmd.cli.run(\"ls -1 /\", hide=True)\n",
      "            assert result.ok is True\n",
      "            root_files = result.stdout.split(\"\\n\")\n",
      "            assert 'bin' in root_files\n",
      "            assert 'etc' in root_files\n",
      "            assert 'usr' in root_files\n",
      "            assert 'tmp' in root_files\n",
      "            assert 'sbin' in root_files\n",
      "test_run(suspend_capture={capmanager=<CaptureManager _method='no' _global_capturing=<MultiCapture out=None err=None in_=None _state='started' _in_suspended=False> _capture_fixture=None>}, host={addr='local', ssh_port=22, context={}, ssh_user=None, ssh_password=None, ssh_connect_timeout=10})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "assert result.ok is True\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_pty(suspend_capture, host):\n",
      "    with suspend_capture:\n",
      "        with global_context.SetContext(host):\n",
      "            result = cmd.cli.pty(\"ls -1 / | grep bin\", hide=True)\n",
      "            assert result.ok is True\n",
      "            root_files = result.stdout.split(\"\\n\")\n",
      "            assert 'bin\\r' in root_files\n",
      "            assert 'sbin\\r' in root_files\n",
      "test_pty(suspend_capture={capmanager=<CaptureManager _method='no' _global_capturing=<MultiCapture out=None err=None in_=None _state='started' _in_suspended=False> _capture_fixture=None>}, host={addr='local', ssh_port=22, context={}, ssh_user=None, ssh_password=None, ssh_connect_timeout=10})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "result = cmd.cli.pty(\"ls -1 / | grep bin\", hide=True)\n",
      "State:\n",
      "<Result cmd='ls -1 / | grep bin' exited=0>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_pty(suspend_capture, host):\n",
      "    with suspend_capture:\n",
      "        with global_context.SetContext(host):\n",
      "            result = cmd.cli.pty(\"ls -1 / | grep bin\", hide=True)\n",
      "            assert result.ok is True\n",
      "            root_files = result.stdout.split(\"\\n\")\n",
      "            assert 'bin\\r' in root_files\n",
      "            assert 'sbin\\r' in root_files\n",
      "test_pty(suspend_capture={capmanager=<CaptureManager _method='no' _global_capturing=<MultiCapture out=None err=None in_=None _state='started' _in_suspended=False> _capture_fixture=None>}, host={addr='local', ssh_port=22, context={}, ssh_user=None, ssh_password=None, ssh_connect_timeout=10})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "assert result.ok is True\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_is_dir_exists(suspend_capture, host):\n",
      "    with suspend_capture:\n",
      "        with global_context.SetContext(host):\n",
      "            assert cmd.fs.is_dir_exists(\"/etc\")\n",
      "            assert cmd.fs.is_dir_exists(\"/bin\")\n",
      "test_is_dir_exists(suspend_capture={capmanager=<CaptureManager _method='no' _global_capturing=<MultiCapture out=None err=None in_=None _state='started' _in_suspended=False> _capture_fixture=None>}, host={addr='local', ssh_port=22, context={}, ssh_user=None, ssh_password=None, ssh_connect_timeout=10})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "assert cmd.fs.is_dir_exists(\"/etc\")\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_is_dir_exists(suspend_capture, host):\n",
      "    with suspend_capture:\n",
      "        with global_context.SetContext(host):\n",
      "            assert cmd.fs.is_dir_exists(\"/etc\")\n",
      "            assert cmd.fs.is_dir_exists(\"/bin\")\n",
      "test_is_dir_exists(suspend_capture={capmanager=<CaptureManager _method='no' _global_capturing=<MultiCapture out=None err=None in_=None _state='started' _in_suspended=False> _capture_fixture=None>}, host={addr='local', ssh_port=22, context={}, ssh_user=None, ssh_password=None, ssh_connect_timeout=10})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "assert cmd.fs.is_dir_exists(\"/bin\")\n",
      "State:\n",
      "{capmanager=<CaptureManager _method='no' _global_capturing=<MultiCapture out=None err=None in_=None _state='started' _in_suspended=False> _capture_fixture=None>}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _build_kwargs(fn, context: Dict[str, Any]):\n",
      "    arg_names: List[str] = _get_arg_names(fn)\n",
      "    kwargs = {}\n",
      "    for context_name, context_val in context.items():\n",
      "        if context_name in arg_names:\n",
      "            kwargs[context_name] = context_val\n",
      "    return kwargs\n",
      "_build_kwargs(fn=<bound method run of <carnival.step.Step object at 0x7f61e69897c0>>, context={'secrets': {'STATIC_SECRET': 'SECRET_VALUE'}, 'host': <carnival.host.Host object at 0x7f61e69896a0>, 'add': 'context', 'another': 'context1'})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "arg_names: List[str] = _get_arg_names(fn)\n",
      "State:\n",
      "['self']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _build_kwargs(fn, context: Dict[str, Any]):\n",
      "    arg_names: List[str] = _get_arg_names(fn)\n",
      "    kwargs = {}\n",
      "    for context_name, context_val in context.items():\n",
      "        if context_name in arg_names:\n",
      "            kwargs[context_name] = context_val\n",
      "    return kwargs\n",
      "_build_kwargs(fn=<bound method run of <carnival.step.Step object at 0x7f61e69897c0>>, context={'secrets': {'STATIC_SECRET': 'SECRET_VALUE'}, 'host': <carnival.host.Host object at 0x7f61e69896a0>, 'add': 'context', 'another': 'context1'})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "kwargs = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_arg_names(fn) -> List[str]:\n",
      "    arg_names: List[str] = []\n",
      "    spec = inspect.getfullargspec(fn)\n",
      "    arg_names += spec.args\n",
      "    arg_names += spec.kwonlyargs\n",
      "    return arg_names\n",
      "_get_arg_names(fn=<bound method run of <carnival.step.Step object at 0x7f61e69897c0>>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "arg_names: List[str] = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_arg_names(fn) -> List[str]:\n",
      "    arg_names: List[str] = []\n",
      "    spec = inspect.getfullargspec(fn)\n",
      "    arg_names += spec.args\n",
      "    arg_names += spec.kwonlyargs\n",
      "    return arg_names\n",
      "_get_arg_names(fn=<bound method run of <carnival.step.Step object at 0x7f61e69897c0>>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "spec = inspect.getfullargspec(fn)\n",
      "State:\n",
      "FullArgSpec(args=['self'], varargs=None, varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={})\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_arg_names(fn) -> List[str]:\n",
      "    arg_names: List[str] = []\n",
      "    spec = inspect.getfullargspec(fn)\n",
      "    arg_names += spec.args\n",
      "    arg_names += spec.kwonlyargs\n",
      "    return arg_names\n",
      "_get_arg_names(fn=<bound method run of <carnival.step.Step object at 0x7f61e69897c0>>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "arg_names += spec.args\n",
      "State:\n",
      "['self']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _underscore(word: str) -> str:\n",
      "    word = re.sub(r\"([A-Z]+)([A-Z][a-z])\", r'\\1_\\2', word)\n",
      "    word = re.sub(r\"([a-z\\d])([A-Z])\", r'\\1_\\2', word)\n",
      "    word = word.replace(\"-\", \"_\")\n",
      "    return word.lower()\n",
      "_underscore(word='TestKlass')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "word = re.sub(r\"([a-z\\d])([A-Z])\", r'\\1_\\2', word)\n",
      "State:\n",
      "'Test_Klass'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_tasks_from_runtime(carnival_tasks_module: str) -> Dict[str, Type[Task]]:\n",
      "    tasks: Dict[str, Type[Task]] = {}\n",
      "    for task_class in task_subclasses(Task):\n",
      "        task_full_name = get_task_full_name(carnival_tasks_module, task_class)\n",
      "        tasks[task_full_name] = task_class\n",
      "    return tasks\n",
      "get_tasks_from_runtime(carnival_tasks_module='')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "tasks: Dict[str, Type[Task]] = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _make_text_tag(column, attributes, text, cell_width):\n",
      "    text_tag_attributes = {\n",
      "        'x': str(column * cell_width),\n",
      "        'textLength': str(wcswidth(text) * cell_width),\n",
      "    }\n",
      "    if attributes['bold']:\n",
      "        text_tag_attributes['font-weight'] = 'bold'\n",
      "    if attributes['italics']:\n",
      "        text_tag_attributes['font-style'] = 'italic'\n",
      "    decoration = ''\n",
      "    if attributes['underscore']:\n",
      "        decoration = 'underline'\n",
      "    if attributes['strikethrough']:\n",
      "        decoration += ' line-through'\n",
      "    if decoration:\n",
      "        text_tag_attributes['text-decoration'] = decoration\n",
      "    if attributes['color'].startswith('\n",
      "        text_tag_attributes['fill'] = attributes['color']\n",
      "    else:\n",
      "        text_tag_attributes['class'] = attributes['color']\n",
      "    text_tag = etree.Element('text', text_tag_attributes)\n",
      "    text_tag.text = text\n",
      "    return text_tag\n",
      "_make_text_tag(column=0, attributes={'color': 'red', 'bold': False, 'italics': False, 'underscore': False, 'strikethrough': False}, text='A', cell_width=8)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "text_tag_attributes = {\n",
      "State:\n",
      "{'x': '0', 'textLength': '8'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _make_rect_tag(column, length, height, cell_width, cell_height, background_color):\n",
      "    attributes = {\n",
      "        'x': str(column * cell_width),\n",
      "        'y': str(height),\n",
      "        'width': str(length * cell_width),\n",
      "        'height': str(cell_height)\n",
      "    }\n",
      "    if background_color.startswith('\n",
      "        attributes['fill'] = background_color\n",
      "    else:\n",
      "        attributes['class'] = background_color\n",
      "    rect_tag = etree.Element('rect', attributes)\n",
      "    return rect_tag\n",
      "_make_rect_tag(column=0, length=2, height=0, cell_width=8, cell_height=1, background_color='red')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "attributes = {\n",
      "State:\n",
      "{'x': '0', 'y': '0', 'width': '16', 'height': '1'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _render_line(offset, row_number, row, cell_height, cell_width, definitions):\n",
      "    tags = _render_line_bg_colors(screen_line=row,\n",
      "                                  height=offset + row_number * cell_height,\n",
      "                                  cell_height=cell_height,\n",
      "                                  cell_width=cell_width)\n",
      "    text_group_tag = etree.Element('g')\n",
      "    text_tags = _render_characters(row, cell_width)\n",
      "    for tag in text_tags:\n",
      "        text_group_tag.append(tag)\n",
      "    text_group_tag_str = etree.tostring(text_group_tag)\n",
      "    if text_group_tag_str in definitions:\n",
      "        group_id = definitions[text_group_tag_str].attrib['id']\n",
      "        new_definitions = {}\n",
      "    else:\n",
      "        group_id = 'g{}'.format(len(definitions) + 1)\n",
      "        assert group_id not in definitions.values()\n",
      "        text_group_tag.attrib['id'] = group_id\n",
      "        new_definitions = {text_group_tag_str: text_group_tag}\n",
      "    use_attributes = {\n",
      "        '{{{}}}href'.format(XLINK_NS): '\n",
      "        'y': str(offset + row_number * cell_height),\n",
      "    }\n",
      "    tags.append(etree.Element('use', use_attributes))\n",
      "    return tags, new_definitions\n",
      "_render_line(offset=0, row_number=1, row={0: CharacterCell(text='a', color='foreground', background_color='background', bold=False, italics=False, underscore=False, strikethrough=False)}, cell_height=17, cell_width=9, definitions={})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "tags = _render_line_bg_colors(screen_line=row,\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def line(i):\n",
      "    chars = []\n",
      "    for c in 'line{}'.format(i):\n",
      "        chars.append(anim.CharacterCell(c, '\n",
      "                                        False, False, False, False))\n",
      "    return dict(enumerate(chars))\n",
      "line(i=1)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "chars = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def integral_duration_validation(duration):\n",
      "    if duration.lower().endswith('ms'):\n",
      "        duration = duration[:-len('ms')]\n",
      "    if duration.isdigit() and int(duration) >= 1:\n",
      "        return int(duration)\n",
      "    raise ValueError('duration must be an integer greater than 0')\n",
      "integral_duration_validation(duration='100ms')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "duration = duration[:-len('ms')]\n",
      "State:\n",
      "'100'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _to_initial(self):\n",
      "        self.current_pos = self.initial_pos\n",
      "_to_initial(self=Bot(index=0, initial_pos=(1, 1), team_index=0, homezone=(0, 3), current_pos=(4, 1), noisy=False), self.current_pos=(4, 1), self.homezone=(0, 3), self.index=0, self.initial_pos=(1, 1), self.noisy=False, self.team_index=0)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.current_pos = self.initial_pos\n",
      "State:\n",
      "Bot(index=0, initial_pos=(1, 1), team_index=0, homezone=(0, 3), current_pos=(1, 1), noisy=False)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def initial_positions(walls):\n",
      "    width = max(walls)[0] + 1\n",
      "    height = max(walls)[1] + 1\n",
      "    left_start = (1, height - 2)\n",
      "    left = []\n",
      "    right_start = (width - 2, 1)\n",
      "    right = []\n",
      "    dist = 0\n",
      "    while len(left) < 2:\n",
      "        for x_dist in range(dist + 1):\n",
      "            y_dist = dist - x_dist\n",
      "            pos = (left_start[0] + x_dist, left_start[1] - y_dist)\n",
      "            if not (0 <= pos[0] < width) and not (0 <= pos[1] < height):\n",
      "                raise ValueError(\"Not enough free initial positions.\")\n",
      "            if not (0 <= pos[0] < width) or not (0 <= pos[1] < height):\n",
      "                continue\n",
      "            if pos not in walls:\n",
      "                left.append(pos)\n",
      "            if len(left) == 2:\n",
      "                break\n",
      "        dist += 1\n",
      "    dist = 0\n",
      "    while len(right) < 2:\n",
      "        for x_dist in range(dist + 1):\n",
      "            y_dist = dist - x_dist\n",
      "            pos = (right_start[0] - x_dist, right_start[1] + y_dist)\n",
      "            if not (0 <= pos[0] < width) and not (0 <= pos[1] < height):\n",
      "                raise ValueError(\"Not enough free initial positions.\")\n",
      "            if not (0 <= pos[0] < width) or not (0 <= pos[1] < height):\n",
      "                continue\n",
      "            if pos not in walls:\n",
      "                right.append(pos)\n",
      "            if len(right) == 2:\n",
      "                break\n",
      "        dist += 1\n",
      "    left.reverse()\n",
      "    right.reverse()\n",
      "    return [left[0], right[0], left[1], right[1]]\n",
      "initial_positions(walls=[(0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 3), (2, 0), (2, 1), (2, 3), (3, 0), (3, 1), (3, 3), (4, 0), (4, 1), (4, 3), (5, 0), (5, 3), (6, 0), (6, 3), (7, 0), (7, 1), (7, 2), (7, 3)])\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "height = max(walls)[1] + 1\n",
      "State:\n",
      "4\n",
      "==================================================\n",
      "Clean Code:\n",
      "def initial_positions(walls):\n",
      "    width = max(walls)[0] + 1\n",
      "    height = max(walls)[1] + 1\n",
      "    left_start = (1, height - 2)\n",
      "    left = []\n",
      "    right_start = (width - 2, 1)\n",
      "    right = []\n",
      "    dist = 0\n",
      "    while len(left) < 2:\n",
      "        for x_dist in range(dist + 1):\n",
      "            y_dist = dist - x_dist\n",
      "            pos = (left_start[0] + x_dist, left_start[1] - y_dist)\n",
      "            if not (0 <= pos[0] < width) and not (0 <= pos[1] < height):\n",
      "                raise ValueError(\"Not enough free initial positions.\")\n",
      "            if not (0 <= pos[0] < width) or not (0 <= pos[1] < height):\n",
      "                continue\n",
      "            if pos not in walls:\n",
      "                left.append(pos)\n",
      "            if len(left) == 2:\n",
      "                break\n",
      "        dist += 1\n",
      "    dist = 0\n",
      "    while len(right) < 2:\n",
      "        for x_dist in range(dist + 1):\n",
      "            y_dist = dist - x_dist\n",
      "            pos = (right_start[0] - x_dist, right_start[1] + y_dist)\n",
      "            if not (0 <= pos[0] < width) and not (0 <= pos[1] < height):\n",
      "                raise ValueError(\"Not enough free initial positions.\")\n",
      "            if not (0 <= pos[0] < width) or not (0 <= pos[1] < height):\n",
      "                continue\n",
      "            if pos not in walls:\n",
      "                right.append(pos)\n",
      "            if len(right) == 2:\n",
      "                break\n",
      "        dist += 1\n",
      "    left.reverse()\n",
      "    right.reverse()\n",
      "    return [left[0], right[0], left[1], right[1]]\n",
      "initial_positions(walls=[(0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 3), (2, 0), (2, 1), (2, 3), (3, 0), (3, 1), (3, 3), (4, 0), (4, 1), (4, 3), (5, 0), (5, 3), (6, 0), (6, 3), (7, 0), (7, 1), (7, 2), (7, 3)])\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "right = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _call_wrapped(self, method, ctx):\n",
      "        ctx_0 = ff.deep_extend(ctx)\n",
      "        try:\n",
      "            if self.middleware_items:\n",
      "                for middleware_item in self.middleware_items:\n",
      "                    ctx = middleware_item(method, ctx, **{\"__step\": self.step})\n",
      "            else:\n",
      "                ctx = method(ctx)\n",
      "        except Exception as e:\n",
      "            self.failure = Failure(e, at=f'method: {method.__name__}; module: {method.__module__}')\n",
      "            return ctx_0\n",
      "        return ctx\n",
      "_call_wrapped(self=<fairways.chains.Chain object at 0x7f4e6dfefc70>, method=<function ChainsTestCase.test_all.<locals>.<lambda> at 0x7f4e6e0533a0>, ctx=2, self.ctx=2, self.failure=None, self.middleware_items=[], self.step=0)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "ctx_0 = ff.deep_extend(ctx)\n",
      "State:\n",
      "2\n",
      "==================================================\n",
      "Clean Code:\n",
      "def catch(self, err_method):\n",
      "        ctx = self.ctx\n",
      "        if self.failure is not None:\n",
      "            last_failure = self.failure\n",
      "            self.failure = None\n",
      "            result = err_method(last_failure)\n",
      "        return Chain(self)\n",
      "catch(self=<fairways.chains.Chain object at 0x7f4e6df91130>, err_method=<function ChainsTestCase.test_catch_no_error.<locals>.handle_error at 0x7f4e6dc12ee0>, self.ctx=[1, 2, 3], self.failure=None, self.middleware_items=[], self.step=3)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "ctx = self.ctx\n",
      "State:\n",
      "[1, 2, 3]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def on(self, keypath, method):\n",
      "        if self.failure is None:\n",
      "            ctx = self.ctx\n",
      "            try:\n",
      "                value = get_nested(ctx, keypath)\n",
      "                parent = get_parent(ctx, keypath)\n",
      "            except KeyError:\n",
      "                return self\n",
      "            result = self._call_wrapped(method, value)\n",
      "            last_key = get_lastkey(keypath)\n",
      "            parent.update({\n",
      "                last_key: result\n",
      "            })\n",
      "            self.ctx = ctx\n",
      "        return Chain(self)\n",
      "on(self=<fairways.chains.Chain object at 0x7f4e6dc01160>, keypath='nested/event', method=<function ChainsTestCase.test_nested_on_if_found.<locals>.<lambda> at 0x7f4e6dc12ca0>, self.ctx={'nested': {'data': 'should be unchanged', 'event': None}}, self.failure=None, self.middleware_items=[], self.step=0)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "value = get_nested(ctx, keypath)\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def extend(*args):\n",
      "        args = list(args)\n",
      "        dest = args.pop(0)\n",
      "        for source in args:\n",
      "            if source:\n",
      "                dest.update(source)\n",
      "        return dest\n",
      "extend(args=({}, {}))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "args = list(args)\n",
      "State:\n",
      "[{}, {}]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def extend(*args):\n",
      "        args = list(args)\n",
      "        dest = args.pop(0)\n",
      "        for source in args:\n",
      "            if source:\n",
      "                dest.update(source)\n",
      "        return dest\n",
      "extend(args=({}, {}))\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "dest = args.pop(0)\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def getLogger():        \n",
      "    handler = logging.StreamHandler()\n",
      "    if formatter:\n",
      "        handler.setFormatter(formatter)\n",
      "    root = logging.getLogger()\n",
      "    root.setLevel(logging.DEBUG)\n",
      "    root.addHandler(handler)\n",
      "    return root\n",
      "getLogger()\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "handler = logging.StreamHandler()\n",
      "State:\n",
      "<StreamHandler <stderr> (NOTSET)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def getLogger():        \n",
      "    handler = logging.StreamHandler()\n",
      "    if formatter:\n",
      "        handler.setFormatter(formatter)\n",
      "    root = logging.getLogger()\n",
      "    root.setLevel(logging.DEBUG)\n",
      "    root.addHandler(handler)\n",
      "    return root\n",
      "getLogger()\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "root = logging.getLogger()\n",
      "State:\n",
      "<RootLogger root (WARNING)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def getLogger():        \n",
      "    handler = logging.StreamHandler()\n",
      "    if formatter:\n",
      "        handler.setFormatter(formatter)\n",
      "    root = logging.getLogger()\n",
      "    root.setLevel(logging.DEBUG)\n",
      "    root.addHandler(handler)\n",
      "    return root\n",
      "getLogger()\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "root.setLevel(logging.DEBUG)\n",
      "State:\n",
      "<RootLogger root (DEBUG)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def register(function):\n",
      "        name = function.__qualname__\n",
      "        mm = registry.get(name)\n",
      "        if mm is None:\n",
      "            mm = registry[name] = MultiMethod(name)\n",
      "        mm.register(types, function)\n",
      "        return mm\n",
      "register(function=<function OverloadTestCase.test_overload_func.<locals>.func at 0x7f4e6d895700>, types=(<class 'int'>,))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "name = function.__qualname__\n",
      "State:\n",
      "'OverloadTestCase.test_overload_func.<locals>.func'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def register(function):\n",
      "        name = function.__qualname__\n",
      "        mm = registry.get(name)\n",
      "        if mm is None:\n",
      "            mm = registry[name] = MultiMethod(name)\n",
      "        mm.register(types, function)\n",
      "        return mm\n",
      "register(function=<function OverloadTestCase.test_overload_func.<locals>.func at 0x7f4e6d895700>, types=(<class 'int'>,))\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "mm = registry.get(name)\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def register(function):\n",
      "        name = function.__qualname__\n",
      "        mm = registry.get(name)\n",
      "        if mm is None:\n",
      "            mm = registry[name] = MultiMethod(name)\n",
      "        mm.register(types, function)\n",
      "        return mm\n",
      "register(function=<function OverloadTestCase.test_overload_func.<locals>.func at 0x7f4e6d895700>, types=(<class 'int'>,))\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "mm = registry[name] = MultiMethod(name)\n",
      "State:\n",
      "{name='OverloadTestCase.test_overload_func.<locals>.func', typemap={}}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def run_asyn(coro_obj, new_loop=True):\n",
      "    import asyncio\n",
      "    own_loop = None\n",
      "    if new_loop:\n",
      "        own_loop = asyncio.new_event_loop()\n",
      "        asyncio.set_event_loop(own_loop)\n",
      "        loop = own_loop\n",
      "    else:\n",
      "        loop = asyncio.get_event_loop()\n",
      "    try:\n",
      "        task = asyncio.ensure_future(coro_obj, loop=loop)\n",
      "        (result,) = loop.run_until_complete(asyncio.gather(task))\n",
      "        return result\n",
      "    finally:\n",
      "        if own_loop:\n",
      "            own_loop.close()\n",
      "run_asyn(coro_obj=<coroutine object AmqpDriver.execute.<locals>.push at 0x7f4e5f192140>, new_loop=True)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "import asyncio\n",
      "State:\n",
      "<module 'asyncio' from '/local/rcs/XXX/miniforge3/envs/danwin+fairways_py/lib/python3.9/asyncio/__init__.py'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def run_asyn(coro_obj, new_loop=True):\n",
      "    import asyncio\n",
      "    own_loop = None\n",
      "    if new_loop:\n",
      "        own_loop = asyncio.new_event_loop()\n",
      "        asyncio.set_event_loop(own_loop)\n",
      "        loop = own_loop\n",
      "    else:\n",
      "        loop = asyncio.get_event_loop()\n",
      "    try:\n",
      "        task = asyncio.ensure_future(coro_obj, loop=loop)\n",
      "        (result,) = loop.run_until_complete(asyncio.gather(task))\n",
      "        return result\n",
      "    finally:\n",
      "        if own_loop:\n",
      "            own_loop.close()\n",
      "run_asyn(coro_obj=<coroutine object AmqpDriver.execute.<locals>.push at 0x7f4e5f192140>, new_loop=True)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "own_loop = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def run_asyn(coro_obj, new_loop=True):\n",
      "    import asyncio\n",
      "    own_loop = None\n",
      "    if new_loop:\n",
      "        own_loop = asyncio.new_event_loop()\n",
      "        asyncio.set_event_loop(own_loop)\n",
      "        loop = own_loop\n",
      "    else:\n",
      "        loop = asyncio.get_event_loop()\n",
      "    try:\n",
      "        task = asyncio.ensure_future(coro_obj, loop=loop)\n",
      "        (result,) = loop.run_until_complete(asyncio.gather(task))\n",
      "        return result\n",
      "    finally:\n",
      "        if own_loop:\n",
      "            own_loop.close()\n",
      "run_asyn(coro_obj=<coroutine object AmqpDriver.execute.<locals>.push at 0x7f4e5f192140>, new_loop=True)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "own_loop = asyncio.new_event_loop()\n",
      "State:\n",
      "<_UnixSelectorEventLoop running=False closed=False debug=False>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def run_asyn(coro_obj, new_loop=True):\n",
      "    import asyncio\n",
      "    own_loop = None\n",
      "    if new_loop:\n",
      "        own_loop = asyncio.new_event_loop()\n",
      "        asyncio.set_event_loop(own_loop)\n",
      "        loop = own_loop\n",
      "    else:\n",
      "        loop = asyncio.get_event_loop()\n",
      "    try:\n",
      "        task = asyncio.ensure_future(coro_obj, loop=loop)\n",
      "        (result,) = loop.run_until_complete(asyncio.gather(task))\n",
      "        return result\n",
      "    finally:\n",
      "        if own_loop:\n",
      "            own_loop.close()\n",
      "run_asyn(coro_obj=<coroutine object AmqpDriver.execute.<locals>.push at 0x7f4e5f192140>, new_loop=True)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "task = asyncio.ensure_future(coro_obj, loop=loop)\n",
      "State:\n",
      "<Task pending name='Task-6' coro=<AmqpDriver.execute.<locals>.push() running at /local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/danwin+fairways_py/danwin+fairways_py/fairways/io/asyn/amqp.py:43>>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _make_request(self, **params):\n",
      "        uri_parts = self.uri_parts\n",
      "        root_url = f'{uri_parts.scheme}://{uri_parts.host}'\n",
      "        if uri_parts.port:\n",
      "            root_url = f'{root_url}:{uri_parts.port}'\n",
      "        abs_url = urllib.parse.urljoin(root_url, params.pop(\"url\"))\n",
      "        log.debug(f'Abs url: {abs_url}')\n",
      "        handler = getattr(requests, params.pop(\"method\"))\n",
      "        user, password = self.uri_parts.user, self.uri_parts.password\n",
      "        if user and password:\n",
      "            params[\"auth\"] = HTTPBasicAuth(user, password)\n",
      "        print(\"REQUEST PARAMS: %s, %s\" % (abs_url, params))\n",
      "        response = handler(abs_url, **params )\n",
      "        log.debug(\"Response text: %s\", response.text)\n",
      "        response.raise_for_status()\n",
      "        return response\n",
      "_make_request(self=<fairways.io.syn.http.Http object at 0x7f4e5f0b2550>, params={'url': '/json/?fields=61439', 'method': 'get'}, self.conn_str='http://ip-api.com', self.engine=None, self.uri_parts=UriParts(scheme='http', user=None, password=None, host='ip-api.com', port=None, path=None))\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "uri_parts = self.uri_parts\n",
      "State:\n",
      "UriParts(scheme='http', user=None, password=None, host='ip-api.com', port=None, path=None)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def change(self, sql):\n",
      "        try:\n",
      "            self._ensure_connection()\n",
      "            self.engine.execute(sql)\n",
      "            self.engine.commit()\n",
      "        except Exception as e:\n",
      "            log.error(\"DB operation error: {} at {}\".format(e, self.db_name))\n",
      "            raise\n",
      "        finally:\n",
      "            if self.autoclose:\n",
      "                self.close()\n",
      "change(self=<fairways.io.syn.sqlite.SqLite object at 0x7f4e5eb61e20>, sql='CREATE TABLE fairways (\\n                    id integer primary key,\\n                    name varchar\\n                );', self.conn_str='./test_dbi.sqlite', self.engine=None, self.uri_parts=UriParts(scheme=None, user=None, password=None, host=None, port=None, path='./test_dbi.sqlite'))\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "self.close()\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _connect(self):\n",
      "        db_filename = self.conn_str\n",
      "        engine = sqlite3.connect(db_filename)\n",
      "        engine.row_factory = dict_factory\n",
      "        engine.isolation_level = \"IMMEDIATE\"\n",
      "        self.engine = engine\n",
      "_connect(self=<fairways.io.syn.sqlite.SqLite object at 0x7f4e5eb61e20>, self.conn_str='./test_dbi.sqlite', self.engine=None, self.uri_parts=UriParts(scheme=None, user=None, password=None, host=None, port=None, path='./test_dbi.sqlite'))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "db_filename = self.conn_str\n",
      "State:\n",
      "'./test_dbi.sqlite'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _connect(self):\n",
      "        db_filename = self.conn_str\n",
      "        engine = sqlite3.connect(db_filename)\n",
      "        engine.row_factory = dict_factory\n",
      "        engine.isolation_level = \"IMMEDIATE\"\n",
      "        self.engine = engine\n",
      "_connect(self=<fairways.io.syn.sqlite.SqLite object at 0x7f4e5eb61e20>, self.conn_str='./test_dbi.sqlite', self.engine=None, self.uri_parts=UriParts(scheme=None, user=None, password=None, host=None, port=None, path='./test_dbi.sqlite'))\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "engine = sqlite3.connect(db_filename)\n",
      "State:\n",
      "REPR FAILED\n",
      "==================================================\n",
      "Clean Code:\n",
      "def close(self):\n",
      "        if self.is_connected():\n",
      "            self.engine.close()\n",
      "            self.engine = None\n",
      "close(self=<fairways.io.syn.sqlite.SqLite object at 0x7f4e5eb61e20>, self.conn_str='./test_dbi.sqlite', self.engine=<sqlite3.Connection object at 0x7f4e5eb6bb70>, self.uri_parts=UriParts(scheme=None, user=None, password=None, host=None, port=None, path='./test_dbi.sqlite'))\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.engine = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def fetch(self, sql):\n",
      "        cursor = None\n",
      "        try:\n",
      "            self._ensure_connection()\n",
      "            cursor = self.engine.execute(sql)\n",
      "            return cursor.fetchall()\n",
      "        except Exception as e:\n",
      "            log.error(\"DB operation error: {} at {}\".format(e, self.db_name))\n",
      "            raise\n",
      "        finally:\n",
      "            if cursor:\n",
      "                cursor.close()\n",
      "            if self.autoclose:\n",
      "                self.close()\n",
      "fetch(self=<fairways.io.syn.sqlite.SqLite object at 0x7f4e5eb61e20>, sql='select name from fairways where id=1;', self.conn_str='./test_dbi.sqlite', self.engine=None, self.uri_parts=UriParts(scheme=None, user=None, password=None, host=None, port=None, path='./test_dbi.sqlite'))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "cursor = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def fetch(self, sql):\n",
      "        cursor = None\n",
      "        try:\n",
      "            self._ensure_connection()\n",
      "            cursor = self.engine.execute(sql)\n",
      "            return cursor.fetchall()\n",
      "        except Exception as e:\n",
      "            log.error(\"DB operation error: {} at {}\".format(e, self.db_name))\n",
      "            raise\n",
      "        finally:\n",
      "            if cursor:\n",
      "                cursor.close()\n",
      "            if self.autoclose:\n",
      "                self.close()\n",
      "fetch(self=<fairways.io.syn.sqlite.SqLite object at 0x7f4e5eb61e20>, sql='select name from fairways where id=1;', self.conn_str='./test_dbi.sqlite', self.engine=None, self.uri_parts=UriParts(scheme=None, user=None, password=None, host=None, port=None, path='./test_dbi.sqlite'))\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "cursor = self.engine.execute(sql)\n",
      "State:\n",
      "REPR FAILED\n",
      "==================================================\n",
      "Clean Code:\n",
      "def dict_factory(cursor, row):\n",
      "    d = {}\n",
      "    for idx, col in enumerate(cursor.description):\n",
      "        d[col[0]] = row[idx]\n",
      "    return d\n",
      "dict_factory(cursor=REPR FAILED, row=('My Way',))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "d = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def dict_factory(cursor, row):\n",
      "    d = {}\n",
      "    for idx, col in enumerate(cursor.description):\n",
      "        d[col[0]] = row[idx]\n",
      "    return d\n",
      "dict_factory(cursor=REPR FAILED, row=('My Way',))\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "d[col[0]] = row[idx]\n",
      "State:\n",
      "{'name': 'My Way'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _register_scheme(cls, prefixes, loader, force=False):\n",
      "        if isinstance(prefixes, str):\n",
      "            prefixes = [prefixes]\n",
      "        else:\n",
      "            assert isinstance(prefixes, (list, tuple))\n",
      "        for prefix in prefixes:\n",
      "            if (prefix not in cls._schemes) or force:\n",
      "                cls._schemes[prefix] = loader\n",
      "            else:\n",
      "                raise KeyError(\n",
      "                    f'{prefix} is already registered as a loader backend, '\n",
      "                    'add \"force=True\" if you want to override it')\n",
      "        cls._schemes = OrderedDict(\n",
      "            sorted(cls._schemes.items(), key=lambda t: t[0], reverse=True))\n",
      "_register_scheme(cls=<class 'annotator.mmpkg.mmcv.runner.checkpoint.CheckpointLoader'>, prefixes='', loader=<function load_from_local at 0x7f01519a5430>, force=False)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "prefixes = [prefixes]\n",
      "State:\n",
      "['']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __setattr__(self, name: str, value: Any) -> None:\n",
      "        if (\n",
      "            name in self.__dict__.keys()\n",
      "            or \"_BaseAWSObject__initialized\" not in self.__dict__\n",
      "        ):\n",
      "            return dict.__setattr__(self, name, value)\n",
      "        elif name in self.attributes:\n",
      "            if name == \"DependsOn\":\n",
      "                self.resource[name] = depends_on_helper(value)\n",
      "            else:\n",
      "                self.resource[name] = value\n",
      "            return None\n",
      "        elif name in self.propnames:\n",
      "            expected_type = self.props[name][0]\n",
      "            if isinstance(value, AWSHelperFn):\n",
      "                return self.properties.__setitem__(name, value)\n",
      "            elif isinstance(expected_type, types.FunctionType):\n",
      "                try:\n",
      "                    value = expected_type(value)\n",
      "                except Exception:\n",
      "                    sys.stderr.write(\n",
      "                        \"%s: %s.%s function validator '%s' threw \"\n",
      "                        \"exception:\\n\"\n",
      "                        % (self.__class__, self.title, name, expected_type.__name__)\n",
      "                    )\n",
      "                    raise\n",
      "                return self.properties.__setitem__(name, value)\n",
      "            elif isinstance(expected_type, list):\n",
      "                if not isinstance(value, list):\n",
      "                    self._raise_type(name, value, expected_type)\n",
      "                if len(expected_type) == 1 and isinstance(\n",
      "                    expected_type[0], types.FunctionType\n",
      "                ):\n",
      "                    new_value = list(map(expected_type[0], value))\n",
      "                    return self.properties.__setitem__(name, new_value)\n",
      "                for v in cast(List[Any], value):\n",
      "                    if not isinstance(v, tuple(expected_type)) and not isinstance(\n",
      "                        v, AWSHelperFn\n",
      "                    ):\n",
      "                        self._raise_type(name, v, expected_type)\n",
      "                return self.properties.__setitem__(name, value)\n",
      "            elif isinstance(value, cast(type, expected_type)):\n",
      "                return self.properties.__setitem__(name, value)\n",
      "            else:\n",
      "                self._raise_type(name, value, expected_type)\n",
      "        type_name = getattr(self, \"resource_type\", self.__class__.__name__)\n",
      "        if type_name == \"AWS::CloudFormation::CustomResource\" or type_name.startswith(\n",
      "            \"Custom::\"\n",
      "        ):\n",
      "            return self.properties.__setitem__(name, value)\n",
      "        raise AttributeError(\n",
      "            \"%s object does not support attribute %s\" % (type_name, name)\n",
      "        )\n",
      "__setattr__(self=<troposphere.emr.SimpleScalingPolicyConfiguration object at 0x7f711d559340>, name='title', value=None)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "return dict.__setattr__(self, name, value)  # type: ignore\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def validate(self):\n",
      "        validate_model(self)\n",
      "validate(self=<troposphere.apigateway.Model object at 0x7f711cc4f8b0>, self._BaseAWSObject__initialized=True, self.attributes=['Condition', 'CreationPolicy', 'DeletionPolicy', 'DependsOn', 'Metadata', 'UpdatePolicy', 'UpdateReplacePolicy'], self.do_validation=True, self.properties={'RestApiId': 'apiid', 'Schema': '{\"a: \"b\"}'}, self.propnames={'Schema', 'RestApiId', 'Description', 'Name', 'ContentType'}, self.resource={'Properties': {'RestApiId': 'apiid', 'Schema': '{\"a: \"b\"}'}, 'Type': 'AWS::ApiGateway::Model'}, self.template=None, self.title='schema')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "validate_model(self)\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def validate_model(self):\n",
      "    name = \"Schema\"\n",
      "    if name in self.properties:\n",
      "        schema = self.properties.get(name)\n",
      "        self.properties[name] = json_checker(schema)\n",
      "validate_model(self=<troposphere.apigateway.Model object at 0x7f711cd01b80>, self._BaseAWSObject__initialized=True, self.attributes=['Condition', 'CreationPolicy', 'DeletionPolicy', 'DependsOn', 'Metadata', 'UpdatePolicy', 'UpdateReplacePolicy'], self.do_validation=True, self.properties={'RestApiId': 'apiid'}, self.propnames={'Schema', 'RestApiId', 'Description', 'Name', 'ContentType'}, self.resource={'Properties': {'RestApiId': 'apiid'}, 'Type': 'AWS::ApiGateway::Model'}, self.template=None, self.title='schema')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "name = \"Schema\"\n",
      "State:\n",
      "'Schema'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def validate_dashboard(self):\n",
      "    name = \"DashboardBody\"\n",
      "    if name in self.properties:\n",
      "        dashboard_body = self.properties.get(name)\n",
      "        self.properties[name] = json_checker(dashboard_body)\n",
      "validate_dashboard(self=<troposphere.cloudwatch.Dashboard object at 0x7f711d7160d0>, self._BaseAWSObject__initialized=True, self.attributes=['Condition', 'CreationPolicy', 'DeletionPolicy', 'DependsOn', 'Metadata', 'UpdatePolicy', 'UpdateReplacePolicy'], self.do_validation=True, self.properties={'DashboardBody': '{\"a\": \"b\"}'}, self.propnames={'DashboardBody', 'DashboardName'}, self.resource={'Properties': {'DashboardBody': '{\"a\": \"b\"}'}, 'Type': 'AWS::CloudWatch::Dashboard'}, self.template=None, self.title='dashboard')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "name = \"DashboardBody\"\n",
      "State:\n",
      "'DashboardBody'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def validate_trigger(self):\n",
      "    valid = [\n",
      "        \"all\",\n",
      "        \"createReference\",\n",
      "        \"deleteReference\",\n",
      "        \"updateReference\",\n",
      "    ]\n",
      "    events = self.properties.get(\"Events\")\n",
      "    if events and not isinstance(events, AWSHelperFn):\n",
      "        if \"all\" in events and len(events) != 1:\n",
      "            raise ValueError(\"Trigger events: all must be used alone\")\n",
      "        else:\n",
      "            for e in events:\n",
      "                if e not in valid and not isinstance(e, AWSHelperFn):\n",
      "                    raise ValueError(\"Trigger: invalid event %s\" % e)\n",
      "validate_trigger(self=<troposphere.codecommit.Trigger object at 0x7f711cff0640>, self._BaseAWSObject__initialized=True, self.attributes=['Condition', 'CreationPolicy', 'DeletionPolicy', 'DependsOn', 'Metadata', 'UpdatePolicy', 'UpdateReplacePolicy'], self.do_validation=True, self.properties={'DestinationArn': 'arn:random', 'Events': ['all'], 'Name': 'trigger name'}, self.propnames={'Branches', 'DestinationArn', 'Name', 'CustomData', 'Events'}, self.resource={'DestinationArn': 'arn:random', 'Events': ['all'], 'Name': 'trigger name'}, self.template=None, self.title=None)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "valid = [\n",
      "State:\n",
      "['all', 'createReference', 'deleteReference', 'updateReference']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def validate_trigger(self):\n",
      "    valid = [\n",
      "        \"all\",\n",
      "        \"createReference\",\n",
      "        \"deleteReference\",\n",
      "        \"updateReference\",\n",
      "    ]\n",
      "    events = self.properties.get(\"Events\")\n",
      "    if events and not isinstance(events, AWSHelperFn):\n",
      "        if \"all\" in events and len(events) != 1:\n",
      "            raise ValueError(\"Trigger events: all must be used alone\")\n",
      "        else:\n",
      "            for e in events:\n",
      "                if e not in valid and not isinstance(e, AWSHelperFn):\n",
      "                    raise ValueError(\"Trigger: invalid event %s\" % e)\n",
      "validate_trigger(self=<troposphere.codecommit.Trigger object at 0x7f711cff0640>, self._BaseAWSObject__initialized=True, self.attributes=['Condition', 'CreationPolicy', 'DeletionPolicy', 'DependsOn', 'Metadata', 'UpdatePolicy', 'UpdateReplacePolicy'], self.do_validation=True, self.properties={'DestinationArn': 'arn:random', 'Events': ['all'], 'Name': 'trigger name'}, self.propnames={'Branches', 'DestinationArn', 'Name', 'CustomData', 'Events'}, self.resource={'DestinationArn': 'arn:random', 'Events': ['all'], 'Name': 'trigger name'}, self.template=None, self.title=None)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "events = self.properties.get(\"Events\")\n",
      "State:\n",
      "['all']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def is_aws_object_subclass(cls: Any) -> bool:\n",
      "    is_aws_object = False\n",
      "    try:\n",
      "        is_aws_object = issubclass(cls, BaseAWSObject)\n",
      "    except TypeError:\n",
      "        pass\n",
      "    return is_aws_object\n",
      "is_aws_object_subclass(cls=<function integer at 0x7f711d9e0940>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "is_aws_object = False\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def is_aws_object_subclass(cls: Any) -> bool:\n",
      "    is_aws_object = False\n",
      "    try:\n",
      "        is_aws_object = issubclass(cls, BaseAWSObject)\n",
      "    except TypeError:\n",
      "        pass\n",
      "    return is_aws_object\n",
      "is_aws_object_subclass(cls=<function integer at 0x7f711d9e0940>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "is_aws_object = issubclass(cls, BaseAWSObject)\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def check_properties(action_types, props_to_check, required):\n",
      "        for this_type in action_types:\n",
      "            self_props = self.properties\n",
      "            if self_props.get(\"TargetType\") == this_type:\n",
      "                invalid_props = []\n",
      "                for prop in props_to_check:\n",
      "                    if (prop not in self_props and required is True) or (\n",
      "                        prop in self_props and required is False\n",
      "                    ):\n",
      "                        invalid_props.append(prop)\n",
      "                if len(invalid_props) > 0:\n",
      "                    type_msg = (\n",
      "                        \"Omitting TargetType\"\n",
      "                        if this_type is None\n",
      "                        else 'TargetType of \"%s\"' % this_type\n",
      "                    )\n",
      "                    raise ValueError(\n",
      "                        '%s in \"%s\" %s definitions of %s'\n",
      "                        % (\n",
      "                            type_msg,\n",
      "                            self.__class__.__name__,\n",
      "                            \"requires\" if required is True else \"must not contain\",\n",
      "                            str(invalid_props).strip(\"[]\"),\n",
      "                        )\n",
      "                    )\n",
      "check_properties(action_types=[None, 'instance', 'ip'], props_to_check=['Port', 'Protocol', 'VpcId'], required=True, self=<troposphere.elasticloadbalancingv2.TargetGroup object at 0x7f711ce8adf0>, self._BaseAWSObject__initialized=True, self.attributes=['Condition', 'CreationPolicy', 'DeletionPolicy', 'DependsOn', 'Metadata', 'UpdatePolicy', 'UpdateReplacePolicy'], self.do_validation=True, self.properties={'TargetType': 'instance'}, self.propnames={'Targets', 'TargetType', 'Protocol', 'HealthyThresholdCount', 'HealthCheckPort', 'HealthCheckProtocol', 'IpAddressType', 'HealthCheckEnabled', 'HealthCheckPath', 'VpcId', 'Name', 'Tags', 'Matcher', 'HealthCheckTimeoutSeconds', 'Port', 'UnhealthyThresholdCount', 'HealthCheckIntervalSeconds', 'ProtocolVersion', 'TargetGroupAttributes'}, self.resource={'Properties': {'TargetType': 'instance'}, 'Type': 'AWS::ElasticLoadBalancingV2::TargetGroup'}, self.template=None, self.title='targetGroup')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "invalid_props = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_transform(self, transform: Union[List[object], str]) -> None:\n",
      "        from troposphere.serverless import SERVERLESS_TRANSFORM\n",
      "        if self.globals and transform != SERVERLESS_TRANSFORM:\n",
      "            raise ValueError(\n",
      "                \"Cannot set transform to non-Serverless while using Globals\"\n",
      "            )\n",
      "        self.transform = transform\n",
      "set_transform(self=<troposphere.Template object at 0x7f71133367f0>, transform='AWS::LanguageExtensions', self.conditions={}, self.description=None, self.globals=None, self.mappings={}, self.metadata={}, self.outputs={}, self.parameters={}, self.resources={}, self.rules={}, self.transform=None, self.version='2010-09-09')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "from troposphere.serverless import SERVERLESS_TRANSFORM\n",
      "State:\n",
      "'AWS::Serverless-2016-10-31'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add_mapping(self, name: str, mapping: Dict[str, Any]) -> None:\n",
      "        if len(self.mappings) >= MAX_MAPPINGS:\n",
      "            raise ValueError(\"Maximum mappings %d reached\" % MAX_MAPPINGS)\n",
      "        if name not in self.mappings:\n",
      "            self.mappings[name] = {}\n",
      "        self.mappings[name].update(mapping)\n",
      "add_mapping(self=<troposphere.Template object at 0x7f7113b59040>, name='map', mapping={'k1': {'n1': 'v1'}}, self.conditions={}, self.description=None, self.globals=None, self.mappings={}, self.metadata={}, self.outputs={}, self.parameters={}, self.resources={}, self.rules={}, self.transform=None, self.version=None)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "self.mappings[name] = {}\n",
      "State:\n",
      "{'map': {}}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add_mapping(self, name: str, mapping: Dict[str, Any]) -> None:\n",
      "        if len(self.mappings) >= MAX_MAPPINGS:\n",
      "            raise ValueError(\"Maximum mappings %d reached\" % MAX_MAPPINGS)\n",
      "        if name not in self.mappings:\n",
      "            self.mappings[name] = {}\n",
      "        self.mappings[name].update(mapping)\n",
      "add_mapping(self=<troposphere.Template object at 0x7f7113b59040>, name='map', mapping={'k1': {'n1': 'v1'}}, self.conditions={}, self.description=None, self.globals=None, self.mappings={}, self.metadata={}, self.outputs={}, self.parameters={}, self.resources={}, self.rules={}, self.transform=None, self.version=None)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "self.mappings[name].update(mapping)\n",
      "State:\n",
      "{'map': {'k1': {'n1': 'v1'}}}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __add__(self, newtags):\n",
      "        newtags.tags = self.tags + newtags.tags\n",
      "        return newtags\n",
      "__add__(self=<troposphere.validators.autoscaling.Tags object at 0x7f711cd48160>, newtags={tags=[{'Key': 'bar', 'Value': 'barval', 'PropagateAtLaunch': False}]}, self.tags=[{'Key': 'foo', 'Value': 'fooval', 'PropagateAtLaunch': True}])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "newtags.tags = self.tags + newtags.tags\n",
      "State:\n",
      "{tags=[{'Key': 'foo', 'Value': 'fooval', 'PropagateAtLaunch': True}, {'Key': 'bar', 'Value': 'barval', 'PropagateAtLaunch': False}]}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_description(self, description: str) -> None:\n",
      "        self.description = description\n",
      "set_description(self=<troposphere.template_generator.TemplateGenerator object at 0x7f7113b978e0>, description='Description', self._reference_map={}, self.conditions={}, self.description=None, self.globals=None, self.mappings={}, self.metadata={}, self.outputs={}, self.parameters={}, self.resources={}, self.rules={}, self.transform=None, self.version='2010-09-09')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.description = description\n",
      "State:\n",
      "'Description'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def create_result(self, file, delimiter=\"\"):\n",
      "        file = os.path.join(self.filepath, file)\n",
      "        self.instance.UserData = userdata.from_file(file, delimiter)\n",
      "        return self.instance.UserData.to_dict()\n",
      "create_result(self=<tests.test_userdata.TestUserdata testMethod=test_char_escaping>, file='char_escaping.sh', delimiter='', self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7f711c955c70>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_char_escaping', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.filepath='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/cloudtools+troposphere/cloudtools+troposphere/tests/userdata_test_scripts/', self.instance=<troposphere.ec2.Instance object at 0x7f711c955df0>, self.test_char_escaping=<bound method TestUserdata.test_char_escaping of <tests.test_userdata.TestUserdata testMethod=test_char_escaping>>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "file = os.path.join(self.filepath, file)\n",
      "State:\n",
      "'/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/cloudtools+troposphere/cloudtools+troposphere/tests/userdata_test_scripts/char_escaping.sh'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def notification_event(events):\n",
      "    valid_events = [\"All\", \"InProgress\", \"Success\", \"TimedOut\", \"Cancelled\", \"Failed\"]\n",
      "    for event in events:\n",
      "        if event not in valid_events:\n",
      "            raise ValueError(\n",
      "                'NotificationEvents must be at least one of: \"%s\"'\n",
      "                % (\", \".join(valid_events))\n",
      "            )\n",
      "    return events\n",
      "notification_event(events=['All', 'InProgress', 'Success', 'TimedOut', 'Cancelled', 'Failed'])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "valid_events = [\"All\", \"InProgress\", \"Success\", \"TimedOut\", \"Cancelled\", \"Failed\"]\n",
      "State:\n",
      "['All', 'InProgress', 'Success', 'TimedOut', 'Cancelled', 'Failed']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_air_quality(city):\n",
      "    if not city or not city.strip():\n",
      "        return\n",
      "    print(' {} ...'.format(city))\n",
      "    try:\n",
      "        url = 'http://api.waqi.info/feed/{city}/?token={token}'.format(city=city, token=AQICN_TOKEN)\n",
      "        resp = requests.get(url)\n",
      "        if resp.status_code == 200:\n",
      "            content_dict = resp.json()\n",
      "            if content_dict.get('status') == 'ok':\n",
      "                data_dict = content_dict['data']\n",
      "                aqi = data_dict['aqi']\n",
      "                air_status = ''\n",
      "                for key in sorted(AIR_STATUS_DICT):\n",
      "                    if key >= aqi:\n",
      "                        air_status = AIR_STATUS_DICT[key]\n",
      "                        break\n",
      "                aqi_info = '{city} PM2.5{aqi} {air_status}'.format(city=city, aqi=aqi, air_status=air_status)\n",
      "                return aqi_info\n",
      "            else:\n",
      "                print(':{}'.format(content_dict['data']))\n",
      "                return None\n",
      "        print('')\n",
      "    except Exception as exception:\n",
      "        print(str(exception))\n",
      "    return None\n",
      "get_air_quality(city='')\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "resp = requests.get(url)\n",
      "State:\n",
      "<Response [200]>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_air_quality(city):\n",
      "    if not city or not city.strip():\n",
      "        return\n",
      "    print(' {} ...'.format(city))\n",
      "    try:\n",
      "        url = 'http://api.waqi.info/feed/{city}/?token={token}'.format(city=city, token=AQICN_TOKEN)\n",
      "        resp = requests.get(url)\n",
      "        if resp.status_code == 200:\n",
      "            content_dict = resp.json()\n",
      "            if content_dict.get('status') == 'ok':\n",
      "                data_dict = content_dict['data']\n",
      "                aqi = data_dict['aqi']\n",
      "                air_status = ''\n",
      "                for key in sorted(AIR_STATUS_DICT):\n",
      "                    if key >= aqi:\n",
      "                        air_status = AIR_STATUS_DICT[key]\n",
      "                        break\n",
      "                aqi_info = '{city} PM2.5{aqi} {air_status}'.format(city=city, aqi=aqi, air_status=air_status)\n",
      "                return aqi_info\n",
      "            else:\n",
      "                print(':{}'.format(content_dict['data']))\n",
      "                return None\n",
      "        print('')\n",
      "    except Exception as exception:\n",
      "        print(str(exception))\n",
      "    return None\n",
      "get_air_quality(city='')\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "data_dict = content_dict['data']\n",
      "State:\n",
      "{'aqi': 50, 'idx': 1552, 'attributions': [{'url': 'http://sthjt.gxzf.gov.cn/', 'name': 'Guangxi Zhuang Autonomous Region Environmental Protection Agency ()'}, {'url': 'https://waqi.info/', 'name': 'World Air Quality Index Project'}], 'city': {'geo': [25.273566, 110.290195], 'name': 'Guilin ()', 'url': 'https://aqicn.org/city/guilin', 'location': ''}, 'dominentpol': 'pm25', 'iaqi': {'co': {'v': 9.1}, 'h': {'v': 77}, 'no2': {'v': 4.6}, 'p': {'v': 1012}, 'pm10': {'v': 20}, 'pm25': {'v': 50}, 'so2': {'v': 4.1}, 't': {'v': 18}, 'w': {'v': 4.6}}, 'time': {'s': '2024-04-04 19:00:00', 'tz': '+08:00', 'v': 1712257200, 'iso': '2024-04-04T19:00:00+08:00'}, 'forecast': {'daily': {'o3': [{'avg': 7, 'day': '2024-04-02', 'max': 22, 'min': 4}, {'avg': 3, 'day': '2024-04-03', 'max': 8, 'min': 2}, {'avg': 6, 'day': '2024-04-04', 'max': 12, 'min': 2}, {'avg': 3, 'day': '2024-04-05', 'max': 5, 'min': 1}, {'avg': 6, 'day': '2024-04-06', 'max': 7, 'min': 5}, {'avg': 8, 'day': '2024-04-07', 'max': 13, 'min': 5}, {'avg': 15, 'day': '2024-04-08', 'max': 23, 'min': 10}, {'avg': 15, 'day': '2024-04-09', 'max': 17, 'min': 14}], 'pm10': [{'avg': 47, 'day': '2024-04-02', 'max': 58, 'min': 32}, {'avg': 56, 'day': '2024-04-03', 'max': 83, 'min': 46}, {'avg': 45, 'day': '2024-04-04', 'max': 46, 'min': 41}, {'avg': 29, 'day': '2024-04-05', 'max': 38, 'min': 24}, {'avg': 22, 'day': '2024-04-06', 'max': 28, 'min': 19}, {'avg': 24, 'day': '2024-04-07', 'max': 28, 'min': 19}, {'avg': 22, 'day': '2024-04-08', 'max': 28, 'min': 19}, {'avg': 30, 'day': '2024-04-09', 'max': 38, 'min': 28}, {'avg': 44, 'day': '2024-04-10', 'max': 46, 'min': 28}], 'pm25': [{'avg': 140, 'day': '2024-04-02', 'max': 158, 'min': 98}, {'avg': 153, 'day': '2024-04-03', 'max': 185, 'min': 138}, {'avg': 136, 'day': '2024-04-04', 'max': 138, 'min': 123}, {'avg': 90, 'day': '2024-04-05', 'max': 115, 'min': 70}, {'avg': 72, 'day': '2024-04-06', 'max': 89, 'min': 68}, {'avg': 80, 'day': '2024-04-07', 'max': 89, 'min': 68}, {'avg': 74, 'day': '2024-04-08', 'max': 88, 'min': 68}, {'avg': 89, 'day': '2024-04-09', 'max': 89, 'min': 89}, {'avg': 119, 'day': '2024-04-10', 'max': 138, 'min': 89}], 'uvi': [{'avg': 0, 'day': '2022-10-15', 'max': 0, 'min': 0}, {'avg': 1, 'day': '2022-10-16', 'max': 7, 'min': 0}, {'avg': 1, 'day': '2022-10-17', 'max': 8, 'min': 0}, {'avg': 1, 'day': '2022-10-18', 'max': 8, 'min': 0}, {'avg': 1, 'day': '2022-10-19', 'max': 8, 'min': 0}, {'avg': 1, 'day': '2022-10-20', 'max': 5, 'min': 0}]}}, 'debug': {'sync': '2024-04-04T20:26:23+09:00'}}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_air_quality(city):\n",
      "    if not city or not city.strip():\n",
      "        return\n",
      "    print(' {} ...'.format(city))\n",
      "    try:\n",
      "        url = 'http://api.waqi.info/feed/{city}/?token={token}'.format(city=city, token=AQICN_TOKEN)\n",
      "        resp = requests.get(url)\n",
      "        if resp.status_code == 200:\n",
      "            content_dict = resp.json()\n",
      "            if content_dict.get('status') == 'ok':\n",
      "                data_dict = content_dict['data']\n",
      "                aqi = data_dict['aqi']\n",
      "                air_status = ''\n",
      "                for key in sorted(AIR_STATUS_DICT):\n",
      "                    if key >= aqi:\n",
      "                        air_status = AIR_STATUS_DICT[key]\n",
      "                        break\n",
      "                aqi_info = '{city} PM2.5{aqi} {air_status}'.format(city=city, aqi=aqi, air_status=air_status)\n",
      "                return aqi_info\n",
      "            else:\n",
      "                print(':{}'.format(content_dict['data']))\n",
      "                return None\n",
      "        print('')\n",
      "    except Exception as exception:\n",
      "        print(str(exception))\n",
      "    return None\n",
      "get_air_quality(city='')\n",
      "\n",
      "n:\n",
      "25\n",
      "Statement:\n",
      "air_status = AIR_STATUS_DICT[key]\n",
      "State:\n",
      "''\n",
      "==================================================\n",
      "Clean Code:\n",
      "def getDefaultSaveDirectoryWithName(cls, appName):\n",
      "        if os.name == 'nt':\n",
      "            logFolder = os.path.join(os.path.expandvars(\"%APPDATA%\"), appName)\n",
      "        else:\n",
      "            tmpPath = Path(f'/var/log/{uuid.uuid4()}')\n",
      "            try:\n",
      "                tmpPath.touch()\n",
      "                tmpPath.unlink()\n",
      "                tmpPath = tmpPath.parent\n",
      "            except PermissionError:\n",
      "                tmpPath = Path.home() / 'log'\n",
      "                tmpPath.mkdir(exist_ok=True)\n",
      "            logFolder = tmpPath / appName\n",
      "        if not os.path.isdir(logFolder):\n",
      "            os.makedirs(logFolder)\n",
      "        return logFolder\n",
      "getDefaultSaveDirectoryWithName(cls=<class 'csmlog.CSMLogger'>, appName='csmlog_test')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "tmpPath = Path(f'/var/log/{uuid.uuid4()}')\n",
      "State:\n",
      "PosixPath('/var/log/8db00105-ced9-4191-9269-c2aad6e71766')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __getLoggerWithName(self, loggerName):\n",
      "        logger = logging.getLogger(loggerName)\n",
      "        logger.setLevel(1)\n",
      "        logFolder = self.getDefaultSaveDirectory()\n",
      "        logFile = os.path.join(logFolder, loggerName + \".txt\")\n",
      "        formatter = self.getFormatter()\n",
      "        rfh = logging.handlers.RotatingFileHandler(logFile, maxBytes=1024*1024*8, backupCount=10)\n",
      "        rfh.setFormatter(formatter)\n",
      "        logger.addHandler(rfh)\n",
      "        logger.logFile = logFile\n",
      "        logger.logFolder = logFolder\n",
      "        logger.loggerName = loggerName\n",
      "        return logger\n",
      "__getLoggerWithName(self=<csmlog.CSMLogger object at 0x7f031056a8b0>, loggerName='csmlog_test', self.appName='csmlog_test', self.googleSheetShareEmail=None, self.udpLogging=True)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "logger = logging.getLogger(loggerName)\n",
      "State:\n",
      "<Logger csmlog_test (WARNING)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __getLoggerWithName(self, loggerName):\n",
      "        logger = logging.getLogger(loggerName)\n",
      "        logger.setLevel(1)\n",
      "        logFolder = self.getDefaultSaveDirectory()\n",
      "        logFile = os.path.join(logFolder, loggerName + \".txt\")\n",
      "        formatter = self.getFormatter()\n",
      "        rfh = logging.handlers.RotatingFileHandler(logFile, maxBytes=1024*1024*8, backupCount=10)\n",
      "        rfh.setFormatter(formatter)\n",
      "        logger.addHandler(rfh)\n",
      "        logger.logFile = logFile\n",
      "        logger.logFolder = logFolder\n",
      "        logger.loggerName = loggerName\n",
      "        return logger\n",
      "__getLoggerWithName(self=<csmlog.CSMLogger object at 0x7f031056a8b0>, loggerName='csmlog_test', self.appName='csmlog_test', self.googleSheetShareEmail=None, self.udpLogging=True)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "logger.setLevel(1) # log all\n",
      "State:\n",
      "<Logger csmlog_test (Level 1)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __getLoggerWithName(self, loggerName):\n",
      "        logger = logging.getLogger(loggerName)\n",
      "        logger.setLevel(1)\n",
      "        logFolder = self.getDefaultSaveDirectory()\n",
      "        logFile = os.path.join(logFolder, loggerName + \".txt\")\n",
      "        formatter = self.getFormatter()\n",
      "        rfh = logging.handlers.RotatingFileHandler(logFile, maxBytes=1024*1024*8, backupCount=10)\n",
      "        rfh.setFormatter(formatter)\n",
      "        logger.addHandler(rfh)\n",
      "        logger.logFile = logFile\n",
      "        logger.logFolder = logFolder\n",
      "        logger.loggerName = loggerName\n",
      "        return logger\n",
      "__getLoggerWithName(self=<csmlog.CSMLogger object at 0x7f031056a8b0>, loggerName='csmlog_test', self.appName='csmlog_test', self.googleSheetShareEmail=None, self.udpLogging=True)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "logFolder = self.getDefaultSaveDirectory()\n",
      "State:\n",
      "PosixPath('/home/XXX/log/csmlog_test')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __getLoggerWithName(self, loggerName):\n",
      "        logger = logging.getLogger(loggerName)\n",
      "        logger.setLevel(1)\n",
      "        logFolder = self.getDefaultSaveDirectory()\n",
      "        logFile = os.path.join(logFolder, loggerName + \".txt\")\n",
      "        formatter = self.getFormatter()\n",
      "        rfh = logging.handlers.RotatingFileHandler(logFile, maxBytes=1024*1024*8, backupCount=10)\n",
      "        rfh.setFormatter(formatter)\n",
      "        logger.addHandler(rfh)\n",
      "        logger.logFile = logFile\n",
      "        logger.logFolder = logFolder\n",
      "        logger.loggerName = loggerName\n",
      "        return logger\n",
      "__getLoggerWithName(self=<csmlog.CSMLogger object at 0x7f031056a8b0>, loggerName='csmlog_test', self.appName='csmlog_test', self.googleSheetShareEmail=None, self.udpLogging=True)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "rfh = logging.handlers.RotatingFileHandler(logFile, maxBytes=1024*1024*8, backupCount=10)\n",
      "State:\n",
      "<RotatingFileHandler /home/XXX/log/csmlog_test/csmlog_test.txt (NOTSET)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def emit(self, record):\n",
      "        msg = (self.format(record) + \"\\n\").encode()\n",
      "        msgLen = len(msg)\n",
      "        offset = 0\n",
      "        while offset < msgLen:\n",
      "            offset += self.socket.sendto(msg[offset:offset+MAX_BYTES_PER_SEND], (self.ip, self.port))\n",
      "emit(self=<UdpHandler 127.0.0.1:5123>, record=<LogRecord: csmlog_test, 10, /local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/csm10495+csmlog/csm10495+csmlog/csmlog/__init__.py, 206, \"==== csmlog_test is starting ====\">, self._name=None, self.filters=[], self.formatter=<logging.Formatter object at 0x7f0310534040>, self.ip='127.0.0.1', self.level=0, self.lock=<locked _thread.RLock object owner=139651175405376 count=1 at 0x7f03105342a0>, self.port=5123, self.socket=<socket.socket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_DGRAM, proto=0, laddr=('0.0.0.0', 0)>, self.stream=<_io.TextIOWrapper name='<stderr>' mode='w' encoding='utf-8'>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "msg = (self.format(record) + \"\\n\").encode()\n",
      "State:\n",
      "b'2024-04-03 22:19:41,573 - csmlog_test:206 - DEBUG - ==== csmlog_test is starting ====\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def emit(self, record):\n",
      "        msg = (self.format(record) + \"\\n\").encode()\n",
      "        msgLen = len(msg)\n",
      "        offset = 0\n",
      "        while offset < msgLen:\n",
      "            offset += self.socket.sendto(msg[offset:offset+MAX_BYTES_PER_SEND], (self.ip, self.port))\n",
      "emit(self=<UdpHandler 127.0.0.1:5123>, record=<LogRecord: csmlog_test, 10, /local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/csm10495+csmlog/csm10495+csmlog/csmlog/__init__.py, 206, \"==== csmlog_test is starting ====\">, self._name=None, self.filters=[], self.formatter=<logging.Formatter object at 0x7f0310534040>, self.ip='127.0.0.1', self.level=0, self.lock=<locked _thread.RLock object owner=139651175405376 count=1 at 0x7f03105342a0>, self.port=5123, self.socket=<socket.socket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_DGRAM, proto=0, laddr=('0.0.0.0', 0)>, self.stream=<_io.TextIOWrapper name='<stderr>' mode='w' encoding='utf-8'>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "msgLen = len(msg)\n",
      "State:\n",
      "86\n",
      "==================================================\n",
      "Clean Code:\n",
      "def emit(self, record):\n",
      "        msg = (self.format(record) + \"\\n\").encode()\n",
      "        msgLen = len(msg)\n",
      "        offset = 0\n",
      "        while offset < msgLen:\n",
      "            offset += self.socket.sendto(msg[offset:offset+MAX_BYTES_PER_SEND], (self.ip, self.port))\n",
      "emit(self=<UdpHandler 127.0.0.1:5123>, record=<LogRecord: csmlog_test, 10, /local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/csm10495+csmlog/csm10495+csmlog/csmlog/__init__.py, 206, \"==== csmlog_test is starting ====\">, self._name=None, self.filters=[], self.formatter=<logging.Formatter object at 0x7f0310534040>, self.ip='127.0.0.1', self.level=0, self.lock=<locked _thread.RLock object owner=139651175405376 count=1 at 0x7f03105342a0>, self.port=5123, self.socket=<socket.socket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_DGRAM, proto=0, laddr=('0.0.0.0', 0)>, self.stream=<_io.TextIOWrapper name='<stderr>' mode='w' encoding='utf-8'>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "offset = 0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def disableConsoleLogging(self):\n",
      "        if not self.consoleLoggingStream:\n",
      "            raise RuntimeError(\"Managed console logging is not active\")\n",
      "        self.parentLogger.removeHandler(self.consoleLoggingStream)\n",
      "        self.consoleLoggingStream = None\n",
      "disableConsoleLogging(self=<csmlog.CSMLogger object at 0x7f03104363d0>, self._loggers=[<Logger csmlog_test (Level 1)>, <Logger csmlog_test.tmp (Level 1)>], self.appName='csmlog_test', self.consoleLoggingStream=<StreamHandler (Level 1)>, self.googleSheetShareEmail=None, self.parentLogger=<Logger csmlog_test (Level 1)>, self.udpLogging=True)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "self.consoleLoggingStream = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def check_output(self, cmd, shell=False):\n",
      "        self.logger.debug(\"About to call: %s\" % cmd)\n",
      "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=shell)\n",
      "        output = ''\n",
      "        while process.poll() is None:\n",
      "            output += self._logIo(process)\n",
      "            time.sleep(.00001)\n",
      "        self.logger.debug(\".. Exit Code: %d\" % process.returncode)\n",
      "        if process.returncode != 0:\n",
      "            raise subprocess.CalledProcessError(process.returncode, cmd, output)\n",
      "        return output\n",
      "check_output(self=<csmlog.system_call.LoggedSystemCall object at 0x7f03104366a0>, cmd='echo hi', shell=True, self.logger=<Logger csmlog_test.tmp (Level 1)>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=shell)\n",
      "State:\n",
      "<Popen: returncode: None args: 'echo hi'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def check_output(self, cmd, shell=False):\n",
      "        self.logger.debug(\"About to call: %s\" % cmd)\n",
      "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=shell)\n",
      "        output = ''\n",
      "        while process.poll() is None:\n",
      "            output += self._logIo(process)\n",
      "            time.sleep(.00001)\n",
      "        self.logger.debug(\".. Exit Code: %d\" % process.returncode)\n",
      "        if process.returncode != 0:\n",
      "            raise subprocess.CalledProcessError(process.returncode, cmd, output)\n",
      "        return output\n",
      "check_output(self=<csmlog.system_call.LoggedSystemCall object at 0x7f03104366a0>, cmd='echo hi', shell=True, self.logger=<Logger csmlog_test.tmp (Level 1)>)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "while process.poll() is None:\n",
      "State:\n",
      "<Popen: returncode: 0 args: 'echo hi'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __getattr__(self, item):\n",
      "        res = None\n",
      "        if self.__configured is not None:\n",
      "            try:\n",
      "                res = self.__configured[item]\n",
      "            except Exception:\n",
      "                res = getattr(self.__configured, item)\n",
      "        if res is None:\n",
      "            raise RequiredParameterException(f'Missing argument for \"{item}\"')\n",
      "        return res\n",
      "__getattr__(self=<class 'clima.core.Schema'>, item='__bases__', self._Configurable__configured=<class 'clima.core.Schema'>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "res = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __getattr__(self, item):\n",
      "        res = None\n",
      "        if self.__configured is not None:\n",
      "            try:\n",
      "                res = self.__configured[item]\n",
      "            except Exception:\n",
      "                res = getattr(self.__configured, item)\n",
      "        if res is None:\n",
      "            raise RequiredParameterException(f'Missing argument for \"{item}\"')\n",
      "        return res\n",
      "__getattr__(self=<class 'clima.core.Schema'>, item='__bases__', self._Configurable__configured=<class 'clima.core.Schema'>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "res = self.__configured[item]\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def prepare_docstring_help(N):\n",
      "    args = []\n",
      "    if hasattr(N, '__annotations__'):\n",
      "        for attr_name, cls in N.__annotations__.items():\n",
      "            filtered = filter_params(N)\n",
      "            parsed = parse_source_for_params(filtered)\n",
      "            attr = attr_map(parsed).get(attr_name)\n",
      "            if attr is None:\n",
      "                continue\n",
      "            args.append(argument_help(attr_name, attr))\n",
      "    return '\\n'.join(args)\n",
      "prepare_docstring_help(N={})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "args = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def prepare_docstring_help(N):\n",
      "    args = []\n",
      "    if hasattr(N, '__annotations__'):\n",
      "        for attr_name, cls in N.__annotations__.items():\n",
      "            filtered = filter_params(N)\n",
      "            parsed = parse_source_for_params(filtered)\n",
      "            attr = attr_map(parsed).get(attr_name)\n",
      "            if attr is None:\n",
      "                continue\n",
      "            args.append(argument_help(attr_name, attr))\n",
      "    return '\\n'.join(args)\n",
      "prepare_docstring_help(N={})\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "attr = attr_map(parsed).get(attr_name)\n",
      "State:\n",
      "{'type': 'int', 'default': '0', 'description': ''}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def filter_params(N):\n",
      "    filtered_source = []\n",
      "    for line in inspect.getsourcelines(N.__class__)[0][1:]:\n",
      "        if line.strip().startswith('def '):\n",
      "            break\n",
      "        filtered_source.append(line)\n",
      "    return filtered_source\n",
      "filter_params(N={})\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "filtered_source = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def attr_map(parsed_params):\n",
      "    mapped_attrs = {}\n",
      "    for param_type, def_desc in parsed_params.items():\n",
      "        if ':' in param_type:\n",
      "            param, _type = param_type.split(':', 1)\n",
      "            _type = _type.strip()\n",
      "        else:\n",
      "            param = param_type\n",
      "            _type = None\n",
      "        if '\n",
      "            default, description = def_desc.split('\n",
      "            default = default.strip()\n",
      "            description = description.strip()\n",
      "        else:\n",
      "            default = def_desc.strip()\n",
      "            description = ''\n",
      "        mapped_attrs.update(\n",
      "            {\n",
      "                param: {\n",
      "                    'type': _type,\n",
      "                    'default': default,\n",
      "                    'description': description,\n",
      "                }\n",
      "            }\n",
      "        )\n",
      "    return mapped_attrs\n",
      "attr_map(parsed_params=OrderedDict([('bar: int', '0')]))\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "param, _type = param_type.split(':', 1)\n",
      "State:\n",
      "'bar'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def attr_map(parsed_params):\n",
      "    mapped_attrs = {}\n",
      "    for param_type, def_desc in parsed_params.items():\n",
      "        if ':' in param_type:\n",
      "            param, _type = param_type.split(':', 1)\n",
      "            _type = _type.strip()\n",
      "        else:\n",
      "            param = param_type\n",
      "            _type = None\n",
      "        if '\n",
      "            default, description = def_desc.split('\n",
      "            default = default.strip()\n",
      "            description = description.strip()\n",
      "        else:\n",
      "            default = def_desc.strip()\n",
      "            description = ''\n",
      "        mapped_attrs.update(\n",
      "            {\n",
      "                param: {\n",
      "                    'type': _type,\n",
      "                    'default': default,\n",
      "                    'description': description,\n",
      "                }\n",
      "            }\n",
      "        )\n",
      "    return mapped_attrs\n",
      "attr_map(parsed_params=OrderedDict([('bar: int', '0')]))\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "mapped_attrs.update(\n",
      "State:\n",
      "{'bar': {'type': 'int', 'default': '0', 'description': ''}}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def replace_docstring(func, args):\n",
      "    docstring = func.__doc__\n",
      "    docstring = (docstring if docstring is not None else '') + '\\nArgs:\\n' + args\n",
      "    func.__doc__ = docstring\n",
      "replace_docstring(func=<function TestConfigFromWorkingDir.test_configfile.<locals>.Cli.x at 0x7f29d9417f70>, args='    --bar (int):  (Default is 0)')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "docstring = func.__doc__\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def replace_docstring(func, args):\n",
      "    docstring = func.__doc__\n",
      "    docstring = (docstring if docstring is not None else '') + '\\nArgs:\\n' + args\n",
      "    func.__doc__ = docstring\n",
      "replace_docstring(func=<function TestConfigFromWorkingDir.test_configfile.<locals>.Cli.x at 0x7f29d9417f70>, args='    --bar (int):  (Default is 0)')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "docstring = (docstring if docstring is not None else '') + '\\nArgs:\\n' + args\n",
      "State:\n",
      "'\\nArgs:\\n    --bar (int):  (Default is 0)'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _ParseFn(args):\n",
      "    kwargs, remaining_kwargs, remaining_args = _ParseKeywordArgs(\n",
      "        args, all_args, fn_spec.varkw)\n",
      "    parsed_args, kwargs, remaining_args, capacity = _ParseArgs(\n",
      "        fn_spec.args, fn_spec.defaults, num_required_args, kwargs,\n",
      "        remaining_args, metadata)\n",
      "    if fn_spec.varargs or fn_spec.varkw:\n",
      "      capacity = True\n",
      "    extra_kw = set(kwargs) - set(fn_spec.kwonlyargs)\n",
      "    if fn_spec.varkw is None and extra_kw:\n",
      "      raise FireError('Unexpected kwargs present:', extra_kw)\n",
      "    missing_kwonly = set(required_kwonly) - set(kwargs)\n",
      "    if missing_kwonly:\n",
      "      raise FireError('Missing required flags:', missing_kwonly)\n",
      "    if fn_spec.varargs is not None:\n",
      "      varargs, remaining_args = remaining_args, []\n",
      "    else:\n",
      "      varargs = []\n",
      "    for index, value in enumerate(varargs):\n",
      "      varargs[index] = _ParseValue(value, None, None, metadata)\n",
      "    varargs = parsed_args + varargs\n",
      "    remaining_args += remaining_kwargs\n",
      "    consumed_args = args[:len(args) - len(remaining_args)]\n",
      "    return (varargs, kwargs), consumed_args, remaining_args, capacity\n",
      "_ParseFn(args=['x'], all_args=[], fn_spec={args=[], varargs=None, varkw='cli_args', defaults=(), kwonlyargs=[], kwonlydefaults={}, annotations={}}, metadata={'ACCEPTS_POSITIONAL_ARGS': False}, num_required_args=0, required_kwonly=set())\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "kwargs, remaining_kwargs, remaining_args = _ParseKeywordArgs(\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _setup(self, params: dict, configuration_tuple):\n",
      "        self.__configured = dict(\n",
      "            ChainMap(\n",
      "                params,\n",
      "                config_dict(configuration_tuple),\n",
      "                utils.filter_fields(os.environ, configuration_tuple),\n",
      "                configuration_tuple._asdict()\n",
      "            )\n",
      "        )\n",
      "        return self.__configured\n",
      "_setup(self=<class 'tests.test_configfile.TestConfigFromWorkingDir.setUp.<locals>.C'>, params={}, configuration_tuple={}, self._Configurable__configured=<class 'tests.test_configfile.TestConfigFromWorkingDir.setUp.<locals>.C'>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.__configured = dict(\n",
      "State:\n",
      "{'bar': 42}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def config_dict(configuration_tuple):\n",
      "    config_dict = {}\n",
      "    config_file = configuration_tuple._asdict().get('CFG')\n",
      "    if config_file is None:\n",
      "        config_file = configfile.get_config_path(configuration_tuple)\n",
      "    if config_file is not None:\n",
      "        config_dict = utils.filter_fields(configfile.read_config(config_file), configuration_tuple)\n",
      "        config_dict = utils.type_correct_with(config_dict, configuration_tuple)\n",
      "    return config_dict\n",
      "config_dict(configuration_tuple={})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "config_dict = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def filter_fields(d: dict, nt):\n",
      "    res = {}\n",
      "    for k, v in d.items():\n",
      "        if k in nt._fields:\n",
      "            res.update({k: v})\n",
      "    return res\n",
      "filter_fields(d={'bar': '42'}, nt={})\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "res.update({k: v})\n",
      "State:\n",
      "{'bar': '42'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def type_correct_with(cdict, cfg_tuple):\n",
      "    res = {}\n",
      "    for k, v in cdict.items():\n",
      "        typename = getattr(cfg_tuple, k)\n",
      "        res.update({k: type(typename)(v)})\n",
      "    return res\n",
      "type_correct_with(cdict={'bar': '42'}, cfg_tuple={})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "res = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def AddAccessedProperty(self, component, target, args, filename, lineno):\n",
      "    element = FireTraceElement(\n",
      "        component=component,\n",
      "        action=ACCESSED_PROPERTY,\n",
      "        target=target,\n",
      "        args=args,\n",
      "        filename=filename,\n",
      "        lineno=lineno,\n",
      "    )\n",
      "    self.elements.append(element)\n",
      "AddAccessedProperty(self=<clima.fire.trace.FireTrace object at 0x7f29d95dd610>, component=<bound method TestConfigFromWorkingDir.test_configfile.<locals>.Cli.x of <clima.core.Cli object at 0x7f29d94a60a0>>, target='x', args=['x'], filename='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/d3rp+clima/d3rp+clima/tests/test_configfile.py', lineno=34, self.elements=[<clima.fire.trace.FireTraceElement object at 0x7f29d95dd910>, <clima.fire.trace.FireTraceElement object at 0x7f29d94a67c0>], self.name='test', self.separator='-', self.show_help=False, self.show_trace=False, self.verbose=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "element = FireTraceElement(\n",
      "State:\n",
      "{_action='Accessed property', _target='x', args=['x'], _filename='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/d3rp+clima/d3rp+clima/tests/test_configfile.py', _lineno=34, _error=None, _separator=False, _capacity=None}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def data(self):\n",
      "        active_base_url = BASE_URL_NOT_SET_TEXT\n",
      "        if self.__data['settings']['application'].get('base_url'):\n",
      "            active_base_url = self.__data['settings']['application'].get('base_url')\n",
      "        elif os.getenv('BASE_URL'):\n",
      "            active_base_url = os.getenv('BASE_URL')\n",
      "        d = self.__data\n",
      "        d['settings']['application']['active_base_url'] = active_base_url.strip('\" ')\n",
      "        return d\n",
      "data(self=<changedetectionio.store.ChangeDetectionStore object at 0x7faeffddbd60>, self._ChangeDetectionStore__data={'note': 'Hello! If you change this file manually, please be sure to restart your changedetection.io instance!', 'watching': {}, 'settings': {'headers': {}, 'requests': {'extra_proxies': [], 'extra_browsers': [], 'jitter_seconds': 0, 'proxy': None, 'time_between_check': {'weeks': None, 'days': None, 'hours': 3, 'minutes': None, 'seconds': None}, 'timeout': 45, 'workers': 10}, 'application': {'api_access_token_enabled': True, 'base_url': None, 'empty_pages_are_a_change': False, 'extract_title_as_title': False, 'fetch_backend': 'html_requests', 'filter_failure_notification_threshold_attempts': 6, 'global_ignore_text': [], 'global_subtractive_selectors': [], 'ignore_whitespace': True, 'notification_body': '{{watch_url}} had a change.\\n---\\n{{diff}}\\n---\\n', 'notification_format': 'Text', 'notification_title': 'ChangeDetection.io Notification - {{watch_url}}', 'notification_urls': [], 'pager_size': 50, 'password': False, 'render_anchor_tag_content': False, 'schema_version': 0, 'shared_diff_access': False, 'webdriver_delay': None, 'tags': {}}}}, self.datastore_path='./test-datastore', self.generic_definition={'body': None, 'browser_steps': [], 'browser_steps_last_error_step': None, 'check_unique_lines': False, 'check_count': 0, 'date_created': None, 'consecutive_filter_failures': 0, 'extract_text': [], 'extract_title_as_title': False, 'fetch_backend': 'system', 'fetch_time': 0.0, 'processor': 'text_json_diff', 'filter_failure_notification_send': 1, 'filter_text_added': True, 'filter_text_replaced': True, 'filter_text_removed': True, 'has_ldjson_price_data': None, 'track_ldjson_price_data': None, 'headers': {}, 'ignore_text': [], 'in_stock': None, 'in_stock_only': True, 'include_filters': [], 'last_checked': 0, 'last_error': False, 'last_viewed': 0, 'method': 'GET', 'notification_body': None, 'notification_format': 'System default', 'notification_muted': False, 'notification_title': None, 'notification_screenshot': False, 'notification_urls': [], 'paused': False, 'previous_md5': False, 'previous_md5_before_filters': False, 'proxy': None, 'subtractive_selectors': [], 'tag': '', 'tags': [], 'text_should_not_be_present': [], 'time_between_check': {'weeks': None, 'days': None, 'hours': None, 'minutes': None, 'seconds': None}, 'title': None, 'trigger_text': [], 'url': '', 'uuid': 'fbac52a3-b68a-41be-909a-b647415c2c08', 'webdriver_delay': None, 'webdriver_js_execute_code': None, 'default': {}}, self.json_store_path='./test-datastore/url-watches.json', self.needs_write=False, self.start_time=1712199619.5485861, self.stop_thread=False)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "active_base_url = BASE_URL_NOT_SET_TEXT\n",
      "State:\n",
      "'(\"Base URL\" not set - see settings - notifications)'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __setattr__(self, name, value):\n",
      "        if not self.__dict__[AttrDict.IMMUTABLE]:\n",
      "            if name in self.__dict__:\n",
      "                self.__dict__[name] = value\n",
      "            else:\n",
      "                self[name] = value\n",
      "        else:\n",
      "            raise AttributeError(\n",
      "                'Attempted to set \"{}\" to \"{}\", but AttrDict is immutable'.\n",
      "                format(name, value)\n",
      "            )\n",
      "__setattr__(self={}, name='TRAIN', value={})\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "self[name] = value\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def mock_request_get_content(url, headers):\n",
      "    with open(os.path.join(TEST_DATA_DIR, URL_MAP_FILE)) as f:\n",
      "        url_map = json.loads(f.read())\n",
      "    resp_file = url_map[url]\n",
      "    mock = Mock()\n",
      "    with open(resp_file, \"rb\") as f:\n",
      "        mock.content = f.read()\n",
      "    return mock\n",
      "mock_request_get_content(url='https://weibo.cn/album/166564740000001980768563?rl=1', headers={'User_Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36', 'Cookie': ''})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "url_map = json.loads(f.read())\n",
      "State:\n",
      "{'https://weibo.cn/1669879400/profile': 'tests/testdata/a4437630f3bdfa2757bae1595186ac063fe5ec25cf2f98116ece83cb.html', 'https://weibo.cn/1669879400/info': 'tests/testdata/ca5f2a555e8d62f728c66fa90afb2d54d19f8c898e164204a61bdf03.html', 'https://weibo.cn/1669879400/profile?page=1': 'tests/testdata/4957814af5a123b82e974b5537dea736dfb34e48d8835203a45d2e67.html', 'https://weibo.cn/mblog/picAll/J6k49kbTc?rl=1': 'tests/testdata/e97222acd5bc7d8d1bfbd3f352f8cad3e36fdd19e40b69e1c33fb3c3.html', 'https://weibo.cn/mblog/picAll/J5ZcSnCAg?rl=1': 'tests/testdata/63a98849ec82b2c87ec55bca03cbf5988f7eac233a23d86b4fdf5ffd.html', 'https://weibo.cn/1669879400/profile?page=2': 'tests/testdata/2f62165fa3ca1e85e0d398d385c377a068b76eb95765f7020ffffd3e.html', 'https://weibo.cn/1669879400/profile?page=3': 'tests/testdata/d486235d4a17dd0accb0f2cc77b3648abfa03580b9e0cdb61f1e618f.html', 'https://weibo.cn/mblog/picAll/J3xfm61AZ?rl=1': 'tests/testdata/76233b3f90394581aac6f19cfa5d674a610e8b442b1f83de7673ab49.html', 'https://weibo.cn/comment/J5cVGuUNq': 'tests/testdata/4d5ed0a3ebd0303cb45edd544dbc0ab5e86d43e103405f0c60515884.html', 'https://weibo.cn/1980768563/photo?tf=6_008': 'tests/testdata/e4d541ecb02253c14abc1d52605fc00d91279df9ac4c1465c85b91b3.html', 'https://weibo.cn/album/166564740000001980768563?rl=1': 'tests/testdata/b541fd1751117498b6d6f40d3321686ddf871651237c4ac854a5c3eb.html'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def mock_request_get_content(url, headers):\n",
      "    with open(os.path.join(TEST_DATA_DIR, URL_MAP_FILE)) as f:\n",
      "        url_map = json.loads(f.read())\n",
      "    resp_file = url_map[url]\n",
      "    mock = Mock()\n",
      "    with open(resp_file, \"rb\") as f:\n",
      "        mock.content = f.read()\n",
      "    return mock\n",
      "mock_request_get_content(url='https://weibo.cn/album/166564740000001980768563?rl=1', headers={'User_Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36', 'Cookie': ''})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "resp_file = url_map[url]\n",
      "State:\n",
      "'tests/testdata/b541fd1751117498b6d6f40d3321686ddf871651237c4ac854a5c3eb.html'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def mock_request_get_content(url, headers):\n",
      "    with open(os.path.join(TEST_DATA_DIR, URL_MAP_FILE)) as f:\n",
      "        url_map = json.loads(f.read())\n",
      "    resp_file = url_map[url]\n",
      "    mock = Mock()\n",
      "    with open(resp_file, \"rb\") as f:\n",
      "        mock.content = f.read()\n",
      "    return mock\n",
      "mock_request_get_content(url='https://weibo.cn/album/166564740000001980768563?rl=1', headers={'User_Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36', 'Cookie': ''})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "mock = Mock()\n",
      "State:\n",
      "<Mock id='140426606241008'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_long_weibo(self):\n",
      "        try:\n",
      "            for i in range(5):\n",
      "                self.selector = handle_html(self.cookie, self.url)\n",
      "                if self.selector is not None:\n",
      "                    info = self.selector.xpath(\"//div[@class='c']\")[1]\n",
      "                    wb_content = handle_garbled(info)\n",
      "                    wb_time = info.xpath(\"//span[@class='ct']/text()\")[0]\n",
      "                    weibo_content = wb_content[wb_content.find(':') +\n",
      "                                               1:wb_content.rfind(wb_time)]\n",
      "                    if weibo_content is not None:\n",
      "                        return weibo_content\n",
      "                sleep(random.randint(6, 10))\n",
      "        except Exception:\n",
      "            logger.exception(u'')\n",
      "get_long_weibo(self=<weibo_spider.parser.comment_parser.CommentParser object at 0x7fb79dfb6910>, self.cookie='', self.selector=<Element html at 0x7fb79df53f80>, self.url='https://weibo.cn/comment/J5cVGuUNq')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "for i in range(5):\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def handle_garbled(info):\n",
      "    try:\n",
      "        info = (info.xpath('string(.)').replace(u'\\u200b', '').encode(\n",
      "            sys.stdout.encoding, 'ignore').decode(sys.stdout.encoding))\n",
      "        return info\n",
      "    except Exception as e:\n",
      "        logger.exception(e)\n",
      "        return u''\n",
      "handle_garbled(info=<Element div at 0x7fb79dfb7a80>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "info = (info.xpath('string(.)').replace(u'\\u200b', '').encode(\n",
      "State:\n",
      "'    Dear-    \\xa0\\xa0@WCS\\xa0:@Dear- @ We Stand for Wildlife.  68309                    \\xa0[1000000]    \\xa0[38688]        :                    \\xa0    0605 11:11        \\xa0        \\xa0\\xa0\\xa0    '\n",
      "==================================================\n",
      "Clean Code:\n",
      "def extract_user_info(self):\n",
      "        try:\n",
      "            user = User()\n",
      "            nickname = self.selector.xpath('//title/text()')[0]\n",
      "            nickname = nickname[:-3]\n",
      "            if nickname == u' - ' or nickname == u'':\n",
      "                logger.warning(u'cookie,README')\n",
      "                sys.exit()\n",
      "            user.nickname = nickname\n",
      "            basic_info = self.selector.xpath(\"//div[@class='c'][3]/text()\")\n",
      "            zh_list = [u'', u'', u'', u'', u'', u'']\n",
      "            en_list = [\n",
      "                'gender', 'location', 'birthday', 'description',\n",
      "                'verified_reason', 'talent'\n",
      "            ]\n",
      "            for i in basic_info:\n",
      "                if i.split(':', 1)[0] in zh_list:\n",
      "                    setattr(user, en_list[zh_list.index(i.split(':', 1)[0])],\n",
      "                            i.split(':', 1)[1].replace('\\u3000', ''))\n",
      "            experienced = self.selector.xpath(\"//div[@class='tip'][2]/text()\") \n",
      "            if experienced and experienced[0] == u'':\n",
      "                user.education = self.selector.xpath(\n",
      "                    \"//div[@class='c'][4]/text()\")[0][1:].replace(\n",
      "                        u'\\xa0', u' ')\n",
      "                if self.selector.xpath(\n",
      "                        \"//div[@class='tip'][3]/text()\")[0] == u'':\n",
      "                    user.work = self.selector.xpath(\n",
      "                        \"//div[@class='c'][5]/text()\")[0][1:].replace(\n",
      "                            u'\\xa0', u' ')\n",
      "            elif experienced and experienced[0] == u'':\n",
      "                user.work = self.selector.xpath(\n",
      "                    \"//div[@class='c'][4]/text()\")[0][1:].replace(\n",
      "                        u'\\xa0', u' ')\n",
      "            return user\n",
      "        except Exception as e:\n",
      "            logger.exception(e)\n",
      "extract_user_info(self=<weibo_spider.parser.info_parser.InfoParser object at 0x7fb79dfb6040>, self.cookie='', self.selector=<Element html at 0x7fb79df165c0>, self.url='https://weibo.cn/1669879400/info')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "user = User()\n",
      "State:\n",
      "{id='', nickname='', gender='', location='', birthday='', description='', verified_reason='', talent='', education='', work='', weibo_num=0, following=0, followers=0}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def extract_user_info(self):\n",
      "        try:\n",
      "            user = User()\n",
      "            nickname = self.selector.xpath('//title/text()')[0]\n",
      "            nickname = nickname[:-3]\n",
      "            if nickname == u' - ' or nickname == u'':\n",
      "                logger.warning(u'cookie,README')\n",
      "                sys.exit()\n",
      "            user.nickname = nickname\n",
      "            basic_info = self.selector.xpath(\"//div[@class='c'][3]/text()\")\n",
      "            zh_list = [u'', u'', u'', u'', u'', u'']\n",
      "            en_list = [\n",
      "                'gender', 'location', 'birthday', 'description',\n",
      "                'verified_reason', 'talent'\n",
      "            ]\n",
      "            for i in basic_info:\n",
      "                if i.split(':', 1)[0] in zh_list:\n",
      "                    setattr(user, en_list[zh_list.index(i.split(':', 1)[0])],\n",
      "                            i.split(':', 1)[1].replace('\\u3000', ''))\n",
      "            experienced = self.selector.xpath(\"//div[@class='tip'][2]/text()\") \n",
      "            if experienced and experienced[0] == u'':\n",
      "                user.education = self.selector.xpath(\n",
      "                    \"//div[@class='c'][4]/text()\")[0][1:].replace(\n",
      "                        u'\\xa0', u' ')\n",
      "                if self.selector.xpath(\n",
      "                        \"//div[@class='tip'][3]/text()\")[0] == u'':\n",
      "                    user.work = self.selector.xpath(\n",
      "                        \"//div[@class='c'][5]/text()\")[0][1:].replace(\n",
      "                            u'\\xa0', u' ')\n",
      "            elif experienced and experienced[0] == u'':\n",
      "                user.work = self.selector.xpath(\n",
      "                    \"//div[@class='c'][4]/text()\")[0][1:].replace(\n",
      "                        u'\\xa0', u' ')\n",
      "            return user\n",
      "        except Exception as e:\n",
      "            logger.exception(e)\n",
      "extract_user_info(self=<weibo_spider.parser.info_parser.InfoParser object at 0x7fb79dfb6040>, self.cookie='', self.selector=<Element html at 0x7fb79df165c0>, self.url='https://weibo.cn/1669879400/info')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "nickname = self.selector.xpath('//title/text()')[0]\n",
      "State:\n",
      "'Dear-'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_one_weibo(self, info):\n",
      "        try:\n",
      "            weibo = Weibo()\n",
      "            is_original = self.is_original(info)\n",
      "            weibo.original = is_original\n",
      "            if (not self.filter) or is_original:\n",
      "                weibo.id = info.xpath('@id')[0][2:]\n",
      "                weibo.content = self.get_weibo_content(info,\n",
      "                                                       is_original)\n",
      "                weibo.article_url = self.get_article_url(info)\n",
      "                picture_urls = self.get_picture_urls(info, is_original)\n",
      "                weibo.original_pictures = picture_urls[\n",
      "                    'original_pictures']\n",
      "                if not self.filter:\n",
      "                    weibo.retweet_pictures = picture_urls[\n",
      "                        'retweet_pictures']\n",
      "                weibo.video_url = self.get_video_url(info)\n",
      "                weibo.publish_place = self.get_publish_place(info)\n",
      "                weibo.publish_time = self.get_publish_time(info)\n",
      "                weibo.publish_tool = self.get_publish_tool(info)\n",
      "                footer = self.get_weibo_footer(info)\n",
      "                weibo.up_num = footer['up_num']\n",
      "                weibo.retweet_num = footer['retweet_num']\n",
      "                weibo.comment_num = footer['comment_num']\n",
      "            else:\n",
      "                weibo = None\n",
      "                logger.info(u'')\n",
      "            return weibo\n",
      "        except Exception as e:\n",
      "            logger.exception(e)\n",
      "get_one_weibo(self=<weibo_spider.parser.page_parser.PageParser object at 0x7fb79dfb6a60>, info=<Element div at 0x7fb79def8780>, self.cookie='', self.end_date='now', self.filter=True, self.page=2, self.selector=<Element html at 0x7fb79def1ac0>, self.since_date='2020-06-01', self.to_continue=True, self.url='https://weibo.cn/1669879400/profile?page=2', self.user_uri='1669879400')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "weibo = Weibo()\n",
      "State:\n",
      "{id='', user_id='', content='', article_url='', original_pictures=[], retweet_pictures=None, original=None, video_url='', publish_place='', publish_time='', publish_tool='', up_num=0, retweet_num=0, comment_num=0}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_one_weibo(self, info):\n",
      "        try:\n",
      "            weibo = Weibo()\n",
      "            is_original = self.is_original(info)\n",
      "            weibo.original = is_original\n",
      "            if (not self.filter) or is_original:\n",
      "                weibo.id = info.xpath('@id')[0][2:]\n",
      "                weibo.content = self.get_weibo_content(info,\n",
      "                                                       is_original)\n",
      "                weibo.article_url = self.get_article_url(info)\n",
      "                picture_urls = self.get_picture_urls(info, is_original)\n",
      "                weibo.original_pictures = picture_urls[\n",
      "                    'original_pictures']\n",
      "                if not self.filter:\n",
      "                    weibo.retweet_pictures = picture_urls[\n",
      "                        'retweet_pictures']\n",
      "                weibo.video_url = self.get_video_url(info)\n",
      "                weibo.publish_place = self.get_publish_place(info)\n",
      "                weibo.publish_time = self.get_publish_time(info)\n",
      "                weibo.publish_tool = self.get_publish_tool(info)\n",
      "                footer = self.get_weibo_footer(info)\n",
      "                weibo.up_num = footer['up_num']\n",
      "                weibo.retweet_num = footer['retweet_num']\n",
      "                weibo.comment_num = footer['comment_num']\n",
      "            else:\n",
      "                weibo = None\n",
      "                logger.info(u'')\n",
      "            return weibo\n",
      "        except Exception as e:\n",
      "            logger.exception(e)\n",
      "get_one_weibo(self=<weibo_spider.parser.page_parser.PageParser object at 0x7fb79dfb6a60>, info=<Element div at 0x7fb79def8780>, self.cookie='', self.end_date='now', self.filter=True, self.page=2, self.selector=<Element html at 0x7fb79def1ac0>, self.since_date='2020-06-01', self.to_continue=True, self.url='https://weibo.cn/1669879400/profile?page=2', self.user_uri='1669879400')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "is_original = self.is_original(info)\n",
      "State:\n",
      "True\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_one_weibo(self, info):\n",
      "        try:\n",
      "            weibo = Weibo()\n",
      "            is_original = self.is_original(info)\n",
      "            weibo.original = is_original\n",
      "            if (not self.filter) or is_original:\n",
      "                weibo.id = info.xpath('@id')[0][2:]\n",
      "                weibo.content = self.get_weibo_content(info,\n",
      "                                                       is_original)\n",
      "                weibo.article_url = self.get_article_url(info)\n",
      "                picture_urls = self.get_picture_urls(info, is_original)\n",
      "                weibo.original_pictures = picture_urls[\n",
      "                    'original_pictures']\n",
      "                if not self.filter:\n",
      "                    weibo.retweet_pictures = picture_urls[\n",
      "                        'retweet_pictures']\n",
      "                weibo.video_url = self.get_video_url(info)\n",
      "                weibo.publish_place = self.get_publish_place(info)\n",
      "                weibo.publish_time = self.get_publish_time(info)\n",
      "                weibo.publish_tool = self.get_publish_tool(info)\n",
      "                footer = self.get_weibo_footer(info)\n",
      "                weibo.up_num = footer['up_num']\n",
      "                weibo.retweet_num = footer['retweet_num']\n",
      "                weibo.comment_num = footer['comment_num']\n",
      "            else:\n",
      "                weibo = None\n",
      "                logger.info(u'')\n",
      "            return weibo\n",
      "        except Exception as e:\n",
      "            logger.exception(e)\n",
      "get_one_weibo(self=<weibo_spider.parser.page_parser.PageParser object at 0x7fb79dfb6a60>, info=<Element div at 0x7fb79def8780>, self.cookie='', self.end_date='now', self.filter=True, self.page=2, self.selector=<Element html at 0x7fb79def1ac0>, self.since_date='2020-06-01', self.to_continue=True, self.url='https://weibo.cn/1669879400/profile?page=2', self.user_uri='1669879400')\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "weibo.publish_tool = self.get_publish_tool(info)  # \n",
      "State:\n",
      "{id='J4PGk4yMw', user_id='', content=' \\xa0', article_url='', original_pictures='', retweet_pictures=None, original=True, video_url='', publish_place='', publish_time='2020-06-03 00:00', publish_tool='', up_num=0, retweet_num=0, comment_num=0}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_original_weibo(self, info, weibo_id):\n",
      "        try:\n",
      "            weibo_content = handle_garbled(info)\n",
      "            weibo_content = weibo_content[:weibo_content.rfind(u'')]\n",
      "            a_text = info.xpath('div//a/text()')\n",
      "            if u'' in a_text:\n",
      "                wb_content = CommentParser(self.cookie,\n",
      "                                           weibo_id).get_long_weibo()\n",
      "                if wb_content:\n",
      "                    weibo_content = wb_content\n",
      "            return weibo_content\n",
      "        except Exception as e:\n",
      "            logger.exception(e)\n",
      "get_original_weibo(self=<weibo_spider.parser.page_parser.PageParser object at 0x7fb79dfb6a60>, info=<Element div at 0x7fb79def8780>, weibo_id='J4PGk4yMw', self.cookie='', self.end_date='now', self.filter=True, self.page=2, self.selector=<Element html at 0x7fb79def1ac0>, self.since_date='2020-06-01', self.to_continue=True, self.url='https://weibo.cn/1669879400/profile?page=2', self.user_uri='1669879400')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "weibo_content = handle_garbled(info)\n",
      "State:\n",
      "' \\xa0[1499675]\\xa0[1000000]\\xa0[1000000]\\xa0\\xa02020-06-03 00:00\\xa0'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_original_weibo(self, info, weibo_id):\n",
      "        try:\n",
      "            weibo_content = handle_garbled(info)\n",
      "            weibo_content = weibo_content[:weibo_content.rfind(u'')]\n",
      "            a_text = info.xpath('div//a/text()')\n",
      "            if u'' in a_text:\n",
      "                wb_content = CommentParser(self.cookie,\n",
      "                                           weibo_id).get_long_weibo()\n",
      "                if wb_content:\n",
      "                    weibo_content = wb_content\n",
      "            return weibo_content\n",
      "        except Exception as e:\n",
      "            logger.exception(e)\n",
      "get_original_weibo(self=<weibo_spider.parser.page_parser.PageParser object at 0x7fb79dfb6a60>, info=<Element div at 0x7fb79def8780>, weibo_id='J4PGk4yMw', self.cookie='', self.end_date='now', self.filter=True, self.page=2, self.selector=<Element html at 0x7fb79def1ac0>, self.since_date='2020-06-01', self.to_continue=True, self.url='https://weibo.cn/1669879400/profile?page=2', self.user_uri='1669879400')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "weibo_content = weibo_content[:weibo_content.rfind(u'')]\n",
      "State:\n",
      "' \\xa0'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_picture_urls(self, info, is_original):\n",
      "        try:\n",
      "            weibo_id = info.xpath('@id')[0][2:]\n",
      "            picture_urls = {}\n",
      "            if is_original:\n",
      "                original_pictures = self.extract_picture_urls(info, weibo_id)\n",
      "                picture_urls['original_pictures'] = original_pictures\n",
      "                if not self.filter:\n",
      "                    picture_urls['retweet_pictures'] = u''\n",
      "            else:\n",
      "                retweet_url = info.xpath(\"div/a[@class='cc']/@href\")[0]\n",
      "                retweet_id = retweet_url.split('/')[-1].split('?')[0]\n",
      "                retweet_pictures = self.extract_picture_urls(info, retweet_id)\n",
      "                picture_urls['retweet_pictures'] = retweet_pictures\n",
      "                a_list = info.xpath('div[last()]/a/@href')\n",
      "                original_picture = u''\n",
      "                for a in a_list:\n",
      "                    if a.endswith(('.gif', '.jpeg', '.jpg', '.png')):\n",
      "                        original_picture = a\n",
      "                        break\n",
      "                picture_urls['original_pictures'] = original_picture\n",
      "            return picture_urls\n",
      "        except Exception as e:\n",
      "            logger.exception(e)\n",
      "get_picture_urls(self=<weibo_spider.parser.page_parser.PageParser object at 0x7fb79dfb6a60>, info=<Element div at 0x7fb79def8780>, is_original=True, self.cookie='', self.end_date='now', self.filter=True, self.page=2, self.selector=<Element html at 0x7fb79def1ac0>, self.since_date='2020-06-01', self.to_continue=True, self.url='https://weibo.cn/1669879400/profile?page=2', self.user_uri='1669879400')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "weibo_id = info.xpath('@id')[0][2:]\n",
      "State:\n",
      "'J4PGk4yMw'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def extract_picture_urls(self, info, weibo_id):\n",
      "        try:\n",
      "            a_list = info.xpath('div/a/@href')\n",
      "            first_pic = 'https://weibo.cn/mblog/pic/' + weibo_id\n",
      "            all_pic = 'https://weibo.cn/mblog/picAll/' + weibo_id\n",
      "            picture_urls = u''\n",
      "            if first_pic in ''.join(a_list):\n",
      "                if all_pic in ''.join(a_list):\n",
      "                    preview_picture_list = MblogPicAllParser(\n",
      "                        self.cookie, weibo_id).extract_preview_picture_list()\n",
      "                    picture_list = [\n",
      "                        p.replace('/thumb180/', '/large/')\n",
      "                        for p in preview_picture_list\n",
      "                    ]\n",
      "                    picture_urls = ','.join(picture_list)\n",
      "                else:\n",
      "                    if info.xpath('.//img/@src'):\n",
      "                        for link in info.xpath('div/a'):\n",
      "                            if len(link.xpath('@href')) > 0:\n",
      "                                if first_pic in link.xpath('@href')[0]:\n",
      "                                    if len(link.xpath('img/@src')) > 0:\n",
      "                                        preview_picture = link.xpath(\n",
      "                                            'img/@src')[0]\n",
      "                                        picture_urls = preview_picture.replace(\n",
      "                                            '/wap180/', '/large/')\n",
      "                                        break\n",
      "                    else:\n",
      "                        logger.warning(\n",
      "                            u'\"\"'\n",
      "                            u'\"https://weibo.cn/account/customize/pic\"\"\"'\n",
      "                        )\n",
      "                        sys.exit()\n",
      "            return picture_urls\n",
      "        except Exception as e:\n",
      "            logger.exception(e)\n",
      "            return u''\n",
      "extract_picture_urls(self=<weibo_spider.parser.page_parser.PageParser object at 0x7fb79dfb6a60>, info=<Element div at 0x7fb79def8780>, weibo_id='J4PGk4yMw', self.cookie='', self.end_date='now', self.filter=True, self.page=2, self.selector=<Element html at 0x7fb79def1ac0>, self.since_date='2020-06-01', self.to_continue=True, self.url='https://weibo.cn/1669879400/profile?page=2', self.user_uri='1669879400')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "a_list = info.xpath('div/a/@href')\n",
      "State:\n",
      "['https://weibo.cn/attitude/J4PGk4yMw/add?uid=3113276555&rl=0&st=46d484', 'https://weibo.cn/repost/J4PGk4yMw?uid=1669879400&rl=0', 'https://weibo.cn/comment/J4PGk4yMw?uid=1669879400&rl=0#cmtfrm', 'https://weibo.cn/fav/addFav/J4PGk4yMw?rl=0&st=46d484']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def extract_picture_urls(self, info, weibo_id):\n",
      "        try:\n",
      "            a_list = info.xpath('div/a/@href')\n",
      "            first_pic = 'https://weibo.cn/mblog/pic/' + weibo_id\n",
      "            all_pic = 'https://weibo.cn/mblog/picAll/' + weibo_id\n",
      "            picture_urls = u''\n",
      "            if first_pic in ''.join(a_list):\n",
      "                if all_pic in ''.join(a_list):\n",
      "                    preview_picture_list = MblogPicAllParser(\n",
      "                        self.cookie, weibo_id).extract_preview_picture_list()\n",
      "                    picture_list = [\n",
      "                        p.replace('/thumb180/', '/large/')\n",
      "                        for p in preview_picture_list\n",
      "                    ]\n",
      "                    picture_urls = ','.join(picture_list)\n",
      "                else:\n",
      "                    if info.xpath('.//img/@src'):\n",
      "                        for link in info.xpath('div/a'):\n",
      "                            if len(link.xpath('@href')) > 0:\n",
      "                                if first_pic in link.xpath('@href')[0]:\n",
      "                                    if len(link.xpath('img/@src')) > 0:\n",
      "                                        preview_picture = link.xpath(\n",
      "                                            'img/@src')[0]\n",
      "                                        picture_urls = preview_picture.replace(\n",
      "                                            '/wap180/', '/large/')\n",
      "                                        break\n",
      "                    else:\n",
      "                        logger.warning(\n",
      "                            u'\"\"'\n",
      "                            u'\"https://weibo.cn/account/customize/pic\"\"\"'\n",
      "                        )\n",
      "                        sys.exit()\n",
      "            return picture_urls\n",
      "        except Exception as e:\n",
      "            logger.exception(e)\n",
      "            return u''\n",
      "extract_picture_urls(self=<weibo_spider.parser.page_parser.PageParser object at 0x7fb79dfb6a60>, info=<Element div at 0x7fb79def8780>, weibo_id='J4PGk4yMw', self.cookie='', self.end_date='now', self.filter=True, self.page=2, self.selector=<Element html at 0x7fb79def1ac0>, self.since_date='2020-06-01', self.to_continue=True, self.url='https://weibo.cn/1669879400/profile?page=2', self.user_uri='1669879400')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "first_pic = 'https://weibo.cn/mblog/pic/' + weibo_id\n",
      "State:\n",
      "'https://weibo.cn/mblog/pic/J4PGk4yMw'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def extract_picture_urls(self, info, weibo_id):\n",
      "        try:\n",
      "            a_list = info.xpath('div/a/@href')\n",
      "            first_pic = 'https://weibo.cn/mblog/pic/' + weibo_id\n",
      "            all_pic = 'https://weibo.cn/mblog/picAll/' + weibo_id\n",
      "            picture_urls = u''\n",
      "            if first_pic in ''.join(a_list):\n",
      "                if all_pic in ''.join(a_list):\n",
      "                    preview_picture_list = MblogPicAllParser(\n",
      "                        self.cookie, weibo_id).extract_preview_picture_list()\n",
      "                    picture_list = [\n",
      "                        p.replace('/thumb180/', '/large/')\n",
      "                        for p in preview_picture_list\n",
      "                    ]\n",
      "                    picture_urls = ','.join(picture_list)\n",
      "                else:\n",
      "                    if info.xpath('.//img/@src'):\n",
      "                        for link in info.xpath('div/a'):\n",
      "                            if len(link.xpath('@href')) > 0:\n",
      "                                if first_pic in link.xpath('@href')[0]:\n",
      "                                    if len(link.xpath('img/@src')) > 0:\n",
      "                                        preview_picture = link.xpath(\n",
      "                                            'img/@src')[0]\n",
      "                                        picture_urls = preview_picture.replace(\n",
      "                                            '/wap180/', '/large/')\n",
      "                                        break\n",
      "                    else:\n",
      "                        logger.warning(\n",
      "                            u'\"\"'\n",
      "                            u'\"https://weibo.cn/account/customize/pic\"\"\"'\n",
      "                        )\n",
      "                        sys.exit()\n",
      "            return picture_urls\n",
      "        except Exception as e:\n",
      "            logger.exception(e)\n",
      "            return u''\n",
      "extract_picture_urls(self=<weibo_spider.parser.page_parser.PageParser object at 0x7fb79dfb6a60>, info=<Element div at 0x7fb79def8780>, weibo_id='J4PGk4yMw', self.cookie='', self.end_date='now', self.filter=True, self.page=2, self.selector=<Element html at 0x7fb79def1ac0>, self.since_date='2020-06-01', self.to_continue=True, self.url='https://weibo.cn/1669879400/profile?page=2', self.user_uri='1669879400')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "all_pic = 'https://weibo.cn/mblog/picAll/' + weibo_id\n",
      "State:\n",
      "'https://weibo.cn/mblog/picAll/J4PGk4yMw'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_video_url(self, info):\n",
      "        video_url = u''\n",
      "        weibo_id = info.xpath('@id')[0][2:]\n",
      "        try:\n",
      "            video_page_url = ''\n",
      "            a_text = info.xpath('./div[1]//a/text()')\n",
      "            if u'' in a_text:\n",
      "                video_page_url = CommentParser(self.cookie,\n",
      "                                               weibo_id).get_video_page_url()\n",
      "            else:\n",
      "                a_list = info.xpath('./div[1]//a')\n",
      "                for a in a_list:\n",
      "                    if 'm.weibo.cn/s/video/show?object_id=' in a.xpath(\n",
      "                            '@href')[0]:\n",
      "                        video_page_url = a.xpath('@href')[0]\n",
      "                        break\n",
      "            if video_page_url != '':\n",
      "                video_url = to_video_download_url(self.cookie, video_page_url)\n",
      "        except Exception as e:\n",
      "            logger.exception(e)\n",
      "        return video_url\n",
      "get_video_url(self=<weibo_spider.parser.page_parser.PageParser object at 0x7fb79dfb6a60>, info=<Element div at 0x7fb79def8780>, self.cookie='', self.end_date='now', self.filter=True, self.page=2, self.selector=<Element html at 0x7fb79def1ac0>, self.since_date='2020-06-01', self.to_continue=True, self.url='https://weibo.cn/1669879400/profile?page=2', self.user_uri='1669879400')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "video_url = u''\n",
      "State:\n",
      "''\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_video_url(self, info):\n",
      "        video_url = u''\n",
      "        weibo_id = info.xpath('@id')[0][2:]\n",
      "        try:\n",
      "            video_page_url = ''\n",
      "            a_text = info.xpath('./div[1]//a/text()')\n",
      "            if u'' in a_text:\n",
      "                video_page_url = CommentParser(self.cookie,\n",
      "                                               weibo_id).get_video_page_url()\n",
      "            else:\n",
      "                a_list = info.xpath('./div[1]//a')\n",
      "                for a in a_list:\n",
      "                    if 'm.weibo.cn/s/video/show?object_id=' in a.xpath(\n",
      "                            '@href')[0]:\n",
      "                        video_page_url = a.xpath('@href')[0]\n",
      "                        break\n",
      "            if video_page_url != '':\n",
      "                video_url = to_video_download_url(self.cookie, video_page_url)\n",
      "        except Exception as e:\n",
      "            logger.exception(e)\n",
      "        return video_url\n",
      "get_video_url(self=<weibo_spider.parser.page_parser.PageParser object at 0x7fb79dfb6a60>, info=<Element div at 0x7fb79def8780>, self.cookie='', self.end_date='now', self.filter=True, self.page=2, self.selector=<Element html at 0x7fb79def1ac0>, self.since_date='2020-06-01', self.to_continue=True, self.url='https://weibo.cn/1669879400/profile?page=2', self.user_uri='1669879400')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "weibo_id = info.xpath('@id')[0][2:]\n",
      "State:\n",
      "'J4PGk4yMw'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_publish_time(self, info):\n",
      "        try:\n",
      "            str_time = info.xpath(\"div/span[@class='ct']\")\n",
      "            str_time = handle_garbled(str_time[0])\n",
      "            publish_time = str_time.split(u'')[0]\n",
      "            if u'' in publish_time:\n",
      "                publish_time = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
      "            elif u'' in publish_time:\n",
      "                minute = publish_time[:publish_time.find(u'')]\n",
      "                minute = timedelta(minutes=int(minute))\n",
      "                publish_time = (datetime.now() -\n",
      "                                minute).strftime('%Y-%m-%d %H:%M')\n",
      "            elif u'' in publish_time:\n",
      "                today = datetime.now().strftime('%Y-%m-%d')\n",
      "                time = publish_time[3:]\n",
      "                publish_time = today + ' ' + time\n",
      "                if len(publish_time) > 16:\n",
      "                    publish_time = publish_time[:16]\n",
      "            elif u'' in publish_time:\n",
      "                year = datetime.now().strftime('%Y')\n",
      "                month = publish_time[0:2]\n",
      "                day = publish_time[3:5]\n",
      "                time = publish_time[7:12]\n",
      "                publish_time = year + '-' + month + '-' + day + ' ' + time\n",
      "            else:\n",
      "                publish_time = publish_time[:16]\n",
      "            return publish_time\n",
      "        except Exception as e:\n",
      "            logger.exception(e)\n",
      "get_publish_time(self=<weibo_spider.parser.page_parser.PageParser object at 0x7fb79dfb6a60>, info=<Element div at 0x7fb79def8780>, self.cookie='', self.end_date='now', self.filter=True, self.page=2, self.selector=<Element html at 0x7fb79def1ac0>, self.since_date='2020-06-01', self.to_continue=True, self.url='https://weibo.cn/1669879400/profile?page=2', self.user_uri='1669879400')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "str_time = handle_garbled(str_time[0])\n",
      "State:\n",
      "'2020-06-03 00:00\\xa0'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_weibo_footer(self, info):\n",
      "        try:\n",
      "            footer = {}\n",
      "            pattern = r'\\d+'\n",
      "            str_footer = info.xpath('div')[-1]\n",
      "            str_footer = handle_garbled(str_footer)\n",
      "            str_footer = str_footer[str_footer.rfind(u''):]\n",
      "            weibo_footer = re.findall(pattern, str_footer, re.M)\n",
      "            up_num = int(weibo_footer[0])\n",
      "            footer['up_num'] = up_num\n",
      "            retweet_num = int(weibo_footer[1])\n",
      "            footer['retweet_num'] = retweet_num\n",
      "            comment_num = int(weibo_footer[2])\n",
      "            footer['comment_num'] = comment_num\n",
      "            return footer\n",
      "        except Exception as e:\n",
      "            logger.exception(e)\n",
      "get_weibo_footer(self=<weibo_spider.parser.page_parser.PageParser object at 0x7fb79dfb6a60>, info=<Element div at 0x7fb79def8780>, self.cookie='', self.end_date='now', self.filter=True, self.page=2, self.selector=<Element html at 0x7fb79def1ac0>, self.since_date='2020-06-01', self.to_continue=True, self.url='https://weibo.cn/1669879400/profile?page=2', self.user_uri='1669879400')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "footer = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_weibo_footer(self, info):\n",
      "        try:\n",
      "            footer = {}\n",
      "            pattern = r'\\d+'\n",
      "            str_footer = info.xpath('div')[-1]\n",
      "            str_footer = handle_garbled(str_footer)\n",
      "            str_footer = str_footer[str_footer.rfind(u''):]\n",
      "            weibo_footer = re.findall(pattern, str_footer, re.M)\n",
      "            up_num = int(weibo_footer[0])\n",
      "            footer['up_num'] = up_num\n",
      "            retweet_num = int(weibo_footer[1])\n",
      "            footer['retweet_num'] = retweet_num\n",
      "            comment_num = int(weibo_footer[2])\n",
      "            footer['comment_num'] = comment_num\n",
      "            return footer\n",
      "        except Exception as e:\n",
      "            logger.exception(e)\n",
      "get_weibo_footer(self=<weibo_spider.parser.page_parser.PageParser object at 0x7fb79dfb6a60>, info=<Element div at 0x7fb79def8780>, self.cookie='', self.end_date='now', self.filter=True, self.page=2, self.selector=<Element html at 0x7fb79def1ac0>, self.since_date='2020-06-01', self.to_continue=True, self.url='https://weibo.cn/1669879400/profile?page=2', self.user_uri='1669879400')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "pattern = r'\\d+'\n",
      "State:\n",
      "'\\\\d+'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_weibo_footer(self, info):\n",
      "        try:\n",
      "            footer = {}\n",
      "            pattern = r'\\d+'\n",
      "            str_footer = info.xpath('div')[-1]\n",
      "            str_footer = handle_garbled(str_footer)\n",
      "            str_footer = str_footer[str_footer.rfind(u''):]\n",
      "            weibo_footer = re.findall(pattern, str_footer, re.M)\n",
      "            up_num = int(weibo_footer[0])\n",
      "            footer['up_num'] = up_num\n",
      "            retweet_num = int(weibo_footer[1])\n",
      "            footer['retweet_num'] = retweet_num\n",
      "            comment_num = int(weibo_footer[2])\n",
      "            footer['comment_num'] = comment_num\n",
      "            return footer\n",
      "        except Exception as e:\n",
      "            logger.exception(e)\n",
      "get_weibo_footer(self=<weibo_spider.parser.page_parser.PageParser object at 0x7fb79dfb6a60>, info=<Element div at 0x7fb79def8780>, self.cookie='', self.end_date='now', self.filter=True, self.page=2, self.selector=<Element html at 0x7fb79def1ac0>, self.since_date='2020-06-01', self.to_continue=True, self.url='https://weibo.cn/1669879400/profile?page=2', self.user_uri='1669879400')\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "up_num = int(weibo_footer[0])\n",
      "State:\n",
      "1499675\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __str__(self):\n",
      "        result = self.content + '\\n'\n",
      "        result += u'%s\\n' % self.publish_place\n",
      "        result += u'%s\\n' % self.publish_time\n",
      "        result += u'%s\\n' % self.publish_tool\n",
      "        result += u'%d\\n' % self.up_num\n",
      "        result += u'%d\\n' % self.retweet_num\n",
      "        result += u'%d\\n' % self.comment_num\n",
      "        result += u'urlhttps://weibo.cn/comment/%s\\n' % self.id\n",
      "        return result\n",
      "__str__(self=<weibo_spider.weibo.Weibo object at 0x7fb79dfb6130>, self.article_url='', self.comment_num=1000000, self.content=' \\xa0', self.id='J4PGk4yMw', self.original=True, self.original_pictures='', self.publish_place='', self.publish_time='2020-06-03 00:00', self.publish_tool='', self.retweet_num=1000000, self.retweet_pictures=None, self.up_num=1499675, self.user_id='', self.video_url='')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "result = self.content + '\\n'\n",
      "State:\n",
      "' \\xa0\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __str__(self):\n",
      "        result = self.content + '\\n'\n",
      "        result += u'%s\\n' % self.publish_place\n",
      "        result += u'%s\\n' % self.publish_time\n",
      "        result += u'%s\\n' % self.publish_tool\n",
      "        result += u'%d\\n' % self.up_num\n",
      "        result += u'%d\\n' % self.retweet_num\n",
      "        result += u'%d\\n' % self.comment_num\n",
      "        result += u'urlhttps://weibo.cn/comment/%s\\n' % self.id\n",
      "        return result\n",
      "__str__(self=<weibo_spider.weibo.Weibo object at 0x7fb79dfb6130>, self.article_url='', self.comment_num=1000000, self.content=' \\xa0', self.id='J4PGk4yMw', self.original=True, self.original_pictures='', self.publish_place='', self.publish_time='2020-06-03 00:00', self.publish_tool='', self.retweet_num=1000000, self.retweet_pictures=None, self.up_num=1499675, self.user_id='', self.video_url='')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "result += u'%s\\n' % self.publish_place\n",
      "State:\n",
      "' \\xa0\\n\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __str__(self):\n",
      "        result = self.content + '\\n'\n",
      "        result += u'%s\\n' % self.publish_place\n",
      "        result += u'%s\\n' % self.publish_time\n",
      "        result += u'%s\\n' % self.publish_tool\n",
      "        result += u'%d\\n' % self.up_num\n",
      "        result += u'%d\\n' % self.retweet_num\n",
      "        result += u'%d\\n' % self.comment_num\n",
      "        result += u'urlhttps://weibo.cn/comment/%s\\n' % self.id\n",
      "        return result\n",
      "__str__(self=<weibo_spider.weibo.Weibo object at 0x7fb79dfb6130>, self.article_url='', self.comment_num=1000000, self.content=' \\xa0', self.id='J4PGk4yMw', self.original=True, self.original_pictures='', self.publish_place='', self.publish_time='2020-06-03 00:00', self.publish_tool='', self.retweet_num=1000000, self.retweet_pictures=None, self.up_num=1499675, self.user_id='', self.video_url='')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "result += u'%s\\n' % self.publish_time\n",
      "State:\n",
      "' \\xa0\\n\\n2020-06-03 00:00\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __str__(self):\n",
      "        result = self.content + '\\n'\n",
      "        result += u'%s\\n' % self.publish_place\n",
      "        result += u'%s\\n' % self.publish_time\n",
      "        result += u'%s\\n' % self.publish_tool\n",
      "        result += u'%d\\n' % self.up_num\n",
      "        result += u'%d\\n' % self.retweet_num\n",
      "        result += u'%d\\n' % self.comment_num\n",
      "        result += u'urlhttps://weibo.cn/comment/%s\\n' % self.id\n",
      "        return result\n",
      "__str__(self=<weibo_spider.weibo.Weibo object at 0x7fb79dfb6130>, self.article_url='', self.comment_num=1000000, self.content=' \\xa0', self.id='J4PGk4yMw', self.original=True, self.original_pictures='', self.publish_place='', self.publish_time='2020-06-03 00:00', self.publish_tool='', self.retweet_num=1000000, self.retweet_pictures=None, self.up_num=1499675, self.user_id='', self.video_url='')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "result += u'%s\\n' % self.publish_tool\n",
      "State:\n",
      "' \\xa0\\n\\n2020-06-03 00:00\\n\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def create_app(config_name):\n",
      "    app = Flask(__name__)\n",
      "    app.config.from_object(config[config_name])\n",
      "    config[config_name].init_app(app)\n",
      "    bootstrap.init_app(app)\n",
      "    mail.init_app(app)\n",
      "    moment.init_app(app)\n",
      "    db.init_app(app)\n",
      "    login_manager.init_app(app)\n",
      "    pagedown.init_app(app)\n",
      "    if app.config['SSL_REDIRECT']:\n",
      "        from flask_sslify import SSLify\n",
      "        sslify = SSLify(app)\n",
      "    from .main import main as main_blueprint\n",
      "    app.register_blueprint(main_blueprint)\n",
      "    from .auth import auth as auth_blueprint\n",
      "    app.register_blueprint(auth_blueprint, url_prefix='/auth')\n",
      "    from .api import api as api_blueprint\n",
      "    app.register_blueprint(api_blueprint, url_prefix='/api/v1')\n",
      "    return app\n",
      "create_app(config_name='testing')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "app = Flask(__name__)\n",
      "State:\n",
      "<Flask 'app'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def insert_roles():\n",
      "        roles = {\n",
      "            'User': [Permission.FOLLOW, Permission.COMMENT, Permission.WRITE],\n",
      "            'Moderator': [Permission.FOLLOW, Permission.COMMENT,\n",
      "                          Permission.WRITE, Permission.MODERATE],\n",
      "            'Administrator': [Permission.FOLLOW, Permission.COMMENT,\n",
      "                              Permission.WRITE, Permission.MODERATE,\n",
      "                              Permission.ADMIN],\n",
      "        }\n",
      "        default_role = 'User'\n",
      "        for r in roles:\n",
      "            role = Role.query.filter_by(name=r).first()\n",
      "            if role is None:\n",
      "                role = Role(name=r)\n",
      "            role.reset_permissions()\n",
      "            for perm in roles[r]:\n",
      "                role.add_permission(perm)\n",
      "            role.default = (role.name == default_role)\n",
      "            db.session.add(role)\n",
      "        db.session.commit()\n",
      "insert_roles()\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "roles = {\n",
      "State:\n",
      "{'User': [1, 2, 4], 'Moderator': [1, 2, 4, 8], 'Administrator': [1, 2, 4, 8, 16]}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add_permission(self, perm):\n",
      "        if not self.has_permission(perm):\n",
      "            self.permissions += perm\n",
      "add_permission(self=<Role 'User'>, perm=1, self._sa_instance_state=<sqlalchemy.orm.state.InstanceState object at 0x7f63c74b3fa0>, self.name='User', self.permissions=0)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.permissions += perm\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def unauthorized(message):\n",
      "    response = jsonify({'error': 'unauthorized', 'message': message})\n",
      "    response.status_code = 401\n",
      "    return response\n",
      "unauthorized(message='Invalid credentials')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "response = jsonify({'error': 'unauthorized', 'message': message})\n",
      "State:\n",
      "<Response 67 bytes [200 OK]>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def unauthorized(message):\n",
      "    response = jsonify({'error': 'unauthorized', 'message': message})\n",
      "    response.status_code = 401\n",
      "    return response\n",
      "unauthorized(message='Invalid credentials')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "response.status_code = 401\n",
      "State:\n",
      "<Response 67 bytes [401 UNAUTHORIZED]>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def on_changed_body(target, value, oldvalue, initiator):\n",
      "        allowed_tags = ['a', 'abbr', 'acronym', 'b', 'blockquote', 'code',\n",
      "                        'em', 'i', 'li', 'ol', 'pre', 'strong', 'ul',\n",
      "                        'h1', 'h2', 'h3', 'p']\n",
      "        target.body_html = bleach.linkify(bleach.clean(\n",
      "            markdown(value, output_format='html'),\n",
      "            tags=allowed_tags, strip=True))\n",
      "on_changed_body(target={_sa_instance_state=<sqlalchemy.orm.state.InstanceState object at 0x7f63c764d6d0>}, value='body of the post', oldvalue=symbol('NO_VALUE'), initiator=REPR FAILED)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "allowed_tags = ['a', 'abbr', 'acronym', 'b', 'blockquote', 'code',\n",
      "State:\n",
      "['a', 'abbr', 'acronym', 'b', 'blockquote', 'code', 'em', 'i', 'li', 'ol', 'pre', 'strong', 'ul', 'h1', 'h2', 'h3', 'p']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def new_post():\n",
      "    post = Post.from_json(request.json)\n",
      "    post.author = g.current_user\n",
      "    db.session.add(post)\n",
      "    db.session.commit()\n",
      "    return jsonify(post.to_json()), 201, \\\n",
      "        {'Location': url_for('api.get_post', id=post.id)}\n",
      "new_post()\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "post = Post.from_json(request.json)\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def is_eq(value, rep=None):\n",
      "    if rep is None:\n",
      "        rep = repr(value)\n",
      "    def is_valid(data, explain=False):\n",
      "        if not explain:\n",
      "            return data == value\n",
      "        return (\n",
      "            True, 'data is equal to {}'.format(rep)\n",
      "        ) if data == value else (\n",
      "            False, 'data is not equal to {}'.format(rep)\n",
      "        )\n",
      "    return is_valid\n",
      "is_eq(value=0, rep=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "rep = repr(value)\n",
      "State:\n",
      "'0'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def is_geq(value, rep=None):\n",
      "    if rep is None:\n",
      "        rep = repr(value)\n",
      "    def is_valid(data, explain=False):\n",
      "        if not explain:\n",
      "            return data >= value\n",
      "        return (\n",
      "            True, 'data is greater than or equal to {}'.format(rep)\n",
      "        ) if data >= value else (\n",
      "            False, 'data is not greater than or equal to {}'.format(rep)\n",
      "        )\n",
      "    return is_valid\n",
      "is_geq(value=0, rep=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "rep = repr(value)\n",
      "State:\n",
      "'0'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def is_lt(value, rep=None):\n",
      "    if rep is None:\n",
      "        rep = repr(value)\n",
      "    def is_valid(data, explain=False):\n",
      "        if not explain:\n",
      "            return data < value\n",
      "        return (\n",
      "            True, 'data is lower than {}'.format(rep)\n",
      "        ) if data < value else (\n",
      "            False, 'data is not lower than {}'.format(rep)\n",
      "        )\n",
      "    return is_valid\n",
      "is_lt(value=0, rep=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "rep = repr(value)\n",
      "State:\n",
      "'0'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def text_with_user_content(self) -> str:\n",
      "        text = self.text\n",
      "        for uc in self.user_content.values():\n",
      "            key = uc.key\n",
      "            mk = uc.markdown_text\n",
      "            text = text.replace(key, \"\\n\" + mk)\n",
      "        return text\n",
      "text_with_user_content(self=<bardapi.models.draft.BardDraft object at 0x7f873151a6d0>, self._input_list=['rc_ec0ac24caebe36dd', ['Jill Biden has four younger sisters:\\r\\n\\r\\n* Bonny Jean Biden\\r\\n* Barbara Biden\\r\\n* Valerie Biden Owens\\r\\n* Janine Biden\\r\\n\\r\\nSource: Jill Biden - Wikipedia: https://en.wikipedia.org/wiki/Jill_Biden\\r\\n\\r\\n> Jill Tracy Jacobs was born on June 3, 1951, in Hammonton, New Jersey. She is the oldest of five sisters. Her father, Donald Carl Jacobs, was a bank teller and U.S. Navy signalman during World War II who used the G.I. Bill to attend college and become an insurance salesman. Her mother, Bonny Jean Godfrey Jacobs, was a homemaker.'], [[[284, 425, ['http://en.wikipedia.org/wiki/Jill_Biden', '', '', None, '', None, None, '', False, '', '', '', '', '', '', ''], 1, 1, None, [1, 'normal_citation_datasets']], [196, 411, ['https://en.wikipedia.org/wiki/Jill_Biden\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "text = self.text\n",
      "State:\n",
      "'Jill Biden has four younger sisters:\\r\\n\\r\\n* Bonny Jean Biden\\r\\n* Barbara Biden\\r\\n* Valerie Biden Owens\\r\\n* Janine Biden\\r\\n\\r\\nSource: Jill Biden - Wikipedia: https://en.wikipedia.org/wiki/Jill_Biden\\r\\n\\r\\n> Jill Tracy Jacobs was born on June 3, 1951, in Hammonton, New Jersey. She is the oldest of five sisters. Her father, Donald Carl Jacobs, was a bank teller and U.S. Navy signalman during World War II who used the G.I. Bill to attend college and become an insurance salesman. Her mother, Bonny Jean Godfrey Jacobs, was a homemaker.'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def make_repo(path=None):\n",
      "    if not path:\n",
      "        path = \".\"\n",
      "    repo = git.Repo.init(path)\n",
      "    repo.config_writer().set_value(\"user\", \"name\", \"Test User\").release()\n",
      "    repo.config_writer().set_value(\"user\", \"email\", \"testuser@example.com\").release()\n",
      "    return repo\n",
      "make_repo(path='/tmp/tmpbbwa2p3f')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "repo = git.Repo.init(path)\n",
      "State:\n",
      "<git.repo.base.Repo '/tmp/tmpbbwa2p3f/.git'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def append_chat_history(self, text, linebreak=False, blockquote=False):\n",
      "        if blockquote:\n",
      "            text = text.strip()\n",
      "            text = \"> \" + text\n",
      "        if linebreak:\n",
      "            text = text.rstrip()\n",
      "            text = text + \"  \\n\"\n",
      "        if not text.endswith(\"\\n\"):\n",
      "            text += \"\\n\"\n",
      "        if self.chat_history_file is not None:\n",
      "            with self.chat_history_file.open(\"a\", encoding=self.encoding) as f:\n",
      "                f.write(text)\n",
      "append_chat_history(self=<aider.io.InputOutput object at 0x7fa2d23214f0>, text='Model: gpt-4 using diff edit format', linebreak=True, blockquote=True, self.chat_history_file=None, self.console=<console width=105 None>, self.dry_run=False, self.encoding='utf-8', self.input=None, self.input_history_file=None, self.output=None, self.pretty=True, self.tool_error_color='red', self.tool_output_color=None, self.user_input_color='blue', self.yes=True)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "text = \"> \" + text\n",
      "State:\n",
      "'> Model: gpt-4 using diff edit format'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def tool_output(self, *messages, log_only=False):\n",
      "        if messages:\n",
      "            hist = \" \".join(messages)\n",
      "            hist = f\"{hist.strip()}\"\n",
      "            self.append_chat_history(hist, linebreak=True, blockquote=True)\n",
      "        if not log_only:\n",
      "            messages = list(map(Text, messages))\n",
      "            style = dict(style=self.tool_output_color) if self.tool_output_color else dict()\n",
      "            self.console.print(*messages, **style)\n",
      "tool_output(self=<aider.io.InputOutput object at 0x7fa2d23214f0>, log_only=False, messages=('Model: gpt-4 using diff edit format',), self.chat_history_file=None, self.console=<console width=105 None>, self.dry_run=False, self.encoding='utf-8', self.input=None, self.input_history_file=None, self.output=None, self.pretty=True, self.tool_error_color='red', self.tool_output_color=None, self.user_input_color='blue', self.yes=True)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "hist = \" \".join(messages)\n",
      "State:\n",
      "'Model: gpt-4 using diff edit format'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def safe_abs_path(res):\n",
      "    \"Gives an abs path, which safely returns a full (not 8.3) windows path\"\n",
      "    res = Path(res).resolve()\n",
      "    return str(res)\n",
      "safe_abs_path(res='/tmp/tmpbbwa2p3f')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "res = Path(res).resolve()\n",
      "State:\n",
      "PosixPath('/tmp/tmpbbwa2p3f')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def allowed_to_edit(self, path):\n",
      "        full_path = self.abs_root_path(path)\n",
      "        if self.repo:\n",
      "            need_to_add = not self.repo.path_in_repo(path)\n",
      "        else:\n",
      "            need_to_add = False\n",
      "        if full_path in self.abs_fnames:\n",
      "            self.check_for_dirty_commit(path)\n",
      "            return True\n",
      "        if not Path(full_path).exists():\n",
      "            if not self.io.confirm_ask(f\"Allow creation of new file {path}?\"):\n",
      "                self.io.tool_error(f\"Skipping edits to {path}\")\n",
      "                return\n",
      "            if not self.dry_run:\n",
      "                Path(full_path).parent.mkdir(parents=True, exist_ok=True)\n",
      "                Path(full_path).touch()\n",
      "                if need_to_add:\n",
      "                    self.repo.repo.git.add(full_path)\n",
      "            self.abs_fnames.add(full_path)\n",
      "            return True\n",
      "        if not self.io.confirm_ask(\n",
      "            f\"Allow edits to {path} which was not previously added to chat?\"\n",
      "        ):\n",
      "            self.io.tool_error(f\"Skipping edits to {path}\")\n",
      "            return\n",
      "        if need_to_add:\n",
      "            self.repo.repo.git.add(full_path)\n",
      "        self.abs_fnames.add(full_path)\n",
      "        self.check_for_dirty_commit(path)\n",
      "        return True\n",
      "allowed_to_edit(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa2d2321160>, path='added.txt', self.abs_fnames={'/tmp/tmpbbwa2p3f/added.txt'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa2d23216a0>, self.console=<console width=105 None>, self.cur_messages=[], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa2d2321e20>, self.io=<aider.io.InputOutput object at 0x7fa2d23214f0>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.pretty=True, self.repo=<aider.repo.GitRepo object at 0x7fa2d23210a0>, self.repo_map=<aider.repomap.RepoMap object at 0x7fa2d2321f10>, self.root='/tmp/tmpbbwa2p3f', self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa2d2321bb0>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "full_path = self.abs_root_path(path)\n",
      "State:\n",
      "'/tmp/tmpbbwa2p3f/added.txt'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def allowed_to_edit(self, path):\n",
      "        full_path = self.abs_root_path(path)\n",
      "        if self.repo:\n",
      "            need_to_add = not self.repo.path_in_repo(path)\n",
      "        else:\n",
      "            need_to_add = False\n",
      "        if full_path in self.abs_fnames:\n",
      "            self.check_for_dirty_commit(path)\n",
      "            return True\n",
      "        if not Path(full_path).exists():\n",
      "            if not self.io.confirm_ask(f\"Allow creation of new file {path}?\"):\n",
      "                self.io.tool_error(f\"Skipping edits to {path}\")\n",
      "                return\n",
      "            if not self.dry_run:\n",
      "                Path(full_path).parent.mkdir(parents=True, exist_ok=True)\n",
      "                Path(full_path).touch()\n",
      "                if need_to_add:\n",
      "                    self.repo.repo.git.add(full_path)\n",
      "            self.abs_fnames.add(full_path)\n",
      "            return True\n",
      "        if not self.io.confirm_ask(\n",
      "            f\"Allow edits to {path} which was not previously added to chat?\"\n",
      "        ):\n",
      "            self.io.tool_error(f\"Skipping edits to {path}\")\n",
      "            return\n",
      "        if need_to_add:\n",
      "            self.repo.repo.git.add(full_path)\n",
      "        self.abs_fnames.add(full_path)\n",
      "        self.check_for_dirty_commit(path)\n",
      "        return True\n",
      "allowed_to_edit(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa2d2321160>, path='added.txt', self.abs_fnames={'/tmp/tmpbbwa2p3f/added.txt'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa2d23216a0>, self.console=<console width=105 None>, self.cur_messages=[], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa2d2321e20>, self.io=<aider.io.InputOutput object at 0x7fa2d23214f0>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.pretty=True, self.repo=<aider.repo.GitRepo object at 0x7fa2d23210a0>, self.repo_map=<aider.repomap.RepoMap object at 0x7fa2d2321f10>, self.root='/tmp/tmpbbwa2p3f', self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa2d2321bb0>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "need_to_add = not self.repo.path_in_repo(path)\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def confirm_ask(self, question, default=\"y\"):\n",
      "        self.num_user_asks += 1\n",
      "        if self.yes is True:\n",
      "            res = \"yes\"\n",
      "        elif self.yes is False:\n",
      "            res = \"no\"\n",
      "        else:\n",
      "            res = prompt(question + \" \", default=default)\n",
      "        hist = f\"{question.strip()} {res.strip()}\"\n",
      "        self.append_chat_history(hist, linebreak=True, blockquote=True)\n",
      "        if self.yes in (True, False):\n",
      "            self.tool_output(hist)\n",
      "        if not res or not res.strip():\n",
      "            return\n",
      "        return res.strip().lower().startswith(\"y\")\n",
      "confirm_ask(self=<aider.io.InputOutput object at 0x7fa2d23214f0>, question='Allow edits to repo.txt which was not previously added to chat?', default='y', self.chat_history_file=None, self.console=<console width=105 None>, self.dry_run=False, self.encoding='utf-8', self.input=None, self.input_history_file=None, self.output=None, self.pretty=True, self.tool_error_color='red', self.tool_output_color=None, self.user_input_color='blue', self.yes=True)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.num_user_asks += 1\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def tool_error(self, message):\n",
      "        self.num_error_outputs += 1\n",
      "        if message.strip():\n",
      "            hist = f\"{message.strip()}\"\n",
      "            self.append_chat_history(hist, linebreak=True, blockquote=True)\n",
      "        message = Text(message)\n",
      "        style = dict(style=self.tool_error_color) if self.tool_error_color else dict()\n",
      "        self.console.print(message, **style)\n",
      "tool_error(self=<aider.io.InputOutput object at 0x7fa370b98670>, message='Skipping edits to repo.txt', self.chat_history_file=None, self.console=<console width=105 None>, self.dry_run=False, self.encoding='utf-8', self.input=None, self.input_history_file=None, self.num_user_asks=1, self.output=None, self.pretty=True, self.tool_error_color='red', self.tool_output_color=None, self.user_input_color='blue', self.yes=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.num_error_outputs += 1\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def tool_error(self, message):\n",
      "        self.num_error_outputs += 1\n",
      "        if message.strip():\n",
      "            hist = f\"{message.strip()}\"\n",
      "            self.append_chat_history(hist, linebreak=True, blockquote=True)\n",
      "        message = Text(message)\n",
      "        style = dict(style=self.tool_error_color) if self.tool_error_color else dict()\n",
      "        self.console.print(message, **style)\n",
      "tool_error(self=<aider.io.InputOutput object at 0x7fa370b98670>, message='Skipping edits to repo.txt', self.chat_history_file=None, self.console=<console width=105 None>, self.dry_run=False, self.encoding='utf-8', self.input=None, self.input_history_file=None, self.num_user_asks=1, self.output=None, self.pretty=True, self.tool_error_color='red', self.tool_output_color=None, self.user_input_color='blue', self.yes=False)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "hist = f\"{message.strip()}\"\n",
      "State:\n",
      "'Skipping edits to repo.txt'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def check_for_file_mentions(self, content):\n",
      "        words = set(word for word in content.split())\n",
      "        words = set(word.rstrip(\",.!;\") for word in words)\n",
      "        quotes = \"\".join(['\"', \"'\", \"`\"])\n",
      "        words = set(word.strip(quotes) for word in words)\n",
      "        addable_rel_fnames = self.get_addable_relative_files()\n",
      "        mentioned_rel_fnames = set()\n",
      "        fname_to_rel_fnames = {}\n",
      "        for rel_fname in addable_rel_fnames:\n",
      "            if rel_fname in words:\n",
      "                mentioned_rel_fnames.add(str(rel_fname))\n",
      "            fname = os.path.basename(rel_fname)\n",
      "            if fname not in fname_to_rel_fnames:\n",
      "                fname_to_rel_fnames[fname] = []\n",
      "            fname_to_rel_fnames[fname].append(rel_fname)\n",
      "        for fname, rel_fnames in fname_to_rel_fnames.items():\n",
      "            if len(rel_fnames) == 1 and fname in words:\n",
      "                mentioned_rel_fnames.add(rel_fnames[0])\n",
      "        if not mentioned_rel_fnames:\n",
      "            return\n",
      "        for rel_fname in mentioned_rel_fnames:\n",
      "            self.io.tool_output(rel_fname)\n",
      "        if not self.io.confirm_ask(\"Add these files to the chat?\"):\n",
      "            return\n",
      "        for rel_fname in mentioned_rel_fnames:\n",
      "            self.add_rel_fname(rel_fname)\n",
      "        return prompts.added_files.format(fnames=\", \".join(mentioned_rel_fnames))\n",
      "check_for_file_mentions(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa370b5aa60>, content='Please check file1.txt!', self.abs_fnames=set(), self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa370b5ac70>, self.console=<console width=105 None>, self.cur_messages=[], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa370b5a6d0>, self.io=<aider.io.InputOutput object at 0x7fa370b5ab80>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.pretty=True, self.repo=<aider.repo.GitRepo object at 0x7fa370b5a670>, self.repo_map=<aider.repomap.RepoMap object at 0x7fa370b5afa0>, self.root='/tmp/tmpsccfakni', self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa370b5af10>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "addable_rel_fnames = self.get_addable_relative_files()\n",
      "State:\n",
      "{'other/file1.txt', 'file1.txt'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_all_relative_files(self):\n",
      "        if self.repo:\n",
      "            files = self.repo.get_tracked_files()\n",
      "        else:\n",
      "            files = self.get_inchat_relative_files()\n",
      "        files = [fname for fname in files if Path(self.abs_root_path(fname)).is_file()]\n",
      "        return sorted(set(files))\n",
      "get_all_relative_files(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa370b5aa60>, self.abs_fnames=set(), self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa370b5ac70>, self.console=<console width=105 None>, self.cur_messages=[], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa370b5a6d0>, self.io=<aider.io.InputOutput object at 0x7fa370b5ab80>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.pretty=True, self.repo=<aider.repo.GitRepo object at 0x7fa370b5a670>, self.repo_map=<aider.repomap.RepoMap object at 0x7fa370b5afa0>, self.root='/tmp/tmpsccfakni', self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa370b5af10>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "files = self.repo.get_tracked_files()\n",
      "State:\n",
      "{'other/file1.txt', 'file1.txt'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add_rel_fname(self, rel_fname):\n",
      "        self.abs_fnames.add(self.abs_root_path(rel_fname))\n",
      "add_rel_fname(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa370b5aa60>, rel_fname='file1.txt', self.abs_fnames=set(), self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa370b5ac70>, self.console=<console width=105 None>, self.cur_messages=[], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa370b5a6d0>, self.io=<aider.io.InputOutput object at 0x7fa370b5ab80>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.pretty=True, self.repo=<aider.repo.GitRepo object at 0x7fa370b5a670>, self.repo_map=<aider.repomap.RepoMap object at 0x7fa370b5afa0>, self.root='/tmp/tmpsccfakni', self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa370b5af10>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.abs_fnames.add(self.abs_root_path(rel_fname))\n",
      "State:\n",
      "{'/tmp/tmpsccfakni/file1.txt'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def run(self, with_message=None):\n",
      "        while True:\n",
      "            try:\n",
      "                if with_message:\n",
      "                    new_user_message = with_message\n",
      "                    self.io.user_input(with_message)\n",
      "                else:\n",
      "                    new_user_message = self.run_loop()\n",
      "                while new_user_message:\n",
      "                    new_user_message = self.send_new_user_message(new_user_message)\n",
      "                if with_message:\n",
      "                    return self.partial_response_content\n",
      "            except KeyboardInterrupt:\n",
      "                self.keyboard_interrupt()\n",
      "            except EOFError:\n",
      "                return\n",
      "run(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa3708257c0>, with_message='hi', self.abs_fnames={'/tmp/tmpi945g4fh'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3708259d0>, self.console=<console width=105 None>, self.cur_messages=[], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa370825070>, self.io=<aider.io.InputOutput object at 0x7fa370858880>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339944195216'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa370858670>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "new_user_message = with_message\n",
      "State:\n",
      "'hi'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def send_new_user_message(self, inp):\n",
      "        self.cur_messages += [\n",
      "            dict(role=\"user\", content=inp),\n",
      "        ]\n",
      "        messages = self.format_messages()\n",
      "        if self.verbose:\n",
      "            utils.show_messages(messages, functions=self.functions)\n",
      "        exhausted = False\n",
      "        interrupted = False\n",
      "        try:\n",
      "            interrupted = self.send(messages, functions=self.functions)\n",
      "        except ExhaustedContextWindow:\n",
      "            exhausted = True\n",
      "        except openai.BadRequestError as err:\n",
      "            if \"maximum context length\" in str(err):\n",
      "                exhausted = True\n",
      "            else:\n",
      "                raise err\n",
      "        if exhausted:\n",
      "            self.num_exhausted_context_windows += 1\n",
      "            self.io.tool_error(\"The chat session is larger than the context window!\\n\")\n",
      "            self.commands.cmd_tokens(\"\")\n",
      "            self.io.tool_error(\"\\nTo reduce token usage:\")\n",
      "            self.io.tool_error(\" - Use /drop to remove unneeded files from the chat session.\")\n",
      "            self.io.tool_error(\" - Use /clear to clear chat history.\")\n",
      "            return\n",
      "        if self.partial_response_function_call:\n",
      "            args = self.parse_partial_args()\n",
      "            if args:\n",
      "                content = args[\"explanation\"]\n",
      "            else:\n",
      "                content = \"\"\n",
      "        elif self.partial_response_content:\n",
      "            content = self.partial_response_content\n",
      "        else:\n",
      "            content = \"\"\n",
      "        if interrupted:\n",
      "            content += \"\\n^C KeyboardInterrupt\"\n",
      "        self.io.tool_output()\n",
      "        if interrupted:\n",
      "            self.cur_messages += [dict(role=\"assistant\", content=content)]\n",
      "            return\n",
      "        edited, edit_error = self.apply_updates()\n",
      "        if edit_error:\n",
      "            self.update_cur_messages(set())\n",
      "            return edit_error\n",
      "        self.update_cur_messages(edited)\n",
      "        if edited:\n",
      "            if self.repo and self.auto_commits and not self.dry_run:\n",
      "                saved_message = self.auto_commit(edited)\n",
      "            elif hasattr(self.gpt_prompts, \"files_content_gpt_edits_no_repo\"):\n",
      "                saved_message = self.gpt_prompts.files_content_gpt_edits_no_repo\n",
      "            else:\n",
      "                saved_message = None\n",
      "            self.move_back_cur_messages(saved_message)\n",
      "        add_rel_files_message = self.check_for_file_mentions(content)\n",
      "        if add_rel_files_message:\n",
      "            return add_rel_files_message\n",
      "send_new_user_message(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa3708257c0>, inp='hi', self.abs_fnames={'/tmp/tmpi945g4fh'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3708259d0>, self.console=<console width=105 None>, self.cur_messages=[], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa370825070>, self.io=<aider.io.InputOutput object at 0x7fa370858880>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339944195216'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa370858670>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.cur_messages += [\n",
      "State:\n",
      "[{'role': 'user', 'content': 'hi'}]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def send_new_user_message(self, inp):\n",
      "        self.cur_messages += [\n",
      "            dict(role=\"user\", content=inp),\n",
      "        ]\n",
      "        messages = self.format_messages()\n",
      "        if self.verbose:\n",
      "            utils.show_messages(messages, functions=self.functions)\n",
      "        exhausted = False\n",
      "        interrupted = False\n",
      "        try:\n",
      "            interrupted = self.send(messages, functions=self.functions)\n",
      "        except ExhaustedContextWindow:\n",
      "            exhausted = True\n",
      "        except openai.BadRequestError as err:\n",
      "            if \"maximum context length\" in str(err):\n",
      "                exhausted = True\n",
      "            else:\n",
      "                raise err\n",
      "        if exhausted:\n",
      "            self.num_exhausted_context_windows += 1\n",
      "            self.io.tool_error(\"The chat session is larger than the context window!\\n\")\n",
      "            self.commands.cmd_tokens(\"\")\n",
      "            self.io.tool_error(\"\\nTo reduce token usage:\")\n",
      "            self.io.tool_error(\" - Use /drop to remove unneeded files from the chat session.\")\n",
      "            self.io.tool_error(\" - Use /clear to clear chat history.\")\n",
      "            return\n",
      "        if self.partial_response_function_call:\n",
      "            args = self.parse_partial_args()\n",
      "            if args:\n",
      "                content = args[\"explanation\"]\n",
      "            else:\n",
      "                content = \"\"\n",
      "        elif self.partial_response_content:\n",
      "            content = self.partial_response_content\n",
      "        else:\n",
      "            content = \"\"\n",
      "        if interrupted:\n",
      "            content += \"\\n^C KeyboardInterrupt\"\n",
      "        self.io.tool_output()\n",
      "        if interrupted:\n",
      "            self.cur_messages += [dict(role=\"assistant\", content=content)]\n",
      "            return\n",
      "        edited, edit_error = self.apply_updates()\n",
      "        if edit_error:\n",
      "            self.update_cur_messages(set())\n",
      "            return edit_error\n",
      "        self.update_cur_messages(edited)\n",
      "        if edited:\n",
      "            if self.repo and self.auto_commits and not self.dry_run:\n",
      "                saved_message = self.auto_commit(edited)\n",
      "            elif hasattr(self.gpt_prompts, \"files_content_gpt_edits_no_repo\"):\n",
      "                saved_message = self.gpt_prompts.files_content_gpt_edits_no_repo\n",
      "            else:\n",
      "                saved_message = None\n",
      "            self.move_back_cur_messages(saved_message)\n",
      "        add_rel_files_message = self.check_for_file_mentions(content)\n",
      "        if add_rel_files_message:\n",
      "            return add_rel_files_message\n",
      "send_new_user_message(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa3708257c0>, inp='hi', self.abs_fnames={'/tmp/tmpi945g4fh'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3708259d0>, self.console=<console width=105 None>, self.cur_messages=[], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa370825070>, self.io=<aider.io.InputOutput object at 0x7fa370858880>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339944195216'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa370858670>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "exhausted = False\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def format_messages(self):\n",
      "        self.choose_fence()\n",
      "        main_sys = self.fmt_system_prompt(self.gpt_prompts.main_system)\n",
      "        main_sys += \"\\n\" + self.fmt_system_prompt(self.gpt_prompts.system_reminder)\n",
      "        messages = [\n",
      "            dict(role=\"system\", content=main_sys),\n",
      "        ]\n",
      "        self.summarize_end()\n",
      "        messages += self.done_messages\n",
      "        messages += self.get_files_messages()\n",
      "        reminder_message = [\n",
      "            dict(role=\"system\", content=self.fmt_system_prompt(self.gpt_prompts.system_reminder)),\n",
      "        ]\n",
      "        messages_tokens = self.main_model.token_count(messages)\n",
      "        reminder_tokens = self.main_model.token_count(reminder_message)\n",
      "        cur_tokens = self.main_model.token_count(self.cur_messages)\n",
      "        if None not in (messages_tokens, reminder_tokens, cur_tokens):\n",
      "            total_tokens = messages_tokens + reminder_tokens + cur_tokens\n",
      "        else:\n",
      "            total_tokens = 0\n",
      "        messages += self.cur_messages\n",
      "        if total_tokens < self.main_model.max_context_tokens:\n",
      "            messages += reminder_message\n",
      "        return messages\n",
      "format_messages(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa3708257c0>, self.abs_fnames={'/tmp/tmpi945g4fh'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3708259d0>, self.console=<console width=105 None>, self.cur_messages=[{'role': 'user', 'content': 'hi'}], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa370825070>, self.io=<aider.io.InputOutput object at 0x7fa370858880>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339944195216'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa370858670>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.choose_fence()\n",
      "State:\n",
      "('<source>', '</source>')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def format_messages(self):\n",
      "        self.choose_fence()\n",
      "        main_sys = self.fmt_system_prompt(self.gpt_prompts.main_system)\n",
      "        main_sys += \"\\n\" + self.fmt_system_prompt(self.gpt_prompts.system_reminder)\n",
      "        messages = [\n",
      "            dict(role=\"system\", content=main_sys),\n",
      "        ]\n",
      "        self.summarize_end()\n",
      "        messages += self.done_messages\n",
      "        messages += self.get_files_messages()\n",
      "        reminder_message = [\n",
      "            dict(role=\"system\", content=self.fmt_system_prompt(self.gpt_prompts.system_reminder)),\n",
      "        ]\n",
      "        messages_tokens = self.main_model.token_count(messages)\n",
      "        reminder_tokens = self.main_model.token_count(reminder_message)\n",
      "        cur_tokens = self.main_model.token_count(self.cur_messages)\n",
      "        if None not in (messages_tokens, reminder_tokens, cur_tokens):\n",
      "            total_tokens = messages_tokens + reminder_tokens + cur_tokens\n",
      "        else:\n",
      "            total_tokens = 0\n",
      "        messages += self.cur_messages\n",
      "        if total_tokens < self.main_model.max_context_tokens:\n",
      "            messages += reminder_message\n",
      "        return messages\n",
      "format_messages(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa3708257c0>, self.abs_fnames={'/tmp/tmpi945g4fh'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3708259d0>, self.console=<console width=105 None>, self.cur_messages=[{'role': 'user', 'content': 'hi'}], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa370825070>, self.io=<aider.io.InputOutput object at 0x7fa370858880>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339944195216'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa370858670>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "main_sys = self.fmt_system_prompt(self.gpt_prompts.main_system)\n",
      "State:\n",
      "'Act as an expert software developer.\\nYou are diligent and tireless!\\nYou NEVER leave comments describing code without implementing it!\\nYou always COMPLETELY IMPLEMENT the needed code!\\nAlways use best practices when coding.\\nRespect and use existing conventions, libraries, etc that are already present in the code base.\\n\\nTake requests for changes to the supplied code.\\nIf the request is ambiguous, ask questions.\\n\\nOnce you understand the request you MUST:\\n1. List the files you need to modify. Only suggest changes to a *read-write* files. Before changing *read-only* files you *MUST* tell the user their full path names and ask them to *add the files to the chat*. End your reply and wait for their approval.\\n2. Think step-by-step and explain the needed changes with a numbered list of short sentences.\\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\\n\\nAll changes to files must use this *SEARCH/REPLACE block* format.\\n\\n# Example conversation 1:\\n\\n## USER: These are the *read-write* files:\\n\\nmathweb/flask/app.py\\n<source>python\\nfrom flask import Flask\\n\\napp = Flask(__name__)\\n\\n@app.route(\\'/div/<int:n>/<int:d>\\')\\ndef divide(n, d):\\n    if d == 0:\\n        return \\'Error: Division by zero\\'\\n    else:\\n        result = n / d\\n        return str(result)\\n\\ndef factorial(n):\\n    \"compute factorial\"\\n\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\\n\\n@app.route(\\'/factorial/<int:n>\\')\\ndef get_factorial(n):\\n    return str(factorial(n))\\n\\nif __name__ == \\'__main__\\':\\n    print(\"Starting...\")\\n    app.run()\\n</source>\\n\\n## ASSISTANT: Ok.\\n\\n## USER: Change get_factorial() to use math.factorial\\n\\n## ASSISTANT:\\n\\nDo make this change we need to modify `mathweb/flask/app.py` to:\\n\\n1. Import the math package.\\n2. Remove the existing factorial() function.\\n3. Update get_factorial() to call math.factorial instead.\\n\\nHere are the *SEARCH/REPLACE* blocks:\\n\\nmathweb/flask/app.py\\n<source>python\\n<<<<<<< SEARCH\\nfrom flask import Flask\\n=======\\nimport math\\nfrom flask import Flask\\n>>>>>>> REPLACE\\n</source>\\n\\nmathweb/flask/app.py\\n<source>python\\n<<<<<<< SEARCH\\ndef factorial(n):\\n    \"compute factorial\"\\n\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\\n\\n=======\\n>>>>>>> REPLACE\\n</source>\\n\\nmathweb/flask/app.py\\n<source>python\\n<<<<<<< SEARCH\\n    return str(factorial(n))\\n=======\\n    return str(math.factorial(n))\\n>>>>>>> REPLACE\\n</source>\\n\\n# Example conversation 2:\\n\\n## USER: These are the *read-write* files:\\n\\nmain.py\\n<source>python\\ndef hello():\\n    \"print a greeting\"\\n\\n    print(\"hello\")\\n\\nif __name__ == \\'__main__\\':\\n    hello()\\n</source>\\n\\n## ASSISTANT: Ok.\\n\\n## USER: Refactor hello() into its own file.\\n\\n## ASSISTANT:\\n\\nDo make this change we need to modify `main.py` and make a new file `hello.py`:\\n\\n1. Make a new hello.py file with hello() in it.\\n2. Remove hello() from main.py and replace it with an import.\\n\\nHere are the *SEARCH/REPLACE* blocks:\\n\\nhello.py\\n<source>python\\n<<<<<<< SEARCH\\n=======\\ndef hello():\\n    \"print a greeting\"\\n\\n    print(\"hello\")\\n>>>>>>> REPLACE\\n</source>\\n\\nmain.py\\n<source>python\\n<<<<<<< SEARCH\\ndef hello():\\n    \"print a greeting\"\\n\\n    print(\"hello\")\\n=======\\nfrom hello import hello\\n>>>>>>> REPLACE\\n</source>\\n\\n# Rules\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def format_messages(self):\n",
      "        self.choose_fence()\n",
      "        main_sys = self.fmt_system_prompt(self.gpt_prompts.main_system)\n",
      "        main_sys += \"\\n\" + self.fmt_system_prompt(self.gpt_prompts.system_reminder)\n",
      "        messages = [\n",
      "            dict(role=\"system\", content=main_sys),\n",
      "        ]\n",
      "        self.summarize_end()\n",
      "        messages += self.done_messages\n",
      "        messages += self.get_files_messages()\n",
      "        reminder_message = [\n",
      "            dict(role=\"system\", content=self.fmt_system_prompt(self.gpt_prompts.system_reminder)),\n",
      "        ]\n",
      "        messages_tokens = self.main_model.token_count(messages)\n",
      "        reminder_tokens = self.main_model.token_count(reminder_message)\n",
      "        cur_tokens = self.main_model.token_count(self.cur_messages)\n",
      "        if None not in (messages_tokens, reminder_tokens, cur_tokens):\n",
      "            total_tokens = messages_tokens + reminder_tokens + cur_tokens\n",
      "        else:\n",
      "            total_tokens = 0\n",
      "        messages += self.cur_messages\n",
      "        if total_tokens < self.main_model.max_context_tokens:\n",
      "            messages += reminder_message\n",
      "        return messages\n",
      "format_messages(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa3708257c0>, self.abs_fnames={'/tmp/tmpi945g4fh'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3708259d0>, self.console=<console width=105 None>, self.cur_messages=[{'role': 'user', 'content': 'hi'}], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa370825070>, self.io=<aider.io.InputOutput object at 0x7fa370858880>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339944195216'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa370858670>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "main_sys += \"\\n\" + self.fmt_system_prompt(self.gpt_prompts.system_reminder)\n",
      "State:\n",
      "'Act as an expert software developer.\\nYou are diligent and tireless!\\nYou NEVER leave comments describing code without implementing it!\\nYou always COMPLETELY IMPLEMENT the needed code!\\nAlways use best practices when coding.\\nRespect and use existing conventions, libraries, etc that are already present in the code base.\\n\\nTake requests for changes to the supplied code.\\nIf the request is ambiguous, ask questions.\\n\\nOnce you understand the request you MUST:\\n1. List the files you need to modify. Only suggest changes to a *read-write* files. Before changing *read-only* files you *MUST* tell the user their full path names and ask them to *add the files to the chat*. End your reply and wait for their approval.\\n2. Think step-by-step and explain the needed changes with a numbered list of short sentences.\\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\\n\\nAll changes to files must use this *SEARCH/REPLACE block* format.\\n\\n# Example conversation 1:\\n\\n## USER: These are the *read-write* files:\\n\\nmathweb/flask/app.py\\n<source>python\\nfrom flask import Flask\\n\\napp = Flask(__name__)\\n\\n@app.route(\\'/div/<int:n>/<int:d>\\')\\ndef divide(n, d):\\n    if d == 0:\\n        return \\'Error: Division by zero\\'\\n    else:\\n        result = n / d\\n        return str(result)\\n\\ndef factorial(n):\\n    \"compute factorial\"\\n\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\\n\\n@app.route(\\'/factorial/<int:n>\\')\\ndef get_factorial(n):\\n    return str(factorial(n))\\n\\nif __name__ == \\'__main__\\':\\n    print(\"Starting...\")\\n    app.run()\\n</source>\\n\\n## ASSISTANT: Ok.\\n\\n## USER: Change get_factorial() to use math.factorial\\n\\n## ASSISTANT:\\n\\nDo make this change we need to modify `mathweb/flask/app.py` to:\\n\\n1. Import the math package.\\n2. Remove the existing factorial() function.\\n3. Update get_factorial() to call math.factorial instead.\\n\\nHere are the *SEARCH/REPLACE* blocks:\\n\\nmathweb/flask/app.py\\n<source>python\\n<<<<<<< SEARCH\\nfrom flask import Flask\\n=======\\nimport math\\nfrom flask import Flask\\n>>>>>>> REPLACE\\n</source>\\n\\nmathweb/flask/app.py\\n<source>python\\n<<<<<<< SEARCH\\ndef factorial(n):\\n    \"compute factorial\"\\n\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\\n\\n=======\\n>>>>>>> REPLACE\\n</source>\\n\\nmathweb/flask/app.py\\n<source>python\\n<<<<<<< SEARCH\\n    return str(factorial(n))\\n=======\\n    return str(math.factorial(n))\\n>>>>>>> REPLACE\\n</source>\\n\\n# Example conversation 2:\\n\\n## USER: These are the *read-write* files:\\n\\nmain.py\\n<source>python\\ndef hello():\\n    \"print a greeting\"\\n\\n    print(\"hello\")\\n\\nif __name__ == \\'__main__\\':\\n    hello()\\n</source>\\n\\n## ASSISTANT: Ok.\\n\\n## USER: Refactor hello() into its own file.\\n\\n## ASSISTANT:\\n\\nDo make this change we need to modify `main.py` and make a new file `hello.py`:\\n\\n1. Make a new hello.py file with hello() in it.\\n2. Remove hello() from main.py and replace it with an import.\\n\\nHere are the *SEARCH/REPLACE* blocks:\\n\\nhello.py\\n<source>python\\n<<<<<<< SEARCH\\n=======\\ndef hello():\\n    \"print a greeting\"\\n\\n    print(\"hello\")\\n>>>>>>> REPLACE\\n</source>\\n\\nmain.py\\n<source>python\\n<<<<<<< SEARCH\\ndef hello():\\n    \"print a greeting\"\\n\\n    print(\"hello\")\\n=======\\nfrom hello import hello\\n>>>>>>> REPLACE\\n</source>\\n\\n# Rules\\n\\nEvery *SEARCH/REPLACE block* must use this format:\\n1. The file path alone on a line, eg: main.py\\n2. The opening fence and code language, eg: <source>python\\n3. The start of search block: <<<<<<< SEARCH\\n4. A contiguous chunk of lines to search for in the existing source code\\n5. The dividing line: =======\\n6. The lines to replace into the source code\\n7. The end of the replace block: >>>>>>> REPLACE\\n8. The closing fence: </source>\\n\\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\\n\\nInclude *ALL* the code being searched and replaced!\\n\\nOnly *SEARCH/REPLACE* files that are *read-write*.\\n\\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\\n\\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\\n- A new file path, including dir name if needed\\n- An empty `SEARCH` section\\n- The new file\\'s contents in the `REPLACE` section\\n\\nYou are diligent and tireless!\\nYou NEVER leave comments describing code without implementing it!\\nYou always COMPLETELY IMPLEMENT the needed code!\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def format_messages(self):\n",
      "        self.choose_fence()\n",
      "        main_sys = self.fmt_system_prompt(self.gpt_prompts.main_system)\n",
      "        main_sys += \"\\n\" + self.fmt_system_prompt(self.gpt_prompts.system_reminder)\n",
      "        messages = [\n",
      "            dict(role=\"system\", content=main_sys),\n",
      "        ]\n",
      "        self.summarize_end()\n",
      "        messages += self.done_messages\n",
      "        messages += self.get_files_messages()\n",
      "        reminder_message = [\n",
      "            dict(role=\"system\", content=self.fmt_system_prompt(self.gpt_prompts.system_reminder)),\n",
      "        ]\n",
      "        messages_tokens = self.main_model.token_count(messages)\n",
      "        reminder_tokens = self.main_model.token_count(reminder_message)\n",
      "        cur_tokens = self.main_model.token_count(self.cur_messages)\n",
      "        if None not in (messages_tokens, reminder_tokens, cur_tokens):\n",
      "            total_tokens = messages_tokens + reminder_tokens + cur_tokens\n",
      "        else:\n",
      "            total_tokens = 0\n",
      "        messages += self.cur_messages\n",
      "        if total_tokens < self.main_model.max_context_tokens:\n",
      "            messages += reminder_message\n",
      "        return messages\n",
      "format_messages(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa3708257c0>, self.abs_fnames={'/tmp/tmpi945g4fh'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3708259d0>, self.console=<console width=105 None>, self.cur_messages=[{'role': 'user', 'content': 'hi'}], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa370825070>, self.io=<aider.io.InputOutput object at 0x7fa370858880>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339944195216'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa370858670>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "messages = [\n",
      "State:\n",
      "[{'role': 'system', 'content': 'Act as an expert software developer.\\nYou are diligent and tireless!\\nYou NEVER leave comments describing code without implementing it!\\nYou always COMPLETELY IMPLEMENT the needed code!\\nAlways use best practices when coding.\\nRespect and use existing conventions, libraries, etc that are already present in the code base.\\n\\nTake requests for changes to the supplied code.\\nIf the request is ambiguous, ask questions.\\n\\nOnce you understand the request you MUST:\\n1. List the files you need to modify. Only suggest changes to a *read-write* files. Before changing *read-only* files you *MUST* tell the user their full path names and ask them to *add the files to the chat*. End your reply and wait for their approval.\\n2. Think step-by-step and explain the needed changes with a numbered list of short sentences.\\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\\n\\nAll changes to files must use this *SEARCH/REPLACE block* format.\\n\\n# Example conversation 1:\\n\\n## USER: These are the *read-write* files:\\n\\nmathweb/flask/app.py\\n<source>python\\nfrom flask import Flask\\n\\napp = Flask(__name__)\\n\\n@app.route(\\'/div/<int:n>/<int:d>\\')\\ndef divide(n, d):\\n    if d == 0:\\n        return \\'Error: Division by zero\\'\\n    else:\\n        result = n / d\\n        return str(result)\\n\\ndef factorial(n):\\n    \"compute factorial\"\\n\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\\n\\n@app.route(\\'/factorial/<int:n>\\')\\ndef get_factorial(n):\\n    return str(factorial(n))\\n\\nif __name__ == \\'__main__\\':\\n    print(\"Starting...\")\\n    app.run()\\n</source>\\n\\n## ASSISTANT: Ok.\\n\\n## USER: Change get_factorial() to use math.factorial\\n\\n## ASSISTANT:\\n\\nDo make this change we need to modify `mathweb/flask/app.py` to:\\n\\n1. Import the math package.\\n2. Remove the existing factorial() function.\\n3. Update get_factorial() to call math.factorial instead.\\n\\nHere are the *SEARCH/REPLACE* blocks:\\n\\nmathweb/flask/app.py\\n<source>python\\n<<<<<<< SEARCH\\nfrom flask import Flask\\n=======\\nimport math\\nfrom flask import Flask\\n>>>>>>> REPLACE\\n</source>\\n\\nmathweb/flask/app.py\\n<source>python\\n<<<<<<< SEARCH\\ndef factorial(n):\\n    \"compute factorial\"\\n\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\\n\\n=======\\n>>>>>>> REPLACE\\n</source>\\n\\nmathweb/flask/app.py\\n<source>python\\n<<<<<<< SEARCH\\n    return str(factorial(n))\\n=======\\n    return str(math.factorial(n))\\n>>>>>>> REPLACE\\n</source>\\n\\n# Example conversation 2:\\n\\n## USER: These are the *read-write* files:\\n\\nmain.py\\n<source>python\\ndef hello():\\n    \"print a greeting\"\\n\\n    print(\"hello\")\\n\\nif __name__ == \\'__main__\\':\\n    hello()\\n</source>\\n\\n## ASSISTANT: Ok.\\n\\n## USER: Refactor hello() into its own file.\\n\\n## ASSISTANT:\\n\\nDo make this change we need to modify `main.py` and make a new file `hello.py`:\\n\\n1. Make a new hello.py file with hello() in it.\\n2. Remove hello() from main.py and replace it with an import.\\n\\nHere are the *SEARCH/REPLACE* blocks:\\n\\nhello.py\\n<source>python\\n<<<<<<< SEARCH\\n=======\\ndef hello():\\n    \"print a greeting\"\\n\\n    print(\"hello\")\\n>>>>>>> REPLACE\\n</source>\\n\\nmain.py\\n<source>python\\n<<<<<<< SEARCH\\ndef hello():\\n    \"print a greeting\"\\n\\n    print(\"hello\")\\n=======\\nfrom hello import hello\\n>>>>>>> REPLACE\\n</source>\\n\\n# Rules\\n\\nEvery *SEARCH/REPLACE block* must use this format:\\n1. The file path alone on a line, eg: main.py\\n2. The opening fence and code language, eg: <source>python\\n3. The start of search block: <<<<<<< SEARCH\\n4. A contiguous chunk of lines to search for in the existing source code\\n5. The dividing line: =======\\n6. The lines to replace into the source code\\n7. The end of the replace block: >>>>>>> REPLACE\\n8. The closing fence: </source>\\n\\nEvery *SEARCH* section must *EXACTLY MATCH* the existing source code, character for character, including all comments, docstrings, etc.\\n\\nInclude *ALL* the code being searched and replaced!\\n\\nOnly *SEARCH/REPLACE* files that are *read-write*.\\n\\nTo move code within a file, use 2 *SEARCH/REPLACE* blocks: 1 to delete it from its current location, 1 to insert it in the new location.\\n\\nIf you want to put code in a new file, use a *SEARCH/REPLACE block* with:\\n- A new file path, including dir name if needed\\n- An empty `SEARCH` section\\n- The new file\\'s contents in the `REPLACE` section\\n\\nYou are diligent and tireless!\\nYou NEVER leave comments describing code without implementing it!\\nYou always COMPLETELY IMPLEMENT the needed code!\\n'}]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def format_messages(self):\n",
      "        self.choose_fence()\n",
      "        main_sys = self.fmt_system_prompt(self.gpt_prompts.main_system)\n",
      "        main_sys += \"\\n\" + self.fmt_system_prompt(self.gpt_prompts.system_reminder)\n",
      "        messages = [\n",
      "            dict(role=\"system\", content=main_sys),\n",
      "        ]\n",
      "        self.summarize_end()\n",
      "        messages += self.done_messages\n",
      "        messages += self.get_files_messages()\n",
      "        reminder_message = [\n",
      "            dict(role=\"system\", content=self.fmt_system_prompt(self.gpt_prompts.system_reminder)),\n",
      "        ]\n",
      "        messages_tokens = self.main_model.token_count(messages)\n",
      "        reminder_tokens = self.main_model.token_count(reminder_message)\n",
      "        cur_tokens = self.main_model.token_count(self.cur_messages)\n",
      "        if None not in (messages_tokens, reminder_tokens, cur_tokens):\n",
      "            total_tokens = messages_tokens + reminder_tokens + cur_tokens\n",
      "        else:\n",
      "            total_tokens = 0\n",
      "        messages += self.cur_messages\n",
      "        if total_tokens < self.main_model.max_context_tokens:\n",
      "            messages += reminder_message\n",
      "        return messages\n",
      "format_messages(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa3708257c0>, self.abs_fnames={'/tmp/tmpi945g4fh'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3708259d0>, self.console=<console width=105 None>, self.cur_messages=[{'role': 'user', 'content': 'hi'}], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa370825070>, self.io=<aider.io.InputOutput object at 0x7fa370858880>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339944195216'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa370858670>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "cur_tokens = self.main_model.token_count(self.cur_messages)\n",
      "State:\n",
      "14\n",
      "==================================================\n",
      "Clean Code:\n",
      "def choose_fence(self):\n",
      "        all_content = \"\"\n",
      "        for _fname, content in self.get_abs_fnames_content():\n",
      "            all_content += content + \"\\n\"\n",
      "        good = False\n",
      "        for fence_open, fence_close in self.fences:\n",
      "            if fence_open in all_content or fence_close in all_content:\n",
      "                continue\n",
      "            good = True\n",
      "            break\n",
      "        if good:\n",
      "            self.fence = (fence_open, fence_close)\n",
      "        else:\n",
      "            self.fence = self.fences[0]\n",
      "            self.io.tool_error(\n",
      "                \"Unable to find a fencing strategy! Falling back to:\"\n",
      "                \" {self.fence[0]}...{self.fence[1]}\"\n",
      "            )\n",
      "        return\n",
      "choose_fence(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa3708257c0>, self.abs_fnames={'/tmp/tmpi945g4fh'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3708259d0>, self.console=<console width=105 None>, self.cur_messages=[{'role': 'user', 'content': 'hi'}], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa370825070>, self.io=<aider.io.InputOutput object at 0x7fa370858880>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339944195216'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa370858670>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "all_content = \"\"\n",
      "State:\n",
      "''\n",
      "==================================================\n",
      "Clean Code:\n",
      "def choose_fence(self):\n",
      "        all_content = \"\"\n",
      "        for _fname, content in self.get_abs_fnames_content():\n",
      "            all_content += content + \"\\n\"\n",
      "        good = False\n",
      "        for fence_open, fence_close in self.fences:\n",
      "            if fence_open in all_content or fence_close in all_content:\n",
      "                continue\n",
      "            good = True\n",
      "            break\n",
      "        if good:\n",
      "            self.fence = (fence_open, fence_close)\n",
      "        else:\n",
      "            self.fence = self.fences[0]\n",
      "            self.io.tool_error(\n",
      "                \"Unable to find a fencing strategy! Falling back to:\"\n",
      "                \" {self.fence[0]}...{self.fence[1]}\"\n",
      "            )\n",
      "        return\n",
      "choose_fence(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa3708257c0>, self.abs_fnames={'/tmp/tmpi945g4fh'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3708259d0>, self.console=<console width=105 None>, self.cur_messages=[{'role': 'user', 'content': 'hi'}], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa370825070>, self.io=<aider.io.InputOutput object at 0x7fa370858880>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339944195216'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa370858670>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "all_content += content + \"\\n\"\n",
      "State:\n",
      "'this contains ``` backticks\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def read_text(self, filename):\n",
      "        if is_image_file(filename):\n",
      "            return self.read_image(filename)\n",
      "        try:\n",
      "            with open(str(filename), \"r\", encoding=self.encoding) as f:\n",
      "                return f.read()\n",
      "        except FileNotFoundError:\n",
      "            self.tool_error(f\"{filename}: file not found error\")\n",
      "            return\n",
      "        except IsADirectoryError:\n",
      "            self.tool_error(f\"{filename}: is a directory\")\n",
      "            return\n",
      "        except UnicodeError as e:\n",
      "            self.tool_error(f\"{filename}: {e}\")\n",
      "            self.tool_error(\"Use --encoding to set the unicode encoding.\")\n",
      "            return\n",
      "read_text(self=<aider.io.InputOutput object at 0x7fa370858880>, filename='/tmp/tmpi945g4fh', self.chat_history_file=None, self.console=<console width=105 None>, self.dry_run=False, self.encoding='utf-8', self.input=None, self.input_history_file=None, self.output=None, self.pretty=True, self.tool_error_color='red', self.tool_output_color=None, self.user_input_color='blue', self.yes=False)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "with open(str(filename), \"r\", encoding=self.encoding) as f:\n",
      "State:\n",
      "<_io.TextIOWrapper name='/tmp/tmpi945g4fh' mode='r' encoding='utf-8'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def fmt_system_prompt(self, prompt):\n",
      "        prompt = prompt.format(fence=self.fence)\n",
      "        return prompt\n",
      "fmt_system_prompt(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa3708257c0>, prompt='Act as an expert software developer.\\nYou are diligent and tireless!\\nYou NEVER leave comments describing code without implementing it!\\nYou always COMPLETELY IMPLEMENT the needed code!\\nAlways use best practices when coding.\\nRespect and use existing conventions, libraries, etc that are already present in the code base.\\n\\nTake requests for changes to the supplied code.\\nIf the request is ambiguous, ask questions.\\n\\nOnce you understand the request you MUST:\\n1. List the files you need to modify. Only suggest changes to a *read-write* files. Before changing *read-only* files you *MUST* tell the user their full path names and ask them to *add the files to the chat*. End your reply and wait for their approval.\\n2. Think step-by-step and explain the needed changes with a numbered list of short sentences.\\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\\n\\nAll changes to files must use this *SEARCH/REPLACE block* format.\\n\\n\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "prompt = prompt.format(fence=self.fence)\n",
      "State:\n",
      "'Act as an expert software developer.\\nYou are diligent and tireless!\\nYou NEVER leave comments describing code without implementing it!\\nYou always COMPLETELY IMPLEMENT the needed code!\\nAlways use best practices when coding.\\nRespect and use existing conventions, libraries, etc that are already present in the code base.\\n\\nTake requests for changes to the supplied code.\\nIf the request is ambiguous, ask questions.\\n\\nOnce you understand the request you MUST:\\n1. List the files you need to modify. Only suggest changes to a *read-write* files. Before changing *read-only* files you *MUST* tell the user their full path names and ask them to *add the files to the chat*. End your reply and wait for their approval.\\n2. Think step-by-step and explain the needed changes with a numbered list of short sentences.\\n3. Describe each change with a *SEARCH/REPLACE block* per the examples below.\\n\\nAll changes to files must use this *SEARCH/REPLACE block* format.\\n\\n# Example conversation 1:\\n\\n## USER: These are the *read-write* files:\\n\\nmathweb/flask/app.py\\n<source>python\\nfrom flask import Flask\\n\\napp = Flask(__name__)\\n\\n@app.route(\\'/div/<int:n>/<int:d>\\')\\ndef divide(n, d):\\n    if d == 0:\\n        return \\'Error: Division by zero\\'\\n    else:\\n        result = n / d\\n        return str(result)\\n\\ndef factorial(n):\\n    \"compute factorial\"\\n\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\\n\\n@app.route(\\'/factorial/<int:n>\\')\\ndef get_factorial(n):\\n    return str(factorial(n))\\n\\nif __name__ == \\'__main__\\':\\n    print(\"Starting...\")\\n    app.run()\\n</source>\\n\\n## ASSISTANT: Ok.\\n\\n## USER: Change get_factorial() to use math.factorial\\n\\n## ASSISTANT:\\n\\nDo make this change we need to modify `mathweb/flask/app.py` to:\\n\\n1. Import the math package.\\n2. Remove the existing factorial() function.\\n3. Update get_factorial() to call math.factorial instead.\\n\\nHere are the *SEARCH/REPLACE* blocks:\\n\\nmathweb/flask/app.py\\n<source>python\\n<<<<<<< SEARCH\\nfrom flask import Flask\\n=======\\nimport math\\nfrom flask import Flask\\n>>>>>>> REPLACE\\n</source>\\n\\nmathweb/flask/app.py\\n<source>python\\n<<<<<<< SEARCH\\ndef factorial(n):\\n    \"compute factorial\"\\n\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\\n\\n=======\\n>>>>>>> REPLACE\\n</source>\\n\\nmathweb/flask/app.py\\n<source>python\\n<<<<<<< SEARCH\\n    return str(factorial(n))\\n=======\\n    return str(math.factorial(n))\\n>>>>>>> REPLACE\\n</source>\\n\\n# Example conversation 2:\\n\\n## USER: These are the *read-write* files:\\n\\nmain.py\\n<source>python\\ndef hello():\\n    \"print a greeting\"\\n\\n    print(\"hello\")\\n\\nif __name__ == \\'__main__\\':\\n    hello()\\n</source>\\n\\n## ASSISTANT: Ok.\\n\\n## USER: Refactor hello() into its own file.\\n\\n## ASSISTANT:\\n\\nDo make this change we need to modify `main.py` and make a new file `hello.py`:\\n\\n1. Make a new hello.py file with hello() in it.\\n2. Remove hello() from main.py and replace it with an import.\\n\\nHere are the *SEARCH/REPLACE* blocks:\\n\\nhello.py\\n<source>python\\n<<<<<<< SEARCH\\n=======\\ndef hello():\\n    \"print a greeting\"\\n\\n    print(\"hello\")\\n>>>>>>> REPLACE\\n</source>\\n\\nmain.py\\n<source>python\\n<<<<<<< SEARCH\\ndef hello():\\n    \"print a greeting\"\\n\\n    print(\"hello\")\\n=======\\nfrom hello import hello\\n>>>>>>> REPLACE\\n</source>\\n\\n# Rules\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_files_messages(self):\n",
      "        all_content = \"\"\n",
      "        repo_content = self.get_repo_map()\n",
      "        if repo_content:\n",
      "            if all_content:\n",
      "                all_content += \"\\n\"\n",
      "            all_content += repo_content\n",
      "        if self.abs_fnames:\n",
      "            files_content = self.gpt_prompts.files_content_prefix\n",
      "            files_content += self.get_files_content()\n",
      "        else:\n",
      "            files_content = self.gpt_prompts.files_no_full_files\n",
      "        all_content += files_content\n",
      "        files_messages = [\n",
      "            dict(role=\"user\", content=all_content),\n",
      "            dict(role=\"assistant\", content=\"Ok.\"),\n",
      "        ]\n",
      "        images_message = self.get_images_message()\n",
      "        if images_message is not None:\n",
      "            files_messages += [\n",
      "                images_message,\n",
      "                dict(role=\"assistant\", content=\"Ok.\"),\n",
      "            ]\n",
      "        return files_messages\n",
      "get_files_messages(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa3708257c0>, self.abs_fnames={'/tmp/tmpi945g4fh'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3708259d0>, self.console=<console width=105 None>, self.cur_messages=[{'role': 'user', 'content': 'hi'}], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.fence=('<source>', '</source>'), self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa370825070>, self.io=<aider.io.InputOutput object at 0x7fa370858880>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339944195216'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa370858670>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "all_content = \"\"\n",
      "State:\n",
      "''\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_files_messages(self):\n",
      "        all_content = \"\"\n",
      "        repo_content = self.get_repo_map()\n",
      "        if repo_content:\n",
      "            if all_content:\n",
      "                all_content += \"\\n\"\n",
      "            all_content += repo_content\n",
      "        if self.abs_fnames:\n",
      "            files_content = self.gpt_prompts.files_content_prefix\n",
      "            files_content += self.get_files_content()\n",
      "        else:\n",
      "            files_content = self.gpt_prompts.files_no_full_files\n",
      "        all_content += files_content\n",
      "        files_messages = [\n",
      "            dict(role=\"user\", content=all_content),\n",
      "            dict(role=\"assistant\", content=\"Ok.\"),\n",
      "        ]\n",
      "        images_message = self.get_images_message()\n",
      "        if images_message is not None:\n",
      "            files_messages += [\n",
      "                images_message,\n",
      "                dict(role=\"assistant\", content=\"Ok.\"),\n",
      "            ]\n",
      "        return files_messages\n",
      "get_files_messages(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa3708257c0>, self.abs_fnames={'/tmp/tmpi945g4fh'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3708259d0>, self.console=<console width=105 None>, self.cur_messages=[{'role': 'user', 'content': 'hi'}], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.fence=('<source>', '</source>'), self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa370825070>, self.io=<aider.io.InputOutput object at 0x7fa370858880>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339944195216'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa370858670>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "all_content += files_content\n",
      "State:\n",
      "'These are the *read-write* files:\\n\\ntmpi945g4fh\\n<source>\\nthis contains ``` backticks</source>\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_files_content(self, fnames=None):\n",
      "        if not fnames:\n",
      "            fnames = self.abs_fnames\n",
      "        prompt = \"\"\n",
      "        for fname, content in self.get_abs_fnames_content():\n",
      "            if not is_image_file(fname):\n",
      "                relative_fname = self.get_rel_fname(fname)\n",
      "                prompt += \"\\n\"\n",
      "                prompt += relative_fname\n",
      "                prompt += f\"\\n{self.fence[0]}\\n\"\n",
      "                prompt += content\n",
      "                prompt += f\"{self.fence[1]}\\n\"\n",
      "        return prompt\n",
      "get_files_content(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa3708257c0>, fnames=None, self.abs_fnames={'/tmp/tmpi945g4fh'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3708259d0>, self.console=<console width=105 None>, self.cur_messages=[{'role': 'user', 'content': 'hi'}], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.fence=('<source>', '</source>'), self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa370825070>, self.io=<aider.io.InputOutput object at 0x7fa370858880>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339944195216'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa370858670>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "fnames = self.abs_fnames\n",
      "State:\n",
      "{'/tmp/tmpi945g4fh'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_files_content(self, fnames=None):\n",
      "        if not fnames:\n",
      "            fnames = self.abs_fnames\n",
      "        prompt = \"\"\n",
      "        for fname, content in self.get_abs_fnames_content():\n",
      "            if not is_image_file(fname):\n",
      "                relative_fname = self.get_rel_fname(fname)\n",
      "                prompt += \"\\n\"\n",
      "                prompt += relative_fname\n",
      "                prompt += f\"\\n{self.fence[0]}\\n\"\n",
      "                prompt += content\n",
      "                prompt += f\"{self.fence[1]}\\n\"\n",
      "        return prompt\n",
      "get_files_content(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa3708257c0>, fnames=None, self.abs_fnames={'/tmp/tmpi945g4fh'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3708259d0>, self.console=<console width=105 None>, self.cur_messages=[{'role': 'user', 'content': 'hi'}], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.fence=('<source>', '</source>'), self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa370825070>, self.io=<aider.io.InputOutput object at 0x7fa370858880>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339944195216'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa370858670>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "prompt += f\"\\n{self.fence[0]}\\n\"\n",
      "State:\n",
      "'\\ntmpi945g4fh\\n<source>\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def update_files(self):\n",
      "        edits = self.get_edits()\n",
      "        edits = self.prepare_to_edit(edits)\n",
      "        self.apply_edits(edits)\n",
      "        return set(edit[0] for edit in edits)\n",
      "update_files(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa3708257c0>, self.abs_fnames={'/tmp/tmpi945g4fh'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3708259d0>, self.console=<console width=105 None>, self.cur_messages=[{'role': 'user', 'content': 'hi'}], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.fence=('<source>', '</source>'), self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa370825070>, self.io=<aider.io.InputOutput object at 0x7fa370858880>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.partial_response_content='ok', self.partial_response_function_call={}, self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339944195216'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa370858670>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "edits = self.get_edits()\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def find_original_update_blocks(content, fence=DEFAULT_FENCE):\n",
      "    if not content.endswith(\"\\n\"):\n",
      "        content = content + \"\\n\"\n",
      "    pieces = re.split(split_re, content)\n",
      "    pieces.reverse()\n",
      "    processed = []\n",
      "    current_filename = None\n",
      "    try:\n",
      "        while pieces:\n",
      "            cur = pieces.pop()\n",
      "            if cur in (DIVIDER, UPDATED):\n",
      "                processed.append(cur)\n",
      "                raise ValueError(f\"Unexpected {cur}\")\n",
      "            if cur.strip() != HEAD:\n",
      "                processed.append(cur)\n",
      "                continue\n",
      "            processed.append(cur)\n",
      "            filename = strip_filename(processed[-2].splitlines()[-1], fence)\n",
      "            try:\n",
      "                if not filename:\n",
      "                    filename = strip_filename(processed[-2].splitlines()[-2], fence)\n",
      "                if not filename:\n",
      "                    if current_filename:\n",
      "                        filename = current_filename\n",
      "                    else:\n",
      "                        raise ValueError(missing_filename_err)\n",
      "            except IndexError:\n",
      "                if current_filename:\n",
      "                    filename = current_filename\n",
      "                else:\n",
      "                    raise ValueError(missing_filename_err)\n",
      "            current_filename = filename\n",
      "            original_text = pieces.pop()\n",
      "            processed.append(original_text)\n",
      "            divider_marker = pieces.pop()\n",
      "            processed.append(divider_marker)\n",
      "            if divider_marker.strip() != DIVIDER:\n",
      "                raise ValueError(f\"Expected `{DIVIDER}` not {divider_marker.strip()}\")\n",
      "            updated_text = pieces.pop()\n",
      "            processed.append(updated_text)\n",
      "            updated_marker = pieces.pop()\n",
      "            processed.append(updated_marker)\n",
      "            if updated_marker.strip() != UPDATED:\n",
      "                raise ValueError(f\"Expected `{UPDATED}` not `{updated_marker.strip()}\")\n",
      "            yield filename, original_text, updated_text\n",
      "    except ValueError as e:\n",
      "        processed = \"\".join(processed)\n",
      "        err = e.args[0]\n",
      "        raise ValueError(f\"{processed}\\n^^^ {err}\")\n",
      "    except IndexError:\n",
      "        processed = \"\".join(processed)\n",
      "        raise ValueError(f\"{processed}\\n^^^ Incomplete SEARCH/REPLACE block.\")\n",
      "    except Exception:\n",
      "        processed = \"\".join(processed)\n",
      "        raise ValueError(f\"{processed}\\n^^^ Error parsing SEARCH/REPLACE block.\")\n",
      "find_original_update_blocks(content='ok', fence=('<source>', '</source>'))\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "content = content + \"\\n\"\n",
      "State:\n",
      "'ok\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def find_original_update_blocks(content, fence=DEFAULT_FENCE):\n",
      "    if not content.endswith(\"\\n\"):\n",
      "        content = content + \"\\n\"\n",
      "    pieces = re.split(split_re, content)\n",
      "    pieces.reverse()\n",
      "    processed = []\n",
      "    current_filename = None\n",
      "    try:\n",
      "        while pieces:\n",
      "            cur = pieces.pop()\n",
      "            if cur in (DIVIDER, UPDATED):\n",
      "                processed.append(cur)\n",
      "                raise ValueError(f\"Unexpected {cur}\")\n",
      "            if cur.strip() != HEAD:\n",
      "                processed.append(cur)\n",
      "                continue\n",
      "            processed.append(cur)\n",
      "            filename = strip_filename(processed[-2].splitlines()[-1], fence)\n",
      "            try:\n",
      "                if not filename:\n",
      "                    filename = strip_filename(processed[-2].splitlines()[-2], fence)\n",
      "                if not filename:\n",
      "                    if current_filename:\n",
      "                        filename = current_filename\n",
      "                    else:\n",
      "                        raise ValueError(missing_filename_err)\n",
      "            except IndexError:\n",
      "                if current_filename:\n",
      "                    filename = current_filename\n",
      "                else:\n",
      "                    raise ValueError(missing_filename_err)\n",
      "            current_filename = filename\n",
      "            original_text = pieces.pop()\n",
      "            processed.append(original_text)\n",
      "            divider_marker = pieces.pop()\n",
      "            processed.append(divider_marker)\n",
      "            if divider_marker.strip() != DIVIDER:\n",
      "                raise ValueError(f\"Expected `{DIVIDER}` not {divider_marker.strip()}\")\n",
      "            updated_text = pieces.pop()\n",
      "            processed.append(updated_text)\n",
      "            updated_marker = pieces.pop()\n",
      "            processed.append(updated_marker)\n",
      "            if updated_marker.strip() != UPDATED:\n",
      "                raise ValueError(f\"Expected `{UPDATED}` not `{updated_marker.strip()}\")\n",
      "            yield filename, original_text, updated_text\n",
      "    except ValueError as e:\n",
      "        processed = \"\".join(processed)\n",
      "        err = e.args[0]\n",
      "        raise ValueError(f\"{processed}\\n^^^ {err}\")\n",
      "    except IndexError:\n",
      "        processed = \"\".join(processed)\n",
      "        raise ValueError(f\"{processed}\\n^^^ Incomplete SEARCH/REPLACE block.\")\n",
      "    except Exception:\n",
      "        processed = \"\".join(processed)\n",
      "        raise ValueError(f\"{processed}\\n^^^ Error parsing SEARCH/REPLACE block.\")\n",
      "find_original_update_blocks(content='ok', fence=('<source>', '</source>'))\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "pieces = re.split(split_re, content)\n",
      "State:\n",
      "['ok\\n']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def find_original_update_blocks(content, fence=DEFAULT_FENCE):\n",
      "    if not content.endswith(\"\\n\"):\n",
      "        content = content + \"\\n\"\n",
      "    pieces = re.split(split_re, content)\n",
      "    pieces.reverse()\n",
      "    processed = []\n",
      "    current_filename = None\n",
      "    try:\n",
      "        while pieces:\n",
      "            cur = pieces.pop()\n",
      "            if cur in (DIVIDER, UPDATED):\n",
      "                processed.append(cur)\n",
      "                raise ValueError(f\"Unexpected {cur}\")\n",
      "            if cur.strip() != HEAD:\n",
      "                processed.append(cur)\n",
      "                continue\n",
      "            processed.append(cur)\n",
      "            filename = strip_filename(processed[-2].splitlines()[-1], fence)\n",
      "            try:\n",
      "                if not filename:\n",
      "                    filename = strip_filename(processed[-2].splitlines()[-2], fence)\n",
      "                if not filename:\n",
      "                    if current_filename:\n",
      "                        filename = current_filename\n",
      "                    else:\n",
      "                        raise ValueError(missing_filename_err)\n",
      "            except IndexError:\n",
      "                if current_filename:\n",
      "                    filename = current_filename\n",
      "                else:\n",
      "                    raise ValueError(missing_filename_err)\n",
      "            current_filename = filename\n",
      "            original_text = pieces.pop()\n",
      "            processed.append(original_text)\n",
      "            divider_marker = pieces.pop()\n",
      "            processed.append(divider_marker)\n",
      "            if divider_marker.strip() != DIVIDER:\n",
      "                raise ValueError(f\"Expected `{DIVIDER}` not {divider_marker.strip()}\")\n",
      "            updated_text = pieces.pop()\n",
      "            processed.append(updated_text)\n",
      "            updated_marker = pieces.pop()\n",
      "            processed.append(updated_marker)\n",
      "            if updated_marker.strip() != UPDATED:\n",
      "                raise ValueError(f\"Expected `{UPDATED}` not `{updated_marker.strip()}\")\n",
      "            yield filename, original_text, updated_text\n",
      "    except ValueError as e:\n",
      "        processed = \"\".join(processed)\n",
      "        err = e.args[0]\n",
      "        raise ValueError(f\"{processed}\\n^^^ {err}\")\n",
      "    except IndexError:\n",
      "        processed = \"\".join(processed)\n",
      "        raise ValueError(f\"{processed}\\n^^^ Incomplete SEARCH/REPLACE block.\")\n",
      "    except Exception:\n",
      "        processed = \"\".join(processed)\n",
      "        raise ValueError(f\"{processed}\\n^^^ Error parsing SEARCH/REPLACE block.\")\n",
      "find_original_update_blocks(content='ok', fence=('<source>', '</source>'))\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "cur = pieces.pop()\n",
      "State:\n",
      "'ok\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def prepare_to_edit(self, edits):\n",
      "        res = []\n",
      "        seen = dict()\n",
      "        self.need_commit_before_edits = set()\n",
      "        for edit in edits:\n",
      "            path = edit[0]\n",
      "            if path in seen:\n",
      "                allowed = seen[path]\n",
      "            else:\n",
      "                allowed = self.allowed_to_edit(path)\n",
      "                seen[path] = allowed\n",
      "            if allowed:\n",
      "                res.append(edit)\n",
      "        self.dirty_commit()\n",
      "        self.need_commit_before_edits = set()\n",
      "        return res\n",
      "prepare_to_edit(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa3708257c0>, edits=[], self.abs_fnames={'/tmp/tmpi945g4fh'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3708259d0>, self.console=<console width=105 None>, self.cur_messages=[{'role': 'user', 'content': 'hi'}], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.fence=('<source>', '</source>'), self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa370825070>, self.io=<aider.io.InputOutput object at 0x7fa370858880>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.partial_response_content='ok', self.partial_response_function_call={}, self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339944195216'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa370858670>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "res = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def prepare_to_edit(self, edits):\n",
      "        res = []\n",
      "        seen = dict()\n",
      "        self.need_commit_before_edits = set()\n",
      "        for edit in edits:\n",
      "            path = edit[0]\n",
      "            if path in seen:\n",
      "                allowed = seen[path]\n",
      "            else:\n",
      "                allowed = self.allowed_to_edit(path)\n",
      "                seen[path] = allowed\n",
      "            if allowed:\n",
      "                res.append(edit)\n",
      "        self.dirty_commit()\n",
      "        self.need_commit_before_edits = set()\n",
      "        return res\n",
      "prepare_to_edit(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa3708257c0>, edits=[], self.abs_fnames={'/tmp/tmpi945g4fh'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3708259d0>, self.console=<console width=105 None>, self.cur_messages=[{'role': 'user', 'content': 'hi'}], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.fence=('<source>', '</source>'), self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa370825070>, self.io=<aider.io.InputOutput object at 0x7fa370858880>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.partial_response_content='ok', self.partial_response_function_call={}, self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339944195216'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa370858670>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "seen = dict()\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def apply_edits(self, edits):\n",
      "        for path, original, updated in edits:\n",
      "            full_path = self.abs_root_path(path)\n",
      "            content = self.io.read_text(full_path)\n",
      "            content = do_replace(full_path, content, original, updated, self.fence)\n",
      "            if content:\n",
      "                self.io.write_text(full_path, content)\n",
      "                continue\n",
      "            raise ValueError(f\"\"\"InvalidEditBlock: edit failed!\n",
      "{path} does not contain the *exact chunk* of SEARCH lines you specified.\n",
      "Try again.\n",
      "DO NOT skip blank lines, comments, docstrings, etc!\n",
      "The SEARCH block needs to be EXACTLY the same as the lines in {path} with nothing missing!\n",
      "{path} does not contain these {len(original.splitlines())} exact lines in a row:\n",
      "```\n",
      "{original}```\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "full_path = self.abs_root_path(path)\n",
      "State:\n",
      "'/tmp/tmp7g7a2csg/file.txt'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def apply_edits(self, edits):\n",
      "        for path, original, updated in edits:\n",
      "            full_path = self.abs_root_path(path)\n",
      "            content = self.io.read_text(full_path)\n",
      "            content = do_replace(full_path, content, original, updated, self.fence)\n",
      "            if content:\n",
      "                self.io.write_text(full_path, content)\n",
      "                continue\n",
      "            raise ValueError(f\"\"\"InvalidEditBlock: edit failed!\n",
      "{path} does not contain the *exact chunk* of SEARCH lines you specified.\n",
      "Try again.\n",
      "DO NOT skip blank lines, comments, docstrings, etc!\n",
      "The SEARCH block needs to be EXACTLY the same as the lines in {path} with nothing missing!\n",
      "{path} does not contain these {len(original.splitlines())} exact lines in a row:\n",
      "```\n",
      "{original}```\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "content = self.io.read_text(full_path)\n",
      "State:\n",
      "'two\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def apply_edits(self, edits):\n",
      "        for path, original, updated in edits:\n",
      "            full_path = self.abs_root_path(path)\n",
      "            content = self.io.read_text(full_path)\n",
      "            content = do_replace(full_path, content, original, updated, self.fence)\n",
      "            if content:\n",
      "                self.io.write_text(full_path, content)\n",
      "                continue\n",
      "            raise ValueError(f\"\"\"InvalidEditBlock: edit failed!\n",
      "{path} does not contain the *exact chunk* of SEARCH lines you specified.\n",
      "Try again.\n",
      "DO NOT skip blank lines, comments, docstrings, etc!\n",
      "The SEARCH block needs to be EXACTLY the same as the lines in {path} with nothing missing!\n",
      "{path} does not contain these {len(original.splitlines())} exact lines in a row:\n",
      "```\n",
      "{original}```\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "content = do_replace(full_path, content, original, updated, self.fence)\n",
      "State:\n",
      "'three\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def update_cur_messages(self, edited):\n",
      "        if self.partial_response_content:\n",
      "            self.cur_messages += [dict(role=\"assistant\", content=self.partial_response_content)]\n",
      "        if self.partial_response_function_call:\n",
      "            self.cur_messages += [\n",
      "                dict(\n",
      "                    role=\"assistant\",\n",
      "                    content=None,\n",
      "                    function_call=self.partial_response_function_call,\n",
      "                )\n",
      "            ]\n",
      "update_cur_messages(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa3708257c0>, edited=set(), self.abs_fnames={'/tmp/tmpi945g4fh'}, self.apply_update_errors=0, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3708259d0>, self.console=<console width=105 None>, self.cur_messages=[{'role': 'user', 'content': 'hi'}], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.fence=('<source>', '</source>'), self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa370825070>, self.io=<aider.io.InputOutput object at 0x7fa370858880>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.partial_response_content='ok', self.partial_response_function_call={}, self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339944195216'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa370858670>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.cur_messages += [dict(role=\"assistant\", content=self.partial_response_content)]\n",
      "State:\n",
      "[{'role': 'user', 'content': 'hi'}, {'role': 'assistant', 'content': 'ok'}]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_all_abs_files(self):\n",
      "        files = self.get_all_relative_files()\n",
      "        files = [self.abs_root_path(path) for path in files]\n",
      "        return files\n",
      "get_all_abs_files(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa37061eca0>, self.abs_fnames=set(), self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3705c8070>, self.console=<console width=105 None>, self.cur_messages=[], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa37061ed00>, self.io=<MagicMock id='140339941841408'>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.pretty=True, self.repo=<aider.repo.GitRepo object at 0x7fa3705c8c70>, self.repo_map=<aider.repomap.RepoMap object at 0x7fa3706e6100>, self.root='/tmp/tmpum7kw0_8', self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa3706e6970>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "files = self.get_all_relative_files()\n",
      "State:\n",
      "['new.txt']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_ranked_tags_map(self, chat_fnames, other_fnames=None):\n",
      "        if not other_fnames:\n",
      "            other_fnames = list()\n",
      "        ranked_tags = self.get_ranked_tags(chat_fnames, other_fnames)\n",
      "        num_tags = len(ranked_tags)\n",
      "        lower_bound = 0\n",
      "        upper_bound = num_tags\n",
      "        best_tree = None\n",
      "        chat_rel_fnames = [self.get_rel_fname(fname) for fname in chat_fnames]\n",
      "        while lower_bound <= upper_bound:\n",
      "            middle = (lower_bound + upper_bound) // 2\n",
      "            tree = self.to_tree(ranked_tags[:middle], chat_rel_fnames)\n",
      "            num_tokens = self.token_count(tree)\n",
      "            if num_tokens < self.max_map_tokens:\n",
      "                best_tree = tree\n",
      "                lower_bound = middle + 1\n",
      "            else:\n",
      "                upper_bound = middle - 1\n",
      "        return best_tree\n",
      "get_ranked_tags_map(self=<aider.repomap.RepoMap object at 0x7fa37050fc70>, chat_fnames={'/tmp/tmp7g7a2csg/file.txt'}, other_fnames={'/tmp/tmp7g7a2csg/other.txt'}, self.TAGS_CACHE=<diskcache.core.Cache object at 0x7fa37050f520>, self.cache_missing=True, self.io=<aider.io.InputOutput object at 0x7fa37050f700>, self.max_map_tokens=1024, self.repo_content_prefix=\"Below here are summaries of files present in the user's git repository.\\nDo not propose changes to these files, they are *read-only*.\\nTo make a file *read-write*, ask the user to *add it to the chat*.\\n\", self.root='/tmp/tmp7g7a2csg', self.tokenizer=<Encoding 'cl100k_base'>, self.verbose=False)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "ranked_tags = self.get_ranked_tags(chat_fnames, other_fnames)\n",
      "State:\n",
      "[('other.txt',)]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_ranked_tags_map(self, chat_fnames, other_fnames=None):\n",
      "        if not other_fnames:\n",
      "            other_fnames = list()\n",
      "        ranked_tags = self.get_ranked_tags(chat_fnames, other_fnames)\n",
      "        num_tags = len(ranked_tags)\n",
      "        lower_bound = 0\n",
      "        upper_bound = num_tags\n",
      "        best_tree = None\n",
      "        chat_rel_fnames = [self.get_rel_fname(fname) for fname in chat_fnames]\n",
      "        while lower_bound <= upper_bound:\n",
      "            middle = (lower_bound + upper_bound) // 2\n",
      "            tree = self.to_tree(ranked_tags[:middle], chat_rel_fnames)\n",
      "            num_tokens = self.token_count(tree)\n",
      "            if num_tokens < self.max_map_tokens:\n",
      "                best_tree = tree\n",
      "                lower_bound = middle + 1\n",
      "            else:\n",
      "                upper_bound = middle - 1\n",
      "        return best_tree\n",
      "get_ranked_tags_map(self=<aider.repomap.RepoMap object at 0x7fa37050fc70>, chat_fnames={'/tmp/tmp7g7a2csg/file.txt'}, other_fnames={'/tmp/tmp7g7a2csg/other.txt'}, self.TAGS_CACHE=<diskcache.core.Cache object at 0x7fa37050f520>, self.cache_missing=True, self.io=<aider.io.InputOutput object at 0x7fa37050f700>, self.max_map_tokens=1024, self.repo_content_prefix=\"Below here are summaries of files present in the user's git repository.\\nDo not propose changes to these files, they are *read-only*.\\nTo make a file *read-write*, ask the user to *add it to the chat*.\\n\", self.root='/tmp/tmp7g7a2csg', self.tokenizer=<Encoding 'cl100k_base'>, self.verbose=False)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "num_tags = len(ranked_tags)\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_ranked_tags(self, chat_fnames, other_fnames):\n",
      "        defines = defaultdict(set)\n",
      "        references = defaultdict(list)\n",
      "        definitions = defaultdict(set)\n",
      "        personalization = dict()\n",
      "        fnames = set(chat_fnames).union(set(other_fnames))\n",
      "        chat_rel_fnames = set()\n",
      "        fnames = sorted(fnames)\n",
      "        if self.cache_missing:\n",
      "            fnames = tqdm(fnames)\n",
      "        self.cache_missing = False\n",
      "        for fname in fnames:\n",
      "            if not Path(fname).is_file():\n",
      "                if fname not in self.warned_files:\n",
      "                    if Path(fname).exists():\n",
      "                        self.io.tool_error(\n",
      "                            f\"Repo-map can't include {fname}, it is not a normal file\"\n",
      "                        )\n",
      "                    else:\n",
      "                        self.io.tool_error(f\"Repo-map can't include {fname}, it no longer exists\")\n",
      "                self.warned_files.add(fname)\n",
      "                continue\n",
      "            rel_fname = self.get_rel_fname(fname)\n",
      "            if fname in chat_fnames:\n",
      "                personalization[rel_fname] = 1.0\n",
      "                chat_rel_fnames.add(rel_fname)\n",
      "            tags = list(self.get_tags(fname, rel_fname))\n",
      "            if tags is None:\n",
      "                continue\n",
      "            for tag in tags:\n",
      "                if tag.kind == \"def\":\n",
      "                    defines[tag.name].add(rel_fname)\n",
      "                    key = (rel_fname, tag.name)\n",
      "                    definitions[key].add(tag)\n",
      "                if tag.kind == \"ref\":\n",
      "                    references[tag.name].append(rel_fname)\n",
      "        if not references:\n",
      "            references = dict((k, list(v)) for k, v in defines.items())\n",
      "        idents = set(defines.keys()).intersection(set(references.keys()))\n",
      "        G = nx.MultiDiGraph()\n",
      "        for ident in idents:\n",
      "            definers = defines[ident]\n",
      "            for referencer, num_refs in Counter(references[ident]).items():\n",
      "                for definer in definers:\n",
      "                    G.add_edge(referencer, definer, weight=num_refs, ident=ident)\n",
      "        if not references:\n",
      "            pass\n",
      "        if personalization:\n",
      "            pers_args = dict(personalization=personalization, dangling=personalization)\n",
      "        else:\n",
      "            pers_args = dict()\n",
      "        try:\n",
      "            ranked = nx.pagerank(G, weight=\"weight\", **pers_args)\n",
      "        except ZeroDivisionError:\n",
      "            return []\n",
      "        ranked_definitions = defaultdict(float)\n",
      "        for src in G.nodes:\n",
      "            src_rank = ranked[src]\n",
      "            total_weight = sum(data[\"weight\"] for _src, _dst, data in G.out_edges(src, data=True))\n",
      "            for _src, dst, data in G.out_edges(src, data=True):\n",
      "                data[\"rank\"] = src_rank * data[\"weight\"] / total_weight\n",
      "                ident = data[\"ident\"]\n",
      "                ranked_definitions[(dst, ident)] += data[\"rank\"]\n",
      "        ranked_tags = []\n",
      "        ranked_definitions = sorted(ranked_definitions.items(), reverse=True, key=lambda x: x[1])\n",
      "        for (fname, ident), rank in ranked_definitions:\n",
      "            if fname in chat_rel_fnames:\n",
      "                continue\n",
      "            ranked_tags += list(definitions.get((fname, ident), []))\n",
      "        rel_other_fnames_without_tags = set(self.get_rel_fname(fname) for fname in other_fnames)\n",
      "        fnames_already_included = set(rt[0] for rt in ranked_tags)\n",
      "        top_rank = sorted([(rank, node) for (node, rank) in ranked.items()], reverse=True)\n",
      "        for rank, fname in top_rank:\n",
      "            if fname in rel_other_fnames_without_tags:\n",
      "                rel_other_fnames_without_tags.remove(fname)\n",
      "            if fname not in fnames_already_included:\n",
      "                ranked_tags.append((fname,))\n",
      "        for fname in rel_other_fnames_without_tags:\n",
      "            ranked_tags.append((fname,))\n",
      "        return ranked_tags\n",
      "get_ranked_tags(self=<aider.repomap.RepoMap object at 0x7fa37050fc70>, chat_fnames={'/tmp/tmp7g7a2csg/file.txt'}, other_fnames={'/tmp/tmp7g7a2csg/other.txt'}, self.TAGS_CACHE=<diskcache.core.Cache object at 0x7fa37050f520>, self.cache_missing=True, self.io=<aider.io.InputOutput object at 0x7fa37050f700>, self.max_map_tokens=1024, self.repo_content_prefix=\"Below here are summaries of files present in the user's git repository.\\nDo not propose changes to these files, they are *read-only*.\\nTo make a file *read-write*, ask the user to *add it to the chat*.\\n\", self.root='/tmp/tmp7g7a2csg', self.tokenizer=<Encoding 'cl100k_base'>, self.verbose=False)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "personalization = dict()\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_ranked_tags(self, chat_fnames, other_fnames):\n",
      "        defines = defaultdict(set)\n",
      "        references = defaultdict(list)\n",
      "        definitions = defaultdict(set)\n",
      "        personalization = dict()\n",
      "        fnames = set(chat_fnames).union(set(other_fnames))\n",
      "        chat_rel_fnames = set()\n",
      "        fnames = sorted(fnames)\n",
      "        if self.cache_missing:\n",
      "            fnames = tqdm(fnames)\n",
      "        self.cache_missing = False\n",
      "        for fname in fnames:\n",
      "            if not Path(fname).is_file():\n",
      "                if fname not in self.warned_files:\n",
      "                    if Path(fname).exists():\n",
      "                        self.io.tool_error(\n",
      "                            f\"Repo-map can't include {fname}, it is not a normal file\"\n",
      "                        )\n",
      "                    else:\n",
      "                        self.io.tool_error(f\"Repo-map can't include {fname}, it no longer exists\")\n",
      "                self.warned_files.add(fname)\n",
      "                continue\n",
      "            rel_fname = self.get_rel_fname(fname)\n",
      "            if fname in chat_fnames:\n",
      "                personalization[rel_fname] = 1.0\n",
      "                chat_rel_fnames.add(rel_fname)\n",
      "            tags = list(self.get_tags(fname, rel_fname))\n",
      "            if tags is None:\n",
      "                continue\n",
      "            for tag in tags:\n",
      "                if tag.kind == \"def\":\n",
      "                    defines[tag.name].add(rel_fname)\n",
      "                    key = (rel_fname, tag.name)\n",
      "                    definitions[key].add(tag)\n",
      "                if tag.kind == \"ref\":\n",
      "                    references[tag.name].append(rel_fname)\n",
      "        if not references:\n",
      "            references = dict((k, list(v)) for k, v in defines.items())\n",
      "        idents = set(defines.keys()).intersection(set(references.keys()))\n",
      "        G = nx.MultiDiGraph()\n",
      "        for ident in idents:\n",
      "            definers = defines[ident]\n",
      "            for referencer, num_refs in Counter(references[ident]).items():\n",
      "                for definer in definers:\n",
      "                    G.add_edge(referencer, definer, weight=num_refs, ident=ident)\n",
      "        if not references:\n",
      "            pass\n",
      "        if personalization:\n",
      "            pers_args = dict(personalization=personalization, dangling=personalization)\n",
      "        else:\n",
      "            pers_args = dict()\n",
      "        try:\n",
      "            ranked = nx.pagerank(G, weight=\"weight\", **pers_args)\n",
      "        except ZeroDivisionError:\n",
      "            return []\n",
      "        ranked_definitions = defaultdict(float)\n",
      "        for src in G.nodes:\n",
      "            src_rank = ranked[src]\n",
      "            total_weight = sum(data[\"weight\"] for _src, _dst, data in G.out_edges(src, data=True))\n",
      "            for _src, dst, data in G.out_edges(src, data=True):\n",
      "                data[\"rank\"] = src_rank * data[\"weight\"] / total_weight\n",
      "                ident = data[\"ident\"]\n",
      "                ranked_definitions[(dst, ident)] += data[\"rank\"]\n",
      "        ranked_tags = []\n",
      "        ranked_definitions = sorted(ranked_definitions.items(), reverse=True, key=lambda x: x[1])\n",
      "        for (fname, ident), rank in ranked_definitions:\n",
      "            if fname in chat_rel_fnames:\n",
      "                continue\n",
      "            ranked_tags += list(definitions.get((fname, ident), []))\n",
      "        rel_other_fnames_without_tags = set(self.get_rel_fname(fname) for fname in other_fnames)\n",
      "        fnames_already_included = set(rt[0] for rt in ranked_tags)\n",
      "        top_rank = sorted([(rank, node) for (node, rank) in ranked.items()], reverse=True)\n",
      "        for rank, fname in top_rank:\n",
      "            if fname in rel_other_fnames_without_tags:\n",
      "                rel_other_fnames_without_tags.remove(fname)\n",
      "            if fname not in fnames_already_included:\n",
      "                ranked_tags.append((fname,))\n",
      "        for fname in rel_other_fnames_without_tags:\n",
      "            ranked_tags.append((fname,))\n",
      "        return ranked_tags\n",
      "get_ranked_tags(self=<aider.repomap.RepoMap object at 0x7fa37050fc70>, chat_fnames={'/tmp/tmp7g7a2csg/file.txt'}, other_fnames={'/tmp/tmp7g7a2csg/other.txt'}, self.TAGS_CACHE=<diskcache.core.Cache object at 0x7fa37050f520>, self.cache_missing=True, self.io=<aider.io.InputOutput object at 0x7fa37050f700>, self.max_map_tokens=1024, self.repo_content_prefix=\"Below here are summaries of files present in the user's git repository.\\nDo not propose changes to these files, they are *read-only*.\\nTo make a file *read-write*, ask the user to *add it to the chat*.\\n\", self.root='/tmp/tmp7g7a2csg', self.tokenizer=<Encoding 'cl100k_base'>, self.verbose=False)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "fnames = sorted(fnames)\n",
      "State:\n",
      "['/tmp/tmp7g7a2csg/file.txt', '/tmp/tmp7g7a2csg/other.txt']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_tags_raw(self, fname, rel_fname):\n",
      "        lang = filename_to_lang(fname)\n",
      "        if not lang:\n",
      "            return\n",
      "        language = get_language(lang)\n",
      "        parser = get_parser(lang)\n",
      "        scm_fname = pkg_resources.resource_filename(\n",
      "            __name__, os.path.join(\"queries\", f\"tree-sitter-{lang}-tags.scm\")\n",
      "        )\n",
      "        query_scm = Path(scm_fname)\n",
      "        if not query_scm.exists():\n",
      "            return\n",
      "        query_scm = query_scm.read_text()\n",
      "        code = self.io.read_text(fname)\n",
      "        if not code:\n",
      "            return\n",
      "        tree = parser.parse(bytes(code, \"utf-8\"))\n",
      "        query = language.query(query_scm)\n",
      "        captures = query.captures(tree.root_node)\n",
      "        captures = list(captures)\n",
      "        saw = set()\n",
      "        for node, tag in captures:\n",
      "            if tag.startswith(\"name.definition.\"):\n",
      "                kind = \"def\"\n",
      "            elif tag.startswith(\"name.reference.\"):\n",
      "                kind = \"ref\"\n",
      "            else:\n",
      "                continue\n",
      "            saw.add(kind)\n",
      "            result = Tag(\n",
      "                rel_fname=rel_fname,\n",
      "                fname=fname,\n",
      "                name=node.text.decode(\"utf-8\"),\n",
      "                kind=kind,\n",
      "                line=node.start_point[0],\n",
      "            )\n",
      "            yield result\n",
      "        if \"ref\" in saw:\n",
      "            return\n",
      "        if \"def\" not in saw:\n",
      "            return\n",
      "        try:\n",
      "            lexer = guess_lexer_for_filename(fname, code)\n",
      "        except ClassNotFound:\n",
      "            return\n",
      "        tokens = list(lexer.get_tokens(code))\n",
      "        tokens = [token[1] for token in tokens if token[0] in Token.Name]\n",
      "        for token in tokens:\n",
      "            yield Tag(\n",
      "                rel_fname=rel_fname,\n",
      "                fname=fname,\n",
      "                name=token,\n",
      "                kind=\"ref\",\n",
      "                line=-1,\n",
      "            )\n",
      "get_tags_raw(self=<aider.repomap.RepoMap object at 0x7fa37050fc70>, fname='/tmp/tmp7g7a2csg/file.txt', rel_fname='file.txt', self.TAGS_CACHE=<diskcache.core.Cache object at 0x7fa37050f520>, self.cache_missing=False, self.io=<aider.io.InputOutput object at 0x7fa37050f700>, self.max_map_tokens=1024, self.repo_content_prefix=\"Below here are summaries of files present in the user's git repository.\\nDo not propose changes to these files, they are *read-only*.\\nTo make a file *read-write*, ask the user to *add it to the chat*.\\n\", self.root='/tmp/tmp7g7a2csg', self.tokenizer=<Encoding 'cl100k_base'>, self.verbose=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "lang = filename_to_lang(fname)\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def to_tree(self, tags, chat_rel_fnames):\n",
      "        if not tags:\n",
      "            return \"\"\n",
      "        tags = [tag for tag in tags if tag[0] not in chat_rel_fnames]\n",
      "        tags = sorted(tags)\n",
      "        cur_fname = None\n",
      "        context = None\n",
      "        output = \"\"\n",
      "        dummy_tag = (None,)\n",
      "        for tag in tags + [dummy_tag]:\n",
      "            this_rel_fname = tag[0]\n",
      "            if this_rel_fname != cur_fname:\n",
      "                if context:\n",
      "                    context.add_context()\n",
      "                    output += \"\\n\"\n",
      "                    output += cur_fname + \":\\n\"\n",
      "                    output += context.format()\n",
      "                    context = None\n",
      "                elif cur_fname:\n",
      "                    output += \"\\n\" + cur_fname + \"\\n\"\n",
      "                if type(tag) is Tag:\n",
      "                    code = self.io.read_text(tag.fname) or \"\"\n",
      "                    context = TreeContext(\n",
      "                        tag.rel_fname,\n",
      "                        code,\n",
      "                        color=False,\n",
      "                        line_number=False,\n",
      "                        child_context=False,\n",
      "                        last_line=False,\n",
      "                        margin=0,\n",
      "                        mark_lois=False,\n",
      "                        loi_pad=0,\n",
      "                        show_top_of_file_parent_scope=False,\n",
      "                    )\n",
      "                cur_fname = this_rel_fname\n",
      "            if context:\n",
      "                context.add_lines_of_interest([tag.line])\n",
      "        return output\n",
      "to_tree(self=<aider.repomap.RepoMap object at 0x7fa37050fc70>, tags=[('other.txt',)], chat_rel_fnames=['file.txt'], self.TAGS_CACHE=<diskcache.core.Cache object at 0x7fa37050f520>, self.cache_missing=False, self.io=<aider.io.InputOutput object at 0x7fa37050f700>, self.max_map_tokens=1024, self.repo_content_prefix=\"Below here are summaries of files present in the user's git repository.\\nDo not propose changes to these files, they are *read-only*.\\nTo make a file *read-write*, ask the user to *add it to the chat*.\\n\", self.root='/tmp/tmp7g7a2csg', self.tokenizer=<Encoding 'cl100k_base'>, self.verbose=False)\n",
      "\n",
      "n:\n",
      "25\n",
      "Statement:\n",
      "output += \"\\n\" + cur_fname + \"\\n\"\n",
      "State:\n",
      "'\\nother.txt\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def commit(self, fnames=None, context=None, prefix=None, message=None):\n",
      "        if not fnames and not self.repo.is_dirty():\n",
      "            return\n",
      "        diffs = self.get_diffs(fnames)\n",
      "        if not diffs:\n",
      "            return\n",
      "        if message:\n",
      "            commit_message = message\n",
      "        else:\n",
      "            commit_message = self.get_commit_message(diffs, context)\n",
      "        if not commit_message:\n",
      "            commit_message = \"(no commit message provided)\"\n",
      "        if prefix:\n",
      "            commit_message = prefix + commit_message\n",
      "        full_commit_message = commit_message\n",
      "        if context:\n",
      "            full_commit_message += \"\\n\\n\n",
      "        cmd = [\"-m\", full_commit_message, \"--no-verify\"]\n",
      "        if fnames:\n",
      "            fnames = [str(self.abs_root_path(fn)) for fn in fnames]\n",
      "            for fname in fnames:\n",
      "                self.repo.git.add(fname)\n",
      "            cmd += [\"--\"] + fnames\n",
      "        else:\n",
      "            cmd += [\"-a\"]\n",
      "        self.repo.git.commit(cmd)\n",
      "        commit_hash = self.repo.head.commit.hexsha[:7]\n",
      "        self.io.tool_output(f\"Commit {commit_hash} {commit_message}\")\n",
      "        return commit_hash, commit_message\n",
      "commit(self=<aider.repo.GitRepo object at 0x7fa3706e6040>, fnames={'file.txt'}, context=None, prefix=None, message=None, self.client=None, self.get_commit_message=<MagicMock id='140339940750912'>, self.io=<aider.io.InputOutput object at 0x7fa37050f700>, self.repo=<git.repo.base.Repo '/tmp/tmp7g7a2csg/.git'>, self.root='/tmp/tmp7g7a2csg')\n",
      "\n",
      "n:\n",
      "25\n",
      "Statement:\n",
      "fnames = [str(self.abs_root_path(fn)) for fn in fnames]\n",
      "State:\n",
      "['/tmp/tmp7g7a2csg/file.txt']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def do_replace(fname, content, before_text, after_text, fence=None):\n",
      "    before_text = strip_quoted_wrapping(before_text, fname, fence)\n",
      "    after_text = strip_quoted_wrapping(after_text, fname, fence)\n",
      "    fname = Path(fname)\n",
      "    if not fname.exists() and not before_text.strip():\n",
      "        fname.touch()\n",
      "        content = \"\"\n",
      "    if content is None:\n",
      "        return\n",
      "    if not before_text.strip():\n",
      "        new_content = content + after_text\n",
      "    else:\n",
      "        new_content = replace_most_similar_chunk(content, before_text, after_text)\n",
      "    return new_content\n",
      "do_replace(fname='/tmp/tmp7g7a2csg/file.txt', content='two\\n', before_text='two\\n', after_text='three\\n', fence=('```', '```'))\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "fname = Path(fname)\n",
      "State:\n",
      "PosixPath('/tmp/tmp7g7a2csg/file.txt')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def replace_most_similar_chunk(whole, part, replace):\n",
      "    whole, whole_lines = prep(whole)\n",
      "    part, part_lines = prep(part)\n",
      "    replace, replace_lines = prep(replace)\n",
      "    res = perfect_or_whitespace(whole_lines, part_lines, replace_lines)\n",
      "    if res:\n",
      "        return res\n",
      "    if len(part_lines) > 2 and not part_lines[0].strip():\n",
      "        skip_blank_line_part_lines = part_lines[1:]\n",
      "        res = perfect_or_whitespace(whole_lines, skip_blank_line_part_lines, replace_lines)\n",
      "        if res:\n",
      "            return res\n",
      "    try:\n",
      "        res = try_dotdotdots(whole, part, replace)\n",
      "        if res:\n",
      "            return res\n",
      "    except ValueError:\n",
      "        pass\n",
      "    return\n",
      "    res = replace_closest_edit_distance(whole_lines, part, part_lines, replace_lines)\n",
      "    if res:\n",
      "        return res\n",
      "replace_most_similar_chunk(whole='two\\n', part='two\\n', replace='three\\n')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "whole, whole_lines = prep(whole)\n",
      "State:\n",
      "['two\\n']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def replace_most_similar_chunk(whole, part, replace):\n",
      "    whole, whole_lines = prep(whole)\n",
      "    part, part_lines = prep(part)\n",
      "    replace, replace_lines = prep(replace)\n",
      "    res = perfect_or_whitespace(whole_lines, part_lines, replace_lines)\n",
      "    if res:\n",
      "        return res\n",
      "    if len(part_lines) > 2 and not part_lines[0].strip():\n",
      "        skip_blank_line_part_lines = part_lines[1:]\n",
      "        res = perfect_or_whitespace(whole_lines, skip_blank_line_part_lines, replace_lines)\n",
      "        if res:\n",
      "            return res\n",
      "    try:\n",
      "        res = try_dotdotdots(whole, part, replace)\n",
      "        if res:\n",
      "            return res\n",
      "    except ValueError:\n",
      "        pass\n",
      "    return\n",
      "    res = replace_closest_edit_distance(whole_lines, part, part_lines, replace_lines)\n",
      "    if res:\n",
      "        return res\n",
      "replace_most_similar_chunk(whole='two\\n', part='two\\n', replace='three\\n')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "part, part_lines = prep(part)\n",
      "State:\n",
      "['two\\n']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def perfect_replace(whole_lines, part_lines, replace_lines):\n",
      "    part_tup = tuple(part_lines)\n",
      "    part_len = len(part_lines)\n",
      "    for i in range(len(whole_lines) - part_len + 1):\n",
      "        whole_tup = tuple(whole_lines[i : i + part_len])\n",
      "        if part_tup == whole_tup:\n",
      "            res = whole_lines[:i] + replace_lines + whole_lines[i + part_len :]\n",
      "            return \"\".join(res)\n",
      "perfect_replace(whole_lines=['two\\n'], part_lines=['two\\n'], replace_lines=['three\\n'])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "part_tup = tuple(part_lines)\n",
      "State:\n",
      "('two\\n',)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def perfect_replace(whole_lines, part_lines, replace_lines):\n",
      "    part_tup = tuple(part_lines)\n",
      "    part_len = len(part_lines)\n",
      "    for i in range(len(whole_lines) - part_len + 1):\n",
      "        whole_tup = tuple(whole_lines[i : i + part_len])\n",
      "        if part_tup == whole_tup:\n",
      "            res = whole_lines[:i] + replace_lines + whole_lines[i + part_len :]\n",
      "            return \"\".join(res)\n",
      "perfect_replace(whole_lines=['two\\n'], part_lines=['two\\n'], replace_lines=['three\\n'])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "part_len = len(part_lines)\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def perfect_replace(whole_lines, part_lines, replace_lines):\n",
      "    part_tup = tuple(part_lines)\n",
      "    part_len = len(part_lines)\n",
      "    for i in range(len(whole_lines) - part_len + 1):\n",
      "        whole_tup = tuple(whole_lines[i : i + part_len])\n",
      "        if part_tup == whole_tup:\n",
      "            res = whole_lines[:i] + replace_lines + whole_lines[i + part_len :]\n",
      "            return \"\".join(res)\n",
      "perfect_replace(whole_lines=['two\\n'], part_lines=['two\\n'], replace_lines=['three\\n'])\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "for i in range(len(whole_lines) - part_len + 1):\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_context_from_history(self, history):\n",
      "        context = \"\"\n",
      "        if history:\n",
      "            for msg in history:\n",
      "                context += \"\\n\" + msg[\"role\"].upper() + \": \" + msg[\"content\"] + \"\\n\"\n",
      "        return context\n",
      "get_context_from_history(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa37050f1f0>, history=[{'role': 'user', 'content': 'hi'}, {'role': 'assistant', 'content': '\\nDo this:\\n\\nfile.txt\\n<<<<<<< SEARCH\\ntwo\\n=======\\nthree\\n>>>>>>> REPLACE\\n\\n'}], self.abs_fnames={'/tmp/tmp7g7a2csg/file.txt'}, self.apply_update_errors=0, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3706e6910>, self.console=<console width=105 None>, self.cur_messages=[{'role': 'user', 'content': 'hi'}, {'role': 'assistant', 'content': '\\nDo this:\\n\\nfile.txt\\n<<<<<<< SEARCH\\ntwo\\n=======\\nthree\\n>>>>>>> REPLACE\\n\\n'}], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.fence=('```', '```'), self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa37050f280>, self.io=<aider.io.InputOutput object at 0x7fa37050f700>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.partial_response_content='\\nDo this:\\n\\nfile.txt\\n<<<<<<< SEARCH\\ntwo\\n=======\\nthree\\n>>>>>>> REPLACE\\n\\n', self.partial_response_function_call={}, self.pretty=True, self.repo=<aider.repo.GitRepo object at 0x7fa3706e6040>, self.repo_map=<aider.repomap.RepoMap object at 0x7fa37050fc70>, self.root='/tmp/tmp7g7a2csg', self.send=<MagicMock id='140339940751056'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa37050f190>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "context = \"\"\n",
      "State:\n",
      "''\n",
      "==================================================\n",
      "Clean Code:\n",
      "def move_back_cur_messages(self, message):\n",
      "        self.done_messages += self.cur_messages\n",
      "        self.summarize_start()\n",
      "        if message:\n",
      "            self.done_messages += [\n",
      "                dict(role=\"user\", content=message),\n",
      "                dict(role=\"assistant\", content=\"Ok.\"),\n",
      "            ]\n",
      "        self.cur_messages = []\n",
      "move_back_cur_messages(self=<aider.coders.editblock_coder.EditBlockCoder object at 0x7fa37050f1f0>, message='I committed the changes with git hash ec487fd & commit msg: aider: commit message', self.abs_fnames={'/tmp/tmp7g7a2csg/file.txt'}, self.apply_update_errors=0, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3706e6910>, self.console=<console width=105 None>, self.cur_messages=[{'role': 'user', 'content': 'hi'}, {'role': 'assistant', 'content': '\\nDo this:\\n\\nfile.txt\\n<<<<<<< SEARCH\\ntwo\\n=======\\nthree\\n>>>>>>> REPLACE\\n\\n'}], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.fence=('```', '```'), self.gpt_prompts=<aider.coders.editblock_prompts.EditBlockPrompts object at 0x7fa37050f280>, self.io=<aider.io.InputOutput object at 0x7fa37050f700>, self.last_aider_commit_hash='ec487fd', self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.partial_response_content='\\nDo this:\\n\\nfile.txt\\n<<<<<<< SEARCH\\ntwo\\n=======\\nthree\\n>>>>>>> REPLACE\\n\\n', self.partial_response_function_call={}, self.pretty=True, self.repo=<aider.repo.GitRepo object at 0x7fa3706e6040>, self.repo_map=<aider.repomap.RepoMap object at 0x7fa37050fc70>, self.root='/tmp/tmp7g7a2csg', self.send=<MagicMock id='140339940751056'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa37050f190>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.done_messages += self.cur_messages\n",
      "State:\n",
      "[{'role': 'user', 'content': 'hi'}, {'role': 'assistant', 'content': '\\nDo this:\\n\\nfile.txt\\n<<<<<<< SEARCH\\ntwo\\n=======\\nthree\\n>>>>>>> REPLACE\\n\\n'}]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def tokenize(self, messages):\n",
      "        sized = []\n",
      "        for msg in messages:\n",
      "            tokens = len(self.tokenizer.encode(json.dumps(msg)))\n",
      "            sized.append((tokens, msg))\n",
      "        return sized\n",
      "tokenize(self=<aider.history.ChatSummary object at 0x7fa37050f190>, messages=[{'role': 'user', 'content': 'hi'}, {'role': 'assistant', 'content': '\\nDo this:\\n\\nfile.txt\\n<<<<<<< SEARCH\\ntwo\\n=======\\nthree\\n>>>>>>> REPLACE\\n\\n'}], self.client=None, self.max_tokens=1024, self.model=<aider.models.openai.OpenAIModel object at 0x7fa37050ffd0>, self.tokenizer=<Encoding 'cl100k_base'>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "sized = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def cmd_add(self, args):\n",
      "        \"Add files to the chat so GPT can edit them or review them in detail\"\n",
      "        added_fnames = []\n",
      "        all_matched_files = set()\n",
      "        filenames = parse_quoted_filenames(args)\n",
      "        for word in filenames:\n",
      "            if Path(word).is_absolute():\n",
      "                fname = Path(word)\n",
      "            else:\n",
      "                fname = Path(self.coder.root) / word\n",
      "            if fname.exists() and fname.is_file():\n",
      "                all_matched_files.add(str(fname))\n",
      "                continue\n",
      "            matched_files = self.glob_filtered_to_repo(word)\n",
      "            if matched_files:\n",
      "                all_matched_files.update(matched_files)\n",
      "                continue\n",
      "            if self.io.confirm_ask(f\"No files matched '{word}'. Do you want to create {fname}?\"):\n",
      "                fname.touch()\n",
      "                all_matched_files.add(str(fname))\n",
      "        for matched_file in all_matched_files:\n",
      "            abs_file_path = self.coder.abs_root_path(matched_file)\n",
      "            if not abs_file_path.startswith(self.coder.root):\n",
      "                self.io.tool_error(\n",
      "                    f\"Can not add {abs_file_path}, which is not within {self.coder.root}\"\n",
      "                )\n",
      "                continue\n",
      "            if abs_file_path in self.coder.abs_fnames:\n",
      "                self.io.tool_error(f\"{matched_file} is already in the chat\")\n",
      "            else:\n",
      "                if is_image_file(matched_file) and not is_gpt4_with_openai_base_url(\n",
      "                    self.coder.main_model.name, self.coder.client\n",
      "                ):\n",
      "                    self.io.tool_error(\n",
      "                        f\"Cannot add image file {matched_file} as the model does not support image\"\n",
      "                        \" files\"\n",
      "                    )\n",
      "                    continue\n",
      "                content = self.io.read_text(abs_file_path)\n",
      "                if content is None:\n",
      "                    self.io.tool_error(f\"Unable to read {matched_file}\")\n",
      "                else:\n",
      "                    self.coder.abs_fnames.add(abs_file_path)\n",
      "                    self.io.tool_output(f\"Added {matched_file} to the chat\")\n",
      "                    added_fnames.append(matched_file)\n",
      "        if not added_fnames:\n",
      "            return\n",
      "        if not self.coder.cur_messages:\n",
      "            return\n",
      "        reply = prompts.added_files.format(fnames=\", \".join(added_fnames))\n",
      "        return reply\n",
      "cmd_add(self=<aider.commands.Commands object at 0x7fa36b41bdf0>, args='foo.txt bar.txt', self.coder=<aider.coders.wholefile_coder.WholeFileCoder object at 0x7fa2d2472be0>, self.io=<aider.io.InputOutput object at 0x7fa36fe7c040>, self.tokenizer=<Encoding 'cl100k_base'>, self.voice_language=None)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "added_fnames = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse_quoted_filenames(args):\n",
      "    filenames = re.findall(r\"\\\"(.+?)\\\"|(\\S+)\", args)\n",
      "    filenames = [name for sublist in filenames for name in sublist if name]\n",
      "    return filenames\n",
      "parse_quoted_filenames(args='foo.txt bar.txt')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "filenames = re.findall(r\"\\\"(.+?)\\\"|(\\S+)\", args)\n",
      "State:\n",
      "[('', 'foo.txt'), ('', 'bar.txt')]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def glob_filtered_to_repo(self, pattern):\n",
      "        try:\n",
      "            raw_matched_files = list(Path(self.coder.root).glob(pattern))\n",
      "        except ValueError as err:\n",
      "            self.io.tool_error(f\"Error matching {pattern}: {err}\")\n",
      "            raw_matched_files = []\n",
      "        matched_files = []\n",
      "        for fn in raw_matched_files:\n",
      "            matched_files += expand_subdir(fn)\n",
      "        matched_files = [str(Path(fn).relative_to(self.coder.root)) for fn in matched_files]\n",
      "        if self.coder.repo:\n",
      "            git_files = self.coder.repo.get_tracked_files()\n",
      "            matched_files = [fn for fn in matched_files if str(fn) in git_files]\n",
      "        res = list(map(str, matched_files))\n",
      "        return res\n",
      "glob_filtered_to_repo(self=<aider.commands.Commands object at 0x7fa36b41bdf0>, pattern='foo.txt', self.coder=<aider.coders.wholefile_coder.WholeFileCoder object at 0x7fa2d2472be0>, self.io=<aider.io.InputOutput object at 0x7fa36fe7c040>, self.tokenizer=<Encoding 'cl100k_base'>, self.voice_language=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "raw_matched_files = list(Path(self.coder.root).glob(pattern))\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def cmd_drop(self, args):\n",
      "        \"Remove files from the chat session to free up context space\"\n",
      "        if not args.strip():\n",
      "            self.io.tool_output(\"Dropping all files from the chat session.\")\n",
      "            self.coder.abs_fnames = set()\n",
      "        filenames = parse_quoted_filenames(args)\n",
      "        for word in filenames:\n",
      "            matched_files = self.glob_filtered_to_repo(word)\n",
      "            if not matched_files:\n",
      "                matched_files.append(word)\n",
      "            for matched_file in matched_files:\n",
      "                abs_fname = self.coder.abs_root_path(matched_file)\n",
      "                if abs_fname in self.coder.abs_fnames:\n",
      "                    self.coder.abs_fnames.remove(abs_fname)\n",
      "                    self.io.tool_output(f\"Removed {matched_file} from the chat\")\n",
      "cmd_drop(self=<aider.commands.Commands object at 0x7fa36dbb32e0>, args='test_dir/another_dir', self.coder=<aider.coders.wholefile_coder.WholeFileCoder object at 0x7fa36dc10f40>, self.io=<aider.io.InputOutput object at 0x7fa36b1d1ee0>, self.tokenizer=<Encoding 'cl100k_base'>, self.voice_language=None)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "abs_fname = self.coder.abs_root_path(matched_file)\n",
      "State:\n",
      "'/tmp/tmp7ganll4z/test_dir/another_dir/test_file.txt'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def cmd_git(self, args):\n",
      "        \"Run a git command\"\n",
      "        combined_output = None\n",
      "        try:\n",
      "            args = \"git \" + args\n",
      "            env = dict(GIT_EDITOR=\"true\", **subprocess.os.environ)\n",
      "            result = subprocess.run(\n",
      "                args,\n",
      "                stdout=subprocess.PIPE,\n",
      "                stderr=subprocess.STDOUT,\n",
      "                text=True,\n",
      "                env=env,\n",
      "                shell=True,\n",
      "            )\n",
      "            combined_output = result.stdout\n",
      "        except Exception as e:\n",
      "            self.io.tool_error(f\"Error running git command: {e}\")\n",
      "        if combined_output is None:\n",
      "            return\n",
      "        self.io.tool_output(combined_output)\n",
      "cmd_git(self=<aider.commands.Commands object at 0x7fa368d69f70>, args='add test.txt', self.coder=<aider.coders.wholefile_coder.WholeFileCoder object at 0x7fa368d69850>, self.io=<aider.io.InputOutput object at 0x7fa368d69cd0>, self.tokenizer=<Encoding 'cl100k_base'>, self.voice_language=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "combined_output = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def cmd_git(self, args):\n",
      "        \"Run a git command\"\n",
      "        combined_output = None\n",
      "        try:\n",
      "            args = \"git \" + args\n",
      "            env = dict(GIT_EDITOR=\"true\", **subprocess.os.environ)\n",
      "            result = subprocess.run(\n",
      "                args,\n",
      "                stdout=subprocess.PIPE,\n",
      "                stderr=subprocess.STDOUT,\n",
      "                text=True,\n",
      "                env=env,\n",
      "                shell=True,\n",
      "            )\n",
      "            combined_output = result.stdout\n",
      "        except Exception as e:\n",
      "            self.io.tool_error(f\"Error running git command: {e}\")\n",
      "        if combined_output is None:\n",
      "            return\n",
      "        self.io.tool_output(combined_output)\n",
      "cmd_git(self=<aider.commands.Commands object at 0x7fa368d69f70>, args='add test.txt', self.coder=<aider.coders.wholefile_coder.WholeFileCoder object at 0x7fa368d69850>, self.io=<aider.io.InputOutput object at 0x7fa368d69cd0>, self.tokenizer=<Encoding 'cl100k_base'>, self.voice_language=None)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "args = \"git \" + args\n",
      "State:\n",
      "'git add test.txt'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def cmd_git(self, args):\n",
      "        \"Run a git command\"\n",
      "        combined_output = None\n",
      "        try:\n",
      "            args = \"git \" + args\n",
      "            env = dict(GIT_EDITOR=\"true\", **subprocess.os.environ)\n",
      "            result = subprocess.run(\n",
      "                args,\n",
      "                stdout=subprocess.PIPE,\n",
      "                stderr=subprocess.STDOUT,\n",
      "                text=True,\n",
      "                env=env,\n",
      "                shell=True,\n",
      "            )\n",
      "            combined_output = result.stdout\n",
      "        except Exception as e:\n",
      "            self.io.tool_error(f\"Error running git command: {e}\")\n",
      "        if combined_output is None:\n",
      "            return\n",
      "        self.io.tool_output(combined_output)\n",
      "cmd_git(self=<aider.commands.Commands object at 0x7fa368d69f70>, args='add test.txt', self.coder=<aider.coders.wholefile_coder.WholeFileCoder object at 0x7fa368d69850>, self.io=<aider.io.InputOutput object at 0x7fa368d69cd0>, self.tokenizer=<Encoding 'cl100k_base'>, self.voice_language=None)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "env = dict(GIT_EDITOR=\"true\", **subprocess.os.environ)\n",
      "State:\n",
      "{'GIT_EDITOR': 'true', 'SHELL': '/bin/bash', 'LSCOLORS': 'Gxfxcxdxbxegedabagacad', 'USER_ZDOTDIR': '/home/XXX', 'COLORTERM': 'truecolor', 'LESS': '-R', 'TERM_PROGRAM_VERSION': '3.2a', 'GVM_VERSION': '1.0.22', 'CONDA_EXE': '/local/rcs/XXX/miniforge3/bin/conda', '_CE_M': '', 'TMUX': '/tmp/tmux-19200/default,59951,3', 'PKG_CONFIG_PATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib/pkgconfig:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib/pkgconfig:', '_P9K_TTY': '/dev/pts/7', 'GVM_PATH_BACKUP': '/home/XXX/.gvm/bin:/local/rcs/XXX/miniforge3/envs/mal/bin:/local/rcs/XXX/miniforge3/condabin:/home/XXX/.gdrive-downloader:/local/arise/XXX/miniforge3/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/bin:/home/XXX/.gvm/gos/go1.19.1/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/bin:/home/XXX/.gvm/bin:/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/XXX/.local/bin:/home/XXX/.local/bin:/home/XXX/.local/bin', 'P9K_TTY': 'old', 'LC_FIG_SET_PARENT': '4c022497-5122-4b80-b325-c89bab32302a', 'PWD': '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/paul-gauthier+aider/paul-gauthier+aider', 'LOGNAME': 'XXX', 'XDG_SESSION_TYPE': 'tty', 'CONDA_PREFIX': '/local/rcs/XXX/miniforge3/envs/paul-gauthier+aider', 'VSCODE_GIT_ASKPASS_NODE': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/node', 'MOTD_SHOWN': 'pam', 'VSCODE_INJECTION': '1', 'GVM_OVERLAY_PREFIX': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay', 'HOME': '/home/XXX', 'LANG': 'en_US.UTF-8', 'DYLD_LIBRARY_PATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'gvm_pkgset_name': 'global', 'SSL_CERT_DIR': '/usr/lib/ssl/certs', 'CONDA_PROMPT_MODIFIER': '(paul-gauthier+aider) ', 'GIT_ASKPASS': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/extensions/git/dist/askpass.sh', 'GVM_ROOT': '/home/XXX/.gvm', 'SSH_CONNECTION': '127.0.0.1 55664 127.0.0.1 22', 'GOROOT': '/home/XXX/.gvm/gos/go1.19.1', 'NVM_DIR': '/local/rcs/XXX/.nvm', 'VSCODE_GIT_ASKPASS_EXTRA_ARGS': '', 'XDG_SESSION_CLASS': 'user', 'PYTHONPATH': ':/local/rcs/XXX/code/pytrace-collector:/local/rcs/XXX/code/pytrace-collector:/local/rcs/XXX/code/pytrace-collector:/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/paul-gauthier+aider/paul-gauthier+aider', 'TERM': 'screen', 'ZSH': '/home/XXX/.oh-my-zsh', '_CE_CONDA': '', 'VSCODE_NONCE': 'd0bc7031-48a3-4719-8bb5-ef236ddd0016', 'ZDOTDIR': '/home/XXX', 'USER': 'XXX', 'TMUX_PANE': '%5', 'VSCODE_GIT_IPC_HANDLE': '/run/user/19200/vscode-git-13d67c6199.sock', 'CONDA_SHLVL': '3', 'SHLVL': '3', 'PAGER': 'less', '_P9K_SSH_TTY': '/dev/pts/7', 'XDG_SESSION_ID': '43', 'CONDA_PYTHON_EXE': '/local/rcs/XXX/miniforge3/bin/python', 'LD_LIBRARY_PATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib', 'XDG_RUNTIME_DIR': '/run/user/19200', 'SSL_CERT_FILE': '/usr/lib/ssl/certs/ca-certificates.crt', 'SSH_CLIENT': '127.0.0.1 46946 22', 'CONDA_DEFAULT_ENV': 'paul-gauthier+aider', 'P9K_SSH': '1', 'LC_ALL': 'en_US.UTF-8', 'VSCODE_GIT_ASKPASS_MAIN': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/extensions/git/dist/askpass-main.js', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'BROWSER': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/helpers/browser.sh', 'PATH': '/home/XXX/.gdrive-downloader:/local/arise/XXX/miniforge3/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/bin:/home/XXX/.gvm/gos/go1.19.1/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/bin:/home/XXX/.gvm/bin:/local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/bin:/local/rcs/XXX/miniforge3/condabin:/home/XXX/.gdrive-downloader:/local/arise/XXX/miniforge3/bin:/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/XXX/.local/bin:/home/XXX/.local/bin', 'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/19200/bus', 'gvm_go_name': 'go1.19.1', 'CONDA_PREFIX_1': '/local/rcs/XXX/miniforge3', 'CONDA_PREFIX_2': '/local/rcs/XXX/miniforge3/envs/mal', 'OLDPWD': '/local/rcs/XXX/code/pytrace-collector', 'GOPATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global', 'TERM_PROGRAM': 'tmux', 'VSCODE_IPC_HOOK_CLI': '/run/user/19200/vscode-ipc-518d6355-acaf-4714-a359-be3fe9f21e09.sock', '_': '/local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/bin/python', 'PYTEST_CURRENT_TEST': 'tests/test_commands.py::TestCommands::test_cmd_git (call)'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def match_but_for_leading_whitespace(whole_lines, part_lines):\n",
      "    num = len(whole_lines)\n",
      "    if not all(whole_lines[i].lstrip() == part_lines[i].lstrip() for i in range(num)):\n",
      "        return\n",
      "    add = set(\n",
      "        whole_lines[i][: len(whole_lines[i]) - len(part_lines[i])]\n",
      "        for i in range(num)\n",
      "        if whole_lines[i].strip()\n",
      "    )\n",
      "    if len(add) != 1:\n",
      "        return\n",
      "    return add.pop()\n",
      "match_but_for_leading_whitespace(whole_lines=['    line1\\n', '    line2\\n'], part_lines=['line1\\n', 'line2\\n'])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "num = len(whole_lines)\n",
      "State:\n",
      "2\n",
      "==================================================\n",
      "Clean Code:\n",
      "def guessed_wrong_repo(io, git_root, fnames, git_dname):\n",
      "    try:\n",
      "        check_repo = Path(GitRepo(io, fnames, git_dname).root).resolve()\n",
      "    except FileNotFoundError:\n",
      "        return\n",
      "    if not git_root:\n",
      "        return str(check_repo)\n",
      "    git_root = Path(git_root).resolve()\n",
      "    if check_repo == git_root:\n",
      "        return\n",
      "    return str(check_repo)\n",
      "guessed_wrong_repo(io=<MagicMock id='140339689280704'>, git_root='/tmp/tmp65cfsod1', fnames=['/tmp/tmp65cfsod1/foo.py'], git_dname=None)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "check_repo = Path(GitRepo(io, fnames, git_dname).root).resolve()\n",
      "State:\n",
      "PosixPath('/tmp/tmp65cfsod1')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_commit_message(self, diffs, context):\n",
      "        if len(diffs) >= 4 * 1024 * 4:\n",
      "            self.io.tool_error(\"Diff is too large to generate a commit message.\")\n",
      "            return\n",
      "        diffs = \"\n",
      "        content = \"\"\n",
      "        if context:\n",
      "            content += context + \"\\n\"\n",
      "        content += diffs\n",
      "        messages = [\n",
      "            dict(role=\"system\", content=prompts.commit_system),\n",
      "            dict(role=\"user\", content=content),\n",
      "        ]\n",
      "        for model in models.Model.commit_message_models():\n",
      "            commit_message = simple_send_with_retries(self.client, model.name, messages)\n",
      "            if commit_message:\n",
      "                break\n",
      "        if not commit_message:\n",
      "            self.io.tool_error(\"Failed to generate commit message!\")\n",
      "            return\n",
      "        commit_message = commit_message.strip()\n",
      "        if commit_message and commit_message[0] == '\"' and commit_message[-1] == '\"':\n",
      "            commit_message = commit_message[1:-1].strip()\n",
      "        return commit_message\n",
      "get_commit_message(self=<aider.repo.GitRepo object at 0x7fa35bec3460>, diffs='dummy diff', context='dummy context', self.client=None, self.io=<aider.io.InputOutput object at 0x7fa35bf5a880>, self.repo=<git.repo.base.Repo '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/paul-gauthier+aider/paul-gauthier+aider/.git'>, self.root='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/paul-gauthier+aider/paul-gauthier+aider')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "diffs = \"# Diffs:\\n\" + diffs\n",
      "State:\n",
      "'# Diffs:\\ndummy diff'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_commit_message(self, diffs, context):\n",
      "        if len(diffs) >= 4 * 1024 * 4:\n",
      "            self.io.tool_error(\"Diff is too large to generate a commit message.\")\n",
      "            return\n",
      "        diffs = \"\n",
      "        content = \"\"\n",
      "        if context:\n",
      "            content += context + \"\\n\"\n",
      "        content += diffs\n",
      "        messages = [\n",
      "            dict(role=\"system\", content=prompts.commit_system),\n",
      "            dict(role=\"user\", content=content),\n",
      "        ]\n",
      "        for model in models.Model.commit_message_models():\n",
      "            commit_message = simple_send_with_retries(self.client, model.name, messages)\n",
      "            if commit_message:\n",
      "                break\n",
      "        if not commit_message:\n",
      "            self.io.tool_error(\"Failed to generate commit message!\")\n",
      "            return\n",
      "        commit_message = commit_message.strip()\n",
      "        if commit_message and commit_message[0] == '\"' and commit_message[-1] == '\"':\n",
      "            commit_message = commit_message[1:-1].strip()\n",
      "        return commit_message\n",
      "get_commit_message(self=<aider.repo.GitRepo object at 0x7fa35bec3460>, diffs='dummy diff', context='dummy context', self.client=None, self.io=<aider.io.InputOutput object at 0x7fa35bf5a880>, self.repo=<git.repo.base.Repo '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/paul-gauthier+aider/paul-gauthier+aider/.git'>, self.root='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/paul-gauthier+aider/paul-gauthier+aider')\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "content += diffs\n",
      "State:\n",
      "'dummy context\\n# Diffs:\\ndummy diff'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def dump(*vals):\n",
      "    stack = traceback.extract_stack()\n",
      "    vars = stack[-2][3]\n",
      "    vars = \"(\".join(vars.split(\"(\")[1:])\n",
      "    vars = \")\".join(vars.split(\")\")[:-1])\n",
      "    vals = [cvt(v) for v in vals]\n",
      "    has_newline = sum(1 for v in vals if \"\\n\" in v)\n",
      "    if has_newline:\n",
      "        print(\"%s:\" % vars)\n",
      "        print(\", \".join(vals))\n",
      "    else:\n",
      "        print(\"%s:\" % vars, \", \".join(vals))\n",
      "dump(vals=(['/tmp/tmp70qxqdwx/test_file0.py', '/tmp/tmp70qxqdwx/test_file1.txt', '/tmp/tmp70qxqdwx/test_file2.md', '/tmp/tmp70qxqdwx/test_file3.json', '/tmp/tmp70qxqdwx/test_file4.html', '/tmp/tmp70qxqdwx/test_file5.css', '/tmp/tmp70qxqdwx/test_file6.js'],))\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "stack = traceback.extract_stack()\n",
      "State:\n",
      "[<FrameSummary file /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/paul-gauthier+aider/trace_collector.py, line 72 in <module>>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/runpy.py, line 225 in run_module>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/runpy.py, line 97 in _run_module_code>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/runpy.py, line 87 in _run_code>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/pytest/__main__.py, line 7 in <module>>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/_pytest/config/__init__.py, line 197 in console_main>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/_pytest/config/__init__.py, line 174 in main>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/pluggy/_hooks.py, line 501 in __call__>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/pluggy/_manager.py, line 119 in _hookexec>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/pluggy/_callers.py, line 102 in _multicall>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/_pytest/main.py, line 332 in pytest_cmdline_main>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/_pytest/main.py, line 285 in wrap_session>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/_pytest/main.py, line 339 in _main>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/pluggy/_hooks.py, line 501 in __call__>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/pluggy/_manager.py, line 119 in _hookexec>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/pluggy/_callers.py, line 102 in _multicall>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/_pytest/main.py, line 364 in pytest_runtestloop>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/pluggy/_hooks.py, line 501 in __call__>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/pluggy/_manager.py, line 119 in _hookexec>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/pluggy/_callers.py, line 102 in _multicall>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/_pytest/runner.py, line 115 in pytest_runtest_protocol>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/_pytest/runner.py, line 134 in runtestprotocol>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/_pytest/runner.py, line 239 in call_and_report>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/_pytest/runner.py, line 340 in from_call>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/_pytest/runner.py, line 240 in <lambda>>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/pluggy/_hooks.py, line 501 in __call__>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/pluggy/_manager.py, line 119 in _hookexec>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/pluggy/_callers.py, line 102 in _multicall>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/_pytest/runner.py, line 172 in pytest_runtest_call>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/site-packages/_pytest/unittest.py, line 321 in runtest>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/unittest/case.py, line 651 in __call__>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/unittest/case.py, line 592 in run>, <FrameSummary file /local/rcs/XXX/miniforge3/envs/paul-gauthier+aider/lib/python3.9/unittest/case.py, line 550 in _callTestMethod>, <FrameSummary file /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/paul-gauthier+aider/paul-gauthier+aider/tests/test_repomap.py, line 112 in test_get_repo_map_all_files>, <FrameSummary file /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/paul-gauthier+aider/paul-gauthier+aider/aider/dump.py, line 16 in dump>]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def dump(*vals):\n",
      "    stack = traceback.extract_stack()\n",
      "    vars = stack[-2][3]\n",
      "    vars = \"(\".join(vars.split(\"(\")[1:])\n",
      "    vars = \")\".join(vars.split(\")\")[:-1])\n",
      "    vals = [cvt(v) for v in vals]\n",
      "    has_newline = sum(1 for v in vals if \"\\n\" in v)\n",
      "    if has_newline:\n",
      "        print(\"%s:\" % vars)\n",
      "        print(\", \".join(vals))\n",
      "    else:\n",
      "        print(\"%s:\" % vars, \", \".join(vals))\n",
      "dump(vals=(['/tmp/tmp70qxqdwx/test_file0.py', '/tmp/tmp70qxqdwx/test_file1.txt', '/tmp/tmp70qxqdwx/test_file2.md', '/tmp/tmp70qxqdwx/test_file3.json', '/tmp/tmp70qxqdwx/test_file4.html', '/tmp/tmp70qxqdwx/test_file5.css', '/tmp/tmp70qxqdwx/test_file6.js'],))\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "vars = stack[-2][3]\n",
      "State:\n",
      "'dump(other_files)'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def find_diffs(content):\n",
      "    if not content.endswith(\"\\n\"):\n",
      "        content = content + \"\\n\"\n",
      "    lines = content.splitlines(keepends=True)\n",
      "    line_num = 0\n",
      "    edits = []\n",
      "    while line_num < len(lines):\n",
      "        while line_num < len(lines):\n",
      "            line = lines[line_num]\n",
      "            if line.startswith(\"```diff\"):\n",
      "                line_num, these_edits = process_fenced_block(lines, line_num + 1)\n",
      "                edits += these_edits\n",
      "                break\n",
      "            line_num += 1\n",
      "    return edits\n",
      "find_diffs(content='\\nSome text...\\n\\n```diff\\n--- /dev/null\\n+++ file.txt\\n@@ ... @@\\n-Original\\n+Modified\\n```\\n')\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "edits += these_edits\n",
      "State:\n",
      "[('file.txt', ['-Original\\n', '+Modified\\n'])]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_edits(self, mode=\"update\"):\n",
      "        content = self.partial_response_content\n",
      "        chat_files = self.get_inchat_relative_files()\n",
      "        output = []\n",
      "        lines = content.splitlines(keepends=True)\n",
      "        edits = []\n",
      "        saw_fname = None\n",
      "        fname = None\n",
      "        fname_source = None\n",
      "        new_lines = []\n",
      "        for i, line in enumerate(lines):\n",
      "            if line.startswith(self.fence[0]) or line.startswith(self.fence[1]):\n",
      "                if fname is not None:\n",
      "                    saw_fname = None\n",
      "                    full_path = self.abs_root_path(fname)\n",
      "                    if mode == \"diff\":\n",
      "                        output += self.do_live_diff(full_path, new_lines, True)\n",
      "                    else:\n",
      "                        edits.append((fname, fname_source, new_lines))\n",
      "                    fname = None\n",
      "                    fname_source = None\n",
      "                    new_lines = []\n",
      "                    continue\n",
      "                if i > 0:\n",
      "                    fname_source = \"block\"\n",
      "                    fname = lines[i - 1].strip()\n",
      "                    fname = fname.strip(\"*\")\n",
      "                    if fname and fname not in chat_files and Path(fname).name in chat_files:\n",
      "                        fname = Path(fname).name\n",
      "                if not fname:\n",
      "                    if saw_fname:\n",
      "                        fname = saw_fname\n",
      "                        fname_source = \"saw\"\n",
      "                    elif len(chat_files) == 1:\n",
      "                        fname = chat_files[0]\n",
      "                        fname_source = \"chat\"\n",
      "                    else:\n",
      "                        raise ValueError(\n",
      "                            f\"No filename provided before {self.fence[0]} in file listing\"\n",
      "                        )\n",
      "            elif fname is not None:\n",
      "                new_lines.append(line)\n",
      "            else:\n",
      "                for word in line.strip().split():\n",
      "                    word = word.rstrip(\".:,;!\")\n",
      "                    for chat_file in chat_files:\n",
      "                        quoted_chat_file = f\"`{chat_file}`\"\n",
      "                        if word == quoted_chat_file:\n",
      "                            saw_fname = chat_file\n",
      "                output.append(line)\n",
      "        if mode == \"diff\":\n",
      "            if fname is not None:\n",
      "                full_path = (Path(self.root) / fname).absolute()\n",
      "                output += self.do_live_diff(full_path, new_lines, False)\n",
      "            return \"\\n\".join(output)\n",
      "        if fname:\n",
      "            edits.append((fname, fname_source, new_lines))\n",
      "        seen = set()\n",
      "        refined_edits = []\n",
      "        for source in (\"block\", \"saw\", \"chat\"):\n",
      "            for fname, fname_source, new_lines in edits:\n",
      "                if fname_source != source:\n",
      "                    continue\n",
      "                if fname in seen:\n",
      "                    continue\n",
      "                seen.add(fname)\n",
      "                refined_edits.append((fname, fname_source, new_lines))\n",
      "        return refined_edits\n",
      "get_edits(self=<aider.coders.wholefile_coder.WholeFileCoder object at 0x7fa3613bfee0>, mode='update', self.abs_fnames={'/tmp/tmpapnwjv6p'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3613bff10>, self.console=<console width=105 None>, self.cur_messages=[{'role': 'user', 'content': 'hi'}], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.fence=('```', '```'), self.gpt_prompts=<aider.coders.wholefile_prompts.WholeFilePrompts object at 0x7fa3613bf760>, self.io=<aider.io.InputOutput object at 0x7fa3613bfa00>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.partial_response_content='\\nDo this:\\n\\ntmpapnwjv6p\\n```\\nnew\\ntwo\\nthree\\n```\\n\\n', self.partial_response_function_call={}, self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339687715744'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa3613bf820>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "content = self.partial_response_content\n",
      "State:\n",
      "'\\nDo this:\\n\\ntmpapnwjv6p\\n```\\nnew\\ntwo\\nthree\\n```\\n\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_edits(self, mode=\"update\"):\n",
      "        content = self.partial_response_content\n",
      "        chat_files = self.get_inchat_relative_files()\n",
      "        output = []\n",
      "        lines = content.splitlines(keepends=True)\n",
      "        edits = []\n",
      "        saw_fname = None\n",
      "        fname = None\n",
      "        fname_source = None\n",
      "        new_lines = []\n",
      "        for i, line in enumerate(lines):\n",
      "            if line.startswith(self.fence[0]) or line.startswith(self.fence[1]):\n",
      "                if fname is not None:\n",
      "                    saw_fname = None\n",
      "                    full_path = self.abs_root_path(fname)\n",
      "                    if mode == \"diff\":\n",
      "                        output += self.do_live_diff(full_path, new_lines, True)\n",
      "                    else:\n",
      "                        edits.append((fname, fname_source, new_lines))\n",
      "                    fname = None\n",
      "                    fname_source = None\n",
      "                    new_lines = []\n",
      "                    continue\n",
      "                if i > 0:\n",
      "                    fname_source = \"block\"\n",
      "                    fname = lines[i - 1].strip()\n",
      "                    fname = fname.strip(\"*\")\n",
      "                    if fname and fname not in chat_files and Path(fname).name in chat_files:\n",
      "                        fname = Path(fname).name\n",
      "                if not fname:\n",
      "                    if saw_fname:\n",
      "                        fname = saw_fname\n",
      "                        fname_source = \"saw\"\n",
      "                    elif len(chat_files) == 1:\n",
      "                        fname = chat_files[0]\n",
      "                        fname_source = \"chat\"\n",
      "                    else:\n",
      "                        raise ValueError(\n",
      "                            f\"No filename provided before {self.fence[0]} in file listing\"\n",
      "                        )\n",
      "            elif fname is not None:\n",
      "                new_lines.append(line)\n",
      "            else:\n",
      "                for word in line.strip().split():\n",
      "                    word = word.rstrip(\".:,;!\")\n",
      "                    for chat_file in chat_files:\n",
      "                        quoted_chat_file = f\"`{chat_file}`\"\n",
      "                        if word == quoted_chat_file:\n",
      "                            saw_fname = chat_file\n",
      "                output.append(line)\n",
      "        if mode == \"diff\":\n",
      "            if fname is not None:\n",
      "                full_path = (Path(self.root) / fname).absolute()\n",
      "                output += self.do_live_diff(full_path, new_lines, False)\n",
      "            return \"\\n\".join(output)\n",
      "        if fname:\n",
      "            edits.append((fname, fname_source, new_lines))\n",
      "        seen = set()\n",
      "        refined_edits = []\n",
      "        for source in (\"block\", \"saw\", \"chat\"):\n",
      "            for fname, fname_source, new_lines in edits:\n",
      "                if fname_source != source:\n",
      "                    continue\n",
      "                if fname in seen:\n",
      "                    continue\n",
      "                seen.add(fname)\n",
      "                refined_edits.append((fname, fname_source, new_lines))\n",
      "        return refined_edits\n",
      "get_edits(self=<aider.coders.wholefile_coder.WholeFileCoder object at 0x7fa3613bfee0>, mode='update', self.abs_fnames={'/tmp/tmpapnwjv6p'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3613bff10>, self.console=<console width=105 None>, self.cur_messages=[{'role': 'user', 'content': 'hi'}], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.fence=('```', '```'), self.gpt_prompts=<aider.coders.wholefile_prompts.WholeFilePrompts object at 0x7fa3613bf760>, self.io=<aider.io.InputOutput object at 0x7fa3613bfa00>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.partial_response_content='\\nDo this:\\n\\ntmpapnwjv6p\\n```\\nnew\\ntwo\\nthree\\n```\\n\\n', self.partial_response_function_call={}, self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339687715744'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa3613bf820>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "chat_files = self.get_inchat_relative_files()\n",
      "State:\n",
      "['tmpapnwjv6p']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_edits(self, mode=\"update\"):\n",
      "        content = self.partial_response_content\n",
      "        chat_files = self.get_inchat_relative_files()\n",
      "        output = []\n",
      "        lines = content.splitlines(keepends=True)\n",
      "        edits = []\n",
      "        saw_fname = None\n",
      "        fname = None\n",
      "        fname_source = None\n",
      "        new_lines = []\n",
      "        for i, line in enumerate(lines):\n",
      "            if line.startswith(self.fence[0]) or line.startswith(self.fence[1]):\n",
      "                if fname is not None:\n",
      "                    saw_fname = None\n",
      "                    full_path = self.abs_root_path(fname)\n",
      "                    if mode == \"diff\":\n",
      "                        output += self.do_live_diff(full_path, new_lines, True)\n",
      "                    else:\n",
      "                        edits.append((fname, fname_source, new_lines))\n",
      "                    fname = None\n",
      "                    fname_source = None\n",
      "                    new_lines = []\n",
      "                    continue\n",
      "                if i > 0:\n",
      "                    fname_source = \"block\"\n",
      "                    fname = lines[i - 1].strip()\n",
      "                    fname = fname.strip(\"*\")\n",
      "                    if fname and fname not in chat_files and Path(fname).name in chat_files:\n",
      "                        fname = Path(fname).name\n",
      "                if not fname:\n",
      "                    if saw_fname:\n",
      "                        fname = saw_fname\n",
      "                        fname_source = \"saw\"\n",
      "                    elif len(chat_files) == 1:\n",
      "                        fname = chat_files[0]\n",
      "                        fname_source = \"chat\"\n",
      "                    else:\n",
      "                        raise ValueError(\n",
      "                            f\"No filename provided before {self.fence[0]} in file listing\"\n",
      "                        )\n",
      "            elif fname is not None:\n",
      "                new_lines.append(line)\n",
      "            else:\n",
      "                for word in line.strip().split():\n",
      "                    word = word.rstrip(\".:,;!\")\n",
      "                    for chat_file in chat_files:\n",
      "                        quoted_chat_file = f\"`{chat_file}`\"\n",
      "                        if word == quoted_chat_file:\n",
      "                            saw_fname = chat_file\n",
      "                output.append(line)\n",
      "        if mode == \"diff\":\n",
      "            if fname is not None:\n",
      "                full_path = (Path(self.root) / fname).absolute()\n",
      "                output += self.do_live_diff(full_path, new_lines, False)\n",
      "            return \"\\n\".join(output)\n",
      "        if fname:\n",
      "            edits.append((fname, fname_source, new_lines))\n",
      "        seen = set()\n",
      "        refined_edits = []\n",
      "        for source in (\"block\", \"saw\", \"chat\"):\n",
      "            for fname, fname_source, new_lines in edits:\n",
      "                if fname_source != source:\n",
      "                    continue\n",
      "                if fname in seen:\n",
      "                    continue\n",
      "                seen.add(fname)\n",
      "                refined_edits.append((fname, fname_source, new_lines))\n",
      "        return refined_edits\n",
      "get_edits(self=<aider.coders.wholefile_coder.WholeFileCoder object at 0x7fa3613bfee0>, mode='update', self.abs_fnames={'/tmp/tmpapnwjv6p'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3613bff10>, self.console=<console width=105 None>, self.cur_messages=[{'role': 'user', 'content': 'hi'}], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.fence=('```', '```'), self.gpt_prompts=<aider.coders.wholefile_prompts.WholeFilePrompts object at 0x7fa3613bf760>, self.io=<aider.io.InputOutput object at 0x7fa3613bfa00>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.partial_response_content='\\nDo this:\\n\\ntmpapnwjv6p\\n```\\nnew\\ntwo\\nthree\\n```\\n\\n', self.partial_response_function_call={}, self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339687715744'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa3613bf820>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "output = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_edits(self, mode=\"update\"):\n",
      "        content = self.partial_response_content\n",
      "        chat_files = self.get_inchat_relative_files()\n",
      "        output = []\n",
      "        lines = content.splitlines(keepends=True)\n",
      "        edits = []\n",
      "        saw_fname = None\n",
      "        fname = None\n",
      "        fname_source = None\n",
      "        new_lines = []\n",
      "        for i, line in enumerate(lines):\n",
      "            if line.startswith(self.fence[0]) or line.startswith(self.fence[1]):\n",
      "                if fname is not None:\n",
      "                    saw_fname = None\n",
      "                    full_path = self.abs_root_path(fname)\n",
      "                    if mode == \"diff\":\n",
      "                        output += self.do_live_diff(full_path, new_lines, True)\n",
      "                    else:\n",
      "                        edits.append((fname, fname_source, new_lines))\n",
      "                    fname = None\n",
      "                    fname_source = None\n",
      "                    new_lines = []\n",
      "                    continue\n",
      "                if i > 0:\n",
      "                    fname_source = \"block\"\n",
      "                    fname = lines[i - 1].strip()\n",
      "                    fname = fname.strip(\"*\")\n",
      "                    if fname and fname not in chat_files and Path(fname).name in chat_files:\n",
      "                        fname = Path(fname).name\n",
      "                if not fname:\n",
      "                    if saw_fname:\n",
      "                        fname = saw_fname\n",
      "                        fname_source = \"saw\"\n",
      "                    elif len(chat_files) == 1:\n",
      "                        fname = chat_files[0]\n",
      "                        fname_source = \"chat\"\n",
      "                    else:\n",
      "                        raise ValueError(\n",
      "                            f\"No filename provided before {self.fence[0]} in file listing\"\n",
      "                        )\n",
      "            elif fname is not None:\n",
      "                new_lines.append(line)\n",
      "            else:\n",
      "                for word in line.strip().split():\n",
      "                    word = word.rstrip(\".:,;!\")\n",
      "                    for chat_file in chat_files:\n",
      "                        quoted_chat_file = f\"`{chat_file}`\"\n",
      "                        if word == quoted_chat_file:\n",
      "                            saw_fname = chat_file\n",
      "                output.append(line)\n",
      "        if mode == \"diff\":\n",
      "            if fname is not None:\n",
      "                full_path = (Path(self.root) / fname).absolute()\n",
      "                output += self.do_live_diff(full_path, new_lines, False)\n",
      "            return \"\\n\".join(output)\n",
      "        if fname:\n",
      "            edits.append((fname, fname_source, new_lines))\n",
      "        seen = set()\n",
      "        refined_edits = []\n",
      "        for source in (\"block\", \"saw\", \"chat\"):\n",
      "            for fname, fname_source, new_lines in edits:\n",
      "                if fname_source != source:\n",
      "                    continue\n",
      "                if fname in seen:\n",
      "                    continue\n",
      "                seen.add(fname)\n",
      "                refined_edits.append((fname, fname_source, new_lines))\n",
      "        return refined_edits\n",
      "get_edits(self=<aider.coders.wholefile_coder.WholeFileCoder object at 0x7fa3613bfee0>, mode='update', self.abs_fnames={'/tmp/tmpapnwjv6p'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3613bff10>, self.console=<console width=105 None>, self.cur_messages=[{'role': 'user', 'content': 'hi'}], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.fence=('```', '```'), self.gpt_prompts=<aider.coders.wholefile_prompts.WholeFilePrompts object at 0x7fa3613bf760>, self.io=<aider.io.InputOutput object at 0x7fa3613bfa00>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.partial_response_content='\\nDo this:\\n\\ntmpapnwjv6p\\n```\\nnew\\ntwo\\nthree\\n```\\n\\n', self.partial_response_function_call={}, self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339687715744'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa3613bf820>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "full_path = self.abs_root_path(fname)\n",
      "State:\n",
      "'/tmp/tmpapnwjv6p'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_edits(self, mode=\"update\"):\n",
      "        content = self.partial_response_content\n",
      "        chat_files = self.get_inchat_relative_files()\n",
      "        output = []\n",
      "        lines = content.splitlines(keepends=True)\n",
      "        edits = []\n",
      "        saw_fname = None\n",
      "        fname = None\n",
      "        fname_source = None\n",
      "        new_lines = []\n",
      "        for i, line in enumerate(lines):\n",
      "            if line.startswith(self.fence[0]) or line.startswith(self.fence[1]):\n",
      "                if fname is not None:\n",
      "                    saw_fname = None\n",
      "                    full_path = self.abs_root_path(fname)\n",
      "                    if mode == \"diff\":\n",
      "                        output += self.do_live_diff(full_path, new_lines, True)\n",
      "                    else:\n",
      "                        edits.append((fname, fname_source, new_lines))\n",
      "                    fname = None\n",
      "                    fname_source = None\n",
      "                    new_lines = []\n",
      "                    continue\n",
      "                if i > 0:\n",
      "                    fname_source = \"block\"\n",
      "                    fname = lines[i - 1].strip()\n",
      "                    fname = fname.strip(\"*\")\n",
      "                    if fname and fname not in chat_files and Path(fname).name in chat_files:\n",
      "                        fname = Path(fname).name\n",
      "                if not fname:\n",
      "                    if saw_fname:\n",
      "                        fname = saw_fname\n",
      "                        fname_source = \"saw\"\n",
      "                    elif len(chat_files) == 1:\n",
      "                        fname = chat_files[0]\n",
      "                        fname_source = \"chat\"\n",
      "                    else:\n",
      "                        raise ValueError(\n",
      "                            f\"No filename provided before {self.fence[0]} in file listing\"\n",
      "                        )\n",
      "            elif fname is not None:\n",
      "                new_lines.append(line)\n",
      "            else:\n",
      "                for word in line.strip().split():\n",
      "                    word = word.rstrip(\".:,;!\")\n",
      "                    for chat_file in chat_files:\n",
      "                        quoted_chat_file = f\"`{chat_file}`\"\n",
      "                        if word == quoted_chat_file:\n",
      "                            saw_fname = chat_file\n",
      "                output.append(line)\n",
      "        if mode == \"diff\":\n",
      "            if fname is not None:\n",
      "                full_path = (Path(self.root) / fname).absolute()\n",
      "                output += self.do_live_diff(full_path, new_lines, False)\n",
      "            return \"\\n\".join(output)\n",
      "        if fname:\n",
      "            edits.append((fname, fname_source, new_lines))\n",
      "        seen = set()\n",
      "        refined_edits = []\n",
      "        for source in (\"block\", \"saw\", \"chat\"):\n",
      "            for fname, fname_source, new_lines in edits:\n",
      "                if fname_source != source:\n",
      "                    continue\n",
      "                if fname in seen:\n",
      "                    continue\n",
      "                seen.add(fname)\n",
      "                refined_edits.append((fname, fname_source, new_lines))\n",
      "        return refined_edits\n",
      "get_edits(self=<aider.coders.wholefile_coder.WholeFileCoder object at 0x7fa3613bfee0>, mode='update', self.abs_fnames={'/tmp/tmpapnwjv6p'}, self.assistant_output_color='blue', self.auto_commits=True, self.chat_completion_call_hashes=[], self.chat_completion_response_hashes=[], self.client=None, self.code_theme='default', self.commands=<aider.commands.Commands object at 0x7fa3613bff10>, self.console=<console width=105 None>, self.cur_messages=[{'role': 'user', 'content': 'hi'}], self.dirty_commits=True, self.done_messages=[], self.dry_run=False, self.fence=('```', '```'), self.gpt_prompts=<aider.coders.wholefile_prompts.WholeFilePrompts object at 0x7fa3613bf760>, self.io=<aider.io.InputOutput object at 0x7fa3613bfa00>, self.main_model=<aider.models.openai.OpenAIModel object at 0x7fa3771ba760>, self.need_commit_before_edits=set(), self.partial_response_content='\\nDo this:\\n\\ntmpapnwjv6p\\n```\\nnew\\ntwo\\nthree\\n```\\n\\n', self.partial_response_function_call={}, self.pretty=True, self.repo=None, self.root='/tmp', self.send=<MagicMock id='140339687715744'>, self.show_diffs=False, self.stream=True, self.summarized_done_messages=[], self.summarizer=<aider.history.ChatSummary object at 0x7fa3613bf820>, self.summarizer_thread=None, self.verbose=False)\n",
      "\n",
      "n:\n",
      "25\n",
      "Statement:\n",
      "edits.append((fname, fname_source, new_lines))\n",
      "State:\n",
      "[('tmpapnwjv6p', 'block', ['new\\n', 'two\\n', 'three\\n'])]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def from_mix(cls, fuel_tuples: list):\n",
      "        mj = 0\n",
      "        ci = 0\n",
      "        pct_total = 0.   \n",
      "        for fuel, pct in fuel_tuples:\n",
      "            if not isinstance(fuel, Fuel):\n",
      "                raise TypeError('All fuels must be of type Fuel')\n",
      "            mj += fuel.MJ_L * pct\n",
      "            ci += fuel.ci * pct\n",
      "            pct_total += pct\n",
      "        if not (pct_total == 1.):\n",
      "            raise ValueError('Percentages do not add up to 1')\n",
      "        return Fuel(mj, ci)\n",
      "from_mix(cls=<class 'evnrg.fuels.Fuel'>, fuel_tuples=[(Fuel(MJ_L=31.8, ci=99.78), 0.9), (Fuel(MJ_L=21.2, ci=53.49), 0.1)])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "mj = 0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def from_mix(cls, fuel_tuples: list):\n",
      "        mj = 0\n",
      "        ci = 0\n",
      "        pct_total = 0.   \n",
      "        for fuel, pct in fuel_tuples:\n",
      "            if not isinstance(fuel, Fuel):\n",
      "                raise TypeError('All fuels must be of type Fuel')\n",
      "            mj += fuel.MJ_L * pct\n",
      "            ci += fuel.ci * pct\n",
      "            pct_total += pct\n",
      "        if not (pct_total == 1.):\n",
      "            raise ValueError('Percentages do not add up to 1')\n",
      "        return Fuel(mj, ci)\n",
      "from_mix(cls=<class 'evnrg.fuels.Fuel'>, fuel_tuples=[(Fuel(MJ_L=31.8, ci=99.78), 0.9), (Fuel(MJ_L=21.2, ci=53.49), 0.1)])\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "ci = 0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def increment_value(self, item, **kwargs):\n",
      "            assert \"many\" in kwargs\n",
      "            item[\"value\"] += 1\n",
      "            return item\n",
      "increment_value(self=<ExampleSchema(many=False)>, item={'value': 3}, kwargs={'many': False}, self.context={}, self.declared_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.dump_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.dump_only=set(), self.error_messages={'type': 'Invalid input type.', 'unknown': 'Unknown field.'}, self.exclude=set(), self.fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.load_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.load_only=set(), self.many=False, self.only=None, self.ordered=False, self.partial=True, self.unknown='raise')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "assert \"many\" in kwargs\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def increment_value(self, item, **kwargs):\n",
      "            assert \"many\" in kwargs\n",
      "            item[\"value\"] += 1\n",
      "            return item\n",
      "increment_value(self=<ExampleSchema(many=False)>, item={'value': 3}, kwargs={'many': False}, self.context={}, self.declared_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.dump_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.dump_only=set(), self.error_messages={'type': 'Invalid input type.', 'unknown': 'Unknown field.'}, self.exclude=set(), self.fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.load_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.load_only=set(), self.many=False, self.only=None, self.ordered=False, self.partial=True, self.unknown='raise')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "item[\"value\"] += 1\n",
      "State:\n",
      "{'value': 4}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add_tag(self, item, **kwargs):\n",
      "            assert \"many\" in kwargs\n",
      "            item[\"value\"] = self.TAG + item[\"value\"]\n",
      "            return item\n",
      "add_tag(self=<ExampleSchema(many=False)>, item={'value': '4'}, kwargs={'many': False}, self.context={}, self.declared_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.dump_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.dump_only=set(), self.error_messages={'type': 'Invalid input type.', 'unknown': 'Unknown field.'}, self.exclude=set(), self.fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.load_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.load_only=set(), self.many=False, self.only=None, self.ordered=False, self.partial=True, self.unknown='raise')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "assert \"many\" in kwargs\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add_tag(self, item, **kwargs):\n",
      "            assert \"many\" in kwargs\n",
      "            item[\"value\"] = self.TAG + item[\"value\"]\n",
      "            return item\n",
      "add_tag(self=<ExampleSchema(many=False)>, item={'value': '4'}, kwargs={'many': False}, self.context={}, self.declared_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.dump_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.dump_only=set(), self.error_messages={'type': 'Invalid input type.', 'unknown': 'Unknown field.'}, self.exclude=set(), self.fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.load_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.load_only=set(), self.many=False, self.only=None, self.ordered=False, self.partial=True, self.unknown='raise')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "item[\"value\"] = self.TAG + item[\"value\"]\n",
      "State:\n",
      "{'value': 'TAG4'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def remove_tag(self, item, partial, **kwargs):\n",
      "            assert partial is partial_val\n",
      "            assert \"many\" in kwargs\n",
      "            item[\"value\"] = item[\"value\"][len(self.TAG) :]\n",
      "            return item\n",
      "remove_tag(self=<ExampleSchema(many=False)>, item={'value': 'TAG4'}, partial=True, kwargs={'many': False}, partial_val=True, self.context={}, self.declared_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.dump_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.dump_only=set(), self.error_messages={'type': 'Invalid input type.', 'unknown': 'Unknown field.'}, self.exclude=set(), self.fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.load_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.load_only=set(), self.many=False, self.only=None, self.ordered=False, self.partial=True, self.unknown='raise')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "assert partial is partial_val\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def remove_tag(self, item, partial, **kwargs):\n",
      "            assert partial is partial_val\n",
      "            assert \"many\" in kwargs\n",
      "            item[\"value\"] = item[\"value\"][len(self.TAG) :]\n",
      "            return item\n",
      "remove_tag(self=<ExampleSchema(many=False)>, item={'value': 'TAG4'}, partial=True, kwargs={'many': False}, partial_val=True, self.context={}, self.declared_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.dump_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.dump_only=set(), self.error_messages={'type': 'Invalid input type.', 'unknown': 'Unknown field.'}, self.exclude=set(), self.fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.load_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.load_only=set(), self.many=False, self.only=None, self.ordered=False, self.partial=True, self.unknown='raise')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "assert \"many\" in kwargs\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def remove_tag(self, item, partial, **kwargs):\n",
      "            assert partial is partial_val\n",
      "            assert \"many\" in kwargs\n",
      "            item[\"value\"] = item[\"value\"][len(self.TAG) :]\n",
      "            return item\n",
      "remove_tag(self=<ExampleSchema(many=False)>, item={'value': 'TAG4'}, partial=True, kwargs={'many': False}, partial_val=True, self.context={}, self.declared_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.dump_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.dump_only=set(), self.error_messages={'type': 'Invalid input type.', 'unknown': 'Unknown field.'}, self.exclude=set(), self.fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.load_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.load_only=set(), self.many=False, self.only=None, self.ordered=False, self.partial=True, self.unknown='raise')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "item[\"value\"] = item[\"value\"][len(self.TAG) :]\n",
      "State:\n",
      "{'value': '4'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def decrement_value(self, item, partial, **kwargs):\n",
      "            assert partial is partial_val\n",
      "            assert \"many\" in kwargs\n",
      "            item[\"value\"] -= 1\n",
      "            return item\n",
      "decrement_value(self=<ExampleSchema(many=False)>, item={'value': 4}, partial=True, kwargs={'many': False}, partial_val=True, self.context={}, self.declared_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.dump_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.dump_only=set(), self.error_messages={'type': 'Invalid input type.', 'unknown': 'Unknown field.'}, self.exclude=set(), self.fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.load_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.load_only=set(), self.many=False, self.only=None, self.ordered=False, self.partial=True, self.unknown='raise')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "assert partial is partial_val\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def decrement_value(self, item, partial, **kwargs):\n",
      "            assert partial is partial_val\n",
      "            assert \"many\" in kwargs\n",
      "            item[\"value\"] -= 1\n",
      "            return item\n",
      "decrement_value(self=<ExampleSchema(many=False)>, item={'value': 4}, partial=True, kwargs={'many': False}, partial_val=True, self.context={}, self.declared_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.dump_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.dump_only=set(), self.error_messages={'type': 'Invalid input type.', 'unknown': 'Unknown field.'}, self.exclude=set(), self.fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.load_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.load_only=set(), self.many=False, self.only=None, self.ordered=False, self.partial=True, self.unknown='raise')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "assert \"many\" in kwargs\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def decrement_value(self, item, partial, **kwargs):\n",
      "            assert partial is partial_val\n",
      "            assert \"many\" in kwargs\n",
      "            item[\"value\"] -= 1\n",
      "            return item\n",
      "decrement_value(self=<ExampleSchema(many=False)>, item={'value': 4}, partial=True, kwargs={'many': False}, partial_val=True, self.context={}, self.declared_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.dump_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.dump_only=set(), self.error_messages={'type': 'Invalid input type.', 'unknown': 'Unknown field.'}, self.exclude=set(), self.fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.load_fields={'value': <fields.Integer(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid integer.', 'too_large': 'Number too large.'})>}, self.load_only=set(), self.many=False, self.only=None, self.ordered=False, self.partial=True, self.unknown='raise')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "item[\"value\"] -= 1\n",
      "State:\n",
      "{'value': 3}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def inherited(self, item, **kwargs):\n",
      "            item[\"inherited\"] = \"inherited\"\n",
      "            return item\n",
      "inherited(self=<ParentSchema(many=False)>, item={'deleted': 'retained'}, kwargs={'many': False}, self.context={}, self.declared_fields={}, self.dump_fields={}, self.dump_only=set(), self.error_messages={'type': 'Invalid input type.', 'unknown': 'Unknown field.'}, self.exclude=set(), self.fields={}, self.load_fields={}, self.load_only=set(), self.many=False, self.only=None, self.ordered=False, self.partial=None, self.unknown='raise')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "item[\"inherited\"] = \"inherited\"\n",
      "State:\n",
      "{'deleted': 'retained', 'inherited': 'inherited'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_rfc_datetime_field_deserialization(self, fmt, value, expected, aware):\n",
      "        field = fields.DateTime(format=fmt)\n",
      "        assert field.deserialize(value) == expected\n",
      "        field = fields.NaiveDateTime(format=fmt)\n",
      "        if aware:\n",
      "            with pytest.raises(ValidationError, match=\"Not a valid naive datetime.\"):\n",
      "                field.deserialize(value)\n",
      "        else:\n",
      "            assert field.deserialize(value) == expected\n",
      "        field = fields.AwareDateTime(format=fmt)\n",
      "        if not aware:\n",
      "            with pytest.raises(ValidationError, match=\"Not a valid aware datetime.\"):\n",
      "                field.deserialize(value)\n",
      "        else:\n",
      "            assert field.deserialize(value) == expected\n",
      "test_rfc_datetime_field_deserialization(self=<tests.test_deserialization.TestFieldDeserialization object at 0x7f27f99854f0>, fmt='rfc', value='Sun, 10 Nov 2013 01:23:45 -0000', expected=datetime.datetime(2013, 11, 10, 1, 23, 45), aware=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "field = fields.DateTime(format=fmt)\n",
      "State:\n",
      "<fields.DateTime(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid {obj_type}.', 'invalid_awareness': 'Not a valid {awareness} {obj_type}.', 'format': '\"{input}\" cannot be formatted as a {obj_type}.'})>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_rfc_datetime_field_deserialization(self, fmt, value, expected, aware):\n",
      "        field = fields.DateTime(format=fmt)\n",
      "        assert field.deserialize(value) == expected\n",
      "        field = fields.NaiveDateTime(format=fmt)\n",
      "        if aware:\n",
      "            with pytest.raises(ValidationError, match=\"Not a valid naive datetime.\"):\n",
      "                field.deserialize(value)\n",
      "        else:\n",
      "            assert field.deserialize(value) == expected\n",
      "        field = fields.AwareDateTime(format=fmt)\n",
      "        if not aware:\n",
      "            with pytest.raises(ValidationError, match=\"Not a valid aware datetime.\"):\n",
      "                field.deserialize(value)\n",
      "        else:\n",
      "            assert field.deserialize(value) == expected\n",
      "test_rfc_datetime_field_deserialization(self=<tests.test_deserialization.TestFieldDeserialization object at 0x7f27f99854f0>, fmt='rfc', value='Sun, 10 Nov 2013 01:23:45 -0000', expected=datetime.datetime(2013, 11, 10, 1, 23, 45), aware=False)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "assert field.deserialize(value) == expected\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_rfc_datetime_field_deserialization(self, fmt, value, expected, aware):\n",
      "        field = fields.DateTime(format=fmt)\n",
      "        assert field.deserialize(value) == expected\n",
      "        field = fields.NaiveDateTime(format=fmt)\n",
      "        if aware:\n",
      "            with pytest.raises(ValidationError, match=\"Not a valid naive datetime.\"):\n",
      "                field.deserialize(value)\n",
      "        else:\n",
      "            assert field.deserialize(value) == expected\n",
      "        field = fields.AwareDateTime(format=fmt)\n",
      "        if not aware:\n",
      "            with pytest.raises(ValidationError, match=\"Not a valid aware datetime.\"):\n",
      "                field.deserialize(value)\n",
      "        else:\n",
      "            assert field.deserialize(value) == expected\n",
      "test_rfc_datetime_field_deserialization(self=<tests.test_deserialization.TestFieldDeserialization object at 0x7f27f99854f0>, fmt='rfc', value='Sun, 10 Nov 2013 01:23:45 -0000', expected=datetime.datetime(2013, 11, 10, 1, 23, 45), aware=False)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "field = fields.NaiveDateTime(format=fmt)\n",
      "State:\n",
      "<fields.NaiveDateTime(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid {obj_type}.', 'invalid_awareness': 'Not a valid {awareness} {obj_type}.', 'format': '\"{input}\" cannot be formatted as a {obj_type}.'})>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_iso_datetime_field_deserialization(self, fmt, value, expected, aware):\n",
      "        field = fields.DateTime(format=fmt)\n",
      "        assert field.deserialize(value) == expected\n",
      "        field = fields.NaiveDateTime(format=fmt)\n",
      "        if aware:\n",
      "            with pytest.raises(ValidationError, match=\"Not a valid naive datetime.\"):\n",
      "                field.deserialize(value)\n",
      "        else:\n",
      "            assert field.deserialize(value) == expected\n",
      "        field = fields.AwareDateTime(format=fmt)\n",
      "        if not aware:\n",
      "            with pytest.raises(ValidationError, match=\"Not a valid aware datetime.\"):\n",
      "                field.deserialize(value)\n",
      "        else:\n",
      "            assert field.deserialize(value) == expected\n",
      "test_iso_datetime_field_deserialization(self=<tests.test_deserialization.TestFieldDeserialization object at 0x7f27f99c1b80>, fmt='iso', value='2013-11-10T01:23:45', expected=datetime.datetime(2013, 11, 10, 1, 23, 45), aware=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "field = fields.DateTime(format=fmt)\n",
      "State:\n",
      "<fields.DateTime(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid {obj_type}.', 'invalid_awareness': 'Not a valid {awareness} {obj_type}.', 'format': '\"{input}\" cannot be formatted as a {obj_type}.'})>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_iso_datetime_field_deserialization(self, fmt, value, expected, aware):\n",
      "        field = fields.DateTime(format=fmt)\n",
      "        assert field.deserialize(value) == expected\n",
      "        field = fields.NaiveDateTime(format=fmt)\n",
      "        if aware:\n",
      "            with pytest.raises(ValidationError, match=\"Not a valid naive datetime.\"):\n",
      "                field.deserialize(value)\n",
      "        else:\n",
      "            assert field.deserialize(value) == expected\n",
      "        field = fields.AwareDateTime(format=fmt)\n",
      "        if not aware:\n",
      "            with pytest.raises(ValidationError, match=\"Not a valid aware datetime.\"):\n",
      "                field.deserialize(value)\n",
      "        else:\n",
      "            assert field.deserialize(value) == expected\n",
      "test_iso_datetime_field_deserialization(self=<tests.test_deserialization.TestFieldDeserialization object at 0x7f27f99c1b80>, fmt='iso', value='2013-11-10T01:23:45', expected=datetime.datetime(2013, 11, 10, 1, 23, 45), aware=False)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "assert field.deserialize(value) == expected\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_iso_datetime_field_deserialization(self, fmt, value, expected, aware):\n",
      "        field = fields.DateTime(format=fmt)\n",
      "        assert field.deserialize(value) == expected\n",
      "        field = fields.NaiveDateTime(format=fmt)\n",
      "        if aware:\n",
      "            with pytest.raises(ValidationError, match=\"Not a valid naive datetime.\"):\n",
      "                field.deserialize(value)\n",
      "        else:\n",
      "            assert field.deserialize(value) == expected\n",
      "        field = fields.AwareDateTime(format=fmt)\n",
      "        if not aware:\n",
      "            with pytest.raises(ValidationError, match=\"Not a valid aware datetime.\"):\n",
      "                field.deserialize(value)\n",
      "        else:\n",
      "            assert field.deserialize(value) == expected\n",
      "test_iso_datetime_field_deserialization(self=<tests.test_deserialization.TestFieldDeserialization object at 0x7f27f99c1b80>, fmt='iso', value='2013-11-10T01:23:45', expected=datetime.datetime(2013, 11, 10, 1, 23, 45), aware=False)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "field = fields.NaiveDateTime(format=fmt)\n",
      "State:\n",
      "<fields.NaiveDateTime(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid {obj_type}.', 'invalid_awareness': 'Not a valid {awareness} {obj_type}.', 'format': '\"{input}\" cannot be formatted as a {obj_type}.'})>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_timestamp_field_deserialization(self, fmt, value, expected):\n",
      "        field = fields.DateTime(format=fmt)\n",
      "        assert field.deserialize(value) == expected\n",
      "        field = fields.NaiveDateTime(format=fmt)\n",
      "        assert field.deserialize(value) == expected\n",
      "        field = fields.AwareDateTime(format=fmt)\n",
      "        with pytest.raises(ValidationError, match=\"Not a valid aware datetime.\"):\n",
      "            field.deserialize(value)\n",
      "        field = fields.AwareDateTime(format=fmt, default_timezone=central)\n",
      "        expected_aware = expected.replace(tzinfo=central)\n",
      "        assert field.deserialize(value) == expected_aware\n",
      "test_timestamp_field_deserialization(self=<tests.test_deserialization.TestFieldDeserialization object at 0x7f27f99915b0>, fmt='timestamp', value=1384043025, expected=datetime.datetime(2013, 11, 10, 0, 23, 45))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "field = fields.DateTime(format=fmt)\n",
      "State:\n",
      "<fields.DateTime(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid {obj_type}.', 'invalid_awareness': 'Not a valid {awareness} {obj_type}.', 'format': '\"{input}\" cannot be formatted as a {obj_type}.'})>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_timestamp_field_deserialization(self, fmt, value, expected):\n",
      "        field = fields.DateTime(format=fmt)\n",
      "        assert field.deserialize(value) == expected\n",
      "        field = fields.NaiveDateTime(format=fmt)\n",
      "        assert field.deserialize(value) == expected\n",
      "        field = fields.AwareDateTime(format=fmt)\n",
      "        with pytest.raises(ValidationError, match=\"Not a valid aware datetime.\"):\n",
      "            field.deserialize(value)\n",
      "        field = fields.AwareDateTime(format=fmt, default_timezone=central)\n",
      "        expected_aware = expected.replace(tzinfo=central)\n",
      "        assert field.deserialize(value) == expected_aware\n",
      "test_timestamp_field_deserialization(self=<tests.test_deserialization.TestFieldDeserialization object at 0x7f27f99915b0>, fmt='timestamp', value=1384043025, expected=datetime.datetime(2013, 11, 10, 0, 23, 45))\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "assert field.deserialize(value) == expected\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_timestamp_field_deserialization(self, fmt, value, expected):\n",
      "        field = fields.DateTime(format=fmt)\n",
      "        assert field.deserialize(value) == expected\n",
      "        field = fields.NaiveDateTime(format=fmt)\n",
      "        assert field.deserialize(value) == expected\n",
      "        field = fields.AwareDateTime(format=fmt)\n",
      "        with pytest.raises(ValidationError, match=\"Not a valid aware datetime.\"):\n",
      "            field.deserialize(value)\n",
      "        field = fields.AwareDateTime(format=fmt, default_timezone=central)\n",
      "        expected_aware = expected.replace(tzinfo=central)\n",
      "        assert field.deserialize(value) == expected_aware\n",
      "test_timestamp_field_deserialization(self=<tests.test_deserialization.TestFieldDeserialization object at 0x7f27f99915b0>, fmt='timestamp', value=1384043025, expected=datetime.datetime(2013, 11, 10, 0, 23, 45))\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "field = fields.NaiveDateTime(format=fmt)\n",
      "State:\n",
      "<fields.NaiveDateTime(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid {obj_type}.', 'invalid_awareness': 'Not a valid {awareness} {obj_type}.', 'format': '\"{input}\" cannot be formatted as a {obj_type}.'})>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_timestamp_field_deserialization(self, fmt, value, expected):\n",
      "        field = fields.DateTime(format=fmt)\n",
      "        assert field.deserialize(value) == expected\n",
      "        field = fields.NaiveDateTime(format=fmt)\n",
      "        assert field.deserialize(value) == expected\n",
      "        field = fields.AwareDateTime(format=fmt)\n",
      "        with pytest.raises(ValidationError, match=\"Not a valid aware datetime.\"):\n",
      "            field.deserialize(value)\n",
      "        field = fields.AwareDateTime(format=fmt, default_timezone=central)\n",
      "        expected_aware = expected.replace(tzinfo=central)\n",
      "        assert field.deserialize(value) == expected_aware\n",
      "test_timestamp_field_deserialization(self=<tests.test_deserialization.TestFieldDeserialization object at 0x7f27f99915b0>, fmt='timestamp', value=1384043025, expected=datetime.datetime(2013, 11, 10, 0, 23, 45))\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "field.deserialize(value)\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_invalid_ip_deserialization(self, in_value):\n",
      "        field = fields.IP()\n",
      "        with pytest.raises(ValidationError) as excinfo:\n",
      "            field.deserialize(in_value)\n",
      "        assert excinfo.value.args[0] == \"Not a valid IP address.\"\n",
      "test_invalid_ip_deserialization(self=<tests.test_deserialization.TestFieldDeserialization object at 0x7f27f9018a90>, in_value='malformed')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "field = fields.IP()\n",
      "State:\n",
      "<fields.IP(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid_ip': 'Not a valid IP address.'})>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_invalid_ip_deserialization(self, in_value):\n",
      "        field = fields.IP()\n",
      "        with pytest.raises(ValidationError) as excinfo:\n",
      "            field.deserialize(in_value)\n",
      "        assert excinfo.value.args[0] == \"Not a valid IP address.\"\n",
      "test_invalid_ip_deserialization(self=<tests.test_deserialization.TestFieldDeserialization object at 0x7f27f9018a90>, in_value='malformed')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "assert excinfo.value.args[0] == \"Not a valid IP address.\"\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_partial_deserialization(self, partial_schema):\n",
      "        class MySchema(Schema):\n",
      "            foo = fields.Field(required=True)\n",
      "            bar = fields.Field(required=True)\n",
      "        schema_args = {}\n",
      "        load_args = {}\n",
      "        if partial_schema:\n",
      "            schema_args[\"partial\"] = True\n",
      "        else:\n",
      "            load_args[\"partial\"] = True\n",
      "        data = MySchema(**schema_args).load({\"foo\": 3}, **load_args)\n",
      "        assert data[\"foo\"] == 3\n",
      "        assert \"bar\" not in data\n",
      "test_partial_deserialization(self=<tests.test_deserialization.TestSchemaDeserialization object at 0x7f27f9affb80>, partial_schema=True)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "schema_args = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_required_field_failure(FieldClass):\n",
      "    class RequireSchema(Schema):\n",
      "        age = FieldClass(required=True)\n",
      "    user_data = {\"name\": \"Phil\"}\n",
      "    with pytest.raises(ValidationError) as excinfo:\n",
      "        RequireSchema().load(user_data)\n",
      "    errors = excinfo.value.messages\n",
      "    assert \"Missing data for required field.\" in errors[\"age\"]\n",
      "test_required_field_failure(FieldClass=<class 'marshmallow.fields.String'>)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "with pytest.raises(ValidationError) as excinfo:\n",
      "State:\n",
      "<ExceptionInfo for raises contextmanager>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_extra_metadata_may_be_added_to_field(self, FieldClass):\n",
      "        with pytest.warns(DeprecationWarning):\n",
      "            field = FieldClass(description=\"Just a normal field.\")\n",
      "        assert field.metadata[\"description\"] == \"Just a normal field.\"\n",
      "        field = FieldClass(\n",
      "            required=True,\n",
      "            dump_default=None,\n",
      "            validate=lambda v: True,\n",
      "            metadata={\"description\": \"foo\", \"widget\": \"select\"},\n",
      "        )\n",
      "        assert field.metadata == {\"description\": \"foo\", \"widget\": \"select\"}\n",
      "test_extra_metadata_may_be_added_to_field(self=<tests.test_fields.TestMetadata object at 0x7f27f8eff2e0>, FieldClass=<class 'marshmallow.fields.String'>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "field = FieldClass(description=\"Just a normal field.\")\n",
      "State:\n",
      "<fields.String(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid string.', 'invalid_utf8': 'Not a valid utf-8 string.'})>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_extra_metadata_may_be_added_to_field(self, FieldClass):\n",
      "        with pytest.warns(DeprecationWarning):\n",
      "            field = FieldClass(description=\"Just a normal field.\")\n",
      "        assert field.metadata[\"description\"] == \"Just a normal field.\"\n",
      "        field = FieldClass(\n",
      "            required=True,\n",
      "            dump_default=None,\n",
      "            validate=lambda v: True,\n",
      "            metadata={\"description\": \"foo\", \"widget\": \"select\"},\n",
      "        )\n",
      "        assert field.metadata == {\"description\": \"foo\", \"widget\": \"select\"}\n",
      "test_extra_metadata_may_be_added_to_field(self=<tests.test_fields.TestMetadata object at 0x7f27f8eff2e0>, FieldClass=<class 'marshmallow.fields.String'>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "assert field.metadata[\"description\"] == \"Just a normal field.\"\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_extra_metadata_may_be_added_to_field(self, FieldClass):\n",
      "        with pytest.warns(DeprecationWarning):\n",
      "            field = FieldClass(description=\"Just a normal field.\")\n",
      "        assert field.metadata[\"description\"] == \"Just a normal field.\"\n",
      "        field = FieldClass(\n",
      "            required=True,\n",
      "            dump_default=None,\n",
      "            validate=lambda v: True,\n",
      "            metadata={\"description\": \"foo\", \"widget\": \"select\"},\n",
      "        )\n",
      "        assert field.metadata == {\"description\": \"foo\", \"widget\": \"select\"}\n",
      "test_extra_metadata_may_be_added_to_field(self=<tests.test_fields.TestMetadata object at 0x7f27f8eff2e0>, FieldClass=<class 'marshmallow.fields.String'>)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "assert field.metadata == {\"description\": \"foo\", \"widget\": \"select\"}\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_field_metadata_added_in_deprecated_style_warns(self, FieldClass):\n",
      "        with pytest.warns(DeprecationWarning):\n",
      "            field = FieldClass(description=\"Just a normal field.\")\n",
      "            assert field.metadata[\"description\"] == \"Just a normal field.\"\n",
      "        with pytest.warns(DeprecationWarning):\n",
      "            field = FieldClass(\n",
      "                required=True,\n",
      "                dump_default=None,\n",
      "                validate=lambda v: True,\n",
      "                description=\"foo\",\n",
      "                metadata={\"widget\": \"select\"},\n",
      "            )\n",
      "        assert field.metadata == {\"description\": \"foo\", \"widget\": \"select\"}\n",
      "test_field_metadata_added_in_deprecated_style_warns(self=<tests.test_fields.TestMetadata object at 0x7f27f94782b0>, FieldClass=<class 'marshmallow.fields.String'>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "field = FieldClass(description=\"Just a normal field.\")\n",
      "State:\n",
      "<fields.String(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid string.', 'invalid_utf8': 'Not a valid utf-8 string.'})>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_load_default_in_deprecated_style_warns(self, FieldClass):\n",
      "        with pytest.warns(\n",
      "            DeprecationWarning,\n",
      "            match=\"The 'missing' argument to fields is deprecated. \"\n",
      "            \"Use 'load_default' instead.\",\n",
      "        ):\n",
      "            FieldClass(missing=None)\n",
      "        myfield = FieldClass(load_default=1)\n",
      "        with pytest.warns(\n",
      "            DeprecationWarning,\n",
      "            match=\"The 'missing' attribute of fields is deprecated. \"\n",
      "            \"Use 'load_default' instead.\",\n",
      "        ):\n",
      "            assert myfield.missing == 1\n",
      "        with pytest.warns(\n",
      "            DeprecationWarning,\n",
      "            match=\"The 'missing' attribute of fields is deprecated. \"\n",
      "            \"Use 'load_default' instead.\",\n",
      "        ):\n",
      "            myfield.missing = 0\n",
      "        assert myfield.load_default == 0\n",
      "test_load_default_in_deprecated_style_warns(self=<tests.test_fields.TestDeprecatedDefaultAndMissing object at 0x7f27f9467bb0>, FieldClass=<class 'marshmallow.fields.String'>)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "myfield = FieldClass(load_default=1)\n",
      "State:\n",
      "<fields.String(dump_default=<marshmallow.missing>, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=1, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid string.', 'invalid_utf8': 'Not a valid utf-8 string.'})>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_dump_default_in_deprecated_style_warns(self, FieldClass):\n",
      "        with pytest.warns(\n",
      "            DeprecationWarning,\n",
      "            match=\"The 'default' argument to fields is deprecated. \"\n",
      "            \"Use 'dump_default' instead.\",\n",
      "        ):\n",
      "            FieldClass(default=None)\n",
      "        myfield = FieldClass(dump_default=1)\n",
      "        with pytest.warns(\n",
      "            DeprecationWarning,\n",
      "            match=\"The 'default' attribute of fields is deprecated. \"\n",
      "            \"Use 'dump_default' instead.\",\n",
      "        ):\n",
      "            assert myfield.default == 1\n",
      "        with pytest.warns(\n",
      "            DeprecationWarning,\n",
      "            match=\"The 'default' attribute of fields is deprecated. \"\n",
      "            \"Use 'dump_default' instead.\",\n",
      "        ):\n",
      "            myfield.default = 0\n",
      "        assert myfield.dump_default == 0\n",
      "test_dump_default_in_deprecated_style_warns(self=<tests.test_fields.TestDeprecatedDefaultAndMissing object at 0x7f27f92eae50>, FieldClass=<class 'marshmallow.fields.String'>)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "myfield = FieldClass(dump_default=1)\n",
      "State:\n",
      "<fields.String(dump_default=1, attribute=None, validate=None, required=False, load_only=False, dump_only=False, load_default=<marshmallow.missing>, allow_none=False, error_messages={'required': 'Missing data for required field.', 'null': 'Field may not be null.', 'validator_failed': 'Invalid value.', 'invalid': 'Not a valid string.', 'invalid_utf8': 'Not a valid utf-8 string.'})>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _pntcoord(sol, i, n, m, distances, prec):\n",
      "    tmppnt = []\n",
      "    state = 1\n",
      "    pntscount = len(sol)\n",
      "    if distances[i, n] < -0.5 or distances[i, m] < -0.5:\n",
      "        return tmppnt, state\n",
      "    if distances[i, n] + distances[i, m] < _dist(sol[n], sol[m]):\n",
      "        state = 2\n",
      "        return tmppnt, state\n",
      "    g = _affinef(*_invtranmat(*_tranmat(sol[n], sol[m])))\n",
      "    x = _xvalue(distances[i, n], distances[i, m], _dist(sol[n], sol[m]))\n",
      "    y1, y2 = _yvalue(distances[i, n], distances[i, m], _dist(sol[n], sol[m]))\n",
      "    pos1 = g(np.array([x, y1]))\n",
      "    pos2 = g(np.array([x, y2]))\n",
      "    valid1 = True\n",
      "    valid2 = True\n",
      "    for k in range(pntscount):\n",
      "        if np.ndim(sol[k]) != 0 and distances[i, k] > -0.5:\n",
      "            valid1 &= abs(_dist(sol[k], pos1) - distances[i, k]) < prec\n",
      "            valid2 &= abs(_dist(sol[k], pos2) - distances[i, k]) < prec\n",
      "    if valid1 or valid2:\n",
      "        state = 0\n",
      "        same = abs(y1 - y2) < prec / 4.0\n",
      "        if valid1:\n",
      "            tmppnt.append(dcopy(pos1))\n",
      "        if valid2 and not same:\n",
      "            tmppnt.append(dcopy(pos2))\n",
      "    else:\n",
      "        state = 2\n",
      "    return tmppnt, state\n",
      "_pntcoord(sol=[array([0., 0.]), array([3., 0.]), 0, 0], i=2, n=0, m=1, distances=array([[ 0.,  3.,  4.,  1.],       [ 3.,  0.,  2.,  3.],       [ 4.,  2.,  0., -1.],       [ 1.,  3., -1.,  0.]]), prec=0.1)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "pntscount = len(sol)\n",
      "State:\n",
      "4\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _solequal(sol1, sol2, prec):\n",
      "    res = True\n",
      "    for sol_1, sol_2 in zip(sol1, sol2):\n",
      "        if np.ndim(sol_1) != 0 and np.ndim(sol_2) != 0:\n",
      "            res &= _dist(sol_1, sol_2) < prec\n",
      "        elif np.ndim(sol_1) != 0 and np.ndim(sol_2) == 0:\n",
      "            return False\n",
      "        elif np.ndim(sol_1) == 0 and np.ndim(sol_2) != 0:\n",
      "            return False\n",
      "    return res\n",
      "_solequal(sol1=[array([0., 0.]), array([3., 0.]), array([3.5       , 1.93649167]), array([ 0.16666667, -0.9860133 ])], sol2=[array([0., 0.]), array([3., 0.]), array([3.5       , 1.93649167]), array([0.16666667, 0.9860133 ])], prec=0.1)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "res &= _dist(sol_1, sol_2) < prec\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def startElement(self, full_name, attrs):\n",
      "        name = self._build_name(full_name)\n",
      "        attrs = self._attrs_to_dict(attrs)\n",
      "        if attrs and self.namespace_declarations:\n",
      "            attrs['xmlns'] = self.namespace_declarations\n",
      "            self.namespace_declarations = self.dict_constructor()\n",
      "        self.path.append((name, attrs or None))\n",
      "        if len(self.path) >= self.item_depth:\n",
      "            self.stack.append((self.item, self.data))\n",
      "            if self.xml_attribs:\n",
      "                attr_entries = []\n",
      "                for key, value in attrs.items():\n",
      "                    key = self.attr_prefix+self._build_name(key)\n",
      "                    if self.postprocessor:\n",
      "                        entry = self.postprocessor(self.path, key, value)\n",
      "                    else:\n",
      "                        entry = (key, value)\n",
      "                    if entry:\n",
      "                        attr_entries.append(entry)\n",
      "                attrs = self.dict_constructor(attr_entries)\n",
      "            else:\n",
      "                attrs = None\n",
      "            self.item = attrs or None\n",
      "            self.data = []\n",
      "startElement(self=<xmltodict._DictSAXHandler object at 0x7f2c78685910>, full_name='root', attrs=['a', '1', 'b', '2', 'c', '3'], self.attr_prefix='@', self.cdata_key='\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "name = self._build_name(full_name)\n",
      "State:\n",
      "'root'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def startElement(self, full_name, attrs):\n",
      "        name = self._build_name(full_name)\n",
      "        attrs = self._attrs_to_dict(attrs)\n",
      "        if attrs and self.namespace_declarations:\n",
      "            attrs['xmlns'] = self.namespace_declarations\n",
      "            self.namespace_declarations = self.dict_constructor()\n",
      "        self.path.append((name, attrs or None))\n",
      "        if len(self.path) >= self.item_depth:\n",
      "            self.stack.append((self.item, self.data))\n",
      "            if self.xml_attribs:\n",
      "                attr_entries = []\n",
      "                for key, value in attrs.items():\n",
      "                    key = self.attr_prefix+self._build_name(key)\n",
      "                    if self.postprocessor:\n",
      "                        entry = self.postprocessor(self.path, key, value)\n",
      "                    else:\n",
      "                        entry = (key, value)\n",
      "                    if entry:\n",
      "                        attr_entries.append(entry)\n",
      "                attrs = self.dict_constructor(attr_entries)\n",
      "            else:\n",
      "                attrs = None\n",
      "            self.item = attrs or None\n",
      "            self.data = []\n",
      "startElement(self=<xmltodict._DictSAXHandler object at 0x7f2c78685910>, full_name='root', attrs=['a', '1', 'b', '2', 'c', '3'], self.attr_prefix='@', self.cdata_key='\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "attrs = self._attrs_to_dict(attrs)\n",
      "State:\n",
      "{'a': '1', 'b': '2', 'c': '3'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def startElement(self, full_name, attrs):\n",
      "        name = self._build_name(full_name)\n",
      "        attrs = self._attrs_to_dict(attrs)\n",
      "        if attrs and self.namespace_declarations:\n",
      "            attrs['xmlns'] = self.namespace_declarations\n",
      "            self.namespace_declarations = self.dict_constructor()\n",
      "        self.path.append((name, attrs or None))\n",
      "        if len(self.path) >= self.item_depth:\n",
      "            self.stack.append((self.item, self.data))\n",
      "            if self.xml_attribs:\n",
      "                attr_entries = []\n",
      "                for key, value in attrs.items():\n",
      "                    key = self.attr_prefix+self._build_name(key)\n",
      "                    if self.postprocessor:\n",
      "                        entry = self.postprocessor(self.path, key, value)\n",
      "                    else:\n",
      "                        entry = (key, value)\n",
      "                    if entry:\n",
      "                        attr_entries.append(entry)\n",
      "                attrs = self.dict_constructor(attr_entries)\n",
      "            else:\n",
      "                attrs = None\n",
      "            self.item = attrs or None\n",
      "            self.data = []\n",
      "startElement(self=<xmltodict._DictSAXHandler object at 0x7f2c78685910>, full_name='root', attrs=['a', '1', 'b', '2', 'c', '3'], self.attr_prefix='@', self.cdata_key='\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "attr_entries = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _build_name(self, full_name):\n",
      "        if self.namespaces is None:\n",
      "            return full_name\n",
      "        i = full_name.rfind(self.namespace_separator)\n",
      "        if i == -1:\n",
      "            return full_name\n",
      "        namespace, name = full_name[:i], full_name[i+1:]\n",
      "        try:\n",
      "            short_namespace = self.namespaces[namespace]\n",
      "        except KeyError:\n",
      "            short_namespace = namespace\n",
      "        if not short_namespace:\n",
      "            return name\n",
      "        else:\n",
      "            return self.namespace_separator.join((short_namespace, name))\n",
      "_build_name(self=<xmltodict._DictSAXHandler object at 0x7f2c78683040>, full_name='http://defaultns.com/:root', self.attr_prefix='@', self.cdata_key='\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "i = full_name.rfind(self.namespace_separator)\n",
      "State:\n",
      "21\n",
      "==================================================\n",
      "Clean Code:\n",
      "def endElement(self, full_name):\n",
      "        name = self._build_name(full_name)\n",
      "        if len(self.path) == self.item_depth:\n",
      "            item = self.item\n",
      "            if item is None:\n",
      "                item = (None if not self.data\n",
      "                        else self.cdata_separator.join(self.data))\n",
      "            should_continue = self.item_callback(self.path, item)\n",
      "            if not should_continue:\n",
      "                raise ParsingInterrupted()\n",
      "        if self.stack:\n",
      "            data = (None if not self.data\n",
      "                    else self.cdata_separator.join(self.data))\n",
      "            item = self.item\n",
      "            self.item, self.data = self.stack.pop()\n",
      "            if self.strip_whitespace and data and item:\n",
      "                data = data.strip() or None\n",
      "            if data and self.force_cdata and item is None:\n",
      "                item = self.dict_constructor()\n",
      "            if item is not None:\n",
      "                if data:\n",
      "                    self.push_data(item, self.cdata_key, data)\n",
      "                self.item = self.push_data(self.item, name, item)\n",
      "            else:\n",
      "                self.item = self.push_data(self.item, name, data)\n",
      "        else:\n",
      "            self.item = None\n",
      "            self.data = []\n",
      "        self.path.pop()\n",
      "endElement(self=<xmltodict._DictSAXHandler object at 0x7f2c78685910>, full_name='root', self.attr_prefix='@', self.cdata_key='\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "name = self._build_name(full_name)\n",
      "State:\n",
      "'root'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _process_namespace(name, namespaces, ns_sep=':', attr_prefix='@'):\n",
      "    if not namespaces:\n",
      "        return name\n",
      "    try:\n",
      "        ns, name = name.rsplit(ns_sep, 1)\n",
      "    except ValueError:\n",
      "        pass\n",
      "    else:\n",
      "        ns_res = namespaces.get(ns.strip(attr_prefix))\n",
      "        name = '{}{}{}{}'.format(\n",
      "            attr_prefix if ns.startswith(attr_prefix) else '',\n",
      "            ns_res, ns_sep, name) if ns_res else name\n",
      "    return name\n",
      "_process_namespace(name='http://defaultns.com/:root', namespaces={'http://defaultns.com/': '', 'http://a.com/': 'a', 'http://b.com/': 'b'}, ns_sep=':', attr_prefix='@')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "ns, name = name.rsplit(ns_sep, 1)\n",
      "State:\n",
      "'http://defaultns.com/'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def characters(self, data):\n",
      "        if not self.data:\n",
      "            self.data = [data]\n",
      "        else:\n",
      "            self.data.append(data)\n",
      "characters(self=<xmltodict._DictSAXHandler object at 0x7f2c786859a0>, data='y', self.attr_prefix='@', self.cdata_key='\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.data = [data]\n",
      "State:\n",
      "['y']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def comments(self, data):\n",
      "        if self.strip_whitespace:\n",
      "            data = data.strip()\n",
      "        self.item = self.push_data(self.item, self.comment_key, data)\n",
      "comments(self=<xmltodict._DictSAXHandler object at 0x7f2c78888c10>, data=' b comment ', self.attr_prefix='@', self.cdata_key='\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "data = data.strip()\n",
      "State:\n",
      "'b comment'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def comments(self, data):\n",
      "        if self.strip_whitespace:\n",
      "            data = data.strip()\n",
      "        self.item = self.push_data(self.item, self.comment_key, data)\n",
      "comments(self=<xmltodict._DictSAXHandler object at 0x7f2c78888c10>, data=' b comment ', self.attr_prefix='@', self.cdata_key='\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.item = self.push_data(self.item, self.comment_key, data)\n",
      "State:\n",
      "{'#comment': 'b comment'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def response_error(code_status, message=None):\n",
      "    payload = {'error': HTTP_STATUS_CODES.get(code_status, \"something went wrong\")}\n",
      "    if message:\n",
      "        payload['message'] = message\n",
      "    response = jsonify(payload)\n",
      "    response.status_code = code_status\n",
      "    return response\n",
      "response_error(code_status=400, message='Invalid input')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "payload = {'error': HTTP_STATUS_CODES.get(code_status, \"something went wrong\")}\n",
      "State:\n",
      "{'error': 'Bad Request'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def response_error(code_status, message=None):\n",
      "    payload = {'error': HTTP_STATUS_CODES.get(code_status, \"something went wrong\")}\n",
      "    if message:\n",
      "        payload['message'] = message\n",
      "    response = jsonify(payload)\n",
      "    response.status_code = code_status\n",
      "    return response\n",
      "response_error(code_status=400, message='Invalid input')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "payload['message'] = message\n",
      "State:\n",
      "{'error': 'Bad Request', 'message': 'Invalid input'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def response_error(code_status, message=None):\n",
      "    payload = {'error': HTTP_STATUS_CODES.get(code_status, \"something went wrong\")}\n",
      "    if message:\n",
      "        payload['message'] = message\n",
      "    response = jsonify(payload)\n",
      "    response.status_code = code_status\n",
      "    return response\n",
      "response_error(code_status=400, message='Invalid input')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "response = jsonify(payload)\n",
      "State:\n",
      "<Response 50 bytes [200 OK]>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def response_error(code_status, message=None):\n",
      "    payload = {'error': HTTP_STATUS_CODES.get(code_status, \"something went wrong\")}\n",
      "    if message:\n",
      "        payload['message'] = message\n",
      "    response = jsonify(payload)\n",
      "    response.status_code = code_status\n",
      "    return response\n",
      "response_error(code_status=400, message='Invalid input')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "response.status_code = code_status\n",
      "State:\n",
      "<Response 50 bytes [400 BAD REQUEST]>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_base_urls(self, urls):\n",
      "        base_urls = []\n",
      "        for i in urls:\n",
      "            parsed_url = urlparse(i)\n",
      "            base_url = parsed_url.scheme + \"://\" + parsed_url.netloc\n",
      "            if base_url not in base_urls:\n",
      "                base_urls.append(base_url)\n",
      "        return base_urls\n",
      "get_base_urls(self=<application.parser.file.openapi3_parser.OpenAPI3Parser object at 0x7fea8cf55d30>, urls=['http://petstore.swagger.io/v1', 'https://api.example.com/v1/resource', 'https://api.example.com/v1/another/resource', 'https://api.example.com/v1/some/endpoint'], self._parser_config=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "base_urls = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_info_from_paths(self, path):\n",
      "        info = \"\"\n",
      "        if path.operations:\n",
      "            for operation in path.operations:\n",
      "                info += (\n",
      "                    f\"\\n{operation.method.value}=\"\n",
      "                    f\"{operation.responses[0].description}\"\n",
      "                )\n",
      "        return info\n",
      "get_info_from_paths(self=<application.parser.file.openapi3_parser.OpenAPI3Parser object at 0x7fea8c9ce700>, path=Path(url='/pets/{petId}', summary=None, description=None, operations=[Operation(method=<OperationMethod.GET: 'get'>, responses=[Response(is_default=False, description='Expected response to a valid request', code=200, content=[Content(type=<ContentType.JSON: 'application/json'>, schema=Object(type=<DataType.OBJECT: 'object'>, title=None, enum=[], example=None, description=None, default=None, nullable=False, read_only=False, write_only=False, deprecated=False, extensions={}, max_properties=None, min_properties=None, required=['id', 'name'], properties=[Property(name='id', schema=Integer(type=<DataType.INTEGER: 'integer'>, title=None, enum=[], example=None, description=None, default=None, nullable=False, read_only=False, write_only=False, deprecated=False, extensions={}, multiple_of=None, maximum=None, exclusive_maximum=None, minimum=None, exclusive_minimum=None, format=<IntegerFormat.INT64: 'int64'>)), Property(name='name', schema=String(type=<DataType.STRING: 'string'>, title=None, enum=[], example=None, description=None, default=None, nullable=False, read_only=False, write_only=False, deprecated=False, extensions={}, max_length=None, min_length=None, pattern=None, format=None)), Property(name='tag', schema=String(type=<DataType.STRING: 'string'>, title=None, enum=[], example=None, description=None, default=None, nullable=False, read_only=False, write_only=False, deprecated=False, extensions={}, max_length=None, min_length=None, pattern=None, format=None))]))], headers=[]), Response(is_default=True, description='unexpected error', code=None, content=[Content(type=<ContentType.JSON: 'application/json'>, schema=Object(type=<DataType.OBJECT: 'object'>, title=None, enum=[], example=None, description=None, default=None, nullable=False, read_only=False, write_only=False, deprecated=False, extensions={}, max_properties=None, min_properties=None, required=['code', 'message'], properties=[Property(name='code', schema=Integer(type=<DataType.INTEGER: 'integer'>, title=None, enum=[], example=None, description=None, default=None, nullable=False, read_only=False, write_only=False, deprecated=False, extensions={}, multiple_of=None, maximum=None, exclusive_maximum=None, minimum=None, exclusive_minimum=None, format=<IntegerFormat.INT32: 'int32'>)), Property(name='message', schema=String(type=<DataType.STRING: 'string'>, title=None, enum=[], example=None, description=None, default=None, nullable=False, read_only=False, write_only=False, deprecated=False, extensions={}, max_length=None, min_length=None, pattern=None, format=None))]))], headers=[])], summary='Info for a specific pet', description=None, operation_id='showPetById', external_docs=None, request_body=None, deprecated=False, parameters=[Parameter(name='petId', location=<ParameterLocation.PATH: 'path'>, schema=String(type=<DataType.STRING: 'string'>, title=None, enum=[], example=None, description=None, default=None, nullable=False, read_only=False, write_only=False, deprecated=False, extensions={}, max_length=None, min_length=None, pattern=None, format=None), required=True, description='The id of the pet to retrieve', deprecated=False, style=<PathParameterStyle.SIMPLE: 'simple'>, explode=False)], tags=['pets'], security=[])], parameters=[]), self._parser_config=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "info = \"\"\n",
      "State:\n",
      "''\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_info_from_paths(self, path):\n",
      "        info = \"\"\n",
      "        if path.operations:\n",
      "            for operation in path.operations:\n",
      "                info += (\n",
      "                    f\"\\n{operation.method.value}=\"\n",
      "                    f\"{operation.responses[0].description}\"\n",
      "                )\n",
      "        return info\n",
      "get_info_from_paths(self=<application.parser.file.openapi3_parser.OpenAPI3Parser object at 0x7fea8c9ce700>, path=Path(url='/pets/{petId}', summary=None, description=None, operations=[Operation(method=<OperationMethod.GET: 'get'>, responses=[Response(is_default=False, description='Expected response to a valid request', code=200, content=[Content(type=<ContentType.JSON: 'application/json'>, schema=Object(type=<DataType.OBJECT: 'object'>, title=None, enum=[], example=None, description=None, default=None, nullable=False, read_only=False, write_only=False, deprecated=False, extensions={}, max_properties=None, min_properties=None, required=['id', 'name'], properties=[Property(name='id', schema=Integer(type=<DataType.INTEGER: 'integer'>, title=None, enum=[], example=None, description=None, default=None, nullable=False, read_only=False, write_only=False, deprecated=False, extensions={}, multiple_of=None, maximum=None, exclusive_maximum=None, minimum=None, exclusive_minimum=None, format=<IntegerFormat.INT64: 'int64'>)), Property(name='name', schema=String(type=<DataType.STRING: 'string'>, title=None, enum=[], example=None, description=None, default=None, nullable=False, read_only=False, write_only=False, deprecated=False, extensions={}, max_length=None, min_length=None, pattern=None, format=None)), Property(name='tag', schema=String(type=<DataType.STRING: 'string'>, title=None, enum=[], example=None, description=None, default=None, nullable=False, read_only=False, write_only=False, deprecated=False, extensions={}, max_length=None, min_length=None, pattern=None, format=None))]))], headers=[]), Response(is_default=True, description='unexpected error', code=None, content=[Content(type=<ContentType.JSON: 'application/json'>, schema=Object(type=<DataType.OBJECT: 'object'>, title=None, enum=[], example=None, description=None, default=None, nullable=False, read_only=False, write_only=False, deprecated=False, extensions={}, max_properties=None, min_properties=None, required=['code', 'message'], properties=[Property(name='code', schema=Integer(type=<DataType.INTEGER: 'integer'>, title=None, enum=[], example=None, description=None, default=None, nullable=False, read_only=False, write_only=False, deprecated=False, extensions={}, multiple_of=None, maximum=None, exclusive_maximum=None, minimum=None, exclusive_minimum=None, format=<IntegerFormat.INT32: 'int32'>)), Property(name='message', schema=String(type=<DataType.STRING: 'string'>, title=None, enum=[], example=None, description=None, default=None, nullable=False, read_only=False, write_only=False, deprecated=False, extensions={}, max_length=None, min_length=None, pattern=None, format=None))]))], headers=[])], summary='Info for a specific pet', description=None, operation_id='showPetById', external_docs=None, request_body=None, deprecated=False, parameters=[Parameter(name='petId', location=<ParameterLocation.PATH: 'path'>, schema=String(type=<DataType.STRING: 'string'>, title=None, enum=[], example=None, description=None, default=None, nullable=False, read_only=False, write_only=False, deprecated=False, extensions={}, max_length=None, min_length=None, pattern=None, format=None), required=True, description='The id of the pet to retrieve', deprecated=False, style=<PathParameterStyle.SIMPLE: 'simple'>, explode=False)], tags=['pets'], security=[])], parameters=[]), self._parser_config=None)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "info += (\n",
      "State:\n",
      "'\\nget=Expected response to a valid request'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __next__(self):\n",
      "        while True:\n",
      "            self.buffer.seek(self.read_pos)\n",
      "            line = self.buffer.readline()\n",
      "            if line and line[-1] == ord('\\n'):\n",
      "                self.read_pos += len(line)\n",
      "                return line[:-1]\n",
      "            try:\n",
      "                chunk = next(self.byte_iterator)\n",
      "            except StopIteration:\n",
      "                if self.read_pos < self.buffer.getbuffer().nbytes:\n",
      "                    continue\n",
      "                raise\n",
      "            if 'PayloadPart' not in chunk:\n",
      "                print('Unknown event type:' + chunk)\n",
      "                continue\n",
      "            self.buffer.seek(0, io.SEEK_END)\n",
      "            self.buffer.write(chunk['PayloadPart']['Bytes'])\n",
      "__next__(self=<application.llm.sagemaker.LineIterator object at 0x7fea87997070>, self.buffer=<_io.BytesIO object at 0x7fea879a1d10>, self.byte_iterator=<list_iterator object at 0x7fea879a5040>, self.read_pos=0)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "line = self.buffer.readline()\n",
      "State:\n",
      "b''\n",
      "==================================================\n",
      "Clean Code:\n",
      "def check_data(a, b):\n",
      "    if not isinstance(a, np.ndarray):\n",
      "        a = np.array(a)\n",
      "    if not isinstance(b, np.ndarray):\n",
      "        b = np.array(b)\n",
      "    if type(a) != type(b):\n",
      "        raise ValueError(\"Type mismatch: %s and %s\" % (type(a), type(b)))\n",
      "    if a.size != b.size:\n",
      "        raise ValueError(\"Arrays must be equal in length.\")\n",
      "    return a, b\n",
      "check_data(a=[], b=1)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "a = np.array(a)\n",
      "State:\n",
      "array([], dtype=float64)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def wrapper(a, b):\n",
      "        a, b = check_data(a, b)\n",
      "        return function(a, b)\n",
      "wrapper(a=[1, 2, 3, 4], b=[1, 2, 3, 4], function=<function unhot.<locals>.wrapper at 0x7f4414e29790>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "a, b = check_data(a, b)\n",
      "State:\n",
      "array([1, 2, 3, 4])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def logloss(actual, predicted):\n",
      "    predicted = np.clip(predicted, EPS, 1 - EPS)\n",
      "    loss = -np.sum(actual * np.log(predicted))\n",
      "    return loss / float(actual.shape[0])\n",
      "logloss(actual=array([1]), predicted=array([1]))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "predicted = np.clip(predicted, EPS, 1 - EPS)\n",
      "State:\n",
      "array([1.])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def logloss(actual, predicted):\n",
      "    predicted = np.clip(predicted, EPS, 1 - EPS)\n",
      "    loss = -np.sum(actual * np.log(predicted))\n",
      "    return loss / float(actual.shape[0])\n",
      "logloss(actual=array([1]), predicted=array([1]))\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "loss = -np.sum(actual * np.log(predicted))\n",
      "State:\n",
      "9.992007221626415e-16\n",
      "==================================================\n",
      "Clean Code:\n",
      "def clasifier(optimizer):\n",
      "    X, y = make_classification(\n",
      "        n_samples=1000, n_features=100, n_informative=75, random_state=1111, n_classes=2, class_sep=2.5\n",
      "    )\n",
      "    y = one_hot(y)\n",
      "    X -= np.mean(X, axis=0)\n",
      "    X /= np.std(X, axis=0)\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1111)\n",
      "    model = NeuralNet(\n",
      "        layers=[\n",
      "            Dense(128, Parameters(init=\"uniform\")),\n",
      "            Activation(\"relu\"),\n",
      "            Dropout(0.5),\n",
      "            Dense(64, Parameters(init=\"normal\")),\n",
      "            Activation(\"relu\"),\n",
      "            Dense(2),\n",
      "            Activation(\"softmax\"),\n",
      "        ],\n",
      "        loss=\"categorical_crossentropy\",\n",
      "        optimizer=optimizer,\n",
      "        metric=\"accuracy\",\n",
      "        batch_size=64,\n",
      "        max_epochs=10,\n",
      "    )\n",
      "    model.fit(X_train, y_train)\n",
      "    predictions = model.predict(X_test)\n",
      "    return roc_auc_score(y_test[:, 0], predictions[:, 0])\n",
      "clasifier(optimizer={rho=0.95, eps=1e-08, lr=1.0})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "X, y = make_classification(\n",
      "State:\n",
      "array([[  7.80780096,   0.40627657,  -1.48027083, ...,  -6.41386247,         -8.43409921,   0.5602089 ],       [  1.19388878,  -1.47559934,   6.08370626, ...,  -0.97569605,         10.78028251,  -1.31354468],       [ -6.1613578 ,  -0.43920608,  -1.28034138, ...,   4.39277501,          0.02982879,   0.06733291],       ...,       [ -5.51466423, -13.26000331,  10.36091139, ...,   1.89534754,         -3.35155079,  -1.19447879],       [  9.13942123,  -1.21728958,   1.84978723, ...,  -1.75954701,          7.32589326,   3.2035972 ],       [-10.31153907,  -5.04412657,  -0.1021587 , ...,  10.74854013,         -0.48404546,   4.45441204]])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def clasifier(optimizer):\n",
      "    X, y = make_classification(\n",
      "        n_samples=1000, n_features=100, n_informative=75, random_state=1111, n_classes=2, class_sep=2.5\n",
      "    )\n",
      "    y = one_hot(y)\n",
      "    X -= np.mean(X, axis=0)\n",
      "    X /= np.std(X, axis=0)\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1111)\n",
      "    model = NeuralNet(\n",
      "        layers=[\n",
      "            Dense(128, Parameters(init=\"uniform\")),\n",
      "            Activation(\"relu\"),\n",
      "            Dropout(0.5),\n",
      "            Dense(64, Parameters(init=\"normal\")),\n",
      "            Activation(\"relu\"),\n",
      "            Dense(2),\n",
      "            Activation(\"softmax\"),\n",
      "        ],\n",
      "        loss=\"categorical_crossentropy\",\n",
      "        optimizer=optimizer,\n",
      "        metric=\"accuracy\",\n",
      "        batch_size=64,\n",
      "        max_epochs=10,\n",
      "    )\n",
      "    model.fit(X_train, y_train)\n",
      "    predictions = model.predict(X_test)\n",
      "    return roc_auc_score(y_test[:, 0], predictions[:, 0])\n",
      "clasifier(optimizer={rho=0.95, eps=1e-08, lr=1.0})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "y = one_hot(y)\n",
      "State:\n",
      "array([[0., 1.],       [1., 0.],       [1., 0.],       ...,       [1., 0.],       [0., 1.],       [1., 0.]])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def fit(self, X, y=None):\n",
      "        if not self._initialized:\n",
      "            self._setup_layers(X.shape)\n",
      "        if y.ndim == 1:\n",
      "            y = y[:, np.newaxis]\n",
      "        self._setup_input(X, y)\n",
      "        self.is_training = True\n",
      "        self.optimizer.optimize(self)\n",
      "        self.is_training = False\n",
      "fit(self=<mla.neuralnet.nnet.NeuralNet object at 0x7f43904a3700>, X=array([[ 0.52035826, -1.70073552, -0.50697874, ..., -0.74790479,         0.16039734,  1.0323546 ],       [ 0.96925333, -0.29561268,  0.86400006, ...,  1.4457464 ,        -1.32196293, -0.0653791 ],       [ 0.24562753,  0.62176281,  0.95963704, ..., -0.16747796,        -0.60033504,  1.64606491],       ...,       [ 1.14501028,  1.07221843,  1.2764346 , ..., -0.83624018,        -1.87719572,  0.57666069],       [ 1.98506244, -0.27254282,  1.58679531, ..., -0.73272139,         2.03923433, -1.41719355],       [-0.82634613,  1.99698567,  0.0889722 , ...,  0.83719252,        -0.88980401,  0.81743782]]), y=array([[1., 0.],       [1., 0.],       [1., 0.],       ...,       [0., 1.],       [1., 0.],       [1., 0.]]), self._initialized=False, self._n_layers=0, self.batch_size=64, self.bprop_entry=-1, self.layers=[<mla.neuralnet.layers.basic.Dense object at 0x7f43904a3610>, <mla.neuralnet.layers.basic.Activation object at 0x7f43904a33d0>, <mla.neuralnet.layers.basic.Dropout object at 0x7f43904a32b0>, <mla.neuralnet.layers.basic.Dense object at 0x7f43904a3c70>, <mla.neuralnet.layers.basic.Activation object at 0x7f43904a3550>, <mla.neuralnet.layers.basic.Dense object at 0x7f43904a3820>, <mla.neuralnet.layers.basic.Activation object at 0x7f43904a3730>], self.log_metric=True, self.loss=<function logloss at 0x7f4414e29e50>, self.loss_grad=<function NeuralNet.__init__.<locals>.<lambda> at 0x7f439043c790>, self.max_epochs=10, self.metric=<function unhot.<locals>.wrapper at 0x7f4414e29820>, self.metric_name='accuracy', self.optimizer=<mla.neuralnet.optimizers.Adadelta object at 0x7f43904a38e0>, self.shuffle=False, self.training=False, self.verbose=True)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self._setup_layers(X.shape)\n",
      "State:\n",
      "True\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _setup_layers(self, x_shape):\n",
      "        x_shape = list(x_shape)\n",
      "        x_shape[0] = self.batch_size\n",
      "        for layer in self.layers:\n",
      "            layer.setup(x_shape)\n",
      "            x_shape = layer.shape(x_shape)\n",
      "        self._n_layers = len(self.layers)\n",
      "        self.optimizer.setup(self)\n",
      "        self._initialized = True\n",
      "        logging.info(\"Total parameters: %s\" % self.n_params)\n",
      "_setup_layers(self=<mla.neuralnet.nnet.NeuralNet object at 0x7f43904a3700>, x_shape=(850, 100), self._initialized=False, self._n_layers=0, self.batch_size=64, self.bprop_entry=-1, self.layers=[<mla.neuralnet.layers.basic.Dense object at 0x7f43904a3610>, <mla.neuralnet.layers.basic.Activation object at 0x7f43904a33d0>, <mla.neuralnet.layers.basic.Dropout object at 0x7f43904a32b0>, <mla.neuralnet.layers.basic.Dense object at 0x7f43904a3c70>, <mla.neuralnet.layers.basic.Activation object at 0x7f43904a3550>, <mla.neuralnet.layers.basic.Dense object at 0x7f43904a3820>, <mla.neuralnet.layers.basic.Activation object at 0x7f43904a3730>], self.log_metric=True, self.loss=<function logloss at 0x7f4414e29e50>, self.loss_grad=<function NeuralNet.__init__.<locals>.<lambda> at 0x7f439043c790>, self.max_epochs=10, self.metric=<function unhot.<locals>.wrapper at 0x7f4414e29820>, self.metric_name='accuracy', self.optimizer=<mla.neuralnet.optimizers.Adadelta object at 0x7f43904a38e0>, self.shuffle=False, self.training=False, self.verbose=True)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "x_shape = list(x_shape)\n",
      "State:\n",
      "[850, 100]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def setup_weights(self, W_shape, b_shape=None):\n",
      "        if \"W\" not in self._params:\n",
      "            self._params[\"W\"] = self.init(shape=W_shape, scale=self.scale)\n",
      "            if b_shape is None:\n",
      "                self._params[\"b\"] = np.full(W_shape[1], self.initial_bias)\n",
      "            else:\n",
      "                self._params[\"b\"] = np.full(b_shape, self.initial_bias)\n",
      "        self.init_grad()\n",
      "setup_weights(self=<mla.neuralnet.parameters.Parameters object at 0x7f43904a3580>, W_shape=(100, 128), b_shape=None, self._grads={}, self._params={}, self.constraints={}, self.init=<function uniform at 0x7f4414d97e50>, self.initial_bias=1.0, self.regularizers={}, self.scale=0.5)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self._params[\"W\"] = self.init(shape=W_shape, scale=self.scale)\n",
      "State:\n",
      "{'W': array([[ 0.15358959, -0.38499306,  0.45028286, ...,  0.05874991,         0.31386435, -0.18217166],       [-0.10295491,  0.39016825,  0.32541621, ..., -0.22605705,         0.32141707, -0.32946481],       [ 0.04227727,  0.09911175,  0.47375601, ..., -0.28845091,        -0.08877795, -0.04253136],       ...,       [-0.31774367,  0.16705969,  0.08696401, ...,  0.19827451,         0.4498055 , -0.37831116],       [-0.0062262 ,  0.49305314,  0.34609552, ..., -0.4594941 ,        -0.2100812 ,  0.32738647],       [-0.20795677, -0.10486471, -0.37303625, ..., -0.32176153,         0.24184864,  0.13229956]])}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def setup_weights(self, W_shape, b_shape=None):\n",
      "        if \"W\" not in self._params:\n",
      "            self._params[\"W\"] = self.init(shape=W_shape, scale=self.scale)\n",
      "            if b_shape is None:\n",
      "                self._params[\"b\"] = np.full(W_shape[1], self.initial_bias)\n",
      "            else:\n",
      "                self._params[\"b\"] = np.full(b_shape, self.initial_bias)\n",
      "        self.init_grad()\n",
      "setup_weights(self=<mla.neuralnet.parameters.Parameters object at 0x7f43904a3580>, W_shape=(100, 128), b_shape=None, self._grads={}, self._params={}, self.constraints={}, self.init=<function uniform at 0x7f4414d97e50>, self.initial_bias=1.0, self.regularizers={}, self.scale=0.5)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "self._params[\"b\"] = np.full(W_shape[1], self.initial_bias)\n",
      "State:\n",
      "{'W': array([[ 0.15358959, -0.38499306,  0.45028286, ...,  0.05874991,         0.31386435, -0.18217166],       [-0.10295491,  0.39016825,  0.32541621, ..., -0.22605705,         0.32141707, -0.32946481],       [ 0.04227727,  0.09911175,  0.47375601, ..., -0.28845091,        -0.08877795, -0.04253136],       ...,       [-0.31774367,  0.16705969,  0.08696401, ...,  0.19827451,         0.4498055 , -0.37831116],       [-0.0062262 ,  0.49305314,  0.34609552, ..., -0.4594941 ,        -0.2100812 ,  0.32738647],       [-0.20795677, -0.10486471, -0.37303625, ..., -0.32176153,         0.24184864,  0.13229956]]), 'b': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,       1., 1., 1., 1., 1., 1., 1., 1., 1.])}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def update(self, X, y):\n",
      "        y_pred = self.fprop(X)\n",
      "        grad = self.loss_grad(y, y_pred)\n",
      "        for layer in reversed(self.layers[: self.bprop_entry]):\n",
      "            grad = layer.backward_pass(grad)\n",
      "        return self.loss(y, y_pred)\n",
      "update(self=<mla.neuralnet.nnet.NeuralNet object at 0x7f43904a3700>, X=array([[ 0.52035826, -1.70073552, -0.50697874, ..., -0.74790479,         0.16039734,  1.0323546 ],       [ 0.96925333, -0.29561268,  0.86400006, ...,  1.4457464 ,        -1.32196293, -0.0653791 ],       [ 0.24562753,  0.62176281,  0.95963704, ..., -0.16747796,        -0.60033504,  1.64606491],       ...,       [ 1.00615996,  0.0248242 , -0.22444922, ...,  0.59628011,         0.23845454,  0.19828813],       [-1.01925321, -0.91396712,  0.45963942, ...,  0.4610527 ,        -0.72484729,  1.250311  ],       [-0.58603073,  0.99913856, -0.47442535, ..., -1.10245052,        -0.45649948,  0.22987189]]), y=array([[1., 0.],       [1., 0.],       [1., 0.],       [1., 0.],       [0., 1.],       [0., 1.],       [0., 1.],       [0., 1.],       [1., 0.],       [1., 0.],       [0., 1.],       [0., 1.],       [1., 0.],       [0., 1.],       [1., 0.],       [0., 1.],       [0., 1.],       [0., 1.],       [1., 0.],       [1., 0.],       [0., 1.],       [0., 1.],       [0., 1.],       [1., 0.],       [1., 0.],       [0., 1.],       [0., 1.],       [1., 0.],       [1., 0.],       [1., 0.],       [0., 1.],       [1., 0.],       [1., 0.],       [0., 1.],       [1., 0.],       [1., 0.],       [1., 0.],       [1., 0.],       [1., 0.],       [1., 0.],       [0., 1.],       [0., 1.],       [1., 0.],       [1., 0.],       [1., 0.],       [1., 0.],       [1., 0.],       [0., 1.],       [0., 1.],       [1., 0.],       [0., 1.],       [1., 0.],       [0., 1.],       [0., 1.],       [1., 0.],       [0., 1.],       [0., 1.],       [1., 0.],       [0., 1.],       [0., 1.],       [1., 0.],       [0., 1.],       [1., 0.],       [0., 1.]]), self.X=array([[ 0.52035826, -1.70073552, -0.50697874, ..., -0.74790479,         0.16039734,  1.0323546 ],       [ 0.96925333, -0.29561268,  0.86400006, ...,  1.4457464 ,        -1.32196293, -0.0653791 ],       [ 0.24562753,  0.62176281,  0.95963704, ..., -0.16747796,        -0.60033504,  1.64606491],       ...,       [ 1.14501028,  1.07221843,  1.2764346 , ..., -0.83624018,        -1.87719572,  0.57666069],       [ 1.98506244, -0.27254282,  1.58679531, ..., -0.73272139,         2.03923433, -1.41719355],       [-0.82634613,  1.99698567,  0.0889722 , ...,  0.83719252,        -0.88980401,  0.81743782]]), self._initialized=True, self._n_layers=7, self.batch_size=64, self.bprop_entry=-1, self.layers=[<mla.neuralnet.layers.basic.Dense object at 0x7f43904a3610>, <mla.neuralnet.layers.basic.Activation object at 0x7f43904a33d0>, <mla.neuralnet.layers.basic.Dropout object at 0x7f43904a32b0>, <mla.neuralnet.layers.basic.Dense object at 0x7f43904a3c70>, <mla.neuralnet.layers.basic.Activation object at 0x7f43904a3550>, <mla.neuralnet.layers.basic.Dense object at 0x7f43904a3820>, <mla.neuralnet.layers.basic.Activation object at 0x7f43904a3730>], self.log_metric=True, self.loss=<function logloss at 0x7f4414e29e50>, self.loss_grad=<function NeuralNet.__init__.<locals>.<lambda> at 0x7f439043c790>, self.max_epochs=10, self.metric=<function unhot.<locals>.wrapper at 0x7f4414e29820>, self.metric_name='accuracy', self.n_features=100, self.n_samples=850, self.optimizer=<mla.neuralnet.optimizers.Adadelta object at 0x7f43904a38e0>, self.shuffle=False, self.training=True, self.verbose=True, self.y=array([[1., 0.],       [1., 0.],       [1., 0.],       ...,       [0., 1.],       [1., 0.],       [1., 0.]]))\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "y_pred = self.fprop(X)\n",
      "State:\n",
      "array([[7.24528710e-01, 2.75471290e-01],       [4.41944053e-05, 9.99955806e-01],       [2.57782213e-01, 7.42217787e-01],       [8.94990313e-07, 9.99999105e-01],       [1.78382174e-04, 9.99821618e-01],       [2.76923931e-06, 9.99997231e-01],       [8.91199892e-02, 9.10880011e-01],       [3.02136417e-09, 9.99999997e-01],       [1.43793536e-03, 9.98562065e-01],       [1.13881133e-04, 9.99886119e-01],       [2.23825888e-02, 9.77617411e-01],       [1.19273806e-02, 9.88072619e-01],       [2.44887439e-11, 1.00000000e+00],       [1.54219370e-07, 9.99999846e-01],       [1.54507072e-11, 1.00000000e+00],       [1.83966556e-10, 1.00000000e+00],       [2.05411557e-01, 7.94588443e-01],       [2.50351223e-02, 9.74964878e-01],       [4.79204538e-04, 9.99520795e-01],       [5.68875648e-03, 9.94311244e-01],       [1.00899305e-03, 9.98991007e-01],       [6.61715643e-04, 9.99338284e-01],       [8.58865898e-03, 9.91411341e-01],       [1.70273059e-06, 9.99998297e-01],       [6.72817206e-11, 1.00000000e+00],       [1.35639675e-02, 9.86436032e-01],       [9.49823073e-04, 9.99050177e-01],       [7.70221752e-13, 1.00000000e+00],       [9.20476155e-05, 9.99907952e-01],       [2.05871796e-01, 7.94128204e-01],       [4.17055897e-05, 9.99958294e-01],       [4.51873405e-06, 9.99995481e-01],       [3.81555264e-11, 1.00000000e+00],       [1.80050726e-03, 9.98199493e-01],       [1.51031447e-06, 9.99998490e-01],       [6.42569657e-14, 1.00000000e+00],       [9.01205494e-01, 9.87945055e-02],       [9.91829962e-01, 8.17003832e-03],       [9.99983048e-01, 1.69520345e-05],       [2.21210710e-03, 9.97787893e-01],       [1.01156467e-07, 9.99999899e-01],       [2.62614287e-11, 1.00000000e+00],       [1.19076158e-07, 9.99999881e-01],       [1.01908826e-07, 9.99999898e-01],       [2.47487002e-11, 1.00000000e+00],       [9.05522572e-06, 9.99990945e-01],       [1.48616398e-09, 9.99999999e-01],       [8.66418547e-05, 9.99913358e-01],       [7.43261746e-05, 9.99925674e-01],       [7.41052874e-05, 9.99925895e-01],       [3.52878606e-03, 9.96471214e-01],       [9.52698611e-01, 4.73013892e-02],       [3.05386775e-01, 6.94613225e-01],       [7.30758348e-06, 9.99992692e-01],       [6.93356372e-01, 3.06643628e-01],       [5.68178012e-07, 9.99999432e-01],       [1.51776767e-01, 8.48223233e-01],       [3.18976688e-01, 6.81023312e-01],       [9.53343605e-06, 9.99990467e-01],       [1.58052803e-08, 9.99999984e-01],       [8.97741330e-10, 9.99999999e-01],       [1.17715264e-03, 9.98822847e-01],       [1.70904469e-03, 9.98290955e-01],       [4.20623184e-13, 1.00000000e+00]])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def update(self, X, y):\n",
      "        y_pred = self.fprop(X)\n",
      "        grad = self.loss_grad(y, y_pred)\n",
      "        for layer in reversed(self.layers[: self.bprop_entry]):\n",
      "            grad = layer.backward_pass(grad)\n",
      "        return self.loss(y, y_pred)\n",
      "update(self=<mla.neuralnet.nnet.NeuralNet object at 0x7f43904a3700>, X=array([[ 0.52035826, -1.70073552, -0.50697874, ..., -0.74790479,         0.16039734,  1.0323546 ],       [ 0.96925333, -0.29561268,  0.86400006, ...,  1.4457464 ,        -1.32196293, -0.0653791 ],       [ 0.24562753,  0.62176281,  0.95963704, ..., -0.16747796,        -0.60033504,  1.64606491],       ...,       [ 1.00615996,  0.0248242 , -0.22444922, ...,  0.59628011,         0.23845454,  0.19828813],       [-1.01925321, -0.91396712,  0.45963942, ...,  0.4610527 ,        -0.72484729,  1.250311  ],       [-0.58603073,  0.99913856, -0.47442535, ..., -1.10245052,        -0.45649948,  0.22987189]]), y=array([[1., 0.],       [1., 0.],       [1., 0.],       [1., 0.],       [0., 1.],       [0., 1.],       [0., 1.],       [0., 1.],       [1., 0.],       [1., 0.],       [0., 1.],       [0., 1.],       [1., 0.],       [0., 1.],       [1., 0.],       [0., 1.],       [0., 1.],       [0., 1.],       [1., 0.],       [1., 0.],       [0., 1.],       [0., 1.],       [0., 1.],       [1., 0.],       [1., 0.],       [0., 1.],       [0., 1.],       [1., 0.],       [1., 0.],       [1., 0.],       [0., 1.],       [1., 0.],       [1., 0.],       [0., 1.],       [1., 0.],       [1., 0.],       [1., 0.],       [1., 0.],       [1., 0.],       [1., 0.],       [0., 1.],       [0., 1.],       [1., 0.],       [1., 0.],       [1., 0.],       [1., 0.],       [1., 0.],       [0., 1.],       [0., 1.],       [1., 0.],       [0., 1.],       [1., 0.],       [0., 1.],       [0., 1.],       [1., 0.],       [0., 1.],       [0., 1.],       [1., 0.],       [0., 1.],       [0., 1.],       [1., 0.],       [0., 1.],       [1., 0.],       [0., 1.]]), self.X=array([[ 0.52035826, -1.70073552, -0.50697874, ..., -0.74790479,         0.16039734,  1.0323546 ],       [ 0.96925333, -0.29561268,  0.86400006, ...,  1.4457464 ,        -1.32196293, -0.0653791 ],       [ 0.24562753,  0.62176281,  0.95963704, ..., -0.16747796,        -0.60033504,  1.64606491],       ...,       [ 1.14501028,  1.07221843,  1.2764346 , ..., -0.83624018,        -1.87719572,  0.57666069],       [ 1.98506244, -0.27254282,  1.58679531, ..., -0.73272139,         2.03923433, -1.41719355],       [-0.82634613,  1.99698567,  0.0889722 , ...,  0.83719252,        -0.88980401,  0.81743782]]), self._initialized=True, self._n_layers=7, self.batch_size=64, self.bprop_entry=-1, self.layers=[<mla.neuralnet.layers.basic.Dense object at 0x7f43904a3610>, <mla.neuralnet.layers.basic.Activation object at 0x7f43904a33d0>, <mla.neuralnet.layers.basic.Dropout object at 0x7f43904a32b0>, <mla.neuralnet.layers.basic.Dense object at 0x7f43904a3c70>, <mla.neuralnet.layers.basic.Activation object at 0x7f43904a3550>, <mla.neuralnet.layers.basic.Dense object at 0x7f43904a3820>, <mla.neuralnet.layers.basic.Activation object at 0x7f43904a3730>], self.log_metric=True, self.loss=<function logloss at 0x7f4414e29e50>, self.loss_grad=<function NeuralNet.__init__.<locals>.<lambda> at 0x7f439043c790>, self.max_epochs=10, self.metric=<function unhot.<locals>.wrapper at 0x7f4414e29820>, self.metric_name='accuracy', self.n_features=100, self.n_samples=850, self.optimizer=<mla.neuralnet.optimizers.Adadelta object at 0x7f43904a38e0>, self.shuffle=False, self.training=True, self.verbose=True, self.y=array([[1., 0.],       [1., 0.],       [1., 0.],       ...,       [0., 1.],       [1., 0.],       [1., 0.]]))\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "grad = self.loss_grad(y, y_pred)\n",
      "State:\n",
      "array([[-2.75471290e-01,  2.75471290e-01],       [-9.99955806e-01,  9.99955806e-01],       [-7.42217787e-01,  7.42217787e-01],       [-9.99999105e-01,  9.99999105e-01],       [ 1.78382174e-04, -1.78382174e-04],       [ 2.76923931e-06, -2.76923931e-06],       [ 8.91199892e-02, -8.91199892e-02],       [ 3.02136417e-09, -3.02136427e-09],       [-9.98562065e-01,  9.98562065e-01],       [-9.99886119e-01,  9.99886119e-01],       [ 2.23825888e-02, -2.23825888e-02],       [ 1.19273806e-02, -1.19273806e-02],       [-1.00000000e+00,  1.00000000e+00],       [ 1.54219370e-07, -1.54219370e-07],       [-1.00000000e+00,  1.00000000e+00],       [ 1.83966556e-10, -1.83966620e-10],       [ 2.05411557e-01, -2.05411557e-01],       [ 2.50351223e-02, -2.50351223e-02],       [-9.99520795e-01,  9.99520795e-01],       [-9.94311244e-01,  9.94311244e-01],       [ 1.00899305e-03, -1.00899305e-03],       [ 6.61715643e-04, -6.61715643e-04],       [ 8.58865898e-03, -8.58865898e-03],       [-9.99998297e-01,  9.99998297e-01],       [-1.00000000e+00,  1.00000000e+00],       [ 1.35639675e-02, -1.35639675e-02],       [ 9.49823073e-04, -9.49823073e-04],       [-1.00000000e+00,  1.00000000e+00],       [-9.99907952e-01,  9.99907952e-01],       [-7.94128204e-01,  7.94128204e-01],       [ 4.17055897e-05, -4.17055897e-05],       [-9.99995481e-01,  9.99995481e-01],       [-1.00000000e+00,  1.00000000e+00],       [ 1.80050726e-03, -1.80050726e-03],       [-9.99998490e-01,  9.99998490e-01],       [-1.00000000e+00,  1.00000000e+00],       [-9.87945055e-02,  9.87945055e-02],       [-8.17003832e-03,  8.17003832e-03],       [-1.69520345e-05,  1.69520345e-05],       [-9.97787893e-01,  9.97787893e-01],       [ 1.01156467e-07, -1.01156467e-07],       [ 2.62614287e-11, -2.62614375e-11],       [-9.99999881e-01,  9.99999881e-01],       [-9.99999898e-01,  9.99999898e-01],       [-1.00000000e+00,  1.00000000e+00],       [-9.99990945e-01,  9.99990945e-01],       [-9.99999999e-01,  9.99999999e-01],       [ 8.66418547e-05, -8.66418547e-05],       [ 7.43261746e-05, -7.43261746e-05],       [-9.99925895e-01,  9.99925895e-01],       [ 3.52878606e-03, -3.52878606e-03],       [-4.73013892e-02,  4.73013892e-02],       [ 3.05386775e-01, -3.05386775e-01],       [ 7.30758348e-06, -7.30758348e-06],       [-3.06643628e-01,  3.06643628e-01],       [ 5.68178012e-07, -5.68178012e-07],       [ 1.51776767e-01, -1.51776767e-01],       [-6.81023312e-01,  6.81023312e-01],       [ 9.53343605e-06, -9.53343605e-06],       [ 1.58052803e-08, -1.58052804e-08],       [-9.99999999e-01,  9.99999999e-01],       [ 1.17715264e-03, -1.17715264e-03],       [-9.98290955e-01,  9.98290955e-01],       [ 4.20623184e-13, -4.20552482e-13]])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _predict(self, X=None):\n",
      "        if not self._initialized:\n",
      "            self._setup_layers(X.shape)\n",
      "        y = []\n",
      "        X_batch = batch_iterator(X, self.batch_size)\n",
      "        for Xb in X_batch:\n",
      "            y.append(self.fprop(Xb))\n",
      "        return np.concatenate(y)\n",
      "_predict(self=<mla.neuralnet.nnet.NeuralNet object at 0x7f43904a3700>, X=array([[ 0.52035826, -1.70073552, -0.50697874, ..., -0.74790479,         0.16039734,  1.0323546 ],       [ 0.96925333, -0.29561268,  0.86400006, ...,  1.4457464 ,        -1.32196293, -0.0653791 ],       [ 0.24562753,  0.62176281,  0.95963704, ..., -0.16747796,        -0.60033504,  1.64606491],       ...,       [ 1.14501028,  1.07221843,  1.2764346 , ..., -0.83624018,        -1.87719572,  0.57666069],       [ 1.98506244, -0.27254282,  1.58679531, ..., -0.73272139,         2.03923433, -1.41719355],       [-0.82634613,  1.99698567,  0.0889722 , ...,  0.83719252,        -0.88980401,  0.81743782]]), self.X=array([[ 0.52035826, -1.70073552, -0.50697874, ..., -0.74790479,         0.16039734,  1.0323546 ],       [ 0.96925333, -0.29561268,  0.86400006, ...,  1.4457464 ,        -1.32196293, -0.0653791 ],       [ 0.24562753,  0.62176281,  0.95963704, ..., -0.16747796,        -0.60033504,  1.64606491],       ...,       [ 1.14501028,  1.07221843,  1.2764346 , ..., -0.83624018,        -1.87719572,  0.57666069],       [ 1.98506244, -0.27254282,  1.58679531, ..., -0.73272139,         2.03923433, -1.41719355],       [-0.82634613,  1.99698567,  0.0889722 , ...,  0.83719252,        -0.88980401,  0.81743782]]), self._initialized=True, self._n_layers=7, self.batch_size=64, self.bprop_entry=-1, self.layers=[<mla.neuralnet.layers.basic.Dense object at 0x7f43904a3610>, <mla.neuralnet.layers.basic.Activation object at 0x7f43904a33d0>, <mla.neuralnet.layers.basic.Dropout object at 0x7f43904a32b0>, <mla.neuralnet.layers.basic.Dense object at 0x7f43904a3c70>, <mla.neuralnet.layers.basic.Activation object at 0x7f43904a3550>, <mla.neuralnet.layers.basic.Dense object at 0x7f43904a3820>, <mla.neuralnet.layers.basic.Activation object at 0x7f43904a3730>], self.log_metric=True, self.loss=<function logloss at 0x7f4414e29e50>, self.loss_grad=<function NeuralNet.__init__.<locals>.<lambda> at 0x7f439043c790>, self.max_epochs=10, self.metric=<function unhot.<locals>.wrapper at 0x7f4414e29820>, self.metric_name='accuracy', self.n_features=100, self.n_samples=850, self.optimizer=<mla.neuralnet.optimizers.Adadelta object at 0x7f43904a38e0>, self.shuffle=False, self.training=False, self.verbose=True, self.y=array([[1., 0.],       [1., 0.],       [1., 0.],       ...,       [0., 1.],       [1., 0.],       [1., 0.]]))\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "y = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def read_env() -> Dict[str, Any]:\n",
      "    __env_dict: Dict[str, Optional[str]] = {}\n",
      "    for env_file in get_reading_order():\n",
      "        if env_file.exists():\n",
      "            __env_dict.update(**dotenv_values(env_file))\n",
      "    __env_dict_filtered = {\n",
      "        k[len(\"OPENBBB_\") - 1 :]: v\n",
      "        for k, v in __env_dict.items()\n",
      "        if k.startswith(\"OPENBB_\")\n",
      "    }\n",
      "    return __env_dict_filtered\n",
      "read_env()\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "__env_dict: Dict[str, Optional[str]] = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def validate_send_to_s3(cls, values):\n",
      "        if \"posthog\" in values[\"LOGGING_HANDLERS\"] or values[\"LOG_COLLECT\"] is False:\n",
      "            values[\"LOGGING_SEND_TO_S3\"] = False\n",
      "        return values\n",
      "validate_send_to_s3(cls=<class 'openbb_terminal.core.models.system_model.SystemModel'>, values={'OS': 'Linux', 'PYTHON_VERSION': '3.9.19', 'PLATFORM': 'Linux-5.15.0-94-generic-x86_64-with-glibc2.35', 'VERSION': '3.2.4', 'LOGGING_APP_ID': 'REPLACE_ME', 'LOGGING_APP_NAME': 'gst', 'LOGGING_AWS_ACCESS_KEY_ID': 'REPLACE_ME', 'LOGGING_AWS_SECRET_ACCESS_KEY': 'REPLACE_ME', 'LOGGING_COMMIT_HASH': 'REPLACE_ME', 'LOGGING_BRANCH': 'REPLACE_ME', 'LOGGING_FREQUENCY': 'H', 'LOGGING_HANDLERS': ['file', 'posthog'], 'LOGGING_ROLLING_CLOCK': False, 'LOGGING_VERBOSITY': 20, 'LOGGING_SUB_APP': 'terminal', 'LOGGING_SUPPRESS': False, 'LOGGING_SEND_TO_S3': True, 'LOG_COLLECT': True, 'DISABLE_STREAMLIT_WARNING': False, 'DISABLE_FORECASTING_WARNING': False, 'DISABLE_OPTIMIZATION_WARNING': False, 'TEST_MODE': False, 'DEBUG_MODE': False, 'DEV_BACKEND': False, 'ENABLE_AUTHENTICATION': True, 'HEADLESS': False})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "values[\"LOGGING_SEND_TO_S3\"] = False\n",
      "State:\n",
      "{'OS': 'Linux', 'PYTHON_VERSION': '3.9.19', 'PLATFORM': 'Linux-5.15.0-94-generic-x86_64-with-glibc2.35', 'VERSION': '3.2.4', 'LOGGING_APP_ID': 'REPLACE_ME', 'LOGGING_APP_NAME': 'gst', 'LOGGING_AWS_ACCESS_KEY_ID': 'REPLACE_ME', 'LOGGING_AWS_SECRET_ACCESS_KEY': 'REPLACE_ME', 'LOGGING_COMMIT_HASH': 'REPLACE_ME', 'LOGGING_BRANCH': 'REPLACE_ME', 'LOGGING_FREQUENCY': 'H', 'LOGGING_HANDLERS': ['file', 'posthog'], 'LOGGING_ROLLING_CLOCK': False, 'LOGGING_VERBOSITY': 20, 'LOGGING_SUB_APP': 'terminal', 'LOGGING_SUPPRESS': False, 'LOGGING_SEND_TO_S3': False, 'LOG_COLLECT': True, 'DISABLE_STREAMLIT_WARNING': False, 'DISABLE_FORECASTING_WARNING': False, 'DISABLE_OPTIMIZATION_WARNING': False, 'TEST_MODE': False, 'DEBUG_MODE': False, 'DEV_BACKEND': False, 'ENABLE_AUTHENTICATION': True, 'HEADLESS': False}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __new__(mcs: Type[\"PluginMeta\"], *args: Any, **kwargs: Any) -> \"PluginMeta\":\n",
      "        name, bases, attrs = args\n",
      "        indicators: Dict[str, Indicator] = {}\n",
      "        cls_attrs: Dict[str, list] = {\n",
      "            \"__ma_mode__\": [],\n",
      "            \"__inchart__\": [],\n",
      "            \"__subplots__\": [],\n",
      "        }\n",
      "        new_cls = super().__new__(mcs, name, bases, attrs, **kwargs)\n",
      "        for base in reversed(new_cls.__mro__):\n",
      "            for elem, value in base.__dict__.items():\n",
      "                if elem in indicators:\n",
      "                    del indicators[elem]\n",
      "                is_static_method = isinstance(value, staticmethod)\n",
      "                if is_static_method:\n",
      "                    value = value.__func__\n",
      "                if isinstance(value, Indicator):\n",
      "                    if is_static_method:\n",
      "                        raise TypeError(\n",
      "                            f\"Indicator {value.name} can't be a static method\"\n",
      "                        )\n",
      "                    indicators[value.name] = value\n",
      "                elif is_static_method and elem not in new_cls.__static_methods__:\n",
      "                    new_cls.__static_methods__.append(elem)\n",
      "                if elem in [\"__ma_mode__\", \"__inchart__\", \"__subplots__\"]:\n",
      "                    cls_attrs[elem].extend(value)\n",
      "        new_cls.__indicators__ = list(indicators.values())\n",
      "        new_cls.__static_methods__ = list(set(new_cls.__static_methods__))\n",
      "        new_cls.__ma_mode__ = list(set(cls_attrs[\"__ma_mode__\"]))\n",
      "        new_cls.__inchart__ = list(set(cls_attrs[\"__inchart__\"]))\n",
      "        new_cls.__subplots__ = list(set(cls_attrs[\"__subplots__\"]))\n",
      "        return new_cls\n",
      "__new__(mcs=<class 'openbb_terminal.core.plots.plotly_ta.base.PluginMeta'>, args=('PltTA', (), {'__module__': 'openbb_terminal.core.plots.plotly_ta.base', '__qualname__': 'PltTA', '__annotations__': {'indicators': <class 'openbb_terminal.core.plots.plotly_ta.data_classes.ChartIndicators'>, 'intraday': <class 'bool'>, 'df_stock': <class 'pandas.core.frame.DataFrame'>, 'df_ta': <class 'pandas.core.frame.DataFrame'>, 'df_fib': <class 'pandas.core.frame.DataFrame'>, 'close_column': typing.Optional[str], 'params': typing.Dict[str, openbb_terminal.core.plots.plotly_ta.data_classes.TAIndicator], 'inchart_colors': typing.List[str], 'show_volume': <class 'bool'>, '__static_methods__': <class 'list'>, '__indicators__': typing.List[openbb_terminal.core.plots.plotly_ta.base.Indicator], '__ma_mode__': typing.List[str], '__inchart__': typing.List[str], '__subplots__': typing.List[str]}, '__doc__': 'The base class that all Plotly plugins must inherit from.', 'intraday': False, 'close_column': 'Close', 'params': {}, 'inchart_colors': [], 'show_volume': True, '__static_methods__': [], '__indicators__': [], '__ma_mode__': [], '__inchart__': [], '__subplots__': [], '__new__': <function PltTA.__new__ at 0x7fd9130d98b0>, 'ma_mode': <property object at 0x7fd915b6fe00>, 'add_plugins': <function PltTA.add_plugins at 0x7fd9130d9a60>, 'remove_plugins': <function PltTA.remove_plugins at 0x7fd9130d9af0>, '__iter__': <function PltTA.__iter__ at 0x7fd9130d9b80>, 'get_float_precision': <function PltTA.get_float_precision at 0x7fd9130d9c10>, '__classcell__': <cell at 0x7fd9132b88b0: empty>}), kwargs={}, __class__=<class 'openbb_terminal.core.plots.plotly_ta.base.PluginMeta'>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "name, bases, attrs = args\n",
      "State:\n",
      "'PltTA'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __new__(mcs: Type[\"PluginMeta\"], *args: Any, **kwargs: Any) -> \"PluginMeta\":\n",
      "        name, bases, attrs = args\n",
      "        indicators: Dict[str, Indicator] = {}\n",
      "        cls_attrs: Dict[str, list] = {\n",
      "            \"__ma_mode__\": [],\n",
      "            \"__inchart__\": [],\n",
      "            \"__subplots__\": [],\n",
      "        }\n",
      "        new_cls = super().__new__(mcs, name, bases, attrs, **kwargs)\n",
      "        for base in reversed(new_cls.__mro__):\n",
      "            for elem, value in base.__dict__.items():\n",
      "                if elem in indicators:\n",
      "                    del indicators[elem]\n",
      "                is_static_method = isinstance(value, staticmethod)\n",
      "                if is_static_method:\n",
      "                    value = value.__func__\n",
      "                if isinstance(value, Indicator):\n",
      "                    if is_static_method:\n",
      "                        raise TypeError(\n",
      "                            f\"Indicator {value.name} can't be a static method\"\n",
      "                        )\n",
      "                    indicators[value.name] = value\n",
      "                elif is_static_method and elem not in new_cls.__static_methods__:\n",
      "                    new_cls.__static_methods__.append(elem)\n",
      "                if elem in [\"__ma_mode__\", \"__inchart__\", \"__subplots__\"]:\n",
      "                    cls_attrs[elem].extend(value)\n",
      "        new_cls.__indicators__ = list(indicators.values())\n",
      "        new_cls.__static_methods__ = list(set(new_cls.__static_methods__))\n",
      "        new_cls.__ma_mode__ = list(set(cls_attrs[\"__ma_mode__\"]))\n",
      "        new_cls.__inchart__ = list(set(cls_attrs[\"__inchart__\"]))\n",
      "        new_cls.__subplots__ = list(set(cls_attrs[\"__subplots__\"]))\n",
      "        return new_cls\n",
      "__new__(mcs=<class 'openbb_terminal.core.plots.plotly_ta.base.PluginMeta'>, args=('PltTA', (), {'__module__': 'openbb_terminal.core.plots.plotly_ta.base', '__qualname__': 'PltTA', '__annotations__': {'indicators': <class 'openbb_terminal.core.plots.plotly_ta.data_classes.ChartIndicators'>, 'intraday': <class 'bool'>, 'df_stock': <class 'pandas.core.frame.DataFrame'>, 'df_ta': <class 'pandas.core.frame.DataFrame'>, 'df_fib': <class 'pandas.core.frame.DataFrame'>, 'close_column': typing.Optional[str], 'params': typing.Dict[str, openbb_terminal.core.plots.plotly_ta.data_classes.TAIndicator], 'inchart_colors': typing.List[str], 'show_volume': <class 'bool'>, '__static_methods__': <class 'list'>, '__indicators__': typing.List[openbb_terminal.core.plots.plotly_ta.base.Indicator], '__ma_mode__': typing.List[str], '__inchart__': typing.List[str], '__subplots__': typing.List[str]}, '__doc__': 'The base class that all Plotly plugins must inherit from.', 'intraday': False, 'close_column': 'Close', 'params': {}, 'inchart_colors': [], 'show_volume': True, '__static_methods__': [], '__indicators__': [], '__ma_mode__': [], '__inchart__': [], '__subplots__': [], '__new__': <function PltTA.__new__ at 0x7fd9130d98b0>, 'ma_mode': <property object at 0x7fd915b6fe00>, 'add_plugins': <function PltTA.add_plugins at 0x7fd9130d9a60>, 'remove_plugins': <function PltTA.remove_plugins at 0x7fd9130d9af0>, '__iter__': <function PltTA.__iter__ at 0x7fd9130d9b80>, 'get_float_precision': <function PltTA.get_float_precision at 0x7fd9130d9c10>, '__classcell__': <cell at 0x7fd9132b88b0: empty>}), kwargs={}, __class__=<class 'openbb_terminal.core.plots.plotly_ta.base.PluginMeta'>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "indicators: Dict[str, Indicator] = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __new__(mcs: Type[\"PluginMeta\"], *args: Any, **kwargs: Any) -> \"PluginMeta\":\n",
      "        name, bases, attrs = args\n",
      "        indicators: Dict[str, Indicator] = {}\n",
      "        cls_attrs: Dict[str, list] = {\n",
      "            \"__ma_mode__\": [],\n",
      "            \"__inchart__\": [],\n",
      "            \"__subplots__\": [],\n",
      "        }\n",
      "        new_cls = super().__new__(mcs, name, bases, attrs, **kwargs)\n",
      "        for base in reversed(new_cls.__mro__):\n",
      "            for elem, value in base.__dict__.items():\n",
      "                if elem in indicators:\n",
      "                    del indicators[elem]\n",
      "                is_static_method = isinstance(value, staticmethod)\n",
      "                if is_static_method:\n",
      "                    value = value.__func__\n",
      "                if isinstance(value, Indicator):\n",
      "                    if is_static_method:\n",
      "                        raise TypeError(\n",
      "                            f\"Indicator {value.name} can't be a static method\"\n",
      "                        )\n",
      "                    indicators[value.name] = value\n",
      "                elif is_static_method and elem not in new_cls.__static_methods__:\n",
      "                    new_cls.__static_methods__.append(elem)\n",
      "                if elem in [\"__ma_mode__\", \"__inchart__\", \"__subplots__\"]:\n",
      "                    cls_attrs[elem].extend(value)\n",
      "        new_cls.__indicators__ = list(indicators.values())\n",
      "        new_cls.__static_methods__ = list(set(new_cls.__static_methods__))\n",
      "        new_cls.__ma_mode__ = list(set(cls_attrs[\"__ma_mode__\"]))\n",
      "        new_cls.__inchart__ = list(set(cls_attrs[\"__inchart__\"]))\n",
      "        new_cls.__subplots__ = list(set(cls_attrs[\"__subplots__\"]))\n",
      "        return new_cls\n",
      "__new__(mcs=<class 'openbb_terminal.core.plots.plotly_ta.base.PluginMeta'>, args=('PltTA', (), {'__module__': 'openbb_terminal.core.plots.plotly_ta.base', '__qualname__': 'PltTA', '__annotations__': {'indicators': <class 'openbb_terminal.core.plots.plotly_ta.data_classes.ChartIndicators'>, 'intraday': <class 'bool'>, 'df_stock': <class 'pandas.core.frame.DataFrame'>, 'df_ta': <class 'pandas.core.frame.DataFrame'>, 'df_fib': <class 'pandas.core.frame.DataFrame'>, 'close_column': typing.Optional[str], 'params': typing.Dict[str, openbb_terminal.core.plots.plotly_ta.data_classes.TAIndicator], 'inchart_colors': typing.List[str], 'show_volume': <class 'bool'>, '__static_methods__': <class 'list'>, '__indicators__': typing.List[openbb_terminal.core.plots.plotly_ta.base.Indicator], '__ma_mode__': typing.List[str], '__inchart__': typing.List[str], '__subplots__': typing.List[str]}, '__doc__': 'The base class that all Plotly plugins must inherit from.', 'intraday': False, 'close_column': 'Close', 'params': {}, 'inchart_colors': [], 'show_volume': True, '__static_methods__': [], '__indicators__': [], '__ma_mode__': [], '__inchart__': [], '__subplots__': [], '__new__': <function PltTA.__new__ at 0x7fd9130d98b0>, 'ma_mode': <property object at 0x7fd915b6fe00>, 'add_plugins': <function PltTA.add_plugins at 0x7fd9130d9a60>, 'remove_plugins': <function PltTA.remove_plugins at 0x7fd9130d9af0>, '__iter__': <function PltTA.__iter__ at 0x7fd9130d9b80>, 'get_float_precision': <function PltTA.get_float_precision at 0x7fd9130d9c10>, '__classcell__': <cell at 0x7fd9132b88b0: empty>}), kwargs={}, __class__=<class 'openbb_terminal.core.plots.plotly_ta.base.PluginMeta'>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "cls_attrs: Dict[str, list] = {\n",
      "State:\n",
      "{'__ma_mode__': [], '__inchart__': [], '__subplots__': []}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def getLogger(name, stdout=None):\n",
      "    logger = logging.getLogger(name)\n",
      "    logger.handlers = []\n",
      "    if (stdout is None):\n",
      "        logger.addHandler(logging.NullHandler())\n",
      "    if stdout is not None:\n",
      "        handler = logging.StreamHandler()\n",
      "        handler.setLevel(stdout['level'])\n",
      "        handler.setFormatter(CumulusFormatter())\n",
      "        logger.addHandler(handler)\n",
      "    logger.setLevel(1)\n",
      "    return logger\n",
      "getLogger(name='cumulus.aws', stdout=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "logger = logging.getLogger(name)\n",
      "State:\n",
      "<Logger cumulus.aws (WARNING)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_first_duplicate(items):\n",
      "    seen = set()\n",
      "    for item in items:\n",
      "        if item not in seen:\n",
      "            seen.add(item)\n",
      "        else:\n",
      "            return item\n",
      "    return None\n",
      "get_first_duplicate(items=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "seen = set()\n",
      "State:\n",
      "set()\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _define(self, f, o_len, pure):\n",
      "        name = f.__name__\n",
      "        entry = SymbolTableEntry(self, name, o_len, pure)\n",
      "        setattr(self, name, entry)\n",
      "        self._impls[name] = f\n",
      "        return f\n",
      "_define(self=<hyperopt.pyll.base.SymbolTable object at 0x7f2e26fd4b50>, f=<function call at 0x7f2e26e03a60>, o_len=None, pure=False, self._impls={'list': <class 'list'>, 'dict': <class 'dict'>, 'range': <class 'range'>, 'len': <built-in function len>, 'int': <class 'int'>, 'float': <class 'float'>, 'map': <class 'map'>, 'max': <built-in function max>, 'min': <built-in function min>, 'getattr': <built-in function getattr>})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "name = f.__name__\n",
      "State:\n",
      "'call'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_logger(name):\n",
      "    logger = logging.getLogger(name)\n",
      "    logger.setLevel(logging.INFO)\n",
      "    if not logger.handlers and not logging.getLogger().handlers:\n",
      "        handler = logging.StreamHandler(sys.stderr)\n",
      "        logger.addHandler(handler)\n",
      "    return logger\n",
      "_get_logger(name='hyperopt.utils')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "logger = logging.getLogger(name)\n",
      "State:\n",
      "<Logger hyperopt.utils (WARNING)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def arg(self):\n",
      "        binding = {}\n",
      "        fn = scope._impls[self.name]\n",
      "        defaults = fn.__defaults__\n",
      "        code = fn.__code__\n",
      "        extra_args_ok = bool(code.co_flags & 0x04)\n",
      "        extra_kwargs_ok = bool(code.co_flags & 0x08)\n",
      "        try:\n",
      "            if extra_args_ok and extra_kwargs_ok:\n",
      "                assert len(code.co_varnames) >= code.co_argcount + 2\n",
      "                param_names = code.co_varnames[: code.co_argcount + 2]\n",
      "                args_param = param_names[code.co_argcount]\n",
      "                kwargs_param = param_names[code.co_argcount + 1]\n",
      "                pos_params = param_names[: code.co_argcount]\n",
      "            elif extra_kwargs_ok:\n",
      "                assert len(code.co_varnames) >= code.co_argcount + 1\n",
      "                param_names = code.co_varnames[: code.co_argcount + 1]\n",
      "                kwargs_param = param_names[code.co_argcount]\n",
      "                pos_params = param_names[: code.co_argcount]\n",
      "            elif extra_args_ok:\n",
      "                assert len(code.co_varnames) >= code.co_argcount + 1\n",
      "                param_names = code.co_varnames[: code.co_argcount + 1]\n",
      "                args_param = param_names[code.co_argcount]\n",
      "                pos_params = param_names[: code.co_argcount]\n",
      "            else:\n",
      "                assert len(code.co_varnames) >= code.co_argcount\n",
      "                param_names = code.co_varnames[: code.co_argcount]\n",
      "                pos_params = param_names[: code.co_argcount]\n",
      "        except AssertionError:\n",
      "            print(\"YIKES: MISUNDERSTANDING OF CALL PROTOCOL:\")\n",
      "            print(code.co_argcount)\n",
      "            print(code.co_varnames)\n",
      "            print(\"%x\" % code.co_flags)\n",
      "            raise\n",
      "        if extra_args_ok:\n",
      "            binding[args_param] == []\n",
      "        if extra_kwargs_ok:\n",
      "            binding[kwargs_param] == {}\n",
      "        if len(self.pos_args) > code.co_argcount and not extra_args_ok:\n",
      "            raise TypeError(\"Argument count exceeds number of positional params\")\n",
      "        for param_i, arg_i in zip(param_names, self.pos_args):\n",
      "            binding[param_i] = arg_i\n",
      "        if extra_args_ok:\n",
      "            binding[args_param].extend(args[code.co_argcount :])\n",
      "        for aname, aval in self.named_args:\n",
      "            try:\n",
      "                pos = pos_params.index(aname)\n",
      "            except ValueError:\n",
      "                if extra_kwargs_ok:\n",
      "                    binding[kwargs_param][aname] = aval\n",
      "                    continue\n",
      "                else:\n",
      "                    raise TypeError(\"Unrecognized keyword argument\", aname)\n",
      "            param = param_names[pos]\n",
      "            if param in binding:\n",
      "                raise TypeError(\"Duplicate argument for parameter\", param)\n",
      "            binding[param] = aval\n",
      "        assert len(binding) <= len(param_names)\n",
      "        if len(binding) < len(param_names):\n",
      "            for p in param_names:\n",
      "                if p not in binding:\n",
      "                    binding[p] = MissingArgument\n",
      "        return binding\n",
      "arg(self=<hyperopt.pyll.base.Apply object at 0x7f2ec50fafa0>, self.define_params=None, self.name='hyperopt_param', self.named_args=[], self.o_len=None, self.pos_args=[<hyperopt.pyll.base.Literal object at 0x7f2ec50fa910>, <hyperopt.pyll.base.Apply object at 0x7f2ec50faf40>], self.pure=False)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "defaults = fn.__defaults__  # right-aligned default values for params\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def vectorize_stochastic(orig):\n",
      "    if orig.name == \"idxs_map\" and orig.pos_args[1]._obj in stoch:\n",
      "        idxs = orig.pos_args[0]\n",
      "        dist = orig.pos_args[1]._obj\n",
      "        def foo(arg):\n",
      "            assert arg.name == \"pos_args\"\n",
      "            assert len(arg.pos_args) == 2\n",
      "            arg_vals = arg.pos_args[1]\n",
      "            if arg_vals.name == \"idxs_take\":\n",
      "                if arg_vals.arg[\"vals\"].name == \"asarray\":\n",
      "                    if arg_vals.arg[\"vals\"].inputs()[0].name == \"repeat\":\n",
      "                        repeated_thing = arg_vals.arg[\"vals\"].inputs()[0].inputs()[1]\n",
      "                        return repeated_thing\n",
      "            if arg.pos_args[0] is idxs:\n",
      "                return arg_vals\n",
      "            else:\n",
      "                raise NotImplementedError()\n",
      "        new_pos_args = [foo(arg) for arg in orig.pos_args[2:]]\n",
      "        new_named_args = [[aname, foo(arg)] for aname, arg in orig.named_args]\n",
      "        vnode = Apply(dist, new_pos_args, new_named_args, o_len=None)\n",
      "        n_times = scope.len(idxs)\n",
      "        if \"size\" in dict(vnode.named_args):\n",
      "            raise NotImplementedError(\"random node already has size\")\n",
      "        vnode.named_args.append([\"size\", n_times])\n",
      "        return vnode\n",
      "    else:\n",
      "        return orig\n",
      "vectorize_stochastic(orig={name='idxs_map', pos_args=[<hyperopt.pyll.base.Apply object at 0x7f2ec4b084f0>, <hyperopt.pyll.base.Literal object at 0x7f2ec4b083a0>, <hyperopt.pyll.base.Apply object at 0x7f2ec4b082b0>], named_args=[], o_len=None, pure=True, define_params=None})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "dist = orig.pos_args[1]._obj\n",
      "State:\n",
      "'randint'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def delete(self, *, ip_parameter, cascade):\n",
      "        as_address = clean_address(ip_parameter)\n",
      "        as_network = clean_network(ip_parameter)\n",
      "        if as_address in self.__description:\n",
      "            return self.__remove_ip_object(as_address)\n",
      "        elif as_network in self.__description:\n",
      "            return self.__remove_ip_object(as_network)\n",
      "        raise IPObjectNotInSpaceError(\"cannot delete undescribed IP object\")\n",
      "delete(self=AddressSpace(_AddressSpace__strict=False, _AddressSpace__description={IPv4Address('203.0.113.128'): 'an IPv4 test net address'}, _AddressSpace__networks={}, _AddressSpace__addresses={4: {IPv4Address('203.0.113.128')}}, _AddressSpace__parent_supernet={IPv4Address('203.0.113.128'): None}, _AddressSpace__children_ip_object={None: {IPv4Address('203.0.113.128')}}), ip_parameter='203.0.113.128', cascade=True, self._AddressSpace__addresses={4: {IPv4Address('203.0.113.128')}}, self._AddressSpace__children_ip_object={None: {IPv4Address('203.0.113.128')}}, self._AddressSpace__description={IPv4Address('203.0.113.128'): 'an IPv4 test net address'}, self._AddressSpace__networks={}, self._AddressSpace__parent_supernet={IPv4Address('203.0.113.128'): None}, self._AddressSpace__strict=False)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "as_address = clean_address(ip_parameter)\n",
      "State:\n",
      "IPv4Address('203.0.113.128')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _test_grad_nd(n, ndim):\n",
      "    coords = [np.arange(n)] * ndim\n",
      "    xc = np.meshgrid(*coords, indexing=\"ij\")\n",
      "    u = reduce(lambda x,y: x+y**2, xc, 0.0)\n",
      "    ucopy = np.copy(u)\n",
      "    slices = tuple([slice(1,-1,None)] * ndim)\n",
      "    for i in range(ndim):\n",
      "        assert grad(u, axis=i) == pytest.approx(2*xc[i][slices])\n",
      "    assert np.all(u == ucopy)\n",
      "_test_grad_nd(n=92, ndim=1)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "coords = [np.arange(n)] * ndim\n",
      "State:\n",
      "[array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,       85, 86, 87, 88, 89, 90, 91])]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _test_grad_nd(n, ndim):\n",
      "    coords = [np.arange(n)] * ndim\n",
      "    xc = np.meshgrid(*coords, indexing=\"ij\")\n",
      "    u = reduce(lambda x,y: x+y**2, xc, 0.0)\n",
      "    ucopy = np.copy(u)\n",
      "    slices = tuple([slice(1,-1,None)] * ndim)\n",
      "    for i in range(ndim):\n",
      "        assert grad(u, axis=i) == pytest.approx(2*xc[i][slices])\n",
      "    assert np.all(u == ucopy)\n",
      "_test_grad_nd(n=92, ndim=1)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "xc = np.meshgrid(*coords, indexing=\"ij\")\n",
      "State:\n",
      "[array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,       85, 86, 87, 88, 89, 90, 91])]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _test_grad_nd(n, ndim):\n",
      "    coords = [np.arange(n)] * ndim\n",
      "    xc = np.meshgrid(*coords, indexing=\"ij\")\n",
      "    u = reduce(lambda x,y: x+y**2, xc, 0.0)\n",
      "    ucopy = np.copy(u)\n",
      "    slices = tuple([slice(1,-1,None)] * ndim)\n",
      "    for i in range(ndim):\n",
      "        assert grad(u, axis=i) == pytest.approx(2*xc[i][slices])\n",
      "    assert np.all(u == ucopy)\n",
      "_test_grad_nd(n=92, ndim=1)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "u = reduce(lambda x,y: x+y**2, xc, 0.0)\n",
      "State:\n",
      "array([0.000e+00, 1.000e+00, 4.000e+00, 9.000e+00, 1.600e+01, 2.500e+01,       3.600e+01, 4.900e+01, 6.400e+01, 8.100e+01, 1.000e+02, 1.210e+02,       1.440e+02, 1.690e+02, 1.960e+02, 2.250e+02, 2.560e+02, 2.890e+02,       3.240e+02, 3.610e+02, 4.000e+02, 4.410e+02, 4.840e+02, 5.290e+02,       5.760e+02, 6.250e+02, 6.760e+02, 7.290e+02, 7.840e+02, 8.410e+02,       9.000e+02, 9.610e+02, 1.024e+03, 1.089e+03, 1.156e+03, 1.225e+03,       1.296e+03, 1.369e+03, 1.444e+03, 1.521e+03, 1.600e+03, 1.681e+03,       1.764e+03, 1.849e+03, 1.936e+03, 2.025e+03, 2.116e+03, 2.209e+03,       2.304e+03, 2.401e+03, 2.500e+03, 2.601e+03, 2.704e+03, 2.809e+03,       2.916e+03, 3.025e+03, 3.136e+03, 3.249e+03, 3.364e+03, 3.481e+03,       3.600e+03, 3.721e+03, 3.844e+03, 3.969e+03, 4.096e+03, 4.225e+03,       4.356e+03, 4.489e+03, 4.624e+03, 4.761e+03, 4.900e+03, 5.041e+03,       5.184e+03, 5.329e+03, 5.476e+03, 5.625e+03, 5.776e+03, 5.929e+03,       6.084e+03, 6.241e+03, 6.400e+03, 6.561e+03, 6.724e+03, 6.889e+03,       7.056e+03, 7.225e+03, 7.396e+03, 7.569e+03, 7.744e+03, 7.921e+03,       8.100e+03, 8.281e+03])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_idx(ndim, axis=None, axis_idx=None):\n",
      "    s = [midx] * ndim\n",
      "    if axis is not None:\n",
      "        if hasattr(axis, \"__iter__\"):\n",
      "            for ax, axidx in zip(axis, axis_idx):\n",
      "                s[ax] = axidx\n",
      "        else:\n",
      "            s[axis] = axis_idx\n",
      "    return tuple(s)\n",
      "get_idx(ndim=1, axis=0, axis_idx=slice(None, -2, None))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "s = [midx] * ndim\n",
      "State:\n",
      "[slice(1, -1, None)]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _test_grad2_nd(n, ndim):\n",
      "    coords = [np.arange(n)] * ndim\n",
      "    xc = np.meshgrid(*coords, indexing=\"ij\")\n",
      "    u = reduce(lambda x,y: x+y**2, xc, 0.0)\n",
      "    ucopy = np.copy(u)\n",
      "    gu = np.zeros(tuple([n-2]*ndim))\n",
      "    gu2 = gu + 2.0\n",
      "    for i in range(ndim):\n",
      "        for j in range(ndim):\n",
      "            if i == j:\n",
      "                assert grad2(u, axes=(i,j)) == pytest.approx(gu2)\n",
      "            else:\n",
      "                assert grad2(u, axes=(i,j)) == pytest.approx(gu)\n",
      "    assert np.all(u == ucopy)\n",
      "_test_grad2_nd(n=32, ndim=1)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "coords = [np.arange(n)] * ndim\n",
      "State:\n",
      "[array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _test_grad2_nd(n, ndim):\n",
      "    coords = [np.arange(n)] * ndim\n",
      "    xc = np.meshgrid(*coords, indexing=\"ij\")\n",
      "    u = reduce(lambda x,y: x+y**2, xc, 0.0)\n",
      "    ucopy = np.copy(u)\n",
      "    gu = np.zeros(tuple([n-2]*ndim))\n",
      "    gu2 = gu + 2.0\n",
      "    for i in range(ndim):\n",
      "        for j in range(ndim):\n",
      "            if i == j:\n",
      "                assert grad2(u, axes=(i,j)) == pytest.approx(gu2)\n",
      "            else:\n",
      "                assert grad2(u, axes=(i,j)) == pytest.approx(gu)\n",
      "    assert np.all(u == ucopy)\n",
      "_test_grad2_nd(n=32, ndim=1)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "xc = np.meshgrid(*coords, indexing=\"ij\")\n",
      "State:\n",
      "[array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _test_grad2_nd(n, ndim):\n",
      "    coords = [np.arange(n)] * ndim\n",
      "    xc = np.meshgrid(*coords, indexing=\"ij\")\n",
      "    u = reduce(lambda x,y: x+y**2, xc, 0.0)\n",
      "    ucopy = np.copy(u)\n",
      "    gu = np.zeros(tuple([n-2]*ndim))\n",
      "    gu2 = gu + 2.0\n",
      "    for i in range(ndim):\n",
      "        for j in range(ndim):\n",
      "            if i == j:\n",
      "                assert grad2(u, axes=(i,j)) == pytest.approx(gu2)\n",
      "            else:\n",
      "                assert grad2(u, axes=(i,j)) == pytest.approx(gu)\n",
      "    assert np.all(u == ucopy)\n",
      "_test_grad2_nd(n=32, ndim=1)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "u = reduce(lambda x,y: x+y**2, xc, 0.0)\n",
      "State:\n",
      "array([  0.,   1.,   4.,   9.,  16.,  25.,  36.,  49.,  64.,  81., 100.,       121., 144., 169., 196., 225., 256., 289., 324., 361., 400., 441.,       484., 529., 576., 625., 676., 729., 784., 841., 900., 961.])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _test_grad2_nd(n, ndim):\n",
      "    coords = [np.arange(n)] * ndim\n",
      "    xc = np.meshgrid(*coords, indexing=\"ij\")\n",
      "    u = reduce(lambda x,y: x+y**2, xc, 0.0)\n",
      "    ucopy = np.copy(u)\n",
      "    gu = np.zeros(tuple([n-2]*ndim))\n",
      "    gu2 = gu + 2.0\n",
      "    for i in range(ndim):\n",
      "        for j in range(ndim):\n",
      "            if i == j:\n",
      "                assert grad2(u, axes=(i,j)) == pytest.approx(gu2)\n",
      "            else:\n",
      "                assert grad2(u, axes=(i,j)) == pytest.approx(gu)\n",
      "    assert np.all(u == ucopy)\n",
      "_test_grad2_nd(n=32, ndim=1)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "gu2 = gu + 2.0\n",
      "State:\n",
      "array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_default_expanded_coordinate(shape, ndim):\n",
      "    x_coords = []\n",
      "    for i in range(ndim):\n",
      "        idx = [None] * ndim\n",
      "        idx[i] = slice(None, None, None)\n",
      "        x_coords.append(np.arange(shape[i])[tuple(idx)])\n",
      "    return x_coords\n",
      "_get_default_expanded_coordinate(shape=array([102]), ndim=1)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "x_coords = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_default_expanded_coordinate(shape, ndim):\n",
      "    x_coords = []\n",
      "    for i in range(ndim):\n",
      "        idx = [None] * ndim\n",
      "        idx[i] = slice(None, None, None)\n",
      "        x_coords.append(np.arange(shape[i])[tuple(idx)])\n",
      "    return x_coords\n",
      "_get_default_expanded_coordinate(shape=array([102]), ndim=1)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "idx = [None] * ndim\n",
      "State:\n",
      "[None]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_default_expanded_coordinate(shape, ndim):\n",
      "    x_coords = []\n",
      "    for i in range(ndim):\n",
      "        idx = [None] * ndim\n",
      "        idx[i] = slice(None, None, None)\n",
      "        x_coords.append(np.arange(shape[i])[tuple(idx)])\n",
      "    return x_coords\n",
      "_get_default_expanded_coordinate(shape=array([102]), ndim=1)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "idx[i] = slice(None, None, None)\n",
      "State:\n",
      "[slice(None, None, None)]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_default_expanded_coordinate(shape, ndim):\n",
      "    x_coords = []\n",
      "    for i in range(ndim):\n",
      "        idx = [None] * ndim\n",
      "        idx[i] = slice(None, None, None)\n",
      "        x_coords.append(np.arange(shape[i])[tuple(idx)])\n",
      "    return x_coords\n",
      "_get_default_expanded_coordinate(shape=array([102]), ndim=1)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "x_coords.append(np.arange(shape[i])[tuple(idx)])\n",
      "State:\n",
      "[array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101])]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _pad_conserve_grads(phi):\n",
      "    ndim = np.ndim(phi)\n",
      "    pw = [1, 1]\n",
      "    pp = np.pad(phi, [tuple(pw)]*ndim, mode=\"constant\")\n",
      "    for dim in range(ndim):\n",
      "        idx_pad0_l = _get_idx(ndim, dim, slice(pw[0], pw[0]+1, None))\n",
      "        idx_pad0_r = _get_idx(ndim, dim, slice(pw[0]+1, pw[0]+2, None))\n",
      "        idx_pad0 = _get_idx(ndim, dim, slice(pw[0], pw[0]+1, None))\n",
      "        idx_pad0_fill = _get_idx(ndim, dim, slice(None, pw[0], None))\n",
      "        idx_pad1_l = _get_idx(ndim, dim, slice(-pw[1]-2, -pw[1]-1, None))\n",
      "        idx_pad1_r = _get_idx(ndim, dim, slice(-pw[1]-1, -pw[1], None))\n",
      "        idx_pad1 = _get_idx(ndim, dim, slice(-pw[1]-1, -pw[1], None))\n",
      "        idx_pad1_fill = _get_idx(ndim, dim, slice(-pw[1], None, None))\n",
      "        grad0 = pp[idx_pad0_r] - pp[idx_pad0_l]\n",
      "        grad1 = pp[idx_pad1_r] - pp[idx_pad1_l]\n",
      "        pad_arange0 = np.arange(-pw[0],0)\n",
      "        pad_arange1 = np.arange(1,pw[0]+1)\n",
      "        pad0 = pad_arange0 * grad0 + pp[idx_pad0]\n",
      "        pad1 = pad_arange1 * grad1 + pp[idx_pad1]\n",
      "        pp[idx_pad0_fill] = pad0\n",
      "        pp[idx_pad1_fill] = pad1\n",
      "    return pp\n",
      "_pad_conserve_grads(phi=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "ndim = np.ndim(phi)\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _pad_conserve_grads(phi):\n",
      "    ndim = np.ndim(phi)\n",
      "    pw = [1, 1]\n",
      "    pp = np.pad(phi, [tuple(pw)]*ndim, mode=\"constant\")\n",
      "    for dim in range(ndim):\n",
      "        idx_pad0_l = _get_idx(ndim, dim, slice(pw[0], pw[0]+1, None))\n",
      "        idx_pad0_r = _get_idx(ndim, dim, slice(pw[0]+1, pw[0]+2, None))\n",
      "        idx_pad0 = _get_idx(ndim, dim, slice(pw[0], pw[0]+1, None))\n",
      "        idx_pad0_fill = _get_idx(ndim, dim, slice(None, pw[0], None))\n",
      "        idx_pad1_l = _get_idx(ndim, dim, slice(-pw[1]-2, -pw[1]-1, None))\n",
      "        idx_pad1_r = _get_idx(ndim, dim, slice(-pw[1]-1, -pw[1], None))\n",
      "        idx_pad1 = _get_idx(ndim, dim, slice(-pw[1]-1, -pw[1], None))\n",
      "        idx_pad1_fill = _get_idx(ndim, dim, slice(-pw[1], None, None))\n",
      "        grad0 = pp[idx_pad0_r] - pp[idx_pad0_l]\n",
      "        grad1 = pp[idx_pad1_r] - pp[idx_pad1_l]\n",
      "        pad_arange0 = np.arange(-pw[0],0)\n",
      "        pad_arange1 = np.arange(1,pw[0]+1)\n",
      "        pad0 = pad_arange0 * grad0 + pp[idx_pad0]\n",
      "        pad1 = pad_arange1 * grad1 + pp[idx_pad1]\n",
      "        pp[idx_pad0_fill] = pad0\n",
      "        pp[idx_pad1_fill] = pad1\n",
      "    return pp\n",
      "_pad_conserve_grads(phi=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "pw = [1, 1]\n",
      "State:\n",
      "[1, 1]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _pad_conserve_grads(phi):\n",
      "    ndim = np.ndim(phi)\n",
      "    pw = [1, 1]\n",
      "    pp = np.pad(phi, [tuple(pw)]*ndim, mode=\"constant\")\n",
      "    for dim in range(ndim):\n",
      "        idx_pad0_l = _get_idx(ndim, dim, slice(pw[0], pw[0]+1, None))\n",
      "        idx_pad0_r = _get_idx(ndim, dim, slice(pw[0]+1, pw[0]+2, None))\n",
      "        idx_pad0 = _get_idx(ndim, dim, slice(pw[0], pw[0]+1, None))\n",
      "        idx_pad0_fill = _get_idx(ndim, dim, slice(None, pw[0], None))\n",
      "        idx_pad1_l = _get_idx(ndim, dim, slice(-pw[1]-2, -pw[1]-1, None))\n",
      "        idx_pad1_r = _get_idx(ndim, dim, slice(-pw[1]-1, -pw[1], None))\n",
      "        idx_pad1 = _get_idx(ndim, dim, slice(-pw[1]-1, -pw[1], None))\n",
      "        idx_pad1_fill = _get_idx(ndim, dim, slice(-pw[1], None, None))\n",
      "        grad0 = pp[idx_pad0_r] - pp[idx_pad0_l]\n",
      "        grad1 = pp[idx_pad1_r] - pp[idx_pad1_l]\n",
      "        pad_arange0 = np.arange(-pw[0],0)\n",
      "        pad_arange1 = np.arange(1,pw[0]+1)\n",
      "        pad0 = pad_arange0 * grad0 + pp[idx_pad0]\n",
      "        pad1 = pad_arange1 * grad1 + pp[idx_pad1]\n",
      "        pp[idx_pad0_fill] = pad0\n",
      "        pp[idx_pad1_fill] = pad1\n",
      "    return pp\n",
      "_pad_conserve_grads(phi=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "idx_pad0_fill = _get_idx(ndim, dim, slice(None, pw[0], None))\n",
      "State:\n",
      "(slice(None, 1, None),)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _pad_conserve_grads(phi):\n",
      "    ndim = np.ndim(phi)\n",
      "    pw = [1, 1]\n",
      "    pp = np.pad(phi, [tuple(pw)]*ndim, mode=\"constant\")\n",
      "    for dim in range(ndim):\n",
      "        idx_pad0_l = _get_idx(ndim, dim, slice(pw[0], pw[0]+1, None))\n",
      "        idx_pad0_r = _get_idx(ndim, dim, slice(pw[0]+1, pw[0]+2, None))\n",
      "        idx_pad0 = _get_idx(ndim, dim, slice(pw[0], pw[0]+1, None))\n",
      "        idx_pad0_fill = _get_idx(ndim, dim, slice(None, pw[0], None))\n",
      "        idx_pad1_l = _get_idx(ndim, dim, slice(-pw[1]-2, -pw[1]-1, None))\n",
      "        idx_pad1_r = _get_idx(ndim, dim, slice(-pw[1]-1, -pw[1], None))\n",
      "        idx_pad1 = _get_idx(ndim, dim, slice(-pw[1]-1, -pw[1], None))\n",
      "        idx_pad1_fill = _get_idx(ndim, dim, slice(-pw[1], None, None))\n",
      "        grad0 = pp[idx_pad0_r] - pp[idx_pad0_l]\n",
      "        grad1 = pp[idx_pad1_r] - pp[idx_pad1_l]\n",
      "        pad_arange0 = np.arange(-pw[0],0)\n",
      "        pad_arange1 = np.arange(1,pw[0]+1)\n",
      "        pad0 = pad_arange0 * grad0 + pp[idx_pad0]\n",
      "        pad1 = pad_arange1 * grad1 + pp[idx_pad1]\n",
      "        pp[idx_pad0_fill] = pad0\n",
      "        pp[idx_pad1_fill] = pad1\n",
      "    return pp\n",
      "_pad_conserve_grads(phi=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "pad_arange1 = np.arange(1,pw[0]+1)\n",
      "State:\n",
      "array([1])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_idx(ndim, dim, s, defidx=None):\n",
      "    defidx = slice(None, None, None) if defidx is None else defidx\n",
      "    idx = [defidx] * ndim\n",
      "    idx[dim] = s\n",
      "    return tuple(idx)\n",
      "_get_idx(ndim=1, dim=0, s=slice(1, 2, None), defidx=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "defidx = slice(None, None, None) if defidx is None else defidx\n",
      "State:\n",
      "slice(None, None, None)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_idx(ndim, dim, s, defidx=None):\n",
      "    defidx = slice(None, None, None) if defidx is None else defidx\n",
      "    idx = [defidx] * ndim\n",
      "    idx[dim] = s\n",
      "    return tuple(idx)\n",
      "_get_idx(ndim=1, dim=0, s=slice(1, 2, None), defidx=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "idx = [defidx] * ndim\n",
      "State:\n",
      "[slice(None, None, None)]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_idx(ndim, dim, s, defidx=None):\n",
      "    defidx = slice(None, None, None) if defidx is None else defidx\n",
      "    idx = [defidx] * ndim\n",
      "    idx[dim] = s\n",
      "    return tuple(idx)\n",
      "_get_idx(ndim=1, dim=0, s=slice(1, 2, None), defidx=None)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "idx[dim] = s\n",
      "State:\n",
      "[slice(1, 2, None)]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _test_forward_nd_slanted_pot(n, ndim, abs=None):\n",
      "    x = np.arange(n) - n / 2.0\n",
      "    xs = np.meshgrid(*([x]*ndim), indexing=\"ij\")\n",
      "    xs_sq = reduce(lambda x,y:x+y*y, xs, 0.0)\n",
      "    A = n / 6.0\n",
      "    sigma = (n/30.)\n",
      "    source = np.exp(-xs_sq / (2*sigma**2))\n",
      "    source_copy = np.copy(source)\n",
      "    phi = xs[0] * A\n",
      "    target = sb.forward(source, phi)\n",
      "    xs2 = np.copy(xs)\n",
      "    xs2[0] -= A\n",
      "    xs2_sq = reduce(lambda x,y:x+y*y, xs2, 0.0)\n",
      "    target_calc = np.exp(-xs2_sq / (2*sigma**2))\n",
      "    abs = 2.5/n if abs is None else abs\n",
      "    assert target == pytest.approx(target_calc, abs=abs)\n",
      "    assert np.ndim(target) == ndim\n",
      "    assert np.all(source == source_copy)\n",
      "_test_forward_nd_slanted_pot(n=100, ndim=1, abs=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "x = np.arange(n) - n / 2.0\n",
      "State:\n",
      "array([-50., -49., -48., -47., -46., -45., -44., -43., -42., -41., -40.,       -39., -38., -37., -36., -35., -34., -33., -32., -31., -30., -29.,       -28., -27., -26., -25., -24., -23., -22., -21., -20., -19., -18.,       -17., -16., -15., -14., -13., -12., -11., -10.,  -9.,  -8.,  -7.,        -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,   2.,   3.,   4.,         5.,   6.,   7.,   8.,   9.,  10.,  11.,  12.,  13.,  14.,  15.,        16.,  17.,  18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,  26.,        27.,  28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,        38.,  39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,        49.])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _test_forward_nd_slanted_pot(n, ndim, abs=None):\n",
      "    x = np.arange(n) - n / 2.0\n",
      "    xs = np.meshgrid(*([x]*ndim), indexing=\"ij\")\n",
      "    xs_sq = reduce(lambda x,y:x+y*y, xs, 0.0)\n",
      "    A = n / 6.0\n",
      "    sigma = (n/30.)\n",
      "    source = np.exp(-xs_sq / (2*sigma**2))\n",
      "    source_copy = np.copy(source)\n",
      "    phi = xs[0] * A\n",
      "    target = sb.forward(source, phi)\n",
      "    xs2 = np.copy(xs)\n",
      "    xs2[0] -= A\n",
      "    xs2_sq = reduce(lambda x,y:x+y*y, xs2, 0.0)\n",
      "    target_calc = np.exp(-xs2_sq / (2*sigma**2))\n",
      "    abs = 2.5/n if abs is None else abs\n",
      "    assert target == pytest.approx(target_calc, abs=abs)\n",
      "    assert np.ndim(target) == ndim\n",
      "    assert np.all(source == source_copy)\n",
      "_test_forward_nd_slanted_pot(n=100, ndim=1, abs=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "xs = np.meshgrid(*([x]*ndim), indexing=\"ij\")\n",
      "State:\n",
      "[array([-50., -49., -48., -47., -46., -45., -44., -43., -42., -41., -40.,       -39., -38., -37., -36., -35., -34., -33., -32., -31., -30., -29.,       -28., -27., -26., -25., -24., -23., -22., -21., -20., -19., -18.,       -17., -16., -15., -14., -13., -12., -11., -10.,  -9.,  -8.,  -7.,        -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,   2.,   3.,   4.,         5.,   6.,   7.,   8.,   9.,  10.,  11.,  12.,  13.,  14.,  15.,        16.,  17.,  18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,  26.,        27.,  28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,        38.,  39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,        49.])]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _test_forward_nd_slanted_pot(n, ndim, abs=None):\n",
      "    x = np.arange(n) - n / 2.0\n",
      "    xs = np.meshgrid(*([x]*ndim), indexing=\"ij\")\n",
      "    xs_sq = reduce(lambda x,y:x+y*y, xs, 0.0)\n",
      "    A = n / 6.0\n",
      "    sigma = (n/30.)\n",
      "    source = np.exp(-xs_sq / (2*sigma**2))\n",
      "    source_copy = np.copy(source)\n",
      "    phi = xs[0] * A\n",
      "    target = sb.forward(source, phi)\n",
      "    xs2 = np.copy(xs)\n",
      "    xs2[0] -= A\n",
      "    xs2_sq = reduce(lambda x,y:x+y*y, xs2, 0.0)\n",
      "    target_calc = np.exp(-xs2_sq / (2*sigma**2))\n",
      "    abs = 2.5/n if abs is None else abs\n",
      "    assert target == pytest.approx(target_calc, abs=abs)\n",
      "    assert np.ndim(target) == ndim\n",
      "    assert np.all(source == source_copy)\n",
      "_test_forward_nd_slanted_pot(n=100, ndim=1, abs=None)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "xs_sq = reduce(lambda x,y:x+y*y, xs, 0.0)\n",
      "State:\n",
      "array([2.500e+03, 2.401e+03, 2.304e+03, 2.209e+03, 2.116e+03, 2.025e+03,       1.936e+03, 1.849e+03, 1.764e+03, 1.681e+03, 1.600e+03, 1.521e+03,       1.444e+03, 1.369e+03, 1.296e+03, 1.225e+03, 1.156e+03, 1.089e+03,       1.024e+03, 9.610e+02, 9.000e+02, 8.410e+02, 7.840e+02, 7.290e+02,       6.760e+02, 6.250e+02, 5.760e+02, 5.290e+02, 4.840e+02, 4.410e+02,       4.000e+02, 3.610e+02, 3.240e+02, 2.890e+02, 2.560e+02, 2.250e+02,       1.960e+02, 1.690e+02, 1.440e+02, 1.210e+02, 1.000e+02, 8.100e+01,       6.400e+01, 4.900e+01, 3.600e+01, 2.500e+01, 1.600e+01, 9.000e+00,       4.000e+00, 1.000e+00, 0.000e+00, 1.000e+00, 4.000e+00, 9.000e+00,       1.600e+01, 2.500e+01, 3.600e+01, 4.900e+01, 6.400e+01, 8.100e+01,       1.000e+02, 1.210e+02, 1.440e+02, 1.690e+02, 1.960e+02, 2.250e+02,       2.560e+02, 2.890e+02, 3.240e+02, 3.610e+02, 4.000e+02, 4.410e+02,       4.840e+02, 5.290e+02, 5.760e+02, 6.250e+02, 6.760e+02, 7.290e+02,       7.840e+02, 8.410e+02, 9.000e+02, 9.610e+02, 1.024e+03, 1.089e+03,       1.156e+03, 1.225e+03, 1.296e+03, 1.369e+03, 1.444e+03, 1.521e+03,       1.600e+03, 1.681e+03, 1.764e+03, 1.849e+03, 1.936e+03, 2.025e+03,       2.116e+03, 2.209e+03, 2.304e+03, 2.401e+03])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _test_forward_nd_slanted_pot(n, ndim, abs=None):\n",
      "    x = np.arange(n) - n / 2.0\n",
      "    xs = np.meshgrid(*([x]*ndim), indexing=\"ij\")\n",
      "    xs_sq = reduce(lambda x,y:x+y*y, xs, 0.0)\n",
      "    A = n / 6.0\n",
      "    sigma = (n/30.)\n",
      "    source = np.exp(-xs_sq / (2*sigma**2))\n",
      "    source_copy = np.copy(source)\n",
      "    phi = xs[0] * A\n",
      "    target = sb.forward(source, phi)\n",
      "    xs2 = np.copy(xs)\n",
      "    xs2[0] -= A\n",
      "    xs2_sq = reduce(lambda x,y:x+y*y, xs2, 0.0)\n",
      "    target_calc = np.exp(-xs2_sq / (2*sigma**2))\n",
      "    abs = 2.5/n if abs is None else abs\n",
      "    assert target == pytest.approx(target_calc, abs=abs)\n",
      "    assert np.ndim(target) == ndim\n",
      "    assert np.all(source == source_copy)\n",
      "_test_forward_nd_slanted_pot(n=100, ndim=1, abs=None)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "source_copy = np.copy(source)\n",
      "State:\n",
      "array([1.38634329e-49, 1.19303368e-47, 9.38313827e-46, 6.74461286e-44,       4.43077231e-42, 2.66020642e-40, 1.45970379e-38, 7.32027899e-37,       3.35508886e-35, 1.40538048e-33, 5.38018616e-32, 1.88240985e-30,       6.01928028e-29, 1.75909155e-27, 4.69835486e-26, 1.14687658e-24,       2.55859208e-23, 5.21673666e-22, 9.72098502e-21, 1.65552266e-19,       2.57675711e-18, 3.66543340e-17, 4.76530474e-16, 5.66199552e-15,       6.14839641e-14, 6.10193668e-13, 5.53461007e-12, 4.58796249e-11,       3.47589128e-10, 2.40672244e-09, 1.52299797e-08, 8.80817920e-08,       4.65571572e-07, 2.24905597e-06, 9.92950431e-06, 4.00652974e-05,       1.47748360e-04, 4.97955422e-04, 1.53381068e-03, 4.31784001e-03,       1.11089965e-02, 2.61214099e-02, 5.61347628e-02, 1.10250525e-01,       1.97898699e-01, 3.24652467e-01, 4.86752256e-01, 6.66976811e-01,       8.35270211e-01, 9.55997482e-01, 1.00000000e+00, 9.55997482e-01,       8.35270211e-01, 6.66976811e-01, 4.86752256e-01, 3.24652467e-01,       1.97898699e-01, 1.10250525e-01, 5.61347628e-02, 2.61214099e-02,       1.11089965e-02, 4.31784001e-03, 1.53381068e-03, 4.97955422e-04,       1.47748360e-04, 4.00652974e-05, 9.92950431e-06, 2.24905597e-06,       4.65571572e-07, 8.80817920e-08, 1.52299797e-08, 2.40672244e-09,       3.47589128e-10, 4.58796249e-11, 5.53461007e-12, 6.10193668e-13,       6.14839641e-14, 5.66199552e-15, 4.76530474e-16, 3.66543340e-17,       2.57675711e-18, 1.65552266e-19, 9.72098502e-21, 5.21673666e-22,       2.55859208e-23, 1.14687658e-24, 4.69835486e-26, 1.75909155e-27,       6.01928028e-29, 1.88240985e-30, 5.38018616e-32, 1.40538048e-33,       3.35508886e-35, 7.32027899e-37, 1.45970379e-38, 2.66020642e-40,       4.43077231e-42, 6.74461286e-44, 9.38313827e-46, 1.19303368e-47])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _test_forward_nd_slanted_pot(n, ndim, abs=None):\n",
      "    x = np.arange(n) - n / 2.0\n",
      "    xs = np.meshgrid(*([x]*ndim), indexing=\"ij\")\n",
      "    xs_sq = reduce(lambda x,y:x+y*y, xs, 0.0)\n",
      "    A = n / 6.0\n",
      "    sigma = (n/30.)\n",
      "    source = np.exp(-xs_sq / (2*sigma**2))\n",
      "    source_copy = np.copy(source)\n",
      "    phi = xs[0] * A\n",
      "    target = sb.forward(source, phi)\n",
      "    xs2 = np.copy(xs)\n",
      "    xs2[0] -= A\n",
      "    xs2_sq = reduce(lambda x,y:x+y*y, xs2, 0.0)\n",
      "    target_calc = np.exp(-xs2_sq / (2*sigma**2))\n",
      "    abs = 2.5/n if abs is None else abs\n",
      "    assert target == pytest.approx(target_calc, abs=abs)\n",
      "    assert np.ndim(target) == ndim\n",
      "    assert np.all(source == source_copy)\n",
      "_test_forward_nd_slanted_pot(n=100, ndim=1, abs=None)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "xs2[0] -= A\n",
      "State:\n",
      "array([[-66.66666667, -65.66666667, -64.66666667, -63.66666667,        -62.66666667, -61.66666667, -60.66666667, -59.66666667,        -58.66666667, -57.66666667, -56.66666667, -55.66666667,        -54.66666667, -53.66666667, -52.66666667, -51.66666667,        -50.66666667, -49.66666667, -48.66666667, -47.66666667,        -46.66666667, -45.66666667, -44.66666667, -43.66666667,        -42.66666667, -41.66666667, -40.66666667, -39.66666667,        -38.66666667, -37.66666667, -36.66666667, -35.66666667,        -34.66666667, -33.66666667, -32.66666667, -31.66666667,        -30.66666667, -29.66666667, -28.66666667, -27.66666667,        -26.66666667, -25.66666667, -24.66666667, -23.66666667,        -22.66666667, -21.66666667, -20.66666667, -19.66666667,        -18.66666667, -17.66666667, -16.66666667, -15.66666667,        -14.66666667, -13.66666667, -12.66666667, -11.66666667,        -10.66666667,  -9.66666667,  -8.66666667,  -7.66666667,         -6.66666667,  -5.66666667,  -4.66666667,  -3.66666667,         -2.66666667,  -1.66666667,  -0.66666667,   0.33333333,          1.33333333,   2.33333333,   3.33333333,   4.33333333,          5.33333333,   6.33333333,   7.33333333,   8.33333333,          9.33333333,  10.33333333,  11.33333333,  12.33333333,         13.33333333,  14.33333333,  15.33333333,  16.33333333,         17.33333333,  18.33333333,  19.33333333,  20.33333333,         21.33333333,  22.33333333,  23.33333333,  24.33333333,         25.33333333,  26.33333333,  27.33333333,  28.33333333,         29.33333333,  30.33333333,  31.33333333,  32.33333333]])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _test_forward_nd_quad_pot(n, ndim, abs=None):\n",
      "    x = np.arange(n) - n / 2.0\n",
      "    xs = np.meshgrid(*([x]*ndim), indexing=\"ij\")\n",
      "    xs_sq = reduce(lambda x,y:x+y*y, xs, 0.0)\n",
      "    B = 1.0\n",
      "    scale = 1 + B\n",
      "    sigma = (n/30.)\n",
      "    source = np.exp(-xs_sq / (2*sigma**2))\n",
      "    source_copy = np.copy(source)\n",
      "    phi = 0.5*B*xs_sq\n",
      "    target = sb.forward(source, phi)\n",
      "    target_calc = (scale**-ndim)*np.exp(-xs_sq / (2*(sigma*scale)**2))\n",
      "    abs = 2.5/n if abs is None else abs\n",
      "    assert target == pytest.approx(target_calc, abs=abs)\n",
      "    assert np.ndim(target) == ndim\n",
      "    assert np.all(source == source_copy)\n",
      "_test_forward_nd_quad_pot(n=100, ndim=1, abs=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "x = np.arange(n) - n / 2.0\n",
      "State:\n",
      "array([-50., -49., -48., -47., -46., -45., -44., -43., -42., -41., -40.,       -39., -38., -37., -36., -35., -34., -33., -32., -31., -30., -29.,       -28., -27., -26., -25., -24., -23., -22., -21., -20., -19., -18.,       -17., -16., -15., -14., -13., -12., -11., -10.,  -9.,  -8.,  -7.,        -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,   2.,   3.,   4.,         5.,   6.,   7.,   8.,   9.,  10.,  11.,  12.,  13.,  14.,  15.,        16.,  17.,  18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,  26.,        27.,  28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,        38.,  39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,        49.])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _test_forward_nd_quad_pot(n, ndim, abs=None):\n",
      "    x = np.arange(n) - n / 2.0\n",
      "    xs = np.meshgrid(*([x]*ndim), indexing=\"ij\")\n",
      "    xs_sq = reduce(lambda x,y:x+y*y, xs, 0.0)\n",
      "    B = 1.0\n",
      "    scale = 1 + B\n",
      "    sigma = (n/30.)\n",
      "    source = np.exp(-xs_sq / (2*sigma**2))\n",
      "    source_copy = np.copy(source)\n",
      "    phi = 0.5*B*xs_sq\n",
      "    target = sb.forward(source, phi)\n",
      "    target_calc = (scale**-ndim)*np.exp(-xs_sq / (2*(sigma*scale)**2))\n",
      "    abs = 2.5/n if abs is None else abs\n",
      "    assert target == pytest.approx(target_calc, abs=abs)\n",
      "    assert np.ndim(target) == ndim\n",
      "    assert np.all(source == source_copy)\n",
      "_test_forward_nd_quad_pot(n=100, ndim=1, abs=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "xs = np.meshgrid(*([x]*ndim), indexing=\"ij\")\n",
      "State:\n",
      "[array([-50., -49., -48., -47., -46., -45., -44., -43., -42., -41., -40.,       -39., -38., -37., -36., -35., -34., -33., -32., -31., -30., -29.,       -28., -27., -26., -25., -24., -23., -22., -21., -20., -19., -18.,       -17., -16., -15., -14., -13., -12., -11., -10.,  -9.,  -8.,  -7.,        -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,   2.,   3.,   4.,         5.,   6.,   7.,   8.,   9.,  10.,  11.,  12.,  13.,  14.,  15.,        16.,  17.,  18.,  19.,  20.,  21.,  22.,  23.,  24.,  25.,  26.,        27.,  28.,  29.,  30.,  31.,  32.,  33.,  34.,  35.,  36.,  37.,        38.,  39.,  40.,  41.,  42.,  43.,  44.,  45.,  46.,  47.,  48.,        49.])]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _test_forward_nd_quad_pot(n, ndim, abs=None):\n",
      "    x = np.arange(n) - n / 2.0\n",
      "    xs = np.meshgrid(*([x]*ndim), indexing=\"ij\")\n",
      "    xs_sq = reduce(lambda x,y:x+y*y, xs, 0.0)\n",
      "    B = 1.0\n",
      "    scale = 1 + B\n",
      "    sigma = (n/30.)\n",
      "    source = np.exp(-xs_sq / (2*sigma**2))\n",
      "    source_copy = np.copy(source)\n",
      "    phi = 0.5*B*xs_sq\n",
      "    target = sb.forward(source, phi)\n",
      "    target_calc = (scale**-ndim)*np.exp(-xs_sq / (2*(sigma*scale)**2))\n",
      "    abs = 2.5/n if abs is None else abs\n",
      "    assert target == pytest.approx(target_calc, abs=abs)\n",
      "    assert np.ndim(target) == ndim\n",
      "    assert np.all(source == source_copy)\n",
      "_test_forward_nd_quad_pot(n=100, ndim=1, abs=None)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "xs_sq = reduce(lambda x,y:x+y*y, xs, 0.0)\n",
      "State:\n",
      "array([2.500e+03, 2.401e+03, 2.304e+03, 2.209e+03, 2.116e+03, 2.025e+03,       1.936e+03, 1.849e+03, 1.764e+03, 1.681e+03, 1.600e+03, 1.521e+03,       1.444e+03, 1.369e+03, 1.296e+03, 1.225e+03, 1.156e+03, 1.089e+03,       1.024e+03, 9.610e+02, 9.000e+02, 8.410e+02, 7.840e+02, 7.290e+02,       6.760e+02, 6.250e+02, 5.760e+02, 5.290e+02, 4.840e+02, 4.410e+02,       4.000e+02, 3.610e+02, 3.240e+02, 2.890e+02, 2.560e+02, 2.250e+02,       1.960e+02, 1.690e+02, 1.440e+02, 1.210e+02, 1.000e+02, 8.100e+01,       6.400e+01, 4.900e+01, 3.600e+01, 2.500e+01, 1.600e+01, 9.000e+00,       4.000e+00, 1.000e+00, 0.000e+00, 1.000e+00, 4.000e+00, 9.000e+00,       1.600e+01, 2.500e+01, 3.600e+01, 4.900e+01, 6.400e+01, 8.100e+01,       1.000e+02, 1.210e+02, 1.440e+02, 1.690e+02, 1.960e+02, 2.250e+02,       2.560e+02, 2.890e+02, 3.240e+02, 3.610e+02, 4.000e+02, 4.410e+02,       4.840e+02, 5.290e+02, 5.760e+02, 6.250e+02, 6.760e+02, 7.290e+02,       7.840e+02, 8.410e+02, 9.000e+02, 9.610e+02, 1.024e+03, 1.089e+03,       1.156e+03, 1.225e+03, 1.296e+03, 1.369e+03, 1.444e+03, 1.521e+03,       1.600e+03, 1.681e+03, 1.764e+03, 1.849e+03, 1.936e+03, 2.025e+03,       2.116e+03, 2.209e+03, 2.304e+03, 2.401e+03])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _test_forward_nd_quad_pot(n, ndim, abs=None):\n",
      "    x = np.arange(n) - n / 2.0\n",
      "    xs = np.meshgrid(*([x]*ndim), indexing=\"ij\")\n",
      "    xs_sq = reduce(lambda x,y:x+y*y, xs, 0.0)\n",
      "    B = 1.0\n",
      "    scale = 1 + B\n",
      "    sigma = (n/30.)\n",
      "    source = np.exp(-xs_sq / (2*sigma**2))\n",
      "    source_copy = np.copy(source)\n",
      "    phi = 0.5*B*xs_sq\n",
      "    target = sb.forward(source, phi)\n",
      "    target_calc = (scale**-ndim)*np.exp(-xs_sq / (2*(sigma*scale)**2))\n",
      "    abs = 2.5/n if abs is None else abs\n",
      "    assert target == pytest.approx(target_calc, abs=abs)\n",
      "    assert np.ndim(target) == ndim\n",
      "    assert np.all(source == source_copy)\n",
      "_test_forward_nd_quad_pot(n=100, ndim=1, abs=None)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "source = np.exp(-xs_sq / (2*sigma**2))\n",
      "State:\n",
      "array([1.38634329e-49, 1.19303368e-47, 9.38313827e-46, 6.74461286e-44,       4.43077231e-42, 2.66020642e-40, 1.45970379e-38, 7.32027899e-37,       3.35508886e-35, 1.40538048e-33, 5.38018616e-32, 1.88240985e-30,       6.01928028e-29, 1.75909155e-27, 4.69835486e-26, 1.14687658e-24,       2.55859208e-23, 5.21673666e-22, 9.72098502e-21, 1.65552266e-19,       2.57675711e-18, 3.66543340e-17, 4.76530474e-16, 5.66199552e-15,       6.14839641e-14, 6.10193668e-13, 5.53461007e-12, 4.58796249e-11,       3.47589128e-10, 2.40672244e-09, 1.52299797e-08, 8.80817920e-08,       4.65571572e-07, 2.24905597e-06, 9.92950431e-06, 4.00652974e-05,       1.47748360e-04, 4.97955422e-04, 1.53381068e-03, 4.31784001e-03,       1.11089965e-02, 2.61214099e-02, 5.61347628e-02, 1.10250525e-01,       1.97898699e-01, 3.24652467e-01, 4.86752256e-01, 6.66976811e-01,       8.35270211e-01, 9.55997482e-01, 1.00000000e+00, 9.55997482e-01,       8.35270211e-01, 6.66976811e-01, 4.86752256e-01, 3.24652467e-01,       1.97898699e-01, 1.10250525e-01, 5.61347628e-02, 2.61214099e-02,       1.11089965e-02, 4.31784001e-03, 1.53381068e-03, 4.97955422e-04,       1.47748360e-04, 4.00652974e-05, 9.92950431e-06, 2.24905597e-06,       4.65571572e-07, 8.80817920e-08, 1.52299797e-08, 2.40672244e-09,       3.47589128e-10, 4.58796249e-11, 5.53461007e-12, 6.10193668e-13,       6.14839641e-14, 5.66199552e-15, 4.76530474e-16, 3.66543340e-17,       2.57675711e-18, 1.65552266e-19, 9.72098502e-21, 5.21673666e-22,       2.55859208e-23, 1.14687658e-24, 4.69835486e-26, 1.75909155e-27,       6.01928028e-29, 1.88240985e-30, 5.38018616e-32, 1.40538048e-33,       3.35508886e-35, 7.32027899e-37, 1.45970379e-38, 2.66020642e-40,       4.43077231e-42, 6.74461286e-44, 9.38313827e-46, 1.19303368e-47])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _test_forward_nd_quad_pot(n, ndim, abs=None):\n",
      "    x = np.arange(n) - n / 2.0\n",
      "    xs = np.meshgrid(*([x]*ndim), indexing=\"ij\")\n",
      "    xs_sq = reduce(lambda x,y:x+y*y, xs, 0.0)\n",
      "    B = 1.0\n",
      "    scale = 1 + B\n",
      "    sigma = (n/30.)\n",
      "    source = np.exp(-xs_sq / (2*sigma**2))\n",
      "    source_copy = np.copy(source)\n",
      "    phi = 0.5*B*xs_sq\n",
      "    target = sb.forward(source, phi)\n",
      "    target_calc = (scale**-ndim)*np.exp(-xs_sq / (2*(sigma*scale)**2))\n",
      "    abs = 2.5/n if abs is None else abs\n",
      "    assert target == pytest.approx(target_calc, abs=abs)\n",
      "    assert np.ndim(target) == ndim\n",
      "    assert np.all(source == source_copy)\n",
      "_test_forward_nd_quad_pot(n=100, ndim=1, abs=None)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "target_calc = (scale**-ndim)*np.exp(-xs_sq / (2*(sigma*scale)**2))\n",
      "State:\n",
      "array([3.05096834e-13, 9.29251306e-13, 2.76730504e-12, 8.05766599e-12,       2.29398124e-11, 6.38555777e-11, 1.73794564e-10, 4.62489538e-10,       1.20336122e-09, 3.06138876e-09, 7.61498987e-09, 1.85203228e-08,       4.40408960e-08, 1.02398151e-07, 2.32785786e-07, 5.17427106e-07,       1.12452798e-06, 2.38956987e-06, 4.96475215e-06, 1.00856475e-05,       2.00326487e-05, 3.89046343e-05, 7.38741801e-05, 1.37155234e-04,       2.48977711e-04, 4.41913153e-04, 7.66905340e-04, 1.30129263e-03,       2.15892000e-03, 3.50208357e-03, 5.55449827e-03, 8.61373566e-03,       1.30607049e-02, 1.93628852e-02, 2.80673814e-02, 3.97797544e-02,       5.51252627e-02, 7.46908876e-02, 9.89493495e-02, 1.28170076e-01,       1.62326234e-01, 2.01010692e-01, 2.43376128e-01, 2.88114537e-01,       3.33488405e-01, 3.77419801e-01, 4.17635106e-01, 4.51853539e-01,       4.77998741e-01, 4.94406522e-01, 5.00000000e-01, 4.94406522e-01,       4.77998741e-01, 4.51853539e-01, 4.17635106e-01, 3.77419801e-01,       3.33488405e-01, 2.88114537e-01, 2.43376128e-01, 2.01010692e-01,       1.62326234e-01, 1.28170076e-01, 9.89493495e-02, 7.46908876e-02,       5.51252627e-02, 3.97797544e-02, 2.80673814e-02, 1.93628852e-02,       1.30607049e-02, 8.61373566e-03, 5.55449827e-03, 3.50208357e-03,       2.15892000e-03, 1.30129263e-03, 7.66905340e-04, 4.41913153e-04,       2.48977711e-04, 1.37155234e-04, 7.38741801e-05, 3.89046343e-05,       2.00326487e-05, 1.00856475e-05, 4.96475215e-06, 2.38956987e-06,       1.12452798e-06, 5.17427106e-07, 2.32785786e-07, 1.02398151e-07,       4.40408960e-08, 1.85203228e-08, 7.61498987e-09, 3.06138876e-09,       1.20336122e-09, 4.62489538e-10, 1.73794564e-10, 6.38555777e-11,       2.29398124e-11, 8.05766599e-12, 2.76730504e-12, 9.29251306e-13])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write_sectionindex(self):\n",
      "        all_keys = []\n",
      "        all_start = []\n",
      "        all_stop = []\n",
      "        prev_divider = False\n",
      "        for idx, ln in enumerate(self.description):\n",
      "            if prev_divider is True and any([h == ln.rstrip() for h in self._section_headings]):\n",
      "                key = ln[1:].rstrip().lstrip()\n",
      "                if len(all_start) > 0:\n",
      "                    all_stop.append(idx - 1)\n",
      "                all_keys.append(key)\n",
      "                all_start.append(idx)\n",
      "                prev_divider = False\n",
      "            if DIVIDER in ln:\n",
      "                prev_divider = True\n",
      "        all_stop.append(self.data_beginline)\n",
      "        section_map = collections.defaultdict(list)\n",
      "        for (k, start, stop) in zip(all_keys, all_start, all_stop):\n",
      "            section_map[k].append((start, stop))\n",
      "        section_map.default_factory = None\n",
      "        self.sectionindex = section_map\n",
      "_write_sectionindex(self=<proxysiphon.proxychimp.Guts object at 0x7f9f0263ae20>, self._section_headings=['\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "all_keys = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write_sectionindex(self):\n",
      "        all_keys = []\n",
      "        all_start = []\n",
      "        all_stop = []\n",
      "        prev_divider = False\n",
      "        for idx, ln in enumerate(self.description):\n",
      "            if prev_divider is True and any([h == ln.rstrip() for h in self._section_headings]):\n",
      "                key = ln[1:].rstrip().lstrip()\n",
      "                if len(all_start) > 0:\n",
      "                    all_stop.append(idx - 1)\n",
      "                all_keys.append(key)\n",
      "                all_start.append(idx)\n",
      "                prev_divider = False\n",
      "            if DIVIDER in ln:\n",
      "                prev_divider = True\n",
      "        all_stop.append(self.data_beginline)\n",
      "        section_map = collections.defaultdict(list)\n",
      "        for (k, start, stop) in zip(all_keys, all_start, all_stop):\n",
      "            section_map[k].append((start, stop))\n",
      "        section_map.default_factory = None\n",
      "        self.sectionindex = section_map\n",
      "_write_sectionindex(self=<proxysiphon.proxychimp.Guts object at 0x7f9f0263ae20>, self._section_headings=['\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "all_start = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _write_sectionindex(self):\n",
      "        all_keys = []\n",
      "        all_start = []\n",
      "        all_stop = []\n",
      "        prev_divider = False\n",
      "        for idx, ln in enumerate(self.description):\n",
      "            if prev_divider is True and any([h == ln.rstrip() for h in self._section_headings]):\n",
      "                key = ln[1:].rstrip().lstrip()\n",
      "                if len(all_start) > 0:\n",
      "                    all_stop.append(idx - 1)\n",
      "                all_keys.append(key)\n",
      "                all_start.append(idx)\n",
      "                prev_divider = False\n",
      "            if DIVIDER in ln:\n",
      "                prev_divider = True\n",
      "        all_stop.append(self.data_beginline)\n",
      "        section_map = collections.defaultdict(list)\n",
      "        for (k, start, stop) in zip(all_keys, all_start, all_stop):\n",
      "            section_map[k].append((start, stop))\n",
      "        section_map.default_factory = None\n",
      "        self.sectionindex = section_map\n",
      "_write_sectionindex(self=<proxysiphon.proxychimp.Guts object at 0x7f9f0263ae20>, self._section_headings=['\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "all_stop = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def guess_missingvalues(self):\n",
      "        section = self.pull_section('Data')[0]\n",
      "        out = None\n",
      "        for ln in section:\n",
      "            if MISSINGVALUE_LABEL in ln:\n",
      "                out = ln.split(MISSINGVALUE_LABEL)[1]\n",
      "        log.debug('Guessed missing value(s): {}'.format(out))\n",
      "        return out\n",
      "guess_missingvalues(self=<proxysiphon.proxychimp.Guts object at 0x7f9f025d1f10>, self._section_headings=['\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "section = self.pull_section('Data')[0]\n",
      "State:\n",
      "['# Data', '# Data lines follow (have no #)', '# Missing Value: -999']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def yank_chron_df(self, section_name='Chronology_Information', missingvalues=None):\n",
      "        if missingvalues is None:\n",
      "            missingvalues = [-999, 'NaN']\n",
      "        section = self.pull_section(section_name)[0]\n",
      "        start_idx = section.index(CHRON_HEADER)\n",
      "        g_chrond = section[start_idx:]\n",
      "        g_chrond_cleaned = [x[2:].rstrip() for x in g_chrond]\n",
      "        data_bytes = '\\n'.join(g_chrond_cleaned).encode('utf-8')\n",
      "        df = pd.read_csv(BytesIO(data_bytes), sep='\\t', na_values=missingvalues)\n",
      "        return df\n",
      "yank_chron_df(self=<proxysiphon.proxychimp.Guts object at 0x7f9f025d1f10>, section_name='Chronology_Information', missingvalues=None, self._section_headings=['\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "missingvalues = [-999, 'NaN']\n",
      "State:\n",
      "[-999, 'NaN']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def yank_chron_df(self, section_name='Chronology_Information', missingvalues=None):\n",
      "        if missingvalues is None:\n",
      "            missingvalues = [-999, 'NaN']\n",
      "        section = self.pull_section(section_name)[0]\n",
      "        start_idx = section.index(CHRON_HEADER)\n",
      "        g_chrond = section[start_idx:]\n",
      "        g_chrond_cleaned = [x[2:].rstrip() for x in g_chrond]\n",
      "        data_bytes = '\\n'.join(g_chrond_cleaned).encode('utf-8')\n",
      "        df = pd.read_csv(BytesIO(data_bytes), sep='\\t', na_values=missingvalues)\n",
      "        return df\n",
      "yank_chron_df(self=<proxysiphon.proxychimp.Guts object at 0x7f9f025d1f10>, section_name='Chronology_Information', missingvalues=None, self._section_headings=['\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "section = self.pull_section(section_name)[0]\n",
      "State:\n",
      "['# Chronology_Information', '#', '# Labcode\\tdepth_top\\tdepth_bottom\\tmat_dated\\t14C_date\\t14C_1s_err\\tdelta_R\\tdelta_R_1s_err\\tother_date\\tother_1s_err\\tother_type\\t', '#']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def yank_chron_df(self, section_name='Chronology_Information', missingvalues=None):\n",
      "        if missingvalues is None:\n",
      "            missingvalues = [-999, 'NaN']\n",
      "        section = self.pull_section(section_name)[0]\n",
      "        start_idx = section.index(CHRON_HEADER)\n",
      "        g_chrond = section[start_idx:]\n",
      "        g_chrond_cleaned = [x[2:].rstrip() for x in g_chrond]\n",
      "        data_bytes = '\\n'.join(g_chrond_cleaned).encode('utf-8')\n",
      "        df = pd.read_csv(BytesIO(data_bytes), sep='\\t', na_values=missingvalues)\n",
      "        return df\n",
      "yank_chron_df(self=<proxysiphon.proxychimp.Guts object at 0x7f9f025d1f10>, section_name='Chronology_Information', missingvalues=None, self._section_headings=['\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "start_idx = section.index(CHRON_HEADER)\n",
      "State:\n",
      "2\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse(self):\n",
      "        input_chapters = []\n",
      "        for line in self.infos.splitlines()[1:]:\n",
      "            if (\n",
      "                self.duration_tag_separator == \"time=\"\n",
      "                and self.check_duration\n",
      "                and \"time=\" in line\n",
      "            ):\n",
      "                self.result[\"duration\"] = self.parse_duration(line)\n",
      "            elif self._inside_output or line[0] != \" \":\n",
      "                if self.duration_tag_separator == \"time=\" and not self._inside_output:\n",
      "                    self._inside_output = True\n",
      "            elif not self._inside_file_metadata and line.startswith(\"  Metadata:\"):\n",
      "                self._inside_file_metadata = True\n",
      "            elif line.startswith(\"  Duration:\"):\n",
      "                self._inside_file_metadata = False\n",
      "                if self.check_duration and self.duration_tag_separator == \"Duration: \":\n",
      "                    self.result[\"duration\"] = self.parse_duration(line)\n",
      "                bitrate_match = re.search(r\"bitrate: (\\d+) kb/s\", line)\n",
      "                self.result[\"bitrate\"] = (\n",
      "                    int(bitrate_match.group(1)) if bitrate_match else None\n",
      "                )\n",
      "                start_match = re.search(r\"start: (\\d+\\.?\\d+)\", line)\n",
      "                self.result[\"start\"] = (\n",
      "                    float(start_match.group(1)) if start_match else None\n",
      "                )\n",
      "            elif self._inside_file_metadata:\n",
      "                field, value = self.parse_metadata_field_value(line)\n",
      "                if field == \"\":\n",
      "                    field = self._last_metadata_field_added\n",
      "                    value = self.result[\"metadata\"][field] + \"\\n\" + value\n",
      "                else:\n",
      "                    self._last_metadata_field_added = field\n",
      "                self.result[\"metadata\"][field] = value\n",
      "            elif line.lstrip().startswith(\"Stream \"):\n",
      "                if self._current_stream:\n",
      "                    self._current_input_file[\"streams\"].append(self._current_stream)\n",
      "                main_info_match = re.search(\n",
      "                    r\"^Stream\\s\n",
      "                    line.lstrip(),\n",
      "                )\n",
      "                (\n",
      "                    input_number,\n",
      "                    stream_number,\n",
      "                    language,\n",
      "                    stream_type,\n",
      "                ) = main_info_match.groups()\n",
      "                input_number = int(input_number)\n",
      "                stream_number = int(stream_number)\n",
      "                stream_type_lower = stream_type.lower()\n",
      "                if language == \"und\":\n",
      "                    language = None\n",
      "                self._current_stream = {\n",
      "                    \"input_number\": input_number,\n",
      "                    \"stream_number\": stream_number,\n",
      "                    \"stream_type\": stream_type_lower,\n",
      "                    \"language\": language,\n",
      "                    \"default\": not self._default_stream_found\n",
      "                    or line.endswith(\"(default)\"),\n",
      "                }\n",
      "                self._default_stream_found = True\n",
      "                if self._current_stream[\"default\"]:\n",
      "                    self.result[\n",
      "                        f\"default_{stream_type_lower}_input_number\"\n",
      "                    ] = input_number\n",
      "                    self.result[\n",
      "                        f\"default_{stream_type_lower}_stream_number\"\n",
      "                    ] = stream_number\n",
      "                if self._current_chapter:\n",
      "                    input_chapters[input_number].append(self._current_chapter)\n",
      "                    self._current_chapter = None\n",
      "                if \"input_number\" not in self._current_input_file:\n",
      "                    self._current_input_file[\"input_number\"] = input_number\n",
      "                elif self._current_input_file[\"input_number\"] != input_number:\n",
      "                    if len(input_chapters) >= input_number + 1:\n",
      "                        self._current_input_file[\"chapters\"] = input_chapters[\n",
      "                            input_number\n",
      "                        ]\n",
      "                    self.result[\"inputs\"].append(self._current_input_file)\n",
      "                    self._current_input_file = {\"input_number\": input_number}\n",
      "                try:\n",
      "                    global_data, stream_data = self.parse_data_by_stream_type(\n",
      "                        stream_type, line\n",
      "                    )\n",
      "                except NotImplementedError as exc:\n",
      "                    warnings.warn(\n",
      "                        f\"{str(exc)}\\nffmpeg output:\\n\\n{self.infos}\", UserWarning\n",
      "                    )\n",
      "                else:\n",
      "                    self.result.update(global_data)\n",
      "                    self._current_stream.update(stream_data)\n",
      "            elif line.startswith(\"    Metadata:\"):\n",
      "                continue\n",
      "            elif self._current_stream:\n",
      "                if \"metadata\" not in self._current_stream:\n",
      "                    self._current_stream[\"metadata\"] = {}\n",
      "                field, value = self.parse_metadata_field_value(line)\n",
      "                if self._current_stream[\"stream_type\"] == \"video\":\n",
      "                    field, value = self.video_metadata_type_casting(field, value)\n",
      "                    if field == \"rotate\":\n",
      "                        self.result[\"video_rotation\"] = value\n",
      "                if field == \"\":\n",
      "                    field = self._last_metadata_field_added\n",
      "                    value = self._current_stream[\"metadata\"][field] + \"\\n\" + value\n",
      "                else:\n",
      "                    self._last_metadata_field_added = field\n",
      "                self._current_stream[\"metadata\"][field] = value\n",
      "            elif line.startswith(\"    Chapter\"):\n",
      "                if self._current_chapter:\n",
      "                    if len(input_chapters) < self._current_chapter[\"input_number\"] + 1:\n",
      "                        input_chapters.append([])\n",
      "                    input_chapters[self._current_chapter[\"input_number\"]].append(\n",
      "                        self._current_chapter\n",
      "                    )\n",
      "                chapter_data_match = re.search(\n",
      "                    r\"^    Chapter\n",
      "                    line,\n",
      "                )\n",
      "                input_number, chapter_number, start, end = chapter_data_match.groups()\n",
      "                self._current_chapter = {\n",
      "                    \"input_number\": int(input_number),\n",
      "                    \"chapter_number\": int(chapter_number),\n",
      "                    \"start\": float(start),\n",
      "                    \"end\": float(end),\n",
      "                }\n",
      "            elif self._current_chapter:\n",
      "                if \"metadata\" not in self._current_chapter:\n",
      "                    self._current_chapter[\"metadata\"] = {}\n",
      "                field, value = self.parse_metadata_field_value(line)\n",
      "                if field == \"\":\n",
      "                    field = self._last_metadata_field_added\n",
      "                    value = self._current_chapter[\"metadata\"][field] + \"\\n\" + value\n",
      "                else:\n",
      "                    self._last_metadata_field_added = field\n",
      "                self._current_chapter[\"metadata\"][field] = value\n",
      "        if self._current_input_file:\n",
      "            self._current_input_file[\"streams\"].append(self._current_stream)\n",
      "            if len(input_chapters) == self._current_input_file[\"input_number\"] + 1:\n",
      "                self._current_input_file[\"chapters\"] = input_chapters[\n",
      "                    self._current_input_file[\"input_number\"]\n",
      "                ]\n",
      "            self.result[\"inputs\"].append(self._current_input_file)\n",
      "        if self.result[\"video_found\"] and self.check_duration:\n",
      "            self.result[\"video_n_frames\"] = int(\n",
      "                self.result[\"duration\"] * self.result[\"video_fps\"]\n",
      "            )\n",
      "            self.result[\"video_duration\"] = self.result[\"duration\"]\n",
      "        else:\n",
      "            self.result[\"video_n_frames\"] = 1\n",
      "            self.result[\"video_duration\"] = None\n",
      "        if self.result[\"audio_found\"] and not self.result.get(\"audio_bitrate\"):\n",
      "            self.result[\"audio_bitrate\"] = None\n",
      "            for streams_input in self.result[\"inputs\"]:\n",
      "                for stream in streams_input[\"streams\"]:\n",
      "                    if stream[\"stream_type\"] == \"audio\" and stream.get(\"bitrate\"):\n",
      "                        self.result[\"audio_bitrate\"] = stream[\"bitrate\"]\n",
      "                        break\n",
      "                if self.result[\"audio_bitrate\"] is not None:\n",
      "                    break\n",
      "        result = self.result\n",
      "        self._reset_state()\n",
      "        return result\n",
      "parse(self=<moviepy.video.io.ffmpeg_reader.FFmpegInfosParser object at 0x7f8c0a6795e0>, self._current_chapter=None, self._current_input_file={'streams': []}, self._current_stream=None, self._default_stream_found=False, self._inside_file_metadata=False, self._inside_output=False, self._last_metadata_field_added=None, self.check_duration=True, self.duration_tag_separator='Duration: ', self.filename='media/bitmap.mp4', self.fps_source='fps', self.infos=\"Input\n",
      "\n",
      "n:\n",
      "25\n",
      "Statement:\n",
      "self._inside_file_metadata = False\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def initialize(self, start_time=0):\n",
      "        self.close(delete_lastread=False)\n",
      "        if start_time != 0:\n",
      "            offset = min(1, start_time)\n",
      "            i_arg = [\n",
      "                \"-ss\",\n",
      "                \"%.06f\" % (start_time - offset),\n",
      "                \"-i\",\n",
      "                self.filename,\n",
      "                \"-ss\",\n",
      "                \"%.06f\" % offset,\n",
      "            ]\n",
      "        else:\n",
      "            i_arg = [\"-i\", self.filename]\n",
      "        cmd = (\n",
      "            [FFMPEG_BINARY]\n",
      "            + i_arg\n",
      "            + [\n",
      "                \"-loglevel\",\n",
      "                \"error\",\n",
      "                \"-f\",\n",
      "                \"image2pipe\",\n",
      "                \"-vf\",\n",
      "                \"scale=%d:%d\" % tuple(self.size),\n",
      "                \"-sws_flags\",\n",
      "                self.resize_algo,\n",
      "                \"-pix_fmt\",\n",
      "                self.pixel_format,\n",
      "                \"-vcodec\",\n",
      "                \"rawvideo\",\n",
      "                \"-\",\n",
      "            ]\n",
      "        )\n",
      "        popen_params = cross_platform_popen_params(\n",
      "            {\n",
      "                \"bufsize\": self.bufsize,\n",
      "                \"stdout\": sp.PIPE,\n",
      "                \"stderr\": sp.PIPE,\n",
      "                \"stdin\": sp.DEVNULL,\n",
      "            }\n",
      "        )\n",
      "        self.proc = sp.Popen(cmd, **popen_params)\n",
      "        self.pos = self.get_frame_number(start_time)\n",
      "        self.lastread = self.read_frame()\n",
      "initialize(self=<moviepy.video.io.ffmpeg_reader.FFMPEG_VideoReader object at 0x7f8c0a679fd0>, start_time=0, self.bitrate=1, self.bufsize=103, self.depth=3, self.duration=5.0, self.ffmpeg_duration=5.0, self.filename='media/bitmap.mp4', self.fps=1.0, self.infos={'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf58.20.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1, 1], 'bitrate': 1, 'fps': 1.0, 'metadata': {'handler_name': 'VideoHandler'}}], 'input_number': 0}], 'duration': 5.0, 'bitrate': 2, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_size': [1, 1], 'video_bitrate': 1, 'video_fps': 1.0, 'video_n_frames': 5, 'video_duration': 5.0}, self.n_frames=5, self.pixel_format='rgb24', self.proc=None, self.resize_algo='bicubic', self.rotation=0, self.size=[1, 1])\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "i_arg = [\"-i\", self.filename]\n",
      "State:\n",
      "['-i', 'media/bitmap.mp4']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def read_chunk(self, chunksize):\n",
      "        chunksize = int(round(chunksize))\n",
      "        s = self.proc.stdout.read(self.nchannels * chunksize * self.nbytes)\n",
      "        data_type = {1: \"int8\", 2: \"int16\", 4: \"int32\"}[self.nbytes]\n",
      "        if hasattr(np, \"frombuffer\"):\n",
      "            result = np.frombuffer(s, dtype=data_type)\n",
      "        else:\n",
      "            result = np.fromstring(s, dtype=data_type)\n",
      "        result = (1.0 * result / 2 ** (8 * self.nbytes - 1)).reshape(\n",
      "            (int(len(result) / self.nchannels), self.nchannels)\n",
      "        )\n",
      "        pad = np.zeros((chunksize - len(result), self.nchannels), dtype=result.dtype)\n",
      "        result = np.concatenate([result, pad])\n",
      "        self.pos = self.pos + chunksize\n",
      "        return result\n",
      "read_chunk(self=<moviepy.audio.io.readers.FFMPEG_AudioReader object at 0x7f8c0a2042e0>, chunksize=200000, self.bitrate=127, self.buffer=None, self.buffer_startframe=1, self.buffersize=200000, self.codec='pcm_s16le', self.duration=9.64, self.filename='media/chaplin.mp4', self.format='s16le', self.fps=44100, self.infos={'video_found': True, 'audio_found': True, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf57.25.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [640, 360], 'bitrate': 495, 'fps': 25.0, 'metadata': {'handler_name': 'VideoHandler'}}, {'input_number': 0, 'stream_number': 1, 'stream_type': 'audio', 'language': None, 'default': True, 'fps': 44100, 'bitrate': 127, 'metadata': {'handler_name': 'SoundHandler'}}], 'input_number': 0}], 'duration': 9.64, 'bitrate': 631, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_size': [640, 360], 'video_bitrate': 495, 'video_fps': 25.0, 'default_audio_input_number': 0, 'default_audio_stream_number': 1, 'audio_fps': 44100, 'audio_bitrate': 127, 'video_n_frames': 241, 'video_duration': 9.64}, self.n_frames=425124, self.nbytes=2, self.nchannels=2, self.pos=0, self.proc=<Popen: returncode: None args: ['/local/rcs/XXX/miniforge3/envs/Zulko+mov...>)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "data_type = {1: \"int8\", 2: \"int16\", 4: \"int32\"}[self.nbytes]\n",
      "State:\n",
      "'int16'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def read_chunk(self, chunksize):\n",
      "        chunksize = int(round(chunksize))\n",
      "        s = self.proc.stdout.read(self.nchannels * chunksize * self.nbytes)\n",
      "        data_type = {1: \"int8\", 2: \"int16\", 4: \"int32\"}[self.nbytes]\n",
      "        if hasattr(np, \"frombuffer\"):\n",
      "            result = np.frombuffer(s, dtype=data_type)\n",
      "        else:\n",
      "            result = np.fromstring(s, dtype=data_type)\n",
      "        result = (1.0 * result / 2 ** (8 * self.nbytes - 1)).reshape(\n",
      "            (int(len(result) / self.nchannels), self.nchannels)\n",
      "        )\n",
      "        pad = np.zeros((chunksize - len(result), self.nchannels), dtype=result.dtype)\n",
      "        result = np.concatenate([result, pad])\n",
      "        self.pos = self.pos + chunksize\n",
      "        return result\n",
      "read_chunk(self=<moviepy.audio.io.readers.FFMPEG_AudioReader object at 0x7f8c0a2042e0>, chunksize=200000, self.bitrate=127, self.buffer=None, self.buffer_startframe=1, self.buffersize=200000, self.codec='pcm_s16le', self.duration=9.64, self.filename='media/chaplin.mp4', self.format='s16le', self.fps=44100, self.infos={'video_found': True, 'audio_found': True, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf57.25.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [640, 360], 'bitrate': 495, 'fps': 25.0, 'metadata': {'handler_name': 'VideoHandler'}}, {'input_number': 0, 'stream_number': 1, 'stream_type': 'audio', 'language': None, 'default': True, 'fps': 44100, 'bitrate': 127, 'metadata': {'handler_name': 'SoundHandler'}}], 'input_number': 0}], 'duration': 9.64, 'bitrate': 631, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_size': [640, 360], 'video_bitrate': 495, 'video_fps': 25.0, 'default_audio_input_number': 0, 'default_audio_stream_number': 1, 'audio_fps': 44100, 'audio_bitrate': 127, 'video_n_frames': 241, 'video_duration': 9.64}, self.n_frames=425124, self.nbytes=2, self.nchannels=2, self.pos=0, self.proc=<Popen: returncode: None args: ['/local/rcs/XXX/miniforge3/envs/Zulko+mov...>)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "result = (1.0 * result / 2 ** (8 * self.nbytes - 1)).reshape(\n",
      "State:\n",
      "array([[ 0.00198364, -0.00491333],       [ 0.00384521, -0.00723267],       [ 0.0057373 , -0.00927734],       ...,       [ 0.05221558, -0.04415894],       [ 0.04403687, -0.03729248],       [ 0.03845215, -0.03323364]])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_audioclip_stereo_max_volume(nchannels, channel_muted):\n",
      "    def make_frame(t):\n",
      "        frame = []\n",
      "        for i in range(int(nchannels / 2)):\n",
      "            if channel_muted == \"left\":\n",
      "                frame.append(np.sin(t * 0))\n",
      "                frame.append(np.sin(440 * 2 * np.pi * t))\n",
      "            else:\n",
      "                frame.append(np.sin(440 * 2 * np.pi * t))\n",
      "                frame.append(np.sin(t * 0))\n",
      "        return np.array(frame).T\n",
      "    clip = AudioClip(make_frame, fps=44100, duration=1)\n",
      "    max_volume = clip.max_volume(stereo=True)\n",
      "    assert isinstance(max_volume, np.ndarray)\n",
      "    assert len(max_volume) == nchannels\n",
      "    for i, channel_max_volume in enumerate(max_volume):\n",
      "        if i % 2 == 0:\n",
      "            if channel_muted == \"left\":\n",
      "                assert channel_max_volume == 0\n",
      "            else:\n",
      "                assert channel_max_volume > 0\n",
      "        else:\n",
      "            if channel_muted == \"right\":\n",
      "                assert channel_max_volume == 0\n",
      "            else:\n",
      "                assert channel_max_volume > 0\n",
      "test_audioclip_stereo_max_volume(nchannels=2, channel_muted='left')\n",
      "\n",
      "n:\n",
      "25\n",
      "Statement:\n",
      "assert channel_max_volume == 0\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_clip_with_end(duration, start, end, expected_start, expected_duration):\n",
      "    clip = ColorClip(color=(255, 0, 0), size=(2, 2), duration=duration).with_fps(1)\n",
      "    if start is not None:\n",
      "        clip = clip.with_start(start)\n",
      "    else:\n",
      "        clip.start = None\n",
      "    clip = clip.with_end(end)\n",
      "    assert clip.start == expected_start\n",
      "    assert clip.duration == expected_duration\n",
      "test_clip_with_end(duration=3, start=1, end=2, expected_start=1, expected_duration=1)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "clip = ColorClip(color=(255, 0, 0), size=(2, 2), duration=duration).with_fps(1)\n",
      "State:\n",
      "{start=0, end=3, duration=3, memoize=False, memoized_t=None, memoized_frame=None, mask=None, audio=None, relative_pos=False, layer=0, is_mask=False, has_constant_size=True, size=(2, 2), img=array([[[255,   0,   0],        [255,   0,   0]],       [[255,   0,   0],        [255,   0,   0]]]), fps=1}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_clip_with_end(duration, start, end, expected_start, expected_duration):\n",
      "    clip = ColorClip(color=(255, 0, 0), size=(2, 2), duration=duration).with_fps(1)\n",
      "    if start is not None:\n",
      "        clip = clip.with_start(start)\n",
      "    else:\n",
      "        clip.start = None\n",
      "    clip = clip.with_end(end)\n",
      "    assert clip.start == expected_start\n",
      "    assert clip.duration == expected_duration\n",
      "test_clip_with_end(duration=3, start=1, end=2, expected_start=1, expected_duration=1)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "clip = clip.with_start(start)\n",
      "State:\n",
      "{start=1, end=4, duration=3, memoize=False, memoized_t=None, memoized_frame=None, mask=None, audio=None, relative_pos=False, layer=0, is_mask=False, has_constant_size=True, size=(2, 2), img=array([[[255,   0,   0],        [255,   0,   0]],       [[255,   0,   0],        [255,   0,   0]]]), fps=1}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_clip_copy(copy_func):\n",
      "    clip = Clip()\n",
      "    other_clip = Clip()\n",
      "    for attr in clip.__dict__:\n",
      "        setattr(clip, attr, \"foo\")\n",
      "    copied_clip = copy_func(clip)\n",
      "    for attr in copied_clip.__dict__:\n",
      "        assert getattr(copied_clip, attr) == getattr(clip, attr)\n",
      "        assert getattr(copied_clip, attr) != getattr(other_clip, attr)\n",
      "test_clip_copy(copy_func=<function <lambda> at 0x7f8c0a895700>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "clip = Clip()\n",
      "State:\n",
      "{start=0, end=None, duration=None, memoize=False, memoized_t=None, memoized_frame=None}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_clip_copy(copy_func):\n",
      "    clip = Clip()\n",
      "    other_clip = Clip()\n",
      "    for attr in clip.__dict__:\n",
      "        setattr(clip, attr, \"foo\")\n",
      "    copied_clip = copy_func(clip)\n",
      "    for attr in copied_clip.__dict__:\n",
      "        assert getattr(copied_clip, attr) == getattr(clip, attr)\n",
      "        assert getattr(copied_clip, attr) != getattr(other_clip, attr)\n",
      "test_clip_copy(copy_func=<function <lambda> at 0x7f8c0a895700>)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "other_clip = Clip()\n",
      "State:\n",
      "{start=0, end=None, duration=None, memoize=False, memoized_t=None, memoized_frame=None}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def subprocess_call(cmd, logger=\"bar\"):\n",
      "    logger = proglog.default_bar_logger(logger)\n",
      "    logger(message=\"MoviePy - Running:\\n>>> \" + \" \".join(cmd))\n",
      "    popen_params = cross_platform_popen_params(\n",
      "        {\"stdout\": sp.DEVNULL, \"stderr\": sp.PIPE, \"stdin\": sp.DEVNULL}\n",
      "    )\n",
      "    proc = sp.Popen(cmd, **popen_params)\n",
      "    out, err = proc.communicate()\n",
      "    proc.stderr.close()\n",
      "    if proc.returncode:\n",
      "        logger(message=\"MoviePy - Command returned an error\")\n",
      "        raise IOError(err.decode(\"utf8\"))\n",
      "    else:\n",
      "        logger(message=\"MoviePy - Command successful\")\n",
      "    del proc\n",
      "subprocess_call(cmd=['unset', '-background', 'transparent', '-fill', 'white', '-font', 'Liberation-Mono', '-pointsize', '25', '-size', '640x480', '-gravity', 'center', 'caption:@/tmp/tmp6pdbi5v3.txt', '-type', 'truecolormatte', 'PNG32:/tmp/tmpkd0efulh.png'], logger=None)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "logger = proglog.default_bar_logger(logger)\n",
      "State:\n",
      "{state={'bars': OrderedDict()}, stored={}, logs=[], log_indent=0, ignored_bars=None, logged_bars='all', min_time_interval=0, ignore_bars_under=0}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __del__(self):\n",
      "        self.close()\n",
      "__del__(self=<moviepy.audio.io.readers.FFMPEG_AudioReader object at 0x7f8bd103ec10>, self.bitrate=None, self.buffer=array([[ 0.25909424,  0.11651611],       [ 0.26321411,  0.12841797],       [ 0.25817871,  0.13800049],       ...,       [-0.13201904,  0.03997803],       [-0.14248657,  0.03668213],       [-0.14907837,  0.03515625]]), self.buffer_startframe=0, self.buffersize=44101, self.codec='pcm_s16le', self.duration=1.0, self.filename='media/big_buck_bunny_432_433.webm', self.format='s16le', self.fps=44100, self.infos={'video_found': True, 'audio_found': True, 'metadata': {'encoder': 'Lavf57.25.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1280, 720], 'bitrate': None, 'fps': 24.0}, {'input_number': 0, 'stream_number': 1, 'stream_type': 'audio', 'language': None, 'default': True, 'fps': 44100, 'bitrate': None}], 'input_number': 0}], 'duration': 1.0, 'bitrate': 688, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_size': [1280, 720], 'video_bitrate': None, 'video_fps': 24.0, 'default_audio_input_number': 0, 'default_audio_stream_number': 1, 'audio_fps': 44100, 'audio_bitrate': None, 'video_n_frames': 24, 'video_duration': 1.0}, self.n_frames=44100, self.nbytes=2, self.nchannels=2, self.pos=44101, self.proc=<Popen: returncode: None args: ['/local/rcs/XXX/miniforge3/envs/Zulko+mov...>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.close()\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_save_frame(util, with_mask, t, mask_color, frames):\n",
      "    filename = os.path.join(util.TMP_DIR, \"moviepy_VideoClip_save_frame.png\")\n",
      "    if os.path.isfile(filename):\n",
      "        try:\n",
      "            os.remove(filename)\n",
      "        except PermissionError:\n",
      "            pass\n",
      "    width, height = (len(frames[0][0]), len(frames[0]))\n",
      "    clip = BitmapClip(frames, fps=1)\n",
      "    if with_mask:\n",
      "        mask = ColorClip(color=mask_color, is_mask=True, size=(width, height))\n",
      "        clip = clip.with_mask(mask)\n",
      "    clip.save_frame(filename, t)\n",
      "    t = int(convert_to_seconds(t))\n",
      "    e_r, e_g, e_b = BitmapClip.DEFAULT_COLOR_DICT[frames[t][0][0]]\n",
      "    im = Image.open(filename, mode=\"r\")\n",
      "    assert im.width == width\n",
      "    assert im.height == height\n",
      "    for i in range(im.width):\n",
      "        for j in range(im.height):\n",
      "            rgba = im.getpixel((i, j))\n",
      "            if len(rgba) == 4:\n",
      "                r, g, b, a = rgba\n",
      "            else:\n",
      "                r, g, b = rgba\n",
      "            assert r == e_r\n",
      "            assert g == e_g\n",
      "            assert b == e_b\n",
      "            if with_mask:\n",
      "                assert round(a / 254, 2) == mask_color\n",
      "test_save_frame(util=<class 'tests.conftest.util.<locals>.MoviepyTestUtils'>, with_mask=False, t=0, mask_color=0, frames=[['RR', 'RR'], ['GG', 'GG'], ['BB', 'BB']])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "filename = os.path.join(util.TMP_DIR, \"moviepy_VideoClip_save_frame.png\")\n",
      "State:\n",
      "'/tmp/moviepy_VideoClip_save_frame.png'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_save_frame(util, with_mask, t, mask_color, frames):\n",
      "    filename = os.path.join(util.TMP_DIR, \"moviepy_VideoClip_save_frame.png\")\n",
      "    if os.path.isfile(filename):\n",
      "        try:\n",
      "            os.remove(filename)\n",
      "        except PermissionError:\n",
      "            pass\n",
      "    width, height = (len(frames[0][0]), len(frames[0]))\n",
      "    clip = BitmapClip(frames, fps=1)\n",
      "    if with_mask:\n",
      "        mask = ColorClip(color=mask_color, is_mask=True, size=(width, height))\n",
      "        clip = clip.with_mask(mask)\n",
      "    clip.save_frame(filename, t)\n",
      "    t = int(convert_to_seconds(t))\n",
      "    e_r, e_g, e_b = BitmapClip.DEFAULT_COLOR_DICT[frames[t][0][0]]\n",
      "    im = Image.open(filename, mode=\"r\")\n",
      "    assert im.width == width\n",
      "    assert im.height == height\n",
      "    for i in range(im.width):\n",
      "        for j in range(im.height):\n",
      "            rgba = im.getpixel((i, j))\n",
      "            if len(rgba) == 4:\n",
      "                r, g, b, a = rgba\n",
      "            else:\n",
      "                r, g, b = rgba\n",
      "            assert r == e_r\n",
      "            assert g == e_g\n",
      "            assert b == e_b\n",
      "            if with_mask:\n",
      "                assert round(a / 254, 2) == mask_color\n",
      "test_save_frame(util=<class 'tests.conftest.util.<locals>.MoviepyTestUtils'>, with_mask=False, t=0, mask_color=0, frames=[['RR', 'RR'], ['GG', 'GG'], ['BB', 'BB']])\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "clip = BitmapClip(frames, fps=1)\n",
      "State:\n",
      "{color_dict={'R': (255, 0, 0), 'G': (0, 255, 0), 'B': (0, 0, 255), 'O': (0, 0, 0), 'W': (255, 255, 255), 'A': (89, 225, 62), 'C': (113, 157, 108), 'D': (215, 182, 143), 'E': (57, 26, 252), 'F': (225, 135, 33)}, total_frames=3, start=0, end=3.0, duration=3.0, memoize=False, memoized_t=None, memoized_frame=None, mask=None, audio=None, relative_pos=False, layer=0, size=(2, 2), is_mask=False, has_constant_size=True, fps=1}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_save_frame(util, with_mask, t, mask_color, frames):\n",
      "    filename = os.path.join(util.TMP_DIR, \"moviepy_VideoClip_save_frame.png\")\n",
      "    if os.path.isfile(filename):\n",
      "        try:\n",
      "            os.remove(filename)\n",
      "        except PermissionError:\n",
      "            pass\n",
      "    width, height = (len(frames[0][0]), len(frames[0]))\n",
      "    clip = BitmapClip(frames, fps=1)\n",
      "    if with_mask:\n",
      "        mask = ColorClip(color=mask_color, is_mask=True, size=(width, height))\n",
      "        clip = clip.with_mask(mask)\n",
      "    clip.save_frame(filename, t)\n",
      "    t = int(convert_to_seconds(t))\n",
      "    e_r, e_g, e_b = BitmapClip.DEFAULT_COLOR_DICT[frames[t][0][0]]\n",
      "    im = Image.open(filename, mode=\"r\")\n",
      "    assert im.width == width\n",
      "    assert im.height == height\n",
      "    for i in range(im.width):\n",
      "        for j in range(im.height):\n",
      "            rgba = im.getpixel((i, j))\n",
      "            if len(rgba) == 4:\n",
      "                r, g, b, a = rgba\n",
      "            else:\n",
      "                r, g, b = rgba\n",
      "            assert r == e_r\n",
      "            assert g == e_g\n",
      "            assert b == e_b\n",
      "            if with_mask:\n",
      "                assert round(a / 254, 2) == mask_color\n",
      "test_save_frame(util=<class 'tests.conftest.util.<locals>.MoviepyTestUtils'>, with_mask=False, t=0, mask_color=0, frames=[['RR', 'RR'], ['GG', 'GG'], ['BB', 'BB']])\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "e_r, e_g, e_b = BitmapClip.DEFAULT_COLOR_DICT[frames[t][0][0]]\n",
      "State:\n",
      "255\n",
      "==================================================\n",
      "Clean Code:\n",
      "def filter(get_frame, t):\n",
      "        angle = get_angle(t)\n",
      "        im = get_frame(t)\n",
      "        if unit == \"rad\":\n",
      "            angle = math.degrees(angle)\n",
      "        angle %= 360\n",
      "        if not center and not translate and not bg_color:\n",
      "            if (angle == 0) and expand:\n",
      "                return im\n",
      "            if (angle == 90) and expand:\n",
      "                transpose = [1, 0] if len(im.shape) == 2 else [1, 0, 2]\n",
      "                return np.transpose(im, axes=transpose)[::-1]\n",
      "            elif (angle == 270) and expand:\n",
      "                transpose = [1, 0] if len(im.shape) == 2 else [1, 0, 2]\n",
      "                return np.transpose(im, axes=transpose)[:, ::-1]\n",
      "            elif (angle == 180) and expand:\n",
      "                return im[::-1, ::-1]\n",
      "        if not Image:\n",
      "            raise ValueError(\n",
      "                'Without \"Pillow\" installed, only angles that are a multiple of 90'\n",
      "                \" without centering, translation and background color transformations\"\n",
      "                ' are supported, please install \"Pillow\" with `pip install pillow`'\n",
      "            )\n",
      "        kwargs, _locals = ({}, locals())\n",
      "        for PIL_rotate_kw_name, (\n",
      "            kw_name,\n",
      "            supported,\n",
      "            min_version,\n",
      "        ) in PIL_rotate_kwargs_supported.items():\n",
      "            kw_value = _locals[kw_name]\n",
      "            if supported:\n",
      "                kwargs[PIL_rotate_kw_name] = kw_value\n",
      "            else:\n",
      "                if kw_value is not None:\n",
      "                    warnings.warn(\n",
      "                        f\"rotate '{kw_name}' argument is not supported\"\n",
      "                        \" by your Pillow version and is being ignored. Minimum\"\n",
      "                        \" Pillow version required:\"\n",
      "                        f\" v{'.'.join(str(n) for n in min_version)}\",\n",
      "                        UserWarning,\n",
      "                    )\n",
      "        if im.dtype == \"float64\":\n",
      "            a = 255.0\n",
      "        else:\n",
      "            a = 1\n",
      "        return (\n",
      "            np.array(\n",
      "                Image.fromarray(np.array(a * im).astype(np.uint8)).rotate(\n",
      "                    angle, expand=expand, resample=resample, **kwargs\n",
      "                )\n",
      "            )\n",
      "            / a\n",
      "        )\n",
      "filter(get_frame=<bound method Clip.get_frame of <moviepy.video.VideoClip.BitmapClip object at 0x7f8bc4938070>>, t=0, bg_color=None, center=None, expand=True, get_angle=<function rotate.<locals>.<lambda> at 0x7f8bce85de50>, resample=3, translate=None, unit='deg')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "angle = get_angle(t)\n",
      "State:\n",
      "270\n",
      "==================================================\n",
      "Clean Code:\n",
      "def filter(get_frame, t):\n",
      "        angle = get_angle(t)\n",
      "        im = get_frame(t)\n",
      "        if unit == \"rad\":\n",
      "            angle = math.degrees(angle)\n",
      "        angle %= 360\n",
      "        if not center and not translate and not bg_color:\n",
      "            if (angle == 0) and expand:\n",
      "                return im\n",
      "            if (angle == 90) and expand:\n",
      "                transpose = [1, 0] if len(im.shape) == 2 else [1, 0, 2]\n",
      "                return np.transpose(im, axes=transpose)[::-1]\n",
      "            elif (angle == 270) and expand:\n",
      "                transpose = [1, 0] if len(im.shape) == 2 else [1, 0, 2]\n",
      "                return np.transpose(im, axes=transpose)[:, ::-1]\n",
      "            elif (angle == 180) and expand:\n",
      "                return im[::-1, ::-1]\n",
      "        if not Image:\n",
      "            raise ValueError(\n",
      "                'Without \"Pillow\" installed, only angles that are a multiple of 90'\n",
      "                \" without centering, translation and background color transformations\"\n",
      "                ' are supported, please install \"Pillow\" with `pip install pillow`'\n",
      "            )\n",
      "        kwargs, _locals = ({}, locals())\n",
      "        for PIL_rotate_kw_name, (\n",
      "            kw_name,\n",
      "            supported,\n",
      "            min_version,\n",
      "        ) in PIL_rotate_kwargs_supported.items():\n",
      "            kw_value = _locals[kw_name]\n",
      "            if supported:\n",
      "                kwargs[PIL_rotate_kw_name] = kw_value\n",
      "            else:\n",
      "                if kw_value is not None:\n",
      "                    warnings.warn(\n",
      "                        f\"rotate '{kw_name}' argument is not supported\"\n",
      "                        \" by your Pillow version and is being ignored. Minimum\"\n",
      "                        \" Pillow version required:\"\n",
      "                        f\" v{'.'.join(str(n) for n in min_version)}\",\n",
      "                        UserWarning,\n",
      "                    )\n",
      "        if im.dtype == \"float64\":\n",
      "            a = 255.0\n",
      "        else:\n",
      "            a = 1\n",
      "        return (\n",
      "            np.array(\n",
      "                Image.fromarray(np.array(a * im).astype(np.uint8)).rotate(\n",
      "                    angle, expand=expand, resample=resample, **kwargs\n",
      "                )\n",
      "            )\n",
      "            / a\n",
      "        )\n",
      "filter(get_frame=<bound method Clip.get_frame of <moviepy.video.VideoClip.BitmapClip object at 0x7f8bc4938070>>, t=0, bg_color=None, center=None, expand=True, get_angle=<function rotate.<locals>.<lambda> at 0x7f8bce85de50>, resample=3, translate=None, unit='deg')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "im = get_frame(t)\n",
      "State:\n",
      "array([[[255,   0,   0],        [  0, 255,   0]]])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_write_gif(util, clip_class, opt, loop, with_mask, pixel_format):\n",
      "    filename = os.path.join(util.TMP_DIR, \"moviepy_write_gif.gif\")\n",
      "    if os.path.isfile(filename):\n",
      "        try:\n",
      "            os.remove(filename)\n",
      "        except PermissionError:\n",
      "            pass\n",
      "    fps = 10\n",
      "    if clip_class == \"BitmapClip\":\n",
      "        original_clip = BitmapClip([[\"R\"], [\"G\"], [\"B\"]], fps=fps).with_duration(0.3)\n",
      "    else:\n",
      "        original_clip = concatenate_videoclips(\n",
      "            [\n",
      "                ColorClip(\n",
      "                    (1, 1),\n",
      "                    color=color,\n",
      "                )\n",
      "                .with_duration(0.1)\n",
      "                .with_fps(fps)\n",
      "                for color in [(255, 0, 0), (0, 255, 0), (0, 0, 255)]\n",
      "            ]\n",
      "        )\n",
      "    if with_mask:\n",
      "        original_clip = original_clip.with_mask(\n",
      "            ColorClip((1, 1), color=1, is_mask=True).with_fps(fps).with_duration(0.3)\n",
      "        )\n",
      "    kwargs = {}\n",
      "    if pixel_format is not None:\n",
      "        kwargs[\"pixel_format\"] = pixel_format\n",
      "    write_gif(\n",
      "        original_clip,\n",
      "        filename,\n",
      "        fps=fps,\n",
      "        with_mask=with_mask,\n",
      "        program=\"ffmpeg\",\n",
      "        logger=None,\n",
      "        opt=opt,\n",
      "        loop=loop,\n",
      "        **kwargs,\n",
      "    )\n",
      "    if pixel_format != \"invalid\":\n",
      "        final_clip = VideoFileClip(filename)\n",
      "        r, g, b = final_clip.get_frame(0)[0][0]\n",
      "        assert r == 252\n",
      "        assert g == 0\n",
      "        assert b == 0\n",
      "        r, g, b = final_clip.get_frame(0.1)[0][0]\n",
      "        assert r == 0\n",
      "        assert g == 252\n",
      "        assert b == 0\n",
      "        r, g, b = final_clip.get_frame(0.2)[0][0]\n",
      "        assert r == 0\n",
      "        assert g == 0\n",
      "        assert b == 255\n",
      "        assert final_clip.duration == (loop or 1) * round(original_clip.duration, 6)\n",
      "test_write_gif(util=<class 'tests.conftest.util.<locals>.MoviepyTestUtils'>, clip_class='BitmapClip', opt=False, loop=None, with_mask=False, pixel_format='invalid')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "filename = os.path.join(util.TMP_DIR, \"moviepy_write_gif.gif\")\n",
      "State:\n",
      "'/tmp/moviepy_write_gif.gif'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_freeze(t, freeze_duration, total_duration, padding_end, output_frames):\n",
      "    input_frames = [\"R\", \"G\", \"B\"]\n",
      "    clip_duration = len(input_frames)\n",
      "    clip = BitmapClip([list(color) for color in input_frames], fps=1).with_duration(\n",
      "        clip_duration\n",
      "    )\n",
      "    possible_kwargs = {\n",
      "        \"t\": t,\n",
      "        \"freeze_duration\": freeze_duration,\n",
      "        \"total_duration\": total_duration,\n",
      "        \"padding_end\": padding_end,\n",
      "    }\n",
      "    kwargs = {\n",
      "        kw_name: kw_value\n",
      "        for kw_name, kw_value in possible_kwargs.items()\n",
      "        if kw_value is not None\n",
      "    }\n",
      "    if hasattr(output_frames, \"__traceback__\"):\n",
      "        with pytest.raises(output_frames):\n",
      "            freeze(clip, **kwargs)\n",
      "        return\n",
      "    else:\n",
      "        freezed_clip = freeze(clip, **kwargs)\n",
      "    expected_freeze_duration = (\n",
      "        freeze_duration\n",
      "        if freeze_duration is not None\n",
      "        else total_duration - clip_duration\n",
      "    )\n",
      "    assert freezed_clip.duration == clip_duration + expected_freeze_duration\n",
      "    for i, color in enumerate(freezed_clip.iter_frames()):\n",
      "        expected_color = list(BitmapClip.DEFAULT_COLOR_DICT[output_frames[i]])\n",
      "        assert list(color[0][0]) == expected_color\n",
      "test_freeze(t=None, freeze_duration=1, total_duration=None, padding_end=None, output_frames=['R', 'R', 'G', 'B'])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "input_frames = [\"R\", \"G\", \"B\"]\n",
      "State:\n",
      "['R', 'G', 'B']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_freeze(t, freeze_duration, total_duration, padding_end, output_frames):\n",
      "    input_frames = [\"R\", \"G\", \"B\"]\n",
      "    clip_duration = len(input_frames)\n",
      "    clip = BitmapClip([list(color) for color in input_frames], fps=1).with_duration(\n",
      "        clip_duration\n",
      "    )\n",
      "    possible_kwargs = {\n",
      "        \"t\": t,\n",
      "        \"freeze_duration\": freeze_duration,\n",
      "        \"total_duration\": total_duration,\n",
      "        \"padding_end\": padding_end,\n",
      "    }\n",
      "    kwargs = {\n",
      "        kw_name: kw_value\n",
      "        for kw_name, kw_value in possible_kwargs.items()\n",
      "        if kw_value is not None\n",
      "    }\n",
      "    if hasattr(output_frames, \"__traceback__\"):\n",
      "        with pytest.raises(output_frames):\n",
      "            freeze(clip, **kwargs)\n",
      "        return\n",
      "    else:\n",
      "        freezed_clip = freeze(clip, **kwargs)\n",
      "    expected_freeze_duration = (\n",
      "        freeze_duration\n",
      "        if freeze_duration is not None\n",
      "        else total_duration - clip_duration\n",
      "    )\n",
      "    assert freezed_clip.duration == clip_duration + expected_freeze_duration\n",
      "    for i, color in enumerate(freezed_clip.iter_frames()):\n",
      "        expected_color = list(BitmapClip.DEFAULT_COLOR_DICT[output_frames[i]])\n",
      "        assert list(color[0][0]) == expected_color\n",
      "test_freeze(t=None, freeze_duration=1, total_duration=None, padding_end=None, output_frames=['R', 'R', 'G', 'B'])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "clip_duration = len(input_frames)\n",
      "State:\n",
      "3\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_freeze(t, freeze_duration, total_duration, padding_end, output_frames):\n",
      "    input_frames = [\"R\", \"G\", \"B\"]\n",
      "    clip_duration = len(input_frames)\n",
      "    clip = BitmapClip([list(color) for color in input_frames], fps=1).with_duration(\n",
      "        clip_duration\n",
      "    )\n",
      "    possible_kwargs = {\n",
      "        \"t\": t,\n",
      "        \"freeze_duration\": freeze_duration,\n",
      "        \"total_duration\": total_duration,\n",
      "        \"padding_end\": padding_end,\n",
      "    }\n",
      "    kwargs = {\n",
      "        kw_name: kw_value\n",
      "        for kw_name, kw_value in possible_kwargs.items()\n",
      "        if kw_value is not None\n",
      "    }\n",
      "    if hasattr(output_frames, \"__traceback__\"):\n",
      "        with pytest.raises(output_frames):\n",
      "            freeze(clip, **kwargs)\n",
      "        return\n",
      "    else:\n",
      "        freezed_clip = freeze(clip, **kwargs)\n",
      "    expected_freeze_duration = (\n",
      "        freeze_duration\n",
      "        if freeze_duration is not None\n",
      "        else total_duration - clip_duration\n",
      "    )\n",
      "    assert freezed_clip.duration == clip_duration + expected_freeze_duration\n",
      "    for i, color in enumerate(freezed_clip.iter_frames()):\n",
      "        expected_color = list(BitmapClip.DEFAULT_COLOR_DICT[output_frames[i]])\n",
      "        assert list(color[0][0]) == expected_color\n",
      "test_freeze(t=None, freeze_duration=1, total_duration=None, padding_end=None, output_frames=['R', 'R', 'G', 'B'])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "clip = BitmapClip([list(color) for color in input_frames], fps=1).with_duration(\n",
      "State:\n",
      "{color_dict={'R': (255, 0, 0), 'G': (0, 255, 0), 'B': (0, 0, 255), 'O': (0, 0, 0), 'W': (255, 255, 255), 'A': (89, 225, 62), 'C': (113, 157, 108), 'D': (215, 182, 143), 'E': (57, 26, 252), 'F': (225, 135, 33)}, total_frames=3, start=0, end=3, duration=3, memoize=False, memoized_t=None, memoized_frame=None, mask=None, audio=None, relative_pos=False, layer=0, size=(1, 1), is_mask=False, has_constant_size=True, fps=1}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_freeze(t, freeze_duration, total_duration, padding_end, output_frames):\n",
      "    input_frames = [\"R\", \"G\", \"B\"]\n",
      "    clip_duration = len(input_frames)\n",
      "    clip = BitmapClip([list(color) for color in input_frames], fps=1).with_duration(\n",
      "        clip_duration\n",
      "    )\n",
      "    possible_kwargs = {\n",
      "        \"t\": t,\n",
      "        \"freeze_duration\": freeze_duration,\n",
      "        \"total_duration\": total_duration,\n",
      "        \"padding_end\": padding_end,\n",
      "    }\n",
      "    kwargs = {\n",
      "        kw_name: kw_value\n",
      "        for kw_name, kw_value in possible_kwargs.items()\n",
      "        if kw_value is not None\n",
      "    }\n",
      "    if hasattr(output_frames, \"__traceback__\"):\n",
      "        with pytest.raises(output_frames):\n",
      "            freeze(clip, **kwargs)\n",
      "        return\n",
      "    else:\n",
      "        freezed_clip = freeze(clip, **kwargs)\n",
      "    expected_freeze_duration = (\n",
      "        freeze_duration\n",
      "        if freeze_duration is not None\n",
      "        else total_duration - clip_duration\n",
      "    )\n",
      "    assert freezed_clip.duration == clip_duration + expected_freeze_duration\n",
      "    for i, color in enumerate(freezed_clip.iter_frames()):\n",
      "        expected_color = list(BitmapClip.DEFAULT_COLOR_DICT[output_frames[i]])\n",
      "        assert list(color[0][0]) == expected_color\n",
      "test_freeze(t=None, freeze_duration=1, total_duration=None, padding_end=None, output_frames=['R', 'R', 'G', 'B'])\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "possible_kwargs = {\n",
      "State:\n",
      "{'t': None, 'freeze_duration': 1, 'total_duration': None, 'padding_end': None}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_margin(ClipClass, margin_size, margins, color, expected_result):\n",
      "    if ClipClass is BitmapClip:\n",
      "        clip = BitmapClip([[\"RRR\", \"RRR\"], [\"RRR\", \"RRR\"]], fps=1)\n",
      "    else:\n",
      "        clip = ColorClip(color=(255, 0, 0), size=(3, 2), duration=2).with_fps(1)\n",
      "    if color is None:\n",
      "        color = (0, 0, 0)\n",
      "    if margins is None:\n",
      "        margins = [0, 0, 0, 0]\n",
      "    left, right, top, bottom = margins\n",
      "    new_clip = margin(\n",
      "        clip,\n",
      "        margin_size=margin_size,\n",
      "        left=left,\n",
      "        right=right,\n",
      "        top=top,\n",
      "        bottom=bottom,\n",
      "        color=color,\n",
      "    )\n",
      "    assert new_clip == BitmapClip(expected_result, fps=1)\n",
      "test_margin(ClipClass=<class 'moviepy.video.VideoClip.ColorClip'>, margin_size=None, margins=None, color=None, expected_result=[['RRR', 'RRR'], ['RRR', 'RRR']])\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "clip = ColorClip(color=(255, 0, 0), size=(3, 2), duration=2).with_fps(1)\n",
      "State:\n",
      "{start=0, end=2, duration=2, memoize=False, memoized_t=None, memoized_frame=None, mask=None, audio=None, relative_pos=False, layer=0, is_mask=False, has_constant_size=True, size=(3, 2), img=array([[[255,   0,   0],        [255,   0,   0],        [255,   0,   0]],       [[255,   0,   0],        [255,   0,   0],        [255,   0,   0]]]), fps=1}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_mask_and(image_from, duration, color, mask_color, expected_color):\n",
      "    clip_size = tuple(random.randint(3, 10) for i in range(2))\n",
      "    if duration == \"random\":\n",
      "        duration = round(random.uniform(0, 0.5), 2)\n",
      "    clip = ColorClip(color=color, size=clip_size).with_duration(duration)\n",
      "    mask_clip = ColorClip(color=mask_color, size=clip.size)\n",
      "    masked_clip = mask_and(\n",
      "        clip, mask_clip if image_from == \"ImageClip\" else mask_clip.get_frame(0)\n",
      "    )\n",
      "    assert masked_clip.duration == clip.duration\n",
      "    assert np.array_equal(masked_clip.get_frame(0)[0][0], np.array(expected_color))\n",
      "    color_frame, mask_color_frame = (np.array([[color]]), np.array([[mask_color]]))\n",
      "    clip = VideoClip(lambda t: color_frame).with_duration(duration)\n",
      "    mask_clip = VideoClip(lambda t: mask_color_frame).with_duration(duration)\n",
      "    masked_clip = mask_and(clip, mask_clip)\n",
      "    assert np.array_equal(masked_clip.get_frame(0)[0][0], np.array(expected_color))\n",
      "test_mask_and(image_from='np.ndarray', duration=None, color=(0, 0, 0), mask_color=(255, 255, 255), expected_color=(0, 0, 0))\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "masked_clip = mask_and(\n",
      "State:\n",
      "{start=0, end=None, duration=None, memoize=False, memoized_t=None, memoized_frame=None, mask=None, audio=None, relative_pos=False, layer=0, is_mask=False, has_constant_size=True, size=(8, 3), img=array([[[0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0]],       [[0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0]],       [[0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0],        [0, 0, 0]]])}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_mask_and(image_from, duration, color, mask_color, expected_color):\n",
      "    clip_size = tuple(random.randint(3, 10) for i in range(2))\n",
      "    if duration == \"random\":\n",
      "        duration = round(random.uniform(0, 0.5), 2)\n",
      "    clip = ColorClip(color=color, size=clip_size).with_duration(duration)\n",
      "    mask_clip = ColorClip(color=mask_color, size=clip.size)\n",
      "    masked_clip = mask_and(\n",
      "        clip, mask_clip if image_from == \"ImageClip\" else mask_clip.get_frame(0)\n",
      "    )\n",
      "    assert masked_clip.duration == clip.duration\n",
      "    assert np.array_equal(masked_clip.get_frame(0)[0][0], np.array(expected_color))\n",
      "    color_frame, mask_color_frame = (np.array([[color]]), np.array([[mask_color]]))\n",
      "    clip = VideoClip(lambda t: color_frame).with_duration(duration)\n",
      "    mask_clip = VideoClip(lambda t: mask_color_frame).with_duration(duration)\n",
      "    masked_clip = mask_and(clip, mask_clip)\n",
      "    assert np.array_equal(masked_clip.get_frame(0)[0][0], np.array(expected_color))\n",
      "test_mask_and(image_from='np.ndarray', duration=None, color=(0, 0, 0), mask_color=(255, 255, 255), expected_color=(0, 0, 0))\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "assert np.array_equal(masked_clip.get_frame(0)[0][0], np.array(expected_color))\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_reporting_config() -> Dict[str, Any]:\n",
      "    reporting_config: Dict[str, Any] = {\"consent\": False}\n",
      "    try:\n",
      "        if not os.path.exists(REPORTING_CONFIG_FILE_PATH):\n",
      "            client_id = str(uuid.uuid4())\n",
      "            reporting_config[\"client_id\"] = client_id\n",
      "            reporting_config = save_reporting_config(True, client_id)\n",
      "        else:\n",
      "            with open(REPORTING_CONFIG_FILE_PATH, \"r\") as ifp:\n",
      "                reporting_config = json.load(ifp)\n",
      "        reporting_config[\"machine_id\"] = reporting_config[\"client_id\"]\n",
      "        if (\n",
      "            reporting_config.get(\"username\") is not None\n",
      "            and reporting_config[\"client_id\"] != reporting_config[\"username\"]\n",
      "        ):\n",
      "            reporting_config[\"client_id\"] = reporting_config[\"username\"]\n",
      "    except Exception:\n",
      "        pass\n",
      "    return reporting_config\n",
      "get_reporting_config()\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "reporting_config = json.load(ifp)\n",
      "State:\n",
      "{'client_id': '8728ad25-a15b-4b67-91db-a3a3878aba24', 'consent': True}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_reporting_config() -> Dict[str, Any]:\n",
      "    reporting_config: Dict[str, Any] = {\"consent\": False}\n",
      "    try:\n",
      "        if not os.path.exists(REPORTING_CONFIG_FILE_PATH):\n",
      "            client_id = str(uuid.uuid4())\n",
      "            reporting_config[\"client_id\"] = client_id\n",
      "            reporting_config = save_reporting_config(True, client_id)\n",
      "        else:\n",
      "            with open(REPORTING_CONFIG_FILE_PATH, \"r\") as ifp:\n",
      "                reporting_config = json.load(ifp)\n",
      "        reporting_config[\"machine_id\"] = reporting_config[\"client_id\"]\n",
      "        if (\n",
      "            reporting_config.get(\"username\") is not None\n",
      "            and reporting_config[\"client_id\"] != reporting_config[\"username\"]\n",
      "        ):\n",
      "            reporting_config[\"client_id\"] = reporting_config[\"username\"]\n",
      "    except Exception:\n",
      "        pass\n",
      "    return reporting_config\n",
      "get_reporting_config()\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "reporting_config[\"machine_id\"] = reporting_config[\"client_id\"]\n",
      "State:\n",
      "{'client_id': '8728ad25-a15b-4b67-91db-a3a3878aba24', 'consent': True, 'machine_id': '8728ad25-a15b-4b67-91db-a3a3878aba24'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def version_compare(v1, v2):\n",
      "    arr1 = v1.split(\".\")\n",
      "    arr2 = v2.split(\".\")\n",
      "    n = len(arr1)\n",
      "    m = len(arr2)\n",
      "    arr1 = [int(i) for i in arr1]\n",
      "    arr2 = [int(i) for i in arr2]\n",
      "    if n > m:\n",
      "        for i in range(m, n):\n",
      "            arr2.append(0)\n",
      "    elif m > n:\n",
      "        for i in range(n, m):\n",
      "            arr1.append(0)\n",
      "    for i in range(len(arr1)):\n",
      "        if arr1[i] > arr2[i]:\n",
      "            return 1\n",
      "        elif arr2[i] > arr1[i]:\n",
      "            return -1\n",
      "    return 0\n",
      "version_compare(v1='3.8.18', v2='3.8.27')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "arr1 = v1.split(\".\")\n",
      "State:\n",
      "['3', '8', '18']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def version_compare(v1, v2):\n",
      "    arr1 = v1.split(\".\")\n",
      "    arr2 = v2.split(\".\")\n",
      "    n = len(arr1)\n",
      "    m = len(arr2)\n",
      "    arr1 = [int(i) for i in arr1]\n",
      "    arr2 = [int(i) for i in arr2]\n",
      "    if n > m:\n",
      "        for i in range(m, n):\n",
      "            arr2.append(0)\n",
      "    elif m > n:\n",
      "        for i in range(n, m):\n",
      "            arr1.append(0)\n",
      "    for i in range(len(arr1)):\n",
      "        if arr1[i] > arr2[i]:\n",
      "            return 1\n",
      "        elif arr2[i] > arr1[i]:\n",
      "            return -1\n",
      "    return 0\n",
      "version_compare(v1='3.8.18', v2='3.8.27')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "arr2 = v2.split(\".\")\n",
      "State:\n",
      "['3', '8', '27']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse_access_method(access_method: str):\n",
      "    num_workers = 0\n",
      "    scheduler = \"threaded\"\n",
      "    download = access_method.startswith(\"download\")\n",
      "    local = access_method.startswith(\"local\")\n",
      "    if download or local:\n",
      "        split = access_method.split(\":\")\n",
      "        if len(split) == 1:\n",
      "            split.extend((\"threaded\", \"0\"))\n",
      "        elif len(split) == 2:\n",
      "            split.append(\"threaded\" if split[1].isnumeric() else \"0\")\n",
      "        elif len(split) >= 3:\n",
      "            num_integers = sum(1 for i in split if i.isnumeric())\n",
      "            if num_integers != 1 or len(split) > 3:\n",
      "                raise ValueError(\n",
      "                    \"Invalid access_method format. Expected format is one of the following: {download, download:scheduler, download:num_workers, download:scheduler:num_workers, download:num_workers:scheduler}\"\n",
      "                )\n",
      "        access_method = \"download\" if download else \"local\"\n",
      "        num_worker_index = 1 if split[1].isnumeric() else 2\n",
      "        scheduler_index = 3 - num_worker_index\n",
      "        num_workers = int(split[num_worker_index])\n",
      "        scheduler = split[scheduler_index]\n",
      "    return access_method, num_workers, scheduler\n",
      "parse_access_method(access_method='download')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "num_workers = 0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse_access_method(access_method: str):\n",
      "    num_workers = 0\n",
      "    scheduler = \"threaded\"\n",
      "    download = access_method.startswith(\"download\")\n",
      "    local = access_method.startswith(\"local\")\n",
      "    if download or local:\n",
      "        split = access_method.split(\":\")\n",
      "        if len(split) == 1:\n",
      "            split.extend((\"threaded\", \"0\"))\n",
      "        elif len(split) == 2:\n",
      "            split.append(\"threaded\" if split[1].isnumeric() else \"0\")\n",
      "        elif len(split) >= 3:\n",
      "            num_integers = sum(1 for i in split if i.isnumeric())\n",
      "            if num_integers != 1 or len(split) > 3:\n",
      "                raise ValueError(\n",
      "                    \"Invalid access_method format. Expected format is one of the following: {download, download:scheduler, download:num_workers, download:scheduler:num_workers, download:num_workers:scheduler}\"\n",
      "                )\n",
      "        access_method = \"download\" if download else \"local\"\n",
      "        num_worker_index = 1 if split[1].isnumeric() else 2\n",
      "        scheduler_index = 3 - num_worker_index\n",
      "        num_workers = int(split[num_worker_index])\n",
      "        scheduler = split[scheduler_index]\n",
      "    return access_method, num_workers, scheduler\n",
      "parse_access_method(access_method='download')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "scheduler = \"threaded\"\n",
      "State:\n",
      "'threaded'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse_access_method(access_method: str):\n",
      "    num_workers = 0\n",
      "    scheduler = \"threaded\"\n",
      "    download = access_method.startswith(\"download\")\n",
      "    local = access_method.startswith(\"local\")\n",
      "    if download or local:\n",
      "        split = access_method.split(\":\")\n",
      "        if len(split) == 1:\n",
      "            split.extend((\"threaded\", \"0\"))\n",
      "        elif len(split) == 2:\n",
      "            split.append(\"threaded\" if split[1].isnumeric() else \"0\")\n",
      "        elif len(split) >= 3:\n",
      "            num_integers = sum(1 for i in split if i.isnumeric())\n",
      "            if num_integers != 1 or len(split) > 3:\n",
      "                raise ValueError(\n",
      "                    \"Invalid access_method format. Expected format is one of the following: {download, download:scheduler, download:num_workers, download:scheduler:num_workers, download:num_workers:scheduler}\"\n",
      "                )\n",
      "        access_method = \"download\" if download else \"local\"\n",
      "        num_worker_index = 1 if split[1].isnumeric() else 2\n",
      "        scheduler_index = 3 - num_worker_index\n",
      "        num_workers = int(split[num_worker_index])\n",
      "        scheduler = split[scheduler_index]\n",
      "    return access_method, num_workers, scheduler\n",
      "parse_access_method(access_method='download')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "download = access_method.startswith(\"download\")\n",
      "State:\n",
      "True\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse_access_method(access_method: str):\n",
      "    num_workers = 0\n",
      "    scheduler = \"threaded\"\n",
      "    download = access_method.startswith(\"download\")\n",
      "    local = access_method.startswith(\"local\")\n",
      "    if download or local:\n",
      "        split = access_method.split(\":\")\n",
      "        if len(split) == 1:\n",
      "            split.extend((\"threaded\", \"0\"))\n",
      "        elif len(split) == 2:\n",
      "            split.append(\"threaded\" if split[1].isnumeric() else \"0\")\n",
      "        elif len(split) >= 3:\n",
      "            num_integers = sum(1 for i in split if i.isnumeric())\n",
      "            if num_integers != 1 or len(split) > 3:\n",
      "                raise ValueError(\n",
      "                    \"Invalid access_method format. Expected format is one of the following: {download, download:scheduler, download:num_workers, download:scheduler:num_workers, download:num_workers:scheduler}\"\n",
      "                )\n",
      "        access_method = \"download\" if download else \"local\"\n",
      "        num_worker_index = 1 if split[1].isnumeric() else 2\n",
      "        scheduler_index = 3 - num_worker_index\n",
      "        num_workers = int(split[num_worker_index])\n",
      "        scheduler = split[scheduler_index]\n",
      "    return access_method, num_workers, scheduler\n",
      "parse_access_method(access_method='download')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "local = access_method.startswith(\"local\")\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse_access_method(access_method: str):\n",
      "    num_workers = 0\n",
      "    scheduler = \"threaded\"\n",
      "    download = access_method.startswith(\"download\")\n",
      "    local = access_method.startswith(\"local\")\n",
      "    if download or local:\n",
      "        split = access_method.split(\":\")\n",
      "        if len(split) == 1:\n",
      "            split.extend((\"threaded\", \"0\"))\n",
      "        elif len(split) == 2:\n",
      "            split.append(\"threaded\" if split[1].isnumeric() else \"0\")\n",
      "        elif len(split) >= 3:\n",
      "            num_integers = sum(1 for i in split if i.isnumeric())\n",
      "            if num_integers != 1 or len(split) > 3:\n",
      "                raise ValueError(\n",
      "                    \"Invalid access_method format. Expected format is one of the following: {download, download:scheduler, download:num_workers, download:scheduler:num_workers, download:num_workers:scheduler}\"\n",
      "                )\n",
      "        access_method = \"download\" if download else \"local\"\n",
      "        num_worker_index = 1 if split[1].isnumeric() else 2\n",
      "        scheduler_index = 3 - num_worker_index\n",
      "        num_workers = int(split[num_worker_index])\n",
      "        scheduler = split[scheduler_index]\n",
      "    return access_method, num_workers, scheduler\n",
      "parse_access_method(access_method='download')\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "scheduler_index = 3 - num_worker_index\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __getstate__(self) -> Dict[str, Any]:\n",
      "        d = super().__getstate__()\n",
      "        d[\"tensors\"] = self.tensors.copy()\n",
      "        d[\"groups\"] = self.groups.copy()\n",
      "        d[\"tensor_names\"] = self.tensor_names.copy()\n",
      "        d[\"hidden_tensors\"] = self.hidden_tensors.copy()\n",
      "        d[\"default_index\"] = self.default_index.copy()\n",
      "        return d\n",
      "__getstate__(self=<deeplake.core.meta.dataset_meta.DatasetMeta object at 0x7f6b79b0dd90>, __class__=<class 'deeplake.core.meta.dataset_meta.DatasetMeta'>, self.default_index=[{'start': None, 'stop': None, 'step': None}], self.groups=[], self.hidden_tensors=[], self.is_dirty=False, self.tensor_names={}, self.tensors=[], self.version='3.8.18')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "d = super().__getstate__()\n",
      "State:\n",
      "{'version': '3.8.18'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __getstate__(self) -> Dict[str, Any]:\n",
      "        d = super().__getstate__()\n",
      "        d[\"tensors\"] = self.tensors.copy()\n",
      "        d[\"groups\"] = self.groups.copy()\n",
      "        d[\"tensor_names\"] = self.tensor_names.copy()\n",
      "        d[\"hidden_tensors\"] = self.hidden_tensors.copy()\n",
      "        d[\"default_index\"] = self.default_index.copy()\n",
      "        return d\n",
      "__getstate__(self=<deeplake.core.meta.dataset_meta.DatasetMeta object at 0x7f6b79b0dd90>, __class__=<class 'deeplake.core.meta.dataset_meta.DatasetMeta'>, self.default_index=[{'start': None, 'stop': None, 'step': None}], self.groups=[], self.hidden_tensors=[], self.is_dirty=False, self.tensor_names={}, self.tensors=[], self.version='3.8.18')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "d[\"tensors\"] = self.tensors.copy()\n",
      "State:\n",
      "{'version': '3.8.18', 'tensors': []}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __getstate__(self) -> Dict[str, Any]:\n",
      "        d = super().__getstate__()\n",
      "        d[\"tensors\"] = self.tensors.copy()\n",
      "        d[\"groups\"] = self.groups.copy()\n",
      "        d[\"tensor_names\"] = self.tensor_names.copy()\n",
      "        d[\"hidden_tensors\"] = self.hidden_tensors.copy()\n",
      "        d[\"default_index\"] = self.default_index.copy()\n",
      "        return d\n",
      "__getstate__(self=<deeplake.core.meta.dataset_meta.DatasetMeta object at 0x7f6b79b0dd90>, __class__=<class 'deeplake.core.meta.dataset_meta.DatasetMeta'>, self.default_index=[{'start': None, 'stop': None, 'step': None}], self.groups=[], self.hidden_tensors=[], self.is_dirty=False, self.tensor_names={}, self.tensors=[], self.version='3.8.18')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "d[\"groups\"] = self.groups.copy()\n",
      "State:\n",
      "{'version': '3.8.18', 'tensors': [], 'groups': []}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __getstate__(self) -> Dict[str, Any]:\n",
      "        d = super().__getstate__()\n",
      "        d[\"tensors\"] = self.tensors.copy()\n",
      "        d[\"groups\"] = self.groups.copy()\n",
      "        d[\"tensor_names\"] = self.tensor_names.copy()\n",
      "        d[\"hidden_tensors\"] = self.hidden_tensors.copy()\n",
      "        d[\"default_index\"] = self.default_index.copy()\n",
      "        return d\n",
      "__getstate__(self=<deeplake.core.meta.dataset_meta.DatasetMeta object at 0x7f6b79b0dd90>, __class__=<class 'deeplake.core.meta.dataset_meta.DatasetMeta'>, self.default_index=[{'start': None, 'stop': None, 'step': None}], self.groups=[], self.hidden_tensors=[], self.is_dirty=False, self.tensor_names={}, self.tensors=[], self.version='3.8.18')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "d[\"tensor_names\"] = self.tensor_names.copy()\n",
      "State:\n",
      "{'version': '3.8.18', 'tensors': [], 'groups': [], 'tensor_names': {}}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __getstate__(self) -> Dict[str, Any]:\n",
      "        d = super().__getstate__()\n",
      "        d[\"tensors\"] = self.tensors.copy()\n",
      "        d[\"groups\"] = self.groups.copy()\n",
      "        d[\"tensor_names\"] = self.tensor_names.copy()\n",
      "        d[\"hidden_tensors\"] = self.hidden_tensors.copy()\n",
      "        d[\"default_index\"] = self.default_index.copy()\n",
      "        return d\n",
      "__getstate__(self=<deeplake.core.meta.dataset_meta.DatasetMeta object at 0x7f6b79b0dd90>, __class__=<class 'deeplake.core.meta.dataset_meta.DatasetMeta'>, self.default_index=[{'start': None, 'stop': None, 'step': None}], self.groups=[], self.hidden_tensors=[], self.is_dirty=False, self.tensor_names={}, self.tensors=[], self.version='3.8.18')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "d[\"hidden_tensors\"] = self.hidden_tensors.copy()\n",
      "State:\n",
      "{'version': '3.8.18', 'tensors': [], 'groups': [], 'tensor_names': {}, 'hidden_tensors': []}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _validate_htype_overwrites(htype: str, htype_overwrite: dict):\n",
      "    defaults = HTYPE_CONFIGURATIONS[htype]\n",
      "    for key, value in htype_overwrite.items():\n",
      "        if key not in defaults:\n",
      "            raise TensorMetaInvalidHtypeOverwriteKey(htype, key, list(defaults.keys()))\n",
      "        if isinstance(value, str) and value == UNSPECIFIED:\n",
      "            if defaults[key] == REQUIRE_USER_SPECIFICATION:\n",
      "                raise TensorMetaMissingRequiredValue(htype, key)\n",
      "    sc = htype_overwrite[\"sample_compression\"]\n",
      "    cc = htype_overwrite[\"chunk_compression\"]\n",
      "    compr = sc if cc in (None, UNSPECIFIED) else cc\n",
      "    actual_htype = f\"link[{htype}]\" if htype_overwrite[\"is_link\"] else htype\n",
      "    if htype.startswith(\"image\") and sc == UNSPECIFIED and cc == UNSPECIFIED:\n",
      "        raise TensorMetaMissingRequiredValue(\n",
      "            actual_htype, [\"chunk_compression\", \"sample_compression\"]\n",
      "        )\n",
      "    if htype in (\"audio\", \"video\", \"point_cloud\", \"mesh\", \"nifti\"):\n",
      "        if cc not in (UNSPECIFIED, None):\n",
      "            raise UnsupportedCompressionError(\"Chunk compression\", htype=htype)\n",
      "        elif sc == UNSPECIFIED:\n",
      "            raise TensorMetaMissingRequiredValue(\n",
      "                actual_htype, \"sample_compression\"\n",
      "            )\n",
      "    supported_compressions = HTYPE_SUPPORTED_COMPRESSIONS.get(htype)\n",
      "    if (\n",
      "        compr\n",
      "        and compr != UNSPECIFIED\n",
      "        and supported_compressions\n",
      "        and compr not in supported_compressions\n",
      "    ):\n",
      "        raise UnsupportedCompressionError(compr, htype=htype)\n",
      "_validate_htype_overwrites(htype='generic', htype_overwrite={'sample_compression': 'unspecified', 'chunk_compression': 'unspecified', 'dtype': 'unspecified', 'hidden': False, 'tiling_threshold': None, 'max_chunk_size': 2000000, 'is_sequence': False, 'is_link': False, 'verify': True})\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "compr = sc if cc in (None, UNSPECIFIED) else cc\n",
      "State:\n",
      "'unspecified'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _validate_links(links: dict):\n",
      "    if not isinstance(links, dict):\n",
      "        raise InvalidTensorLinkError()\n",
      "    allowed_keys = (\"extend\", \"update\", \"flatten_sequence\")\n",
      "    for out_tensor, args in links.items():\n",
      "        if not isinstance(out_tensor, str):\n",
      "            raise InvalidTensorLinkError()\n",
      "        if not isinstance(args, dict):\n",
      "            raise InvalidTensorLinkError()\n",
      "        if \"extend\" not in args:\n",
      "            raise InvalidTensorLinkError(\n",
      "                f\"extend transform not specified for link {out_tensor}\"\n",
      "            )\n",
      "        if \"flatten_sequence\" not in args:\n",
      "            raise InvalidTensorLinkError(\n",
      "                f\"flatten_sequence arg not specified for link {out_tensor}\"\n",
      "            )\n",
      "        try:\n",
      "            get_link_transform(args[\"extend\"])\n",
      "        except KeyError:\n",
      "            raise InvalidTensorLinkError(f\"Invalid extend transform: {args['extend']}\")\n",
      "        if \"update\" in args:\n",
      "            try:\n",
      "                get_link_transform(args[\"update\"])\n",
      "            except KeyError:\n",
      "                raise InvalidTensorLinkError(\n",
      "                    f\"Invalid update transform: {args['extend']}\"\n",
      "                )\n",
      "        for k in args:\n",
      "            if k not in allowed_keys:\n",
      "                raise InvalidTensorLinkError(f\"Invalid key in link meta: {k}\")\n",
      "_validate_links(links={})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "allowed_keys = (\"extend\", \"update\", \"flatten_sequence\")\n",
      "State:\n",
      "('extend', 'update', 'flatten_sequence')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add_tensor(self, name, key, hidden=False):\n",
      "        if key not in self.tensors:\n",
      "            self.tensor_names[name] = key\n",
      "            self.tensors.append(key)\n",
      "            if hidden:\n",
      "                self.hidden_tensors.append(key)\n",
      "            self.is_dirty = True\n",
      "add_tensor(self=<deeplake.core.meta.dataset_meta.DatasetMeta object at 0x7f6b79b0dd90>, name='image', key='image', hidden=False, self.default_index=[{'start': None, 'stop': None, 'step': None}], self.groups=[], self.hidden_tensors=[], self.is_dirty=False, self.tensor_names={}, self.tensors=[], self.version='3.8.18')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.tensor_names[name] = key\n",
      "State:\n",
      "{'image': 'image'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def serialize_chunkids(version: str, arr: np.ndarray) -> memoryview:\n",
      "    len_version = len(version)\n",
      "    write_dtype = version_compare(version, \"2.7.6\") >= 0\n",
      "    flatbuff = bytearray(1 + int(write_dtype) + len_version + arr.nbytes)\n",
      "    len_version = len(version)\n",
      "    flatbuff[0] = len_version\n",
      "    flatbuff[1 : 1 + len_version] = version.encode(\"ascii\")\n",
      "    offset = 1 + len_version\n",
      "    if write_dtype:\n",
      "        dtype = arr.dtype\n",
      "        num_bytes = int(dtype.itemsize)\n",
      "        flatbuff[offset] = num_bytes\n",
      "        offset += 1\n",
      "    flatbuff[offset : offset + arr.nbytes] = arr.tobytes()\n",
      "    offset += arr.nbytes\n",
      "    return memoryview(flatbuff)\n",
      "serialize_chunkids(version='3.8.18', arr=array([], shape=(0, 2), dtype=uint64))\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "len_version = len(version)\n",
      "State:\n",
      "6\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _create_new_chunk(self, register=True, row: Optional[int] = None) -> BaseChunk:\n",
      "        chunk_id = self.chunk_id_encoder.generate_chunk_id(register=register, row=row)\n",
      "        chunk = self.chunk_class(*self.chunk_args)\n",
      "        chunk_name = ChunkIdEncoder.name_from_id(chunk_id)\n",
      "        chunk_key = get_chunk_key(self.key, chunk_name, self.commit_id)\n",
      "        if self.commit_chunk_map is not None:\n",
      "            self.commit_chunk_map.add(chunk_name)\n",
      "        chunk.key = chunk_key\n",
      "        chunk.id = chunk_id\n",
      "        chunk._update_tensor_meta_length = register\n",
      "        if self.active_appended_chunk is not None:\n",
      "            self.write_chunk_to_storage(self.active_appended_chunk)\n",
      "        self.active_appended_chunk = chunk\n",
      "        return chunk\n",
      "_create_new_chunk(self=<deeplake.core.chunk_engine.ChunkEngine object at 0x7f6b79b0d340>, register=False, row=None, self._active_appended_chunk=None, self._active_updated_chunk=None, self._all_chunk_engines=None, self._chunk_args=None, self._chunk_compression=None, self._chunk_id_encoder=<deeplake.core.meta.encode.chunk_id.ChunkIdEncoder object at 0x7f6b79b0d490>, self._chunk_id_encoder_commit_id='firstdbf9474d461a19e9333c2fd19b46115348f', self._commit_chunk_map=None, self._commit_chunk_map_commit_id=None, self._commit_diff=None, self._commit_diff_commit_id=None, self._hash_label_map=OrderedDict(), self._info=None, self._info_commit_id=None, self._is_temp_label_tensor=False, self._meta_cache=None, self._num_samples_per_chunk=None, self._numpy_extend_optimization_enabled=True, self._pad_encoder=None, self._pad_encoder_commit_id=None, self._sample_compression=None, self._sequence_encoder=None, self._sequence_encoder_commit_id=None, self._tensor_meta=<deeplake.core.meta.tensor_meta.TensorMeta object at 0x7f6b79b0dca0>, self._tensor_meta_commit_id='firstdbf9474d461a19e9333c2fd19b46115348f', self._tile_encoder=None, self._tile_encoder_commit_id=None, self.base_storage=<deeplake.core.storage.memory.MemoryProvider object at 0x7f6b79af7c10>, self.cache=<deeplake.core.storage.lru_cache.LRUCache object at 0x7f6b79af7fa0>, self.cache_enabled=True, self.cache_range=range(0, 0), self.cached_data=None, self.chunk_class=<class 'deeplake.core.chunk.uncompressed_chunk.UncompressedChunk'>, self.compression=None, self.key='image', self.link_creds=None, self.name='image', self.start_chunk=None, self.version_state={'branch': 'main', 'branch_commit_map': {'main': 'firstdbf9474d461a19e9333c2fd19b46115348f'}, 'commit_node_map': {'firstdbf9474d461a19e9333c2fd19b46115348f': Commit : firstdbf9474d461a19e9333c2fd19b46115348f (main) Author : NoneTime   : Message: None}, 'commit_id': 'firstdbf9474d461a19e9333c2fd19b46115348f', 'commit_node': Commit : firstdbf9474d461a19e9333c2fd19b46115348f (main) Author : NoneTime   : Message: None, 'full_tensors': {'image': Tensor(key='image'), '_image_shape': Tensor(key='_image_shape'), '_image_id': Tensor(key='_image_id')}, 'tensor_names': {'image': 'image', '_image_shape': '_image_shape', '_image_id': '_image_id'}, 'meta': <deeplake.core.meta.dataset_meta.DatasetMeta object at 0x7f6b79b0dd90>}, self.write_initialization_done=True)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "chunk_id = self.chunk_id_encoder.generate_chunk_id(register=register, row=row)\n",
      "State:\n",
      "4422882830357057438\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _create_new_chunk(self, register=True, row: Optional[int] = None) -> BaseChunk:\n",
      "        chunk_id = self.chunk_id_encoder.generate_chunk_id(register=register, row=row)\n",
      "        chunk = self.chunk_class(*self.chunk_args)\n",
      "        chunk_name = ChunkIdEncoder.name_from_id(chunk_id)\n",
      "        chunk_key = get_chunk_key(self.key, chunk_name, self.commit_id)\n",
      "        if self.commit_chunk_map is not None:\n",
      "            self.commit_chunk_map.add(chunk_name)\n",
      "        chunk.key = chunk_key\n",
      "        chunk.id = chunk_id\n",
      "        chunk._update_tensor_meta_length = register\n",
      "        if self.active_appended_chunk is not None:\n",
      "            self.write_chunk_to_storage(self.active_appended_chunk)\n",
      "        self.active_appended_chunk = chunk\n",
      "        return chunk\n",
      "_create_new_chunk(self=<deeplake.core.chunk_engine.ChunkEngine object at 0x7f6b79b0d340>, register=False, row=None, self._active_appended_chunk=None, self._active_updated_chunk=None, self._all_chunk_engines=None, self._chunk_args=None, self._chunk_compression=None, self._chunk_id_encoder=<deeplake.core.meta.encode.chunk_id.ChunkIdEncoder object at 0x7f6b79b0d490>, self._chunk_id_encoder_commit_id='firstdbf9474d461a19e9333c2fd19b46115348f', self._commit_chunk_map=None, self._commit_chunk_map_commit_id=None, self._commit_diff=None, self._commit_diff_commit_id=None, self._hash_label_map=OrderedDict(), self._info=None, self._info_commit_id=None, self._is_temp_label_tensor=False, self._meta_cache=None, self._num_samples_per_chunk=None, self._numpy_extend_optimization_enabled=True, self._pad_encoder=None, self._pad_encoder_commit_id=None, self._sample_compression=None, self._sequence_encoder=None, self._sequence_encoder_commit_id=None, self._tensor_meta=<deeplake.core.meta.tensor_meta.TensorMeta object at 0x7f6b79b0dca0>, self._tensor_meta_commit_id='firstdbf9474d461a19e9333c2fd19b46115348f', self._tile_encoder=None, self._tile_encoder_commit_id=None, self.base_storage=<deeplake.core.storage.memory.MemoryProvider object at 0x7f6b79af7c10>, self.cache=<deeplake.core.storage.lru_cache.LRUCache object at 0x7f6b79af7fa0>, self.cache_enabled=True, self.cache_range=range(0, 0), self.cached_data=None, self.chunk_class=<class 'deeplake.core.chunk.uncompressed_chunk.UncompressedChunk'>, self.compression=None, self.key='image', self.link_creds=None, self.name='image', self.start_chunk=None, self.version_state={'branch': 'main', 'branch_commit_map': {'main': 'firstdbf9474d461a19e9333c2fd19b46115348f'}, 'commit_node_map': {'firstdbf9474d461a19e9333c2fd19b46115348f': Commit : firstdbf9474d461a19e9333c2fd19b46115348f (main) Author : NoneTime   : Message: None}, 'commit_id': 'firstdbf9474d461a19e9333c2fd19b46115348f', 'commit_node': Commit : firstdbf9474d461a19e9333c2fd19b46115348f (main) Author : NoneTime   : Message: None, 'full_tensors': {'image': Tensor(key='image'), '_image_shape': Tensor(key='_image_shape'), '_image_id': Tensor(key='_image_id')}, 'tensor_names': {'image': 'image', '_image_shape': '_image_shape', '_image_id': '_image_id'}, 'meta': <deeplake.core.meta.dataset_meta.DatasetMeta object at 0x7f6b79b0dd90>}, self.write_initialization_done=True)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "chunk_name = ChunkIdEncoder.name_from_id(chunk_id)  # type: ignore\n",
      "State:\n",
      "'3d613c80329b4f9e'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def update_shape_interval(self, shape: Sequence[int]):\n",
      "        initial_min_shape = None if self.min_shape is None else self.min_shape.copy()\n",
      "        initial_max_shape = None if self.max_shape is None else self.max_shape.copy()\n",
      "        if not self.min_shape:\n",
      "            self.min_shape = list(shape)\n",
      "            self.max_shape = list(shape)\n",
      "        else:\n",
      "            expected_dims = len(self.min_shape)\n",
      "            if len(shape) != expected_dims:\n",
      "                raise TensorInvalidSampleShapeError(shape, len(self.min_shape))\n",
      "            for i, dim in enumerate(shape):\n",
      "                self.min_shape[i] = min(dim, self.min_shape[i])\n",
      "                self.max_shape[i] = max(dim, self.max_shape[i])\n",
      "        if initial_min_shape != self.min_shape or initial_max_shape != self.max_shape:\n",
      "            self.is_dirty = True\n",
      "update_shape_interval(self=<deeplake.core.meta.tensor_meta.TensorMeta object at 0x7f6b79b0dca0>, shape=(1024, 1024), self._required_meta_keys=('htype', 'min_shape', 'max_shape', 'length', 'hidden', 'dtype', 'sample_compression', 'chunk_compression', 'typestr', 'max_chunk_size', 'tiling_threshold', 'is_sequence', 'is_link', 'links', 'verify'), self.chunk_compression=None, self.dtype='float64', self.hidden=False, self.htype='generic', self.is_dirty=True, self.is_link=False, self.is_sequence=False, self.length=0, self.links={'_image_shape': {'extend': 'extend_shape', 'flatten_sequence': True, 'update': 'update_shape'}, '_image_id': {'extend': 'extend_id', 'flatten_sequence': False}}, self.max_chunk_size=2000000, self.max_shape=[], self.min_shape=[], self.name='image', self.sample_compression=None, self.tiling_threshold=None, self.typestr='<f8', self.verify=True, self.version='3.8.18')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "initial_min_shape = None if self.min_shape is None else self.min_shape.copy()\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_new_player_rack_list(self, num_players):\n",
      "        player_rack_list = []\n",
      "        for _ in range(num_players):\n",
      "            this_rack = []\n",
      "            for _ in range(config.PLAYER_RACK_SIZE):\n",
      "                this_tile = self._draw_random_tile()\n",
      "                this_rack.append(this_tile)\n",
      "            player_rack_list.append(this_rack)\n",
      "        return player_rack_list\n",
      "_get_new_player_rack_list(self=REPR FAILED, num_players=4, self.tile_bag=[*, *, A, A, A, A, A, A, A, A, A, B, B, C, C, D, D, D, D, E, E, E, E, E, E, E, E, E, E, E, E, F, F, G, G, G, H, H, I, I, I, I, I, I, I, I, I, J, K, L, L, L, L, M, M, N, N, N, N, N, N, O, O, O, O, O, O, O, O, P, P, Q, R, R, R, R, R, R, S, S, S, S, T, T, T, T, T, T, U, U, U, U, V, V, W, W, X, Y, Y, Z])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "player_rack_list = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _draw_random_tile(self):\n",
      "        random_index = random.randrange(0, len(self.tile_bag))\n",
      "        selected_tile = self.tile_bag.pop(random_index)\n",
      "        return selected_tile\n",
      "_draw_random_tile(self=REPR FAILED, self.tile_bag=[*, *, A, A, A, A, A, A, A, A, A, B, B, C, C, D, D, D, D, E, E, E, E, E, E, E, E, E, E, E, E, F, F, G, G, G, H, H, I, I, I, I, I, I, I, I, I, J, K, L, L, L, L, M, M, N, N, N, N, N, N, O, O, O, O, O, O, O, O, P, P, Q, R, R, R, R, R, R, S, S, S, S, T, T, T, T, T, T, U, U, U, U, V, V, W, W, X, Y, Y, Z])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "random_index = random.randrange(0, len(self.tile_bag))\n",
      "State:\n",
      "89\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _draw_random_tile(self):\n",
      "        random_index = random.randrange(0, len(self.tile_bag))\n",
      "        selected_tile = self.tile_bag.pop(random_index)\n",
      "        return selected_tile\n",
      "_draw_random_tile(self=REPR FAILED, self.tile_bag=[*, *, A, A, A, A, A, A, A, A, A, B, B, C, C, D, D, D, D, E, E, E, E, E, E, E, E, E, E, E, E, F, F, G, G, G, H, H, I, I, I, I, I, I, I, I, I, J, K, L, L, L, L, M, M, N, N, N, N, N, N, O, O, O, O, O, O, O, O, P, P, Q, R, R, R, R, R, R, S, S, S, S, T, T, T, T, T, T, U, U, U, U, V, V, W, W, X, Y, Y, Z])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "selected_tile = self.tile_bag.pop(random_index)\n",
      "State:\n",
      "U\n",
      "==================================================\n",
      "Clean Code:\n",
      "def initialize_new_board_square_dict():\n",
      "    initial_board_square_dict = {}\n",
      "    for column in config.LETTER_CODE_DICT:\n",
      "        for row in range(1, config.BOARD_NUM_ROWS + 1):\n",
      "            location = (column, row)\n",
      "            word_multiplier = config.WORD_SCORE_MULT_LOCATION_DICT.get(\n",
      "                location,\n",
      "                1\n",
      "            )\n",
      "            letter_multiplier = config.LETTER_SCORE_MULT_LOCATION_DICT.get(\n",
      "                location,\n",
      "                1\n",
      "            )\n",
      "            initial_board_square_dict[location] = BoardSquare(\n",
      "                tile=None,\n",
      "                word_multiplier=word_multiplier,\n",
      "                letter_multiplier=letter_multiplier\n",
      "            )\n",
      "    return initial_board_square_dict\n",
      "initialize_new_board_square_dict()\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "initial_board_square_dict = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def move_is_sublist(letter_list_1, letter_list_2):\n",
      "    letter_counter_1 = collections.Counter(letter_list_1)\n",
      "    letter_counter_2 = collections.Counter(letter_list_2)\n",
      "    for letter, cardinality in letter_counter_1.items():\n",
      "        if cardinality > letter_counter_2[letter]:\n",
      "            return False\n",
      "    return True\n",
      "move_is_sublist(letter_list_1=[1, 2, 3], letter_list_2=[1, 2, 3, 4])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "letter_counter_1 = collections.Counter(letter_list_1)\n",
      "State:\n",
      "Counter({1: 1, 2: 1, 3: 1})\n",
      "==================================================\n",
      "Clean Code:\n",
      "def move_is_sublist(letter_list_1, letter_list_2):\n",
      "    letter_counter_1 = collections.Counter(letter_list_1)\n",
      "    letter_counter_2 = collections.Counter(letter_list_2)\n",
      "    for letter, cardinality in letter_counter_1.items():\n",
      "        if cardinality > letter_counter_2[letter]:\n",
      "            return False\n",
      "    return True\n",
      "move_is_sublist(letter_list_1=[1, 2, 3], letter_list_2=[1, 2, 3, 4])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "letter_counter_2 = collections.Counter(letter_list_2)\n",
      "State:\n",
      "Counter({1: 1, 2: 1, 3: 1, 4: 1})\n",
      "==================================================\n",
      "Clean Code:\n",
      "def cheat_create_rack_word(self, word, player_id):\n",
      "        player_rack = self.player_rack_list[player_id]\n",
      "        for character in word:\n",
      "            tile = scrabble_board.ScrabbleTile(letter=character)\n",
      "            player_rack.append(tile)\n",
      "cheat_create_rack_word(self=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 ______________9 _______________10_______________11_______________12_______________13_______________14_______________15_______________[[Q, E, O, S, G, H, E], [R, Z, L, A, U, L, A], [W, C, E, T, N, E, E], [T, R, O, B, T, N, N]]Moves played: 0Player 1's move72 tiles remain in bagPlayer 1: 0Player 2: 0Player 3: 0Player 4: 0, word='BAKER', player_id=0, self.board=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 ______________9 _______________10_______________11_______________12_______________13_______________14_______________15_______________, self.move_number=0, self.player_rack_list=[[Q, E, O, S, G, H, E], [R, Z, L, A, U, L, A], [W, C, E, T, N, E, E], [T, R, O, B, T, N, N]], self.player_score_list_list=[[], [], [], []], self.tile_bag=[*, *, A, A, A, A, A, A, A, B, C, D, D, D, D, E, E, E, E, E, E, E, F, F, G, G, H, I, I, I, I, I, I, I, I, I, J, K, L, L, M, M, N, N, N, O, O, O, O, O, O, P, P, R, R, R, R, S, S, S, T, T, T, U, U, U, V, V, W, X, Y, Y])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "player_rack = self.player_rack_list[player_id]\n",
      "State:\n",
      "[Q, E, O, S, G, H, E]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def place_word(self, word, start_location, is_vertical_move):\n",
      "        letter_location_set = get_word_letter_location_set(word,\n",
      "                                                           start_location,\n",
      "                                                           is_vertical_move)\n",
      "        return self.next_player_move(letter_location_set)\n",
      "place_word(self=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 ______________9 _______________10_______________11_______________12_______________13_______________14_______________15_______________[[Q, E, O, S, G, H, E, B, A, K, E, R], [R, Z, L, A, U, L, A], [W, C, E, T, N, E, E], [T, R, O, B, T, N, N]]Moves played: 0Player 1's move72 tiles remain in bagPlayer 1: 0Player 2: 0Player 3: 0Player 4: 0, word='BAKER', start_location=('h', 8), is_vertical_move=False, self.board=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 ______________9 _______________10_______________11_______________12_______________13_______________14_______________15_______________, self.move_number=0, self.player_rack_list=[[Q, E, O, S, G, H, E, B, A, K, E, R], [R, Z, L, A, U, L, A], [W, C, E, T, N, E, E], [T, R, O, B, T, N, N]], self.player_score_list_list=[[], [], [], []], self.tile_bag=[*, *, A, A, A, A, A, A, A, B, C, D, D, D, D, E, E, E, E, E, E, E, F, F, G, G, H, I, I, I, I, I, I, I, I, I, J, K, L, L, M, M, N, N, N, O, O, O, O, O, O, P, P, R, R, R, R, S, S, S, T, T, T, U, U, U, V, V, W, X, Y, Y])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "letter_location_set = get_word_letter_location_set(word,\n",
      "State:\n",
      "{('K', ('j', 8)), ('B', ('h', 8)), ('A', ('i', 8)), ('E', ('k', 8)), ('R', ('l', 8))}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def place_word(self, word, start_location, is_vertical_move):\n",
      "        letter_location_set = get_word_letter_location_set(word,\n",
      "                                                           start_location,\n",
      "                                                           is_vertical_move)\n",
      "        return self.next_player_move(letter_location_set)\n",
      "place_word(self=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 ______________9 _______________10_______________11_______________12_______________13_______________14_______________15_______________[[Q, E, O, S, G, H, E, B, A, K, E, R], [R, Z, L, A, U, L, A], [W, C, E, T, N, E, E], [T, R, O, B, T, N, N]]Moves played: 0Player 1's move72 tiles remain in bagPlayer 1: 0Player 2: 0Player 3: 0Player 4: 0, word='BAKER', start_location=('h', 8), is_vertical_move=False, self.board=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 ______________9 _______________10_______________11_______________12_______________13_______________14_______________15_______________, self.move_number=0, self.player_rack_list=[[Q, E, O, S, G, H, E, B, A, K, E, R], [R, Z, L, A, U, L, A], [W, C, E, T, N, E, E], [T, R, O, B, T, N, N]], self.player_score_list_list=[[], [], [], []], self.tile_bag=[*, *, A, A, A, A, A, A, A, B, C, D, D, D, D, E, E, E, E, E, E, E, F, F, G, G, H, I, I, I, I, I, I, I, I, I, J, K, L, L, M, M, N, N, N, O, O, O, O, O, O, P, P, R, R, R, R, S, S, S, T, T, T, U, U, U, V, V, W, X, Y, Y])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "return self.next_player_move(letter_location_set)\n",
      "State:\n",
      "abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 _______BAKER___9 _______________10_______________11_______________12_______________13_______________14_______________15_______________[[Q, O, S, G, H, E, E], [R, Z, L, A, U, L, A], [W, C, E, T, N, E, E], [T, R, O, B, T, N, N]]Moves played: 1Player 2's move72 tiles remain in bagPlayer 1: 24Player 2: 0Player 3: 0Player 4: 0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_word_letter_location_set(word, start_location, is_vertical_move):\n",
      "    letter_location_set = set()\n",
      "    next_location_func = get_next_location_function(\n",
      "        use_positive_seek=True,\n",
      "        use_vertical_words=is_vertical_move\n",
      "    )\n",
      "    current_location = start_location\n",
      "    word_iterator = iter(word)\n",
      "    for character in word_iterator:\n",
      "        if character == '(':\n",
      "            character = next(word_iterator, None)\n",
      "            while character != ')':\n",
      "                current_location = next_location_func(current_location)\n",
      "                character = next(word_iterator, None)\n",
      "            character = next(word_iterator, None)\n",
      "        if character:\n",
      "            letter_location_set.add((character, current_location))\n",
      "            current_location = next_location_func(current_location)\n",
      "    return letter_location_set\n",
      "get_word_letter_location_set(word='BAKER', start_location=('h', 8), is_vertical_move=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "letter_location_set = set()\n",
      "State:\n",
      "set()\n",
      "==================================================\n",
      "Clean Code:\n",
      "def next_player_move(self, letter_location_set):\n",
      "        player_to_move_id, player_rack = get_current_player_data(\n",
      "            self.move_number,\n",
      "            self.player_rack_list\n",
      "        )\n",
      "        is_legal_move = move_is_legal(self.board,\n",
      "                                      self.move_number,\n",
      "                                      letter_location_set,\n",
      "                                      player_rack)\n",
      "        if is_legal_move:\n",
      "            if move_successfully_challenged():\n",
      "                letter_location_set = set()\n",
      "            for move_letter, board_location in letter_location_set:\n",
      "                tile_index = get_rack_tile_index(player_rack, move_letter)\n",
      "                tile_obj = player_rack.pop(tile_index)\n",
      "                self.board[board_location] = tile_obj\n",
      "            move_score = score_move(letter_location_set, self.board)\n",
      "            self.player_score_list_list[player_to_move_id].append(move_score)\n",
      "            self._refill_player_rack(player_rack)\n",
      "            self._cancel_bonus_squares(letter_location_set)\n",
      "            if len(player_rack) == 0 and len(self.tile_bag) == 0:\n",
      "                last_move_score_list = score_end_of_game(self.player_rack_list,\n",
      "                                                         player_to_move_id)\n",
      "                for i, last_move_score in enumerate(last_move_score_list):\n",
      "                    self.player_score_list_list[i].append(last_move_score)\n",
      "                conclude_game(self.player_score_list_list)\n",
      "            self.move_number += 1\n",
      "            return True\n",
      "        else:\n",
      "            return False\n",
      "next_player_move(self=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 ______________9 _______________10_______________11_______________12_______________13_______________14_______________15_______________[[Q, E, O, S, G, H, E, B, A, K, E, R], [R, Z, L, A, U, L, A], [W, C, E, T, N, E, E], [T, R, O, B, T, N, N]]Moves played: 0Player 1's move72 tiles remain in bagPlayer 1: 0Player 2: 0Player 3: 0Player 4: 0, letter_location_set={('K', ('j', 8)), ('B', ('h', 8)), ('A', ('i', 8)), ('E', ('k', 8)), ('R', ('l', 8))}, self.board=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 ______________9 _______________10_______________11_______________12_______________13_______________14_______________15_______________, self.move_number=0, self.player_rack_list=[[Q, E, O, S, G, H, E, B, A, K, E, R], [R, Z, L, A, U, L, A], [W, C, E, T, N, E, E], [T, R, O, B, T, N, N]], self.player_score_list_list=[[], [], [], []], self.tile_bag=[*, *, A, A, A, A, A, A, A, B, C, D, D, D, D, E, E, E, E, E, E, E, F, F, G, G, H, I, I, I, I, I, I, I, I, I, J, K, L, L, M, M, N, N, N, O, O, O, O, O, O, P, P, R, R, R, R, S, S, S, T, T, T, U, U, U, V, V, W, X, Y, Y])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "player_to_move_id, player_rack = get_current_player_data(\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def move_is_legal(board, move_number, letter_location_set, player_rack=None):\n",
      "    letter_list = [letter for letter, _ in letter_location_set]\n",
      "    location_set = set(location for _, location in letter_location_set)\n",
      "    return_bool = (\n",
      "        move_is_rack_size_or_less(location_set) and\n",
      "        move_does_not_misalign_tiles(location_set) and\n",
      "        move_is_not_out_of_bounds(location_set) and\n",
      "        all_move_tiles_connected(board, location_set) and\n",
      "        move_does_not_stack_tiles(letter_list, location_set) and\n",
      "        move_does_not_cover_tiles(board, location_set) and\n",
      "        move_touches_tile(move_number, board, location_set)\n",
      "    )\n",
      "    if player_rack:\n",
      "        player_rack_letter_list = [tile.letter for tile in player_rack]\n",
      "        return_bool = return_bool and move_is_sublist(letter_list,\n",
      "                                                      player_rack_letter_list)\n",
      "    return return_bool\n",
      "move_is_legal(board=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 ______________9 _______________10_______________11_______________12_______________13_______________14_______________15_______________, move_number=0, letter_location_set={('K', ('j', 8)), ('B', ('h', 8)), ('A', ('i', 8)), ('E', ('k', 8)), ('R', ('l', 8))}, player_rack=[Q, E, O, S, G, H, E, B, A, K, E, R])\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "return_bool = (\n",
      "State:\n",
      "True\n",
      "==================================================\n",
      "Clean Code:\n",
      "def move_is_legal(board, move_number, letter_location_set, player_rack=None):\n",
      "    letter_list = [letter for letter, _ in letter_location_set]\n",
      "    location_set = set(location for _, location in letter_location_set)\n",
      "    return_bool = (\n",
      "        move_is_rack_size_or_less(location_set) and\n",
      "        move_does_not_misalign_tiles(location_set) and\n",
      "        move_is_not_out_of_bounds(location_set) and\n",
      "        all_move_tiles_connected(board, location_set) and\n",
      "        move_does_not_stack_tiles(letter_list, location_set) and\n",
      "        move_does_not_cover_tiles(board, location_set) and\n",
      "        move_touches_tile(move_number, board, location_set)\n",
      "    )\n",
      "    if player_rack:\n",
      "        player_rack_letter_list = [tile.letter for tile in player_rack]\n",
      "        return_bool = return_bool and move_is_sublist(letter_list,\n",
      "                                                      player_rack_letter_list)\n",
      "    return return_bool\n",
      "move_is_legal(board=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 ______________9 _______________10_______________11_______________12_______________13_______________14_______________15_______________, move_number=0, letter_location_set={('K', ('j', 8)), ('B', ('h', 8)), ('A', ('i', 8)), ('E', ('k', 8)), ('R', ('l', 8))}, player_rack=[Q, E, O, S, G, H, E, B, A, K, E, R])\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "player_rack_letter_list = [tile.letter for tile in player_rack]\n",
      "State:\n",
      "['Q', 'E', 'O', 'S', 'G', 'H', 'E', 'B', 'A', 'K', 'E', 'R']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def all_move_tiles_connected(board, location_set):\n",
      "    column_list = [column for column, _ in location_set]\n",
      "    row_list = [row for _, row in location_set]\n",
      "    move_is_vertical = (len(set(column_list)) == 1)\n",
      "    if move_is_vertical:\n",
      "        this_column = column_list[0]\n",
      "        for this_row in range(min(row_list), max(row_list) + 1):\n",
      "            this_tile = board[(this_column, this_row)]\n",
      "            if not (this_tile or (this_column, this_row) in location_set):\n",
      "                return False\n",
      "    else:\n",
      "        column_range = range(\n",
      "            config.LETTER_CODE_DICT[min(column_list)],\n",
      "            config.LETTER_CODE_DICT[max(column_list)] + 1\n",
      "        )\n",
      "        this_row = row_list[0]\n",
      "        for this_column_num in column_range:\n",
      "            this_column = chr(this_column_num)\n",
      "            this_tile = board[(this_column, this_row)]\n",
      "            if not (this_tile or (this_column, this_row) in location_set):\n",
      "                return False\n",
      "    return True\n",
      "all_move_tiles_connected(board=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 ______________9 _______________10_______________11_______________12_______________13_______________14_______________15_______________, location_set={('j', 8), ('l', 8), ('h', 8), ('i', 8), ('k', 8)})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "move_is_vertical = (len(set(column_list)) == 1)\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def all_move_tiles_connected(board, location_set):\n",
      "    column_list = [column for column, _ in location_set]\n",
      "    row_list = [row for _, row in location_set]\n",
      "    move_is_vertical = (len(set(column_list)) == 1)\n",
      "    if move_is_vertical:\n",
      "        this_column = column_list[0]\n",
      "        for this_row in range(min(row_list), max(row_list) + 1):\n",
      "            this_tile = board[(this_column, this_row)]\n",
      "            if not (this_tile or (this_column, this_row) in location_set):\n",
      "                return False\n",
      "    else:\n",
      "        column_range = range(\n",
      "            config.LETTER_CODE_DICT[min(column_list)],\n",
      "            config.LETTER_CODE_DICT[max(column_list)] + 1\n",
      "        )\n",
      "        this_row = row_list[0]\n",
      "        for this_column_num in column_range:\n",
      "            this_column = chr(this_column_num)\n",
      "            this_tile = board[(this_column, this_row)]\n",
      "            if not (this_tile or (this_column, this_row) in location_set):\n",
      "                return False\n",
      "    return True\n",
      "all_move_tiles_connected(board=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 ______________9 _______________10_______________11_______________12_______________13_______________14_______________15_______________, location_set={('j', 8), ('l', 8), ('h', 8), ('i', 8), ('k', 8)})\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "column_range = range(\n",
      "State:\n",
      "range(104, 109)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def all_move_tiles_connected(board, location_set):\n",
      "    column_list = [column for column, _ in location_set]\n",
      "    row_list = [row for _, row in location_set]\n",
      "    move_is_vertical = (len(set(column_list)) == 1)\n",
      "    if move_is_vertical:\n",
      "        this_column = column_list[0]\n",
      "        for this_row in range(min(row_list), max(row_list) + 1):\n",
      "            this_tile = board[(this_column, this_row)]\n",
      "            if not (this_tile or (this_column, this_row) in location_set):\n",
      "                return False\n",
      "    else:\n",
      "        column_range = range(\n",
      "            config.LETTER_CODE_DICT[min(column_list)],\n",
      "            config.LETTER_CODE_DICT[max(column_list)] + 1\n",
      "        )\n",
      "        this_row = row_list[0]\n",
      "        for this_column_num in column_range:\n",
      "            this_column = chr(this_column_num)\n",
      "            this_tile = board[(this_column, this_row)]\n",
      "            if not (this_tile or (this_column, this_row) in location_set):\n",
      "                return False\n",
      "    return True\n",
      "all_move_tiles_connected(board=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 ______________9 _______________10_______________11_______________12_______________13_______________14_______________15_______________, location_set={('j', 8), ('l', 8), ('h', 8), ('i', 8), ('k', 8)})\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "this_row = row_list[0]\n",
      "State:\n",
      "8\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_word_location_set(board, initial_location, use_vertical_words):\n",
      "    word_location_set = set()\n",
      "    for use_positive_seek in [True, False]:\n",
      "        current_location = initial_location\n",
      "        current_tile = board[current_location]\n",
      "        next_location_func = get_next_location_function(use_positive_seek,\n",
      "                                                        use_vertical_words)\n",
      "        while current_tile:\n",
      "            word_location_set.add(current_location)\n",
      "            current_location = next_location_func(current_location)\n",
      "            if location_is_out_of_bounds(current_location):\n",
      "                current_tile = None\n",
      "            else:\n",
      "                current_tile = board[current_location]\n",
      "    if len(word_location_set) > 1:\n",
      "        return frozenset(word_location_set)\n",
      "    else:\n",
      "        return frozenset()\n",
      "get_word_location_set(board=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 _______BAKER___9 _______________10_______________11_______________12_______________13_______________14_______________15_______________, initial_location=('j', 8), use_vertical_words=True)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "word_location_set = set()\n",
      "State:\n",
      "set()\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_word_set_total_score(board, word_set, num_move_locations):\n",
      "    total_score = 0\n",
      "    word_score = 0\n",
      "    for word_location_set in word_set:\n",
      "        word_score = 0\n",
      "        word_multiplier = 1\n",
      "        for location in word_location_set:\n",
      "            square = board.board_square_dict[location]\n",
      "            word_multiplier *= square.word_multiplier\n",
      "            word_score += square.tile.point_value * square.letter_multiplier\n",
      "        word_score *= word_multiplier\n",
      "        total_score += word_score\n",
      "    if num_move_locations == config.PLAYER_RACK_SIZE:\n",
      "        total_score += config.BINGO_SCORE\n",
      "    return total_score\n",
      "get_word_set_total_score(board=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 _______BAKER___9 _______________10_______________11_______________12_______________13_______________14_______________15_______________, word_set={frozenset(), frozenset({('h', 8), ('j', 8), ('l', 8), ('i', 8), ('k', 8)})}, num_move_locations=5)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "total_score = 0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_word_set_total_score(board, word_set, num_move_locations):\n",
      "    total_score = 0\n",
      "    word_score = 0\n",
      "    for word_location_set in word_set:\n",
      "        word_score = 0\n",
      "        word_multiplier = 1\n",
      "        for location in word_location_set:\n",
      "            square = board.board_square_dict[location]\n",
      "            word_multiplier *= square.word_multiplier\n",
      "            word_score += square.tile.point_value * square.letter_multiplier\n",
      "        word_score *= word_multiplier\n",
      "        total_score += word_score\n",
      "    if num_move_locations == config.PLAYER_RACK_SIZE:\n",
      "        total_score += config.BINGO_SCORE\n",
      "    return total_score\n",
      "get_word_set_total_score(board=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 _______BAKER___9 _______________10_______________11_______________12_______________13_______________14_______________15_______________, word_set={frozenset(), frozenset({('h', 8), ('j', 8), ('l', 8), ('i', 8), ('k', 8)})}, num_move_locations=5)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "word_score = 0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def location_touches_tile(board, location):\n",
      "    adjacent_location_set = get_adjacent_location_set(location)\n",
      "    for adjacent_location in adjacent_location_set:\n",
      "        if board[adjacent_location]:\n",
      "            return True\n",
      "    return False\n",
      "location_touches_tile(board=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 _______SCRAB___9 _______________10_______________11_______________12_______________13_______________14_______________15_______________, location=('i', 13))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "adjacent_location_set = get_adjacent_location_set(location)\n",
      "State:\n",
      "{('j', 13), ('h', 13), ('i', 14), ('i', 12)}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def exchange(self, letter_list):\n",
      "        if (len(self.tile_bag) < config.PLAYER_RACK_SIZE or\n",
      "                len(letter_list) > config.PLAYER_RACK_SIZE):\n",
      "            return False\n",
      "        else:\n",
      "            _, player_rack = get_current_player_data(self.move_number,\n",
      "                                                     self.player_rack_list)\n",
      "            player_letter_list = [tile.letter for tile in player_rack]\n",
      "            if move_is_sublist(letter_list, player_letter_list):\n",
      "                self._perform_bag_exchange(letter_list, player_rack)\n",
      "                self.move_number += 1\n",
      "                return True\n",
      "            else:\n",
      "                return False\n",
      "exchange(self=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 ______________9 _______________10_______________11_______________12_______________13_______________14_______________15_______________[[E, E, W, E, P, R, T], [A, I, A, *, N, U, T], [Q, A, C, I, U, R, Y]]Moves played: 0Player 1's move79 tiles remain in bagPlayer 1: 0Player 2: 0Player 3: 0, letter_list=['Z', 'Z', 'Z', 'Z'], self.board=  abcdefghijklmno1 _______________2 _______________3 _______________4 _______________5 _______________6 _______________7 _______________8 ______________9 _______________10_______________11_______________12_______________13_______________14_______________15_______________, self.move_number=0, self.player_rack_list=[[E, E, W, E, P, R, T], [A, I, A, *, N, U, T], [Q, A, C, I, U, R, Y]], self.player_score_list_list=[[], [], []], self.tile_bag=[*, A, A, A, A, A, A, B, B, C, D, D, D, D, E, E, E, E, E, E, E, E, E, F, F, G, G, G, H, H, I, I, I, I, I, I, I, J, K, L, L, L, L, M, M, N, N, N, N, N, O, O, O, O, O, O, O, O, P, R, R, R, R, S, S, S, S, T, T, T, T, U, U, V, V, W, X, Y, Z])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "_, player_rack = get_current_player_data(self.move_number,\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def requires_crt(reason=None):\n",
      "    if reason is None:\n",
      "        reason = \"Test requires awscrt to be installed\"\n",
      "    def decorator(func):\n",
      "        return unittest.skipIf(not HAS_CRT, reason)(func)\n",
      "    return decorator\n",
      "requires_crt(reason=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "reason = \"Test requires awscrt to be installed\"\n",
      "State:\n",
      "'Test requires awscrt to be installed'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_has_definition(self):\n",
      "        if self.name not in self._resource_defs:\n",
      "            definition = {}\n",
      "            for name, resource_def in self._resource_defs.items():\n",
      "                found = False\n",
      "                has_items = self._definition.get('has', {}).items()\n",
      "                for has_name, has_def in has_items:\n",
      "                    if has_def.get('resource', {}).get('type') == name:\n",
      "                        definition[has_name] = has_def\n",
      "                        found = True\n",
      "                if not found:\n",
      "                    fake_has = {'resource': {'type': name, 'identifiers': []}}\n",
      "                    for identifier in resource_def.get('identifiers', []):\n",
      "                        fake_has['resource']['identifiers'].append(\n",
      "                            {'target': identifier['name'], 'source': 'input'}\n",
      "                        )\n",
      "                    definition[name] = fake_has\n",
      "        else:\n",
      "            definition = self._definition.get('has', {})\n",
      "        return definition\n",
      "_get_has_definition(self=<boto3.resources.model.ResourceModel object at 0x7fbbe499faf0>, self._definition=OrderedDict([('actions', OrderedDict([('CreateStack', OrderedDict([('request', OrderedDict([('operation', 'CreateStack')])), ('resource', OrderedDict([('type', 'Stack'), ('identifiers', [OrderedDict([('target', 'Name'), ('source', 'requestParameter'), ('path', 'StackName')])])]))]))])), ('has', OrderedDict([('Event', OrderedDict([('resource', OrderedDict([('type', 'Event'), ('identifiers', [OrderedDict([('target', 'Id'), ('source', 'input')])])]))])), ('Stack', OrderedDict([('resource', OrderedDict([('type', 'Stack'), ('identifiers', [OrderedDict([('target', 'Name'), ('source', 'input')])])]))]))])), ('hasMany', OrderedDict([('Stacks', OrderedDict([('request', OrderedDict([('operation', 'DescribeStacks')])), ('resource', OrderedDict([('type', 'Stack'), ('identifiers', [OrderedDict([('target', 'Name'), ('source', 'response'), ('path', 'Stacks[].StackName')])]), ('path', 'Stacks[]')]))]))]))]), self._renamed={}, self._resource_defs=OrderedDict([('Event', OrderedDict([('identifiers', [OrderedDict([('name', 'Id'), ('memberName', 'EventId')])]), ('shape', 'StackEvent')])), ('Stack', OrderedDict([('identifiers', [OrderedDict([('name', 'Name'), ('memberName', 'StackName')])]), ('shape', 'Stack'), ('load', OrderedDict([('request', OrderedDict([('operation', 'DescribeStacks'), ('params', [OrderedDict([('target', 'StackName'), ('source', 'identifier'), ('name', 'Name')])])])), ('path', 'Stacks[0]')])), ('actions', OrderedDict([('CancelUpdate', OrderedDict([('request', OrderedDict([('operation', 'CancelUpdateStack'), ('params', [OrderedDict([('target', 'StackName'), ('source', 'identifier'), ('name', 'Name')])])]))])), ('Delete', OrderedDict([('request', OrderedDict([('operation', 'DeleteStack'), ('params', [OrderedDict([('target', 'StackName'), ('source', 'identifier'), ('name', 'Name')])])]))])), ('Update', OrderedDict([('request', OrderedDict([('operation', 'UpdateStack'), ('params', [OrderedDict([('target', 'StackName'), ('source', 'identifier'), ('name', 'Name')])])]))]))])), ('has', OrderedDict([('Resource', OrderedDict([('resource', OrderedDict([('type', 'StackResource'), ('identifiers', [OrderedDict([('target', 'StackName'), ('source', 'identifier'), ('name', 'Name')]), OrderedDict([('target', 'LogicalId'), ('source', 'input')])])]))]))])), ('hasMany', OrderedDict([('Events', OrderedDict([('request', OrderedDict([('operation', 'DescribeStackEvents'), ('params', [OrderedDict([('target', 'StackName'), ('source', 'identifier'), ('name', 'Name')])])])), ('resource', OrderedDict([('type', 'Event'), ('identifiers', [OrderedDict([('target', 'Id'), ('source', 'response'), ('path', 'StackEvents[].EventId')])]), ('path', 'StackEvents[]')]))])), ('ResourceSummaries', OrderedDict([('request', OrderedDict([('operation', 'ListStackResources'), ('params', [OrderedDict([('target', 'StackName'), ('source', 'identifier'), ('name', 'Name')])])])), ('resource', OrderedDict([('type', 'StackResourceSummary'), ('identifiers', [OrderedDict([('target', 'LogicalId'), ('source', 'response'), ('path', 'StackResourceSummaries[].LogicalResourceId')]), OrderedDict([('target', 'StackName'), ('source', 'requestParameter'), ('path', 'StackName')])]), ('path', 'StackResourceSummaries[]')]))]))]))])), ('StackResource', OrderedDict([('identifiers', [OrderedDict([('name', 'StackName')]), OrderedDict([('name', 'LogicalId'), ('memberName', 'LogicalResourceId')])]), ('shape', 'StackResourceDetail'), ('load', OrderedDict([('request', OrderedDict([('operation', 'DescribeStackResource'), ('params', [OrderedDict([('target', 'LogicalResourceId'), ('source', 'identifier'), ('name', 'LogicalId')]), OrderedDict([('target', 'StackName'), ('source', 'identifier'), ('name', 'StackName')])])])), ('path', 'StackResourceDetail')])), ('has', OrderedDict([('Stack', OrderedDict([('resource', OrderedDict([('type', 'Stack'), ('identifiers', [OrderedDict([('target', 'Name'), ('source', 'identifier'), ('name', 'StackName')])])]))]))]))])), ('StackResourceSummary', OrderedDict([('identifiers', [OrderedDict([('name', 'StackName')]), OrderedDict([('name', 'LogicalId'), ('memberName', 'LogicalResourceId')])]), ('shape', 'StackResourceSummary'), ('has', OrderedDict([('Resource', OrderedDict([('resource', OrderedDict([('type', 'StackResource'), ('identifiers', [OrderedDict([('target', 'LogicalId'), ('source', 'identifier'), ('name', 'LogicalId')]), OrderedDict([('target', 'StackName'), ('source', 'identifier'), ('name', 'StackName')])])]))]))]))]))]), self.name='cloudformation', self.shape=None)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "definition = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _handler(**kwargs):\n",
      "        module, function_name = full_name.rsplit('.', 1)\n",
      "        module = import_module(module)\n",
      "        kwargs.update(parent_kwargs)\n",
      "        return getattr(module, function_name)(**kwargs)\n",
      "_handler(kwargs={'class_attributes': {'meta': ResourceMeta('dynamodb', identifiers=[]), 'batch_get_item': <function ResourceFactory._create_action.<locals>.do_action at 0x7fbbe3a924c0>, 'batch_write_item': <function ResourceFactory._create_action.<locals>.do_action at 0x7fbbe3a92550>, 'create_table': <function ResourceFactory._create_action.<locals>.do_action at 0x7fbbe3957820>, 'tables': <property object at 0x7fbbe37534a0>, 'Table': <function ResourceFactory._create_class_partial.<locals>.create_resource at 0x7fbbe3957e50>, 'get_available_subresources': <function ResourceFactory._create_available_subresources_command.<locals>.get_available_subresources at 0x7fbbe3957ee0>}, 'base_classes': [<class 'boto3.resources.base.ServiceResource'>], 'service_context': ServiceContext(service_name='dynamodb', service_model=ServiceModel(dynamodb), service_waiter_model=<boto3.utils.LazyLoadedWaiterModel object at 0x7fbbe3bb2940>, resource_json_definitions=OrderedDict([('Table', OrderedDict([('identifiers', [OrderedDict([('name', 'Name'), ('memberName', 'TableName')])]), ('shape', 'TableDescription'), ('load', OrderedDict([('request', OrderedDict([('operation', 'DescribeTable'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])])), ('path', 'Table')])), ('actions', OrderedDict([('Delete', OrderedDict([('request', OrderedDict([('operation', 'DeleteTable'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])]))])), ('DeleteItem', OrderedDict([('request', OrderedDict([('operation', 'DeleteItem'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])]))])), ('GetItem', OrderedDict([('request', OrderedDict([('operation', 'GetItem'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])]))])), ('PutItem', OrderedDict([('request', OrderedDict([('operation', 'PutItem'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])]))])), ('Query', OrderedDict([('request', OrderedDict([('operation', 'Query'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])]))])), ('Scan', OrderedDict([('request', OrderedDict([('operation', 'Scan'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])]))])), ('Update', OrderedDict([('request', OrderedDict([('operation', 'UpdateTable'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])])), ('resource', OrderedDict([('type', 'Table'), ('identifiers', [OrderedDict([('target', 'Name'), ('source', 'identifier'), ('name', 'Name')])]), ('path', 'TableDescription')]))])), ('UpdateItem', OrderedDict([('request', OrderedDict([('operation', 'UpdateItem'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])]))]))])), ('waiters', OrderedDict([('Exists', OrderedDict([('waiterName', 'TableExists'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])])), ('NotExists', OrderedDict([('waiterName', 'TableNotExists'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])]))]))]))])), 'event_name': 'creating-resource-class.dynamodb.ServiceResource'}, full_name='boto3.dynamodb.transform.register_high_level_interface', parent_kwargs={})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "module, function_name = full_name.rsplit('.', 1)\n",
      "State:\n",
      "'boto3.dynamodb.transform'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _handler(**kwargs):\n",
      "        module, function_name = full_name.rsplit('.', 1)\n",
      "        module = import_module(module)\n",
      "        kwargs.update(parent_kwargs)\n",
      "        return getattr(module, function_name)(**kwargs)\n",
      "_handler(kwargs={'class_attributes': {'meta': ResourceMeta('dynamodb', identifiers=[]), 'batch_get_item': <function ResourceFactory._create_action.<locals>.do_action at 0x7fbbe3a924c0>, 'batch_write_item': <function ResourceFactory._create_action.<locals>.do_action at 0x7fbbe3a92550>, 'create_table': <function ResourceFactory._create_action.<locals>.do_action at 0x7fbbe3957820>, 'tables': <property object at 0x7fbbe37534a0>, 'Table': <function ResourceFactory._create_class_partial.<locals>.create_resource at 0x7fbbe3957e50>, 'get_available_subresources': <function ResourceFactory._create_available_subresources_command.<locals>.get_available_subresources at 0x7fbbe3957ee0>}, 'base_classes': [<class 'boto3.resources.base.ServiceResource'>], 'service_context': ServiceContext(service_name='dynamodb', service_model=ServiceModel(dynamodb), service_waiter_model=<boto3.utils.LazyLoadedWaiterModel object at 0x7fbbe3bb2940>, resource_json_definitions=OrderedDict([('Table', OrderedDict([('identifiers', [OrderedDict([('name', 'Name'), ('memberName', 'TableName')])]), ('shape', 'TableDescription'), ('load', OrderedDict([('request', OrderedDict([('operation', 'DescribeTable'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])])), ('path', 'Table')])), ('actions', OrderedDict([('Delete', OrderedDict([('request', OrderedDict([('operation', 'DeleteTable'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])]))])), ('DeleteItem', OrderedDict([('request', OrderedDict([('operation', 'DeleteItem'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])]))])), ('GetItem', OrderedDict([('request', OrderedDict([('operation', 'GetItem'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])]))])), ('PutItem', OrderedDict([('request', OrderedDict([('operation', 'PutItem'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])]))])), ('Query', OrderedDict([('request', OrderedDict([('operation', 'Query'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])]))])), ('Scan', OrderedDict([('request', OrderedDict([('operation', 'Scan'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])]))])), ('Update', OrderedDict([('request', OrderedDict([('operation', 'UpdateTable'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])])), ('resource', OrderedDict([('type', 'Table'), ('identifiers', [OrderedDict([('target', 'Name'), ('source', 'identifier'), ('name', 'Name')])]), ('path', 'TableDescription')]))])), ('UpdateItem', OrderedDict([('request', OrderedDict([('operation', 'UpdateItem'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])]))]))])), ('waiters', OrderedDict([('Exists', OrderedDict([('waiterName', 'TableExists'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])])), ('NotExists', OrderedDict([('waiterName', 'TableNotExists'), ('params', [OrderedDict([('target', 'TableName'), ('source', 'identifier'), ('name', 'Name')])])]))]))]))])), 'event_name': 'creating-resource-class.dynamodb.ServiceResource'}, full_name='boto3.dynamodb.transform.register_high_level_interface', parent_kwargs={})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "module = import_module(module)\n",
      "State:\n",
      "<module 'boto3.dynamodb.transform' from '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/boto+boto3/boto+boto3/boto3/dynamodb/transform.py'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _transform_map(self, model, params, transformation, target_shape):\n",
      "        if not isinstance(params, collections_abc.Mapping):\n",
      "            return\n",
      "        value_model = model.value\n",
      "        value_shape = value_model.name\n",
      "        for key, value in params.items():\n",
      "            if value_shape == target_shape:\n",
      "                params[key] = transformation(value)\n",
      "            else:\n",
      "                self._transform_parameters(\n",
      "                    value_model, params[key], transformation, target_shape\n",
      "                )\n",
      "_transform_map(self=<boto3.dynamodb.transform.ParameterTransformer object at 0x7fbbd84c4a60>, model=<MapShape(ExpressionAttributeNameMap)>, params={'\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "value_model = model.value\n",
      "State:\n",
      "<StringShape(AttributeName)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _transform_map(self, model, params, transformation, target_shape):\n",
      "        if not isinstance(params, collections_abc.Mapping):\n",
      "            return\n",
      "        value_model = model.value\n",
      "        value_shape = value_model.name\n",
      "        for key, value in params.items():\n",
      "            if value_shape == target_shape:\n",
      "                params[key] = transformation(value)\n",
      "            else:\n",
      "                self._transform_parameters(\n",
      "                    value_model, params[key], transformation, target_shape\n",
      "                )\n",
      "_transform_map(self=<boto3.dynamodb.transform.ParameterTransformer object at 0x7fbbd84c4a60>, model=<MapShape(ExpressionAttributeNameMap)>, params={'\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "value_shape = value_model.name\n",
      "State:\n",
      "'AttributeName'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_lexer_single_token(expression, result):\n",
      "    lexer = MOALexer()\n",
      "    tokens = tuple(token.type for token in lexer.tokenize(expression))\n",
      "    assert tokens == result\n",
      "test_lexer_single_token(expression='1234', result=('INTEGER',))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "lexer = MOALexer()\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_lexer_single_token(expression, result):\n",
      "    lexer = MOALexer()\n",
      "    tokens = tuple(token.type for token in lexer.tokenize(expression))\n",
      "    assert tokens == result\n",
      "test_lexer_single_token(expression='1234', result=('INTEGER',))\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "assert tokens == result\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def INTEGER(self, t):\n",
      "        t.value = int(t.value)\n",
      "        return t\n",
      "INTEGER(self=<moa.frontend.moa.MOALexer object at 0x7f5da3d53640>, t=Token(type='INTEGER', value='1234', lineno=1, index=0, end=4), self._Lexer__set_state=<function Lexer.tokenize.<locals>._set_state at 0x7f5da3e37b80>, self.accept=<function Lexer.tokenize.<locals>._accept at 0x7f5da3e75f70>, self.index=4, self.lineno=1, self.mark=<function Lexer.tokenize.<locals>._mark at 0x7f5da4760b80>, self.reject=<function Lexer.tokenize.<locals>._reject at 0x7f5da3e75ee0>, self.text='1234')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "t.value = int(t.value)\n",
      "State:\n",
      "Token(type='INTEGER', value=1234, lineno=1, index=0, end=4)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def extract_from_part(parsed, stop_at_punctuation=True):\n",
      "    tbl_prefix_seen = False\n",
      "    for item in parsed.tokens:\n",
      "        if tbl_prefix_seen:\n",
      "            if is_subselect(item):\n",
      "                for x in extract_from_part(item, stop_at_punctuation):\n",
      "                    yield x\n",
      "            elif stop_at_punctuation and item.ttype is Punctuation:\n",
      "                return\n",
      "            elif item.ttype is Keyword and item.value.upper() == 'ON':\n",
      "                tbl_prefix_seen = False\n",
      "                continue\n",
      "            elif item.ttype is Keyword and (\n",
      "                    not item.value.upper() == 'FROM') and (\n",
      "                    not item.value.upper().endswith('JOIN')):\n",
      "                return\n",
      "            else:\n",
      "                yield item\n",
      "        elif ((item.ttype is Keyword or item.ttype is Keyword.DML) and\n",
      "                item.value.upper() in ('COPY', 'FROM', 'INTO', 'UPDATE', 'TABLE', 'JOIN',)):\n",
      "            tbl_prefix_seen = True\n",
      "        elif isinstance(item, IdentifierList):\n",
      "            for identifier in item.get_identifiers():\n",
      "                if (identifier.ttype is Keyword and\n",
      "                        identifier.value.upper() == 'FROM'):\n",
      "                    tbl_prefix_seen = True\n",
      "                    break\n",
      "extract_from_part(parsed=<Statement 'SELECT ' at 0x7F95D0A07120>, stop_at_punctuation=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "tbl_prefix_seen = False\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse_special_command(sql):\n",
      "    command, _, arg = sql.partition(' ')\n",
      "    verbose = '+' in command\n",
      "    command = command.strip().replace('+', '')\n",
      "    return (command, verbose, arg.strip())\n",
      "parse_special_command(sql='\\\\. ')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "command, _, arg = sql.partition(' ')\n",
      "State:\n",
      "'\\\\.'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse_special_command(sql):\n",
      "    command, _, arg = sql.partition(' ')\n",
      "    verbose = '+' in command\n",
      "    command = command.strip().replace('+', '')\n",
      "    return (command, verbose, arg.strip())\n",
      "parse_special_command(sql='\\\\. ')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "verbose = '+' in command\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_mylogin_cnf_path():\n",
      "    mylogin_cnf_path = os.getenv('MYSQL_TEST_LOGIN_FILE')\n",
      "    if mylogin_cnf_path is None:\n",
      "        app_data = os.getenv('APPDATA')\n",
      "        default_dir = os.path.join(app_data, 'MySQL') if app_data else '~'\n",
      "        mylogin_cnf_path = os.path.join(default_dir, '.mylogin.cnf')\n",
      "    mylogin_cnf_path = os.path.expanduser(mylogin_cnf_path)\n",
      "    if exists(mylogin_cnf_path):\n",
      "        logger.debug(\"Found login path file at '{0}'\".format(mylogin_cnf_path))\n",
      "        return mylogin_cnf_path\n",
      "    return None\n",
      "get_mylogin_cnf_path()\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "app_data = os.getenv('APPDATA')\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def read_config_file(f, list_values=True):\n",
      "    if isinstance(f, basestring):\n",
      "        f = os.path.expanduser(f)\n",
      "    try:\n",
      "        config = ConfigObj(f, interpolation=False, encoding='utf8',\n",
      "                           list_values=list_values)\n",
      "    except ConfigObjError as e:\n",
      "        log(logger, logging.WARNING, \"Unable to parse line {0} of config file \"\n",
      "            \"'{1}'.\".format(e.line_number, f))\n",
      "        log(logger, logging.WARNING, \"Using successfully parsed config values.\")\n",
      "        return e.config\n",
      "    except (IOError, OSError) as e:\n",
      "        log(logger, logging.WARNING, \"You don't have permission to read \"\n",
      "            \"config file '{0}'.\".format(e.filename))\n",
      "        return None\n",
      "    return config\n",
      "read_config_file(f={}, list_values=True)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "config = ConfigObj(f, interpolation=False, encoding='utf8',\n",
      "State:\n",
      "ConfigObj({'main': {'weather': 'cloudy with a chance of meatballs'}})\n",
      "==================================================\n",
      "Clean Code:\n",
      "def read_config_files(files, list_values=True):\n",
      "    config = create_default_config(list_values=list_values)\n",
      "    _files = copy(files)\n",
      "    while _files:\n",
      "        _file = _files.pop(0)\n",
      "        _config = read_config_file(_file, list_values=list_values)\n",
      "        if config is not None:\n",
      "            _files = get_included_configs(_file) + _files\n",
      "        if bool(_config) is True:\n",
      "            config.merge(_config)\n",
      "            config.filename = _config.filename\n",
      "    return config\n",
      "read_config_files(files=['/etc/myclirc', '/home/XXX/.config/mycli/myclirc', '~/.myclirc', '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/dbcli+mycli/dbcli+mycli/.myclirc'], list_values=True)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "_files = copy(files)\n",
      "State:\n",
      "['/etc/myclirc', '/home/XXX/.config/mycli/myclirc', '~/.myclirc', '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/dbcli+mycli/dbcli+mycli/.myclirc']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def style_factory_output(name, cli_style):\n",
      "    try:\n",
      "        style = pygments.styles.get_style_by_name(name).styles\n",
      "    except ClassNotFound:\n",
      "        style = pygments.styles.get_style_by_name('native').styles\n",
      "    for token in cli_style:\n",
      "        if token.startswith('Token.'):\n",
      "            token_type, style_value = parse_pygments_style(\n",
      "                token, style, cli_style)\n",
      "            style.update({token_type: style_value})\n",
      "        elif token in PROMPT_STYLE_TO_TOKEN:\n",
      "            token_type = PROMPT_STYLE_TO_TOKEN[token]\n",
      "            style.update({token_type: cli_style[token]})\n",
      "        elif token in OVERRIDE_STYLE_TO_TOKEN:\n",
      "            token_type = OVERRIDE_STYLE_TO_TOKEN[token]\n",
      "            style.update({token_type: cli_style[token]})\n",
      "        else:\n",
      "            logger.error('Unhandled style / class name: %s', token)\n",
      "    class OutputStyle(PygmentsStyle):\n",
      "        default_style = \"\"\n",
      "        styles = style\n",
      "    return OutputStyle\n",
      "style_factory_output(name='default', cli_style={'completion-menu.completion.current': 'bg:\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "style = pygments.styles.get_style_by_name(name).styles\n",
      "State:\n",
      "{Token.Text.Whitespace: '#bbbbbb', Token.Comment: 'italic #3D7B7B', Token.Comment.Preproc: 'noitalic #9C6500', Token.Keyword: 'bold #008000', Token.Keyword.Pseudo: 'nobold', Token.Keyword.Type: 'nobold #B00040', Token.Operator: '#666666', Token.Operator.Word: 'bold #AA22FF', Token.Name.Builtin: '#008000', Token.Name.Function: '#0000FF', Token.Name.Class: 'bold #0000FF', Token.Name.Namespace: 'bold #0000FF', Token.Name.Exception: 'bold #CB3F38', Token.Name.Variable: '#19177C', Token.Name.Constant: '#880000', Token.Name.Label: '#767600', Token.Name.Entity: 'bold #717171', Token.Name.Attribute: '#687822', Token.Name.Tag: 'bold #008000', Token.Name.Decorator: '#AA22FF', Token.Literal.String: '#BA2121', Token.Literal.String.Doc: 'italic', Token.Literal.String.Interpol: 'bold #A45A77', Token.Literal.String.Escape: 'bold #AA5D1F', Token.Literal.String.Regex: '#A45A77', Token.Literal.String.Symbol: '#19177C', Token.Literal.String.Other: '#008000', Token.Literal.Number: '#666666', Token.Generic.Heading: 'bold #000080', Token.Generic.Subheading: 'bold #800080', Token.Generic.Deleted: '#A00000', Token.Generic.Inserted: '#008400', Token.Generic.Error: '#E40000', Token.Generic.Emph: 'italic', Token.Generic.Strong: 'bold', Token.Generic.EmphStrong: 'bold italic', Token.Generic.Prompt: 'bold #000080', Token.Generic.Output: '#717171', Token.Generic.Traceback: '#04D', Token.Error: 'border:#FF0000', Token: '', Token.Text: '', Token.Escape: '', Token.Other: '', Token.Keyword.Constant: '', Token.Keyword.Declaration: '', Token.Keyword.Namespace: '', Token.Keyword.Reserved: '', Token.Name: '', Token.Name.Builtin.Pseudo: '', Token.Name.Function.Magic: '', Token.Name.Property: '', Token.Name.Other: '', Token.Name.Variable.Class: '', Token.Name.Variable.Global: '', Token.Name.Variable.Instance: '', Token.Name.Variable.Magic: '', Token.Literal: '', Token.Literal.Date: '', Token.Literal.String.Affix: '', Token.Literal.String.Backtick: '', Token.Literal.String.Char: '', Token.Literal.String.Delimiter: '', Token.Literal.String.Double: '', Token.Literal.String.Heredoc: '', Token.Literal.String.Single: '', Token.Literal.Number.Bin: '', Token.Literal.Number.Float: '', Token.Literal.Number.Hex: '', Token.Literal.Number.Integer: '', Token.Literal.Number.Integer.Long: '', Token.Literal.Number.Oct: '', Token.Punctuation: '', Token.Punctuation.Marker: '', Token.Comment.Hashbang: '', Token.Comment.Multiline: '', Token.Comment.PreprocFile: '', Token.Comment.Single: '', Token.Comment.Special: '', Token.Generic: ''}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def initialize_logging(self):\n",
      "        log_file = os.path.expanduser(self.config['main']['log_file'])\n",
      "        log_level = self.config['main']['log_level']\n",
      "        level_map = {'CRITICAL': logging.CRITICAL,\n",
      "                     'ERROR': logging.ERROR,\n",
      "                     'WARNING': logging.WARNING,\n",
      "                     'INFO': logging.INFO,\n",
      "                     'DEBUG': logging.DEBUG\n",
      "                     }\n",
      "        if log_level.upper() == \"NONE\":\n",
      "            handler = logging.NullHandler()\n",
      "            log_level = \"CRITICAL\"\n",
      "        elif dir_path_exists(log_file):\n",
      "            handler = logging.FileHandler(log_file)\n",
      "        else:\n",
      "            self.echo(\n",
      "                'Error: Unable to open the log file \"{}\".'.format(log_file),\n",
      "                err=True, fg='red')\n",
      "            return\n",
      "        formatter = logging.Formatter(\n",
      "            '%(asctime)s (%(process)d/%(threadName)s) '\n",
      "            '%(name)s %(levelname)s - %(message)s')\n",
      "        handler.setFormatter(formatter)\n",
      "        root_logger = logging.getLogger('mycli')\n",
      "        root_logger.addHandler(handler)\n",
      "        root_logger.setLevel(level_map[log_level.upper()])\n",
      "        logging.captureWarnings(True)\n",
      "        root_logger.debug('Initializing mycli logging.')\n",
      "        root_logger.debug('Log file %r.', log_file)\n",
      "initialize_logging(self=<mycli.main.MyCli object at 0x7f95d0b74220>, self.auto_vertical_output=False, self.beep_after_seconds=0.0, self.cli_style={'completion-menu.completion.current': 'bg:\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "log_file = os.path.expanduser(self.config['main']['log_file'])\n",
      "State:\n",
      "'/home/XXX/.mycli.log'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def initialize_logging(self):\n",
      "        log_file = os.path.expanduser(self.config['main']['log_file'])\n",
      "        log_level = self.config['main']['log_level']\n",
      "        level_map = {'CRITICAL': logging.CRITICAL,\n",
      "                     'ERROR': logging.ERROR,\n",
      "                     'WARNING': logging.WARNING,\n",
      "                     'INFO': logging.INFO,\n",
      "                     'DEBUG': logging.DEBUG\n",
      "                     }\n",
      "        if log_level.upper() == \"NONE\":\n",
      "            handler = logging.NullHandler()\n",
      "            log_level = \"CRITICAL\"\n",
      "        elif dir_path_exists(log_file):\n",
      "            handler = logging.FileHandler(log_file)\n",
      "        else:\n",
      "            self.echo(\n",
      "                'Error: Unable to open the log file \"{}\".'.format(log_file),\n",
      "                err=True, fg='red')\n",
      "            return\n",
      "        formatter = logging.Formatter(\n",
      "            '%(asctime)s (%(process)d/%(threadName)s) '\n",
      "            '%(name)s %(levelname)s - %(message)s')\n",
      "        handler.setFormatter(formatter)\n",
      "        root_logger = logging.getLogger('mycli')\n",
      "        root_logger.addHandler(handler)\n",
      "        root_logger.setLevel(level_map[log_level.upper()])\n",
      "        logging.captureWarnings(True)\n",
      "        root_logger.debug('Initializing mycli logging.')\n",
      "        root_logger.debug('Log file %r.', log_file)\n",
      "initialize_logging(self=<mycli.main.MyCli object at 0x7f95d0b74220>, self.auto_vertical_output=False, self.beep_after_seconds=0.0, self.cli_style={'completion-menu.completion.current': 'bg:\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "log_level = self.config['main']['log_level']\n",
      "State:\n",
      "'INFO'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def initialize_logging(self):\n",
      "        log_file = os.path.expanduser(self.config['main']['log_file'])\n",
      "        log_level = self.config['main']['log_level']\n",
      "        level_map = {'CRITICAL': logging.CRITICAL,\n",
      "                     'ERROR': logging.ERROR,\n",
      "                     'WARNING': logging.WARNING,\n",
      "                     'INFO': logging.INFO,\n",
      "                     'DEBUG': logging.DEBUG\n",
      "                     }\n",
      "        if log_level.upper() == \"NONE\":\n",
      "            handler = logging.NullHandler()\n",
      "            log_level = \"CRITICAL\"\n",
      "        elif dir_path_exists(log_file):\n",
      "            handler = logging.FileHandler(log_file)\n",
      "        else:\n",
      "            self.echo(\n",
      "                'Error: Unable to open the log file \"{}\".'.format(log_file),\n",
      "                err=True, fg='red')\n",
      "            return\n",
      "        formatter = logging.Formatter(\n",
      "            '%(asctime)s (%(process)d/%(threadName)s) '\n",
      "            '%(name)s %(levelname)s - %(message)s')\n",
      "        handler.setFormatter(formatter)\n",
      "        root_logger = logging.getLogger('mycli')\n",
      "        root_logger.addHandler(handler)\n",
      "        root_logger.setLevel(level_map[log_level.upper()])\n",
      "        logging.captureWarnings(True)\n",
      "        root_logger.debug('Initializing mycli logging.')\n",
      "        root_logger.debug('Log file %r.', log_file)\n",
      "initialize_logging(self=<mycli.main.MyCli object at 0x7f95d0b74220>, self.auto_vertical_output=False, self.beep_after_seconds=0.0, self.cli_style={'completion-menu.completion.current': 'bg:\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "level_map = {'CRITICAL': logging.CRITICAL,\n",
      "State:\n",
      "{'CRITICAL': 50, 'ERROR': 40, 'WARNING': 30, 'INFO': 20, 'DEBUG': 10}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _grid(string: str, attr: Mapping[str, str]) -> Grid:\n",
      "    method = None\n",
      "    try:\n",
      "        if attr[\"source\"] in (\"grid\", \"grid_l\"):\n",
      "            method = \"linear\"\n",
      "        elif attr[\"source\"] in (\"grid_s\", \"grid_c\"):\n",
      "            method = \"spline\"\n",
      "        elif attr[\"source\"] in (\"grid_q\", \"grid_n\"):\n",
      "            method = \"nearest\"\n",
      "    except KeyError:\n",
      "        if re.fullmatch(r\"\\S[\\S ]*\\.nc\", string):\n",
      "            method = \"linear\"\n",
      "    if method:\n",
      "        return Grid(\n",
      "            file=string, x=attr.get(\"x\", \"lon\"), y=attr.get(\"y\", \"lat\"), method=method\n",
      "        )\n",
      "    raise TextParseError(f\"'{string}' does not represent a grid\")\n",
      "_grid(string='abc', attr={})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "method = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _time(string: str) -> datetime:\n",
      "    formats = [\n",
      "        \"%Y-%m-%dT%H:%M:%S\",\n",
      "        \"%Y-%m-%dT%H:%M\",\n",
      "        \"%Y-%m-%dT%H\",\n",
      "        \"%Y-%m-%dT\",\n",
      "        \"%Y-%m-%d\",\n",
      "    ]\n",
      "    for format_ in formats:\n",
      "        try:\n",
      "            return datetime.strptime(string, format_)\n",
      "        except ValueError:\n",
      "            pass\n",
      "    raise ValueError(f\"time data '{string}' does not match format '%Y-%m-%dT%H:%M:%S'\")\n",
      "_time(string='2016-03-01T21:00:11')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "formats = [\n",
      "State:\n",
      "['%Y-%m-%dT%H:%M:%S', '%Y-%m-%dT%H:%M', '%Y-%m-%dT%H', '%Y-%m-%dT', '%Y-%m-%d']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_x(stack: MutableSequence[NumberOrArray]) -> NumberOrArray:\n",
      "    if not stack:\n",
      "        raise StackUnderflowError(\n",
      "            \"attempted to get element from the 'stack' but the stack is empty\"\n",
      "        )\n",
      "    return stack.pop()\n",
      "_get_x(stack=[1])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "return stack.pop()\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def func_wrapper():\n",
      "        nonlocal value\n",
      "        if value is None:\n",
      "            value = func()\n",
      "        return value\n",
      "func_wrapper(func=<function _is_triton_available at 0x7f97569024c0>, value=None)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "value = func()\n",
      "State:\n",
      "True\n",
      "==================================================\n",
      "Clean Code:\n",
      "def import_all_modules(root: str, base_module: str) -> List[str]:\n",
      "    modules: List[str] = []\n",
      "    for file in os.listdir(root):\n",
      "        if file.endswith((\".py\", \".pyc\")) and not file.startswith(\"_\"):\n",
      "            module = file[: file.find(\".py\")]\n",
      "            if module not in sys.modules:\n",
      "                module_name = \".\".join([base_module, module])\n",
      "                importlib.import_module(module_name)\n",
      "                modules.append(module_name)\n",
      "    return modules\n",
      "import_all_modules(root='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/facebookresearch+xformers/facebookresearch+xformers/xformers/components/attention', base_module='xformers.components.attention')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "modules: List[str] = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_instantiate(param_dict):\n",
      "    @distributed_test(world_size=param_dict.pop(\"world_size\", 2))\n",
      "    def wrapper():\n",
      "        run_test_model_instantiation(param_dict=param_dict)\n",
      "    wrapper()\n",
      "test_instantiate(param_dict={'data_path': 'data/enwik8/enwik8_text_document', 'vocab_file': 'data/gpt2-vocab.json', 'merge_file': 'data/gpt2-merges.txt', 'lr_decay_iters': 20, 'train_iters': 20, 'hostfile': 'None', 'include': 'localhost:1', 'use_wandb': False, 'pipe_parallel_size': 0, 'model_parallel_size': 1, 'num_layers': 2, 'hidden_size': 8, 'num_attention_heads': 4, 'seq_length': 1024, 'max_position_embeddings': 1024, 'pos_emb': 'rotary', 'no_weight_tying': False, 'gpt_j_residual': False, 'output_layer_parallelism': 'column', 'scaled_upper_triang_masked_softmax_fusion': True, 'bias_gelu_fusion': False, 'rope_fusion': False, 'optimizer': {'type': 'sm3', 'params': {}}, 'precision': 'fp16', 'init_method': 'small_init', 'output_layer_init_method': 'wang_init', 'train_micro_batch_size_per_gpu': 4, 'gas': 1, 'data_impl': 'mmap', 'num_workers': 1, 'checkpoint_activations': True, 'checkpoint_num_layers': 1, 'partition_activations': True, 'synchronize_each_layer': True, 'gradient_clipping': 1.0, 'weight_decay': 0.1, 'hidden_dropout': 0, 'attention_dropout': 0, 'distributed_backend': 'nccl', 'lr_decay_style': 'cosine', 'warmup': 0.01, 'checkpoint_factor': 1000, 'eval_interval': 100000, 'eval_iters': 10, 'log_interval': 10, 'steps_per_print': 10, 'wall_clock_breakdown': True, 'deepspeed_extra_args': {'comms_logger': {'enabled': True, 'verbose': True, 'prof_all': True, 'debug': False}}, 'attention_config': [[['sparse_variable'], 'all']], 'world_size': 1, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, 'fp32_allreduce': False})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "@distributed_test(world_size=param_dict.pop(\"world_size\", 2))\n",
      "State:\n",
      "{'data_path': 'data/enwik8/enwik8_text_document', 'vocab_file': 'data/gpt2-vocab.json', 'merge_file': 'data/gpt2-merges.txt', 'lr_decay_iters': 20, 'train_iters': 20, 'hostfile': 'None', 'include': 'localhost:1', 'use_wandb': False, 'pipe_parallel_size': 0, 'model_parallel_size': 1, 'num_layers': 2, 'hidden_size': 8, 'num_attention_heads': 4, 'seq_length': 1024, 'max_position_embeddings': 1024, 'pos_emb': 'rotary', 'no_weight_tying': False, 'gpt_j_residual': False, 'output_layer_parallelism': 'column', 'scaled_upper_triang_masked_softmax_fusion': True, 'bias_gelu_fusion': False, 'rope_fusion': False, 'optimizer': {'type': 'sm3', 'params': {}}, 'precision': 'fp16', 'init_method': 'small_init', 'output_layer_init_method': 'wang_init', 'train_micro_batch_size_per_gpu': 4, 'gas': 1, 'data_impl': 'mmap', 'num_workers': 1, 'checkpoint_activations': True, 'checkpoint_num_layers': 1, 'partition_activations': True, 'synchronize_each_layer': True, 'gradient_clipping': 1.0, 'weight_decay': 0.1, 'hidden_dropout': 0, 'attention_dropout': 0, 'distributed_backend': 'nccl', 'lr_decay_style': 'cosine', 'warmup': 0.01, 'checkpoint_factor': 1000, 'eval_interval': 100000, 'eval_iters': 10, 'log_interval': 10, 'steps_per_print': 10, 'wall_clock_breakdown': True, 'deepspeed_extra_args': {'comms_logger': {'enabled': True, 'verbose': True, 'prof_all': True, 'debug': False}}, 'attention_config': [[['sparse_variable'], 'all']], 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, 'fp32_allreduce': False}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_instantiate(param_dict):\n",
      "    @distributed_test(world_size=param_dict.pop(\"world_size\", 2))\n",
      "    def wrapper():\n",
      "        run_test_model_instantiation(param_dict=param_dict)\n",
      "    wrapper()\n",
      "test_instantiate(param_dict={'data_path': 'data/enwik8/enwik8_text_document', 'vocab_file': 'data/gpt2-vocab.json', 'merge_file': 'data/gpt2-merges.txt', 'lr_decay_iters': 20, 'train_iters': 20, 'hostfile': 'None', 'include': 'localhost:1', 'use_wandb': False, 'pipe_parallel_size': 0, 'model_parallel_size': 1, 'num_layers': 2, 'hidden_size': 8, 'num_attention_heads': 4, 'seq_length': 1024, 'max_position_embeddings': 1024, 'pos_emb': 'rotary', 'no_weight_tying': False, 'gpt_j_residual': False, 'output_layer_parallelism': 'column', 'scaled_upper_triang_masked_softmax_fusion': True, 'bias_gelu_fusion': False, 'rope_fusion': False, 'optimizer': {'type': 'sm3', 'params': {}}, 'precision': 'fp16', 'init_method': 'small_init', 'output_layer_init_method': 'wang_init', 'train_micro_batch_size_per_gpu': 4, 'gas': 1, 'data_impl': 'mmap', 'num_workers': 1, 'checkpoint_activations': True, 'checkpoint_num_layers': 1, 'partition_activations': True, 'synchronize_each_layer': True, 'gradient_clipping': 1.0, 'weight_decay': 0.1, 'hidden_dropout': 0, 'attention_dropout': 0, 'distributed_backend': 'nccl', 'lr_decay_style': 'cosine', 'warmup': 0.01, 'checkpoint_factor': 1000, 'eval_interval': 100000, 'eval_iters': 10, 'log_interval': 10, 'steps_per_print': 10, 'wall_clock_breakdown': True, 'deepspeed_extra_args': {'comms_logger': {'enabled': True, 'verbose': True, 'prof_all': True, 'debug': False}}, 'attention_config': [[['sparse_variable'], 'all']], 'world_size': 1, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, 'fp32_allreduce': False})\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "wrapper()\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def dist_launcher(num_procs, *func_args, **func_kwargs):\n",
      "            processes = []\n",
      "            for local_rank in range(num_procs):\n",
      "                p = Process(\n",
      "                    target=dist_init,\n",
      "                    args=(local_rank, num_procs, *func_args),\n",
      "                    kwargs=func_kwargs,\n",
      "                )\n",
      "                p.start()\n",
      "                processes.append(p)\n",
      "            any_done = False\n",
      "            while not any_done:\n",
      "                for p in processes:\n",
      "                    if not p.is_alive():\n",
      "                        any_done = True\n",
      "                        break\n",
      "            for p in processes:\n",
      "                p.join(DEEPSPEED_UNIT_WORKER_TIMEOUT)\n",
      "            failed = [(rank, p) for rank, p in enumerate(processes) if p.exitcode != 0]\n",
      "            for rank, p in failed:\n",
      "                if p.exitcode is None:\n",
      "                    p.terminate()\n",
      "                    pytest.fail(f\"Worker {rank} hung.\", pytrace=False)\n",
      "                if p.exitcode < 0:\n",
      "                    pytest.fail(\n",
      "                        f\"Worker {rank} killed by signal {-p.exitcode}\", pytrace=False\n",
      "                    )\n",
      "                if p.exitcode > 0:\n",
      "                    pytest.fail(\n",
      "                        f\"Worker {rank} exited with code {p.exitcode}\", pytrace=False\n",
      "                    )\n",
      "dist_launcher(num_procs=1, func_args=(), func_kwargs={}, dist_init=<function distributed_test.<locals>.dist_wrap.<locals>.dist_init at 0x7f1b94b9faf0>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "processes = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_up_autotuning(encoded_config, overwrite_values):\n",
      "        config = json.loads(base64.urlsafe_b64decode(encoded_config).decode(\"utf-8\"))\n",
      "        overwrite_values = overwrite_values if overwrite_values else {}\n",
      "        for tuning_param in AUTOTUNING_ARGS:\n",
      "            if tuning_param in config:\n",
      "                overwrite_values[tuning_param] = config[tuning_param]\n",
      "        return overwrite_values\n",
      "set_up_autotuning(encoded_config='eyJ0cmFpbl9iYXRjaF9zaXplIjogNCwgInRyYWluX21pY3JvX2JhdGNoX3NpemVfcGVyX2dwdSI6IDQsICJvcHRpbWl6ZXIiOiB7InR5cGUiOiAic20zIiwgInBhcmFtcyI6IHt9fSwgImZwMTYiOiB7InR5cGUiOiAiZnAxNiIsICJlbmFibGVkIjogdHJ1ZX0sICJ6ZXJvX29wdGltaXphdGlvbiI6IHsic3RhZ2UiOiAwLCAiYWxsZ2F0aGVyX3BhcnRpdGlvbnMiOiB0cnVlLCAicmVkdWNlX3NjYXR0ZXIiOiB0cnVlLCAiYWxsZ2F0aGVyX2J1Y2tldF9zaXplIjogNTAwMDAwMDAwLCAib3ZlcmxhcF9jb21tIjogZmFsc2UsICJyZWR1Y2VfYnVja2V0X3NpemUiOiA1MDAwMDAwMDAsICJjb250aWd1b3VzX2dyYWRpZW50cyI6IGZhbHNlfSwgIndhbGxfY2xvY2tfYnJlYWtkb3duIjogdHJ1ZSwgImNvbW1zX2xvZ2dlciI6IHsiZW5hYmxlZCI6IHRydWUsICJ2ZXJib3NlIjogdHJ1ZSwgInByb2ZfYWxsIjogdHJ1ZSwgImRlYnVnIjogZmFsc2V9fQ==', overwrite_values={'train_iters': 32})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "config = json.loads(base64.urlsafe_b64decode(encoded_config).decode(\"utf-8\"))\n",
      "State:\n",
      "{'train_batch_size': 4, 'train_micro_batch_size_per_gpu': 4, 'optimizer': {'type': 'sm3', 'params': {}}, 'fp16': {'type': 'fp16', 'enabled': True}, 'zero_optimization': {'stage': 0, 'allgather_partitions': True, 'reduce_scatter': True, 'allgather_bucket_size': 500000000, 'overlap_comm': False, 'reduce_bucket_size': 500000000, 'contiguous_gradients': False}, 'wall_clock_breakdown': True, 'comms_logger': {'enabled': True, 'verbose': True, 'prof_all': True, 'debug': False}}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def run_train_test(monkeypatch, overwrite_values: dict):\n",
      "    max_train_iters = 32\n",
      "    checkpoint_args = {\"train_iters\": max_train_iters}\n",
      "    overwrite_values = checkpoint_args\n",
      "    input_args = [\"train.py\", \"tests/config/test_setup.yml\"]\n",
      "    deepspeed_main_args = simulate_deepy_env(monkeypatch, input_args)\n",
      "    loss_per_iteration = []\n",
      "    with patch(\n",
      "        \"megatron.training.collect_loss_for_unit_test\",\n",
      "        side_effect=lambda x: loss_per_iteration.append(x),\n",
      "    ):\n",
      "        train.main(input_args=deepspeed_main_args, overwrite_values=overwrite_values)\n",
      "        assert (\n",
      "            len(loss_per_iteration) == max_train_iters\n",
      "        ), \"patching should have collected loss values from each train step\"\n",
      "        assert min(loss_per_iteration) < loss_per_iteration[0], (\n",
      "            \"training loss should improve within \" + str(max_train_iters) + \" steps\"\n",
      "        )\n",
      "run_train_test(monkeypatch={_setattr=[], _setitem=[], _cwd=None, _savesyspath=None}, overwrite_values={'pos_emb': 'rpe'})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "max_train_iters = 32\n",
      "State:\n",
      "32\n",
      "==================================================\n",
      "Clean Code:\n",
      "def run_train_test(monkeypatch, overwrite_values: dict):\n",
      "    max_train_iters = 32\n",
      "    checkpoint_args = {\"train_iters\": max_train_iters}\n",
      "    overwrite_values = checkpoint_args\n",
      "    input_args = [\"train.py\", \"tests/config/test_setup.yml\"]\n",
      "    deepspeed_main_args = simulate_deepy_env(monkeypatch, input_args)\n",
      "    loss_per_iteration = []\n",
      "    with patch(\n",
      "        \"megatron.training.collect_loss_for_unit_test\",\n",
      "        side_effect=lambda x: loss_per_iteration.append(x),\n",
      "    ):\n",
      "        train.main(input_args=deepspeed_main_args, overwrite_values=overwrite_values)\n",
      "        assert (\n",
      "            len(loss_per_iteration) == max_train_iters\n",
      "        ), \"patching should have collected loss values from each train step\"\n",
      "        assert min(loss_per_iteration) < loss_per_iteration[0], (\n",
      "            \"training loss should improve within \" + str(max_train_iters) + \" steps\"\n",
      "        )\n",
      "run_train_test(monkeypatch={_setattr=[], _setitem=[], _cwd=None, _savesyspath=None}, overwrite_values={'pos_emb': 'rpe'})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "checkpoint_args = {\"train_iters\": max_train_iters}\n",
      "State:\n",
      "{'train_iters': 32}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def run_train_test(monkeypatch, overwrite_values: dict):\n",
      "    max_train_iters = 32\n",
      "    checkpoint_args = {\"train_iters\": max_train_iters}\n",
      "    overwrite_values = checkpoint_args\n",
      "    input_args = [\"train.py\", \"tests/config/test_setup.yml\"]\n",
      "    deepspeed_main_args = simulate_deepy_env(monkeypatch, input_args)\n",
      "    loss_per_iteration = []\n",
      "    with patch(\n",
      "        \"megatron.training.collect_loss_for_unit_test\",\n",
      "        side_effect=lambda x: loss_per_iteration.append(x),\n",
      "    ):\n",
      "        train.main(input_args=deepspeed_main_args, overwrite_values=overwrite_values)\n",
      "        assert (\n",
      "            len(loss_per_iteration) == max_train_iters\n",
      "        ), \"patching should have collected loss values from each train step\"\n",
      "        assert min(loss_per_iteration) < loss_per_iteration[0], (\n",
      "            \"training loss should improve within \" + str(max_train_iters) + \" steps\"\n",
      "        )\n",
      "run_train_test(monkeypatch={_setattr=[], _setitem=[], _cwd=None, _savesyspath=None}, overwrite_values={'pos_emb': 'rpe'})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "overwrite_values = checkpoint_args\n",
      "State:\n",
      "{'train_iters': 32}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def run_train_test(monkeypatch, overwrite_values: dict):\n",
      "    max_train_iters = 32\n",
      "    checkpoint_args = {\"train_iters\": max_train_iters}\n",
      "    overwrite_values = checkpoint_args\n",
      "    input_args = [\"train.py\", \"tests/config/test_setup.yml\"]\n",
      "    deepspeed_main_args = simulate_deepy_env(monkeypatch, input_args)\n",
      "    loss_per_iteration = []\n",
      "    with patch(\n",
      "        \"megatron.training.collect_loss_for_unit_test\",\n",
      "        side_effect=lambda x: loss_per_iteration.append(x),\n",
      "    ):\n",
      "        train.main(input_args=deepspeed_main_args, overwrite_values=overwrite_values)\n",
      "        assert (\n",
      "            len(loss_per_iteration) == max_train_iters\n",
      "        ), \"patching should have collected loss values from each train step\"\n",
      "        assert min(loss_per_iteration) < loss_per_iteration[0], (\n",
      "            \"training loss should improve within \" + str(max_train_iters) + \" steps\"\n",
      "        )\n",
      "run_train_test(monkeypatch={_setattr=[], _setitem=[], _cwd=None, _savesyspath=None}, overwrite_values={'pos_emb': 'rpe'})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "input_args = [\"train.py\", \"tests/config/test_setup.yml\"]\n",
      "State:\n",
      "['train.py', 'tests/config/test_setup.yml']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def run_train_test(monkeypatch, overwrite_values: dict):\n",
      "    max_train_iters = 32\n",
      "    checkpoint_args = {\"train_iters\": max_train_iters}\n",
      "    overwrite_values = checkpoint_args\n",
      "    input_args = [\"train.py\", \"tests/config/test_setup.yml\"]\n",
      "    deepspeed_main_args = simulate_deepy_env(monkeypatch, input_args)\n",
      "    loss_per_iteration = []\n",
      "    with patch(\n",
      "        \"megatron.training.collect_loss_for_unit_test\",\n",
      "        side_effect=lambda x: loss_per_iteration.append(x),\n",
      "    ):\n",
      "        train.main(input_args=deepspeed_main_args, overwrite_values=overwrite_values)\n",
      "        assert (\n",
      "            len(loss_per_iteration) == max_train_iters\n",
      "        ), \"patching should have collected loss values from each train step\"\n",
      "        assert min(loss_per_iteration) < loss_per_iteration[0], (\n",
      "            \"training loss should improve within \" + str(max_train_iters) + \" steps\"\n",
      "        )\n",
      "run_train_test(monkeypatch={_setattr=[], _setitem=[], _cwd=None, _savesyspath=None}, overwrite_values={'pos_emb': 'rpe'})\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "deepspeed_main_args = simulate_deepy_env(monkeypatch, input_args)\n",
      "State:\n",
      "['--hostfile', 'None', '--include', 'localhost:1', 'train.py', '--deepspeed_config', 'eyJ0cmFpbl9iYXRjaF9zaXplIjogNCwgInRyYWluX21pY3JvX2JhdGNoX3NpemVfcGVyX2dwdSI6IDQsICJvcHRpbWl6ZXIiOiB7InR5cGUiOiAic20zIiwgInBhcmFtcyI6IHt9fSwgImZwMTYiOiB7InR5cGUiOiAiZnAxNiIsICJlbmFibGVkIjogdHJ1ZX0sICJ6ZXJvX29wdGltaXphdGlvbiI6IHsic3RhZ2UiOiAwLCAiYWxsZ2F0aGVyX3BhcnRpdGlvbnMiOiB0cnVlLCAicmVkdWNlX3NjYXR0ZXIiOiB0cnVlLCAiYWxsZ2F0aGVyX2J1Y2tldF9zaXplIjogNTAwMDAwMDAwLCAib3ZlcmxhcF9jb21tIjogZmFsc2UsICJyZWR1Y2VfYnVja2V0X3NpemUiOiA1MDAwMDAwMDAsICJjb250aWd1b3VzX2dyYWRpZW50cyI6IGZhbHNlfSwgIndhbGxfY2xvY2tfYnJlYWtkb3duIjogdHJ1ZSwgImNvbW1zX2xvZ2dlciI6IHsiZW5hYmxlZCI6IHRydWUsICJ2ZXJib3NlIjogdHJ1ZSwgInByb2ZfYWxsIjogdHJ1ZSwgImRlYnVnIjogZmFsc2V9fQ==', '--megatron_config', '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']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def run_neox_args_load_test(yaml_files):\n",
      "    from megatron.neox_arguments import NeoXArgs\n",
      "    yaml_list = get_configs_with_path(yaml_files)\n",
      "    args_loaded = NeoXArgs.from_ymls(yaml_list)\n",
      "    assert isinstance(args_loaded, NeoXArgs)\n",
      "    config = dict()\n",
      "    for conf_file_name in yaml_list:\n",
      "        with open(conf_file_name) as conf_file:\n",
      "            conf = yaml.load(conf_file, Loader=yaml.FullLoader)\n",
      "        for conf_key, conf_value in conf.items():\n",
      "            if conf_key in config:\n",
      "                raise ValueError(\n",
      "                    f\"Conf file {conf_file_name} has the following duplicate keys with previously loaded file: {conf_key}\"\n",
      "                )\n",
      "            conf_key_converted = conf_key.replace(\n",
      "                \"-\", \"_\"\n",
      "            )\n",
      "            config[conf_key_converted] = conf_value\n",
      "    for k, v in config.items():\n",
      "        neox_args_value = getattr(args_loaded, k)\n",
      "        assert v == neox_args_value, (\n",
      "            \"loaded neox args value \"\n",
      "            + str(k)\n",
      "            + \" == \"\n",
      "            + str(neox_args_value)\n",
      "            + \" different from config file \"\n",
      "            + str(v)\n",
      "        )\n",
      "run_neox_args_load_test(yaml_files=['125M.yml', 'local_setup.yml', 'cpu_mock_config.yml'])\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "yaml_list = get_configs_with_path(yaml_files)\n",
      "State:\n",
      "['/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/EleutherAI+gpt-neox/EleutherAI+gpt-neox/configs/125M.yml', '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/EleutherAI+gpt-neox/EleutherAI+gpt-neox/configs/local_setup.yml', '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/EleutherAI+gpt-neox/EleutherAI+gpt-neox/configs/cpu_mock_config.yml']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def run_neox_args_load_test(yaml_files):\n",
      "    from megatron.neox_arguments import NeoXArgs\n",
      "    yaml_list = get_configs_with_path(yaml_files)\n",
      "    args_loaded = NeoXArgs.from_ymls(yaml_list)\n",
      "    assert isinstance(args_loaded, NeoXArgs)\n",
      "    config = dict()\n",
      "    for conf_file_name in yaml_list:\n",
      "        with open(conf_file_name) as conf_file:\n",
      "            conf = yaml.load(conf_file, Loader=yaml.FullLoader)\n",
      "        for conf_key, conf_value in conf.items():\n",
      "            if conf_key in config:\n",
      "                raise ValueError(\n",
      "                    f\"Conf file {conf_file_name} has the following duplicate keys with previously loaded file: {conf_key}\"\n",
      "                )\n",
      "            conf_key_converted = conf_key.replace(\n",
      "                \"-\", \"_\"\n",
      "            )\n",
      "            config[conf_key_converted] = conf_value\n",
      "    for k, v in config.items():\n",
      "        neox_args_value = getattr(args_loaded, k)\n",
      "        assert v == neox_args_value, (\n",
      "            \"loaded neox args value \"\n",
      "            + str(k)\n",
      "            + \" == \"\n",
      "            + str(neox_args_value)\n",
      "            + \" different from config file \"\n",
      "            + str(v)\n",
      "        )\n",
      "run_neox_args_load_test(yaml_files=['125M.yml', 'local_setup.yml', 'cpu_mock_config.yml'])\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "args_loaded = NeoXArgs.from_ymls(yaml_list)\n",
      "State:\n",
      "NeoXArgs(distributed_backend='nccl', local_rank=None, rank=None, lazy_mpu_init=False, short_seq_prob=0.1, eod_mask_loss=False, adlr_autoresume=False, adlr_autoresume_interval=1000, seed=1234, onnx_safe=False, deepscale=False, deepscale_config=None, deepspeed_mpi=False, deepspeed_slurm=False, user_script=None, iteration=None, do_train=None, do_valid=None, do_test=None, save_iters=[10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000, 110000, 120000, 130000, 140000, 150000, 160000, 170000, 180000, 190000, 200000, 210000, 220000, 230000, 240000, 250000, 260000, 270000, 280000, 290000, 300000, 310000], global_num_gpus=1, text_gen_type='unconditional', temperature=0.0, top_p=0.0, top_k=0, return_logits=False, maximum_tokens=64, prompt_end='\\n', sample_input_file=None, sample_output_file='samples.txt', num_samples=1, recompute=False, eval_results_prefix='', eval_tasks=None, use_wandb=True, wandb_group=None, wandb_team=None, wandb_project='neox', wandb_host='https://api.wandb.ai', wandb_init_all_ranks=False, git_hash='7a8fa2f0', log_dir='logs', tensorboard_dir='tensorboard', log_interval=100, log_grad_pct_zeros=False, log_param_norm=False, log_grad_norm=False, log_optimizer_states=False, log_gradient_noise_scale=False, gradient_noise_scale_n_batches=5, gradient_noise_scale_cpu_offload=False, pipe_parallel_size=1, model_parallel_size=1, pipe_partition_method='type:transformer|mlp', world_size=None, is_pipe_parallel=True, data_path='data/enwik8/enwik8_text_document', use_shared_fs=True, train_data_paths=None, label_data_paths=None, test_data_paths=None, valid_data_paths=None, train_data_weights=None, valid_data_weights=None, test_data_weights=None, weight_by_num_documents=False, weighted_sampler_alpha=1.0, data_impl='mmap', mmap_warmup=False, save='checkpoints', s3_path=None, s3_chunk_size=104857600, config_files={'125M.yml': '# GPT-2 pretraining setup\\n{\\n   # parallelism settings ( you will want to change these based on your cluster setup, ideally scheduling pipeline stages\\n   # across the node boundaries )\\n   \"pipe_parallel_size\": 1,\\n   \"model_parallel_size\": 1,\\n\\n   # model settings\\n   \"num_layers\": 12,\\n   \"hidden_size\": 768,\\n   \"num_attention_heads\": 12,\\n   \"seq_length\": 2048,\\n   \"max_position_embeddings\": 2048,\\n   \"norm\": \"layernorm\",\\n   \"pos_emb\": \"rotary\",\\n   \"no_weight_tying\": true,\\n   \"gpt_j_residual\": false,\\n   \"output_layer_parallelism\": \"column\",\\n\\n   # these should provide some speedup but takes a while to build, set to true if desired\\n   \"scaled_upper_triang_masked_softmax_fusion\": false,\\n   \"bias_gelu_fusion\": false,\\n   \"rope_fusion\": false,\\n\\n   # init methods\\n   \"init_method\": \"small_init\",\\n   \"output_layer_init_method\": \"wang_init\",\\n\\n\\n   # optimizer settings\\n   \"optimizer\": {\\n     \"type\": \"Adam\",\\n     \"params\": {\\n       \"lr\": 0.0006,\\n       \"betas\": [0.9, 0.95],\\n       \"eps\": 1.0e-8,\\n     }\\n   },\\n   \"min_lr\": 0.00006,\\n\\n   # for all zero_optimization options, see https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training\\n   \"zero_optimization\": {\\n    \"stage\": 1,\\n    \"allgather_partitions\": True,\\n    \"allgather_bucket_size\": 500000000,\\n    \"overlap_comm\": True,\\n    \"reduce_scatter\": True,\\n    \"reduce_bucket_size\": 500000000,\\n    \"contiguous_gradients\": True,\\n  },\\n\\n   # batch / data settings\\n   \"train_micro_batch_size_per_gpu\": 4,\\n   \"data_impl\": \"mmap\",\\n\\n   # activation checkpointing\\n   \"checkpoint_activations\": true,\\n   \"checkpoint_num_layers\": 1,\\n   \"partition_activations\": true,\\n   \"synchronize_each_layer\": true,\\n\\n   # regularization\\n   \"gradient_clipping\": 1.0,\\n   \"weight_decay\": 0.1,\\n   \"hidden_dropout\": 0.0,\\n   \"attention_dropout\": 0.0,\\n\\n   # precision settings\\n   \"fp16\": {\\n     \"enabled\": true,\\n     \"loss_scale\": 0,\\n     \"loss_scale_window\": 1000,\\n     \"hysteresis\": 2,\\n     \"min_loss_scale\": 1\\n   },\\n\\n   # misc. training settings\\n   \"train_iters\": 320000,\\n   \"lr_decay_iters\": 320000,\\n   \"distributed_backend\": \"nccl\",\\n   \"lr_decay_style\": \"cosine\",\\n   \"warmup\": 0.01,\\n   \"checkpoint_factor\": 10...\\n{\\n  \"global_num_gpus\": 1\\n}\\n'}, load='checkpoints', checkpoint_validation_with_forward_pass=False, checkpoint_scale='linear', checkpoint_factor=10000, extra_save_iters=None, no_save_optim=False, no_save_rng=False, no_load_optim=False, no_load_rng=False, finetune=False, batch_size=4, train_iters=320000, eval_iters=10, keep_last_n_checkpoints=4, eval_interval=1000, split='969, 30, 1', vocab_file='data/gpt2-vocab.json', merge_file='data/gpt2-merges.txt', num_workers=2, exit_interval=None, attention_dropout=0.0, hidden_dropout=0.0, weight_decay=0.1, checkpoint_activations=True, checkpoint_num_layers=1, deepspeed_activation_checkpointing=True, contiguous_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=True, profile_backward=False, partition_activations=True, gas=1, clip_grad=1.0, hysteresis=2, dynamic_loss_scale=True, loss_scale=None, loss_scale_window=1000.0, min_scale=1.0, char_level_ppl=False, use_mup=False, coord_check=False, save_base_shapes=False, base_shapes_file=None, mup_init_scale=1.0, mup_attn_temp=1.0, mup_output_temp=1.0, mup_embedding_mult=1.0, mup_rp_embedding_mult=1.0, mup_width_scale=2, tokenizer_type='GPT2BPETokenizer', padded_vocab_size=None, optimizer_type='Adam', use_bnb_optimizer=False, zero_stage=1, zero_reduce_scatter=True, zero_contiguous_gradients=True, zero_reduce_bucket_size=500000000, zero_allgather_bucket_size=500000000, lr=0.0006, lr_decay_style='cosine', lr_decay_iters=320000, min_lr=6e-05, warmup=0.01, override_lr_scheduler=False, use_checkpoint_lr_scheduler=False, precision='fp16', num_layers=12, hidden_size=768, num_attention_heads=12, seq_length=2048, max_position_embeddings=2048, norm='layernorm', use_qk_layernorm=False, layernorm_epsilon=1e-05, rms_norm_epsilon=1e-08, scalenorm_epsilon=1e-08, pos_emb='rotary', rpe_num_buckets=32, rpe_max_distance=128, opt_pos_emb_offset=0, no_weight_tying=True, attention_config=['global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global'], sparsity_config={}, num_unique_layers=None, param_sharing_style='grouped', make_vocab_size_divisible_by=128, activation='gelu', scaled_upper_triang_masked_softmax_fusion=False, scaled_masked_softmax_fusion=False, bias_gelu_fusion=False, bias_dropout_fusion=False, rope_fusion=False, fp16_lm_cross_entropy=False, init_method_std=0.02, apply_query_key_layer_scaling=False, use_cpu_initialization=False, attention_softmax_in_fp32=False, rotary_pct=1.0, rotary_emb_base=10000, init_method='small_init', output_layer_init_method='wang_init', gmlp_attn_dim=64, gpt_j_residual=False, gpt_j_tied=False, use_bias_in_norms=True, use_bias_in_attn_linear=True, mlp_type='regular', soft_prompt_tuning=None, output_layer_parallelism='column', deepspeed=True, train_batch_size=4, train_micro_batch_size_per_gpu=4, gradient_accumulation_steps=1, optimizer={'type': 'Adam', 'params': {'lr': 0.0006, 'betas': [0.9, 0.95], 'eps': 1e-08}}, scheduler=None, fp32_allreduce=False, prescale_gradients=False, gradient_predivide_factor=1.0, sparse_gradients=False, fp16={'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'hysteresis': 2, 'min_loss_scale': 1}, bf16=None, amp=None, gradient_clipping=1.0, zero_optimization={'stage': 1, 'allgather_partitions': True, 'allgather_bucket_size': 500000000, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 500000000, 'contiguous_gradients': True}, curriculum_learning=None, curriculum_seqlen=0, steps_per_print=10, wall_clock_breakdown=True, dump_state=False, flops_profiler=None, communication_data_type=None, autotuning=None, activation_checkpointing=None, sparse_attention=None, data_efficiency=None, tensorboard=None, wandb=None, csv_monitor=None, elasticity=None, comms_logger=None, compression_training=None, checkpoint=None, data_types=None, deepspeed_extra_args=None, hostfile='/mock_path', include=None, exclude=None, num_nodes=-1, num_gpus=None, master_port=29500, master_addr=None, launcher='pdsh', force_multi=False, detect_nvlink_pairs=False, autotuning_run=None, no_ssh_check=False, comment=None, account=None)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def run_neox_args_load_test(yaml_files):\n",
      "    from megatron.neox_arguments import NeoXArgs\n",
      "    yaml_list = get_configs_with_path(yaml_files)\n",
      "    args_loaded = NeoXArgs.from_ymls(yaml_list)\n",
      "    assert isinstance(args_loaded, NeoXArgs)\n",
      "    config = dict()\n",
      "    for conf_file_name in yaml_list:\n",
      "        with open(conf_file_name) as conf_file:\n",
      "            conf = yaml.load(conf_file, Loader=yaml.FullLoader)\n",
      "        for conf_key, conf_value in conf.items():\n",
      "            if conf_key in config:\n",
      "                raise ValueError(\n",
      "                    f\"Conf file {conf_file_name} has the following duplicate keys with previously loaded file: {conf_key}\"\n",
      "                )\n",
      "            conf_key_converted = conf_key.replace(\n",
      "                \"-\", \"_\"\n",
      "            )\n",
      "            config[conf_key_converted] = conf_value\n",
      "    for k, v in config.items():\n",
      "        neox_args_value = getattr(args_loaded, k)\n",
      "        assert v == neox_args_value, (\n",
      "            \"loaded neox args value \"\n",
      "            + str(k)\n",
      "            + \" == \"\n",
      "            + str(neox_args_value)\n",
      "            + \" different from config file \"\n",
      "            + str(v)\n",
      "        )\n",
      "run_neox_args_load_test(yaml_files=['125M.yml', 'local_setup.yml', 'cpu_mock_config.yml'])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "assert isinstance(args_loaded, NeoXArgs)\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_pointers(sizes):\n",
      "                    pointers = np.zeros(len(sizes), dtype=np.int64)\n",
      "                    sizes = np.array(sizes, dtype=np.int64)\n",
      "                    np.cumsum(sizes[:-1], out=pointers[1:])\n",
      "                    pointers = pointers * dtype().itemsize\n",
      "                    return pointers\n",
      "_get_pointers(sizes=[1545], dtype=<class 'numpy.uint16'>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "pointers = np.zeros(len(sizes), dtype=np.int64)\n",
      "State:\n",
      "array([0])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_pointers(sizes):\n",
      "                    pointers = np.zeros(len(sizes), dtype=np.int64)\n",
      "                    sizes = np.array(sizes, dtype=np.int64)\n",
      "                    np.cumsum(sizes[:-1], out=pointers[1:])\n",
      "                    pointers = pointers * dtype().itemsize\n",
      "                    return pointers\n",
      "_get_pointers(sizes=[1545], dtype=<class 'numpy.uint16'>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "sizes = np.array(sizes, dtype=np.int64)\n",
      "State:\n",
      "array([1545])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __get(self, url, session, headers):\n",
      "        h = {}\n",
      "        if headers:\n",
      "            for k, v in headers.items():\n",
      "                h[k] = v\n",
      "        if self.headers:\n",
      "            for k, v in self.headers.items():\n",
      "                h[k] = v\n",
      "        resp = session.get(url, headers=h, **self.requests_kwargs)\n",
      "        if not resp.ok:\n",
      "            raise WechatSogouRequestsException('WechatSogouAPI get error', resp)\n",
      "        return resp\n",
      "__get(self=<wechatsogou.api.WechatSogouAPI object at 0x7f3e6c1d4370>, url='http://weixin.sogou.com/weixin?type=1&page=1&ie=utf8&query=%E9%AB%98%E8%80%83', session={headers={'User-Agent': 'python-requests/2.31.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}, auth=None, proxies={}, hooks={'response': []}, params={}, stream=False, verify=True, cert=None, max_redirects=30, trust_env=True, cookies=<RequestsCookieJar[]>, adapters=OrderedDict([('https://', <requests.adapters.HTTPAdapter object at 0x7f3e6a69cb50>), ('http://', <requests.adapters.HTTPAdapter object at 0x7f3e6a69c4f0>)])}, headers={'Cookie': 'SUV=None;SNUID=None;'}, self.captcha_break_times=3, self.headers={'User-Agent': 'Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52'}, self.requests_kwargs={})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "h = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __get(self, url, session, headers):\n",
      "        h = {}\n",
      "        if headers:\n",
      "            for k, v in headers.items():\n",
      "                h[k] = v\n",
      "        if self.headers:\n",
      "            for k, v in self.headers.items():\n",
      "                h[k] = v\n",
      "        resp = session.get(url, headers=h, **self.requests_kwargs)\n",
      "        if not resp.ok:\n",
      "            raise WechatSogouRequestsException('WechatSogouAPI get error', resp)\n",
      "        return resp\n",
      "__get(self=<wechatsogou.api.WechatSogouAPI object at 0x7f3e6c1d4370>, url='http://weixin.sogou.com/weixin?type=1&page=1&ie=utf8&query=%E9%AB%98%E8%80%83', session={headers={'User-Agent': 'python-requests/2.31.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}, auth=None, proxies={}, hooks={'response': []}, params={}, stream=False, verify=True, cert=None, max_redirects=30, trust_env=True, cookies=<RequestsCookieJar[]>, adapters=OrderedDict([('https://', <requests.adapters.HTTPAdapter object at 0x7f3e6a69cb50>), ('http://', <requests.adapters.HTTPAdapter object at 0x7f3e6a69c4f0>)])}, headers={'Cookie': 'SUV=None;SNUID=None;'}, self.captcha_break_times=3, self.headers={'User-Agent': 'Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52'}, self.requests_kwargs={})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "h[k] = v\n",
      "State:\n",
      "{'Cookie': 'SUV=None;SNUID=None;'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __handle_content_url(content_url):\n",
      "        content_url = replace_html(content_url)\n",
      "        return ('http://mp.weixin.qq.com{}'.format(\n",
      "            content_url) if 'http://mp.weixin.qq.com' not in content_url else content_url) if content_url else ''\n",
      "__handle_content_url(content_url='/s?timestamp=1500903767&amp;src=3&amp;ver=1&amp;signature=X4l0IQ091w0DY2ERU7fD*h0VUwBxeHPOJH-Uk-vAfaPamMl6ij7fqAIHomnXQ2X2*2J94H0pixVjsjEkL0TbILtKInZ4hqPp3-lC1nQZcN9Fd*BGbTQp7WlZyzLvCXy0Z8yFVF*lIDlo75pemv7kW8wov4Hz5-uiVzBT5q*Nwaw=')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "content_url = replace_html(content_url)\n",
      "State:\n",
      "'/s?timestamp=1500903767&src=3&ver=1&signature=X4l0IQ091w0DY2ERU7fD*h0VUwBxeHPOJH-Uk-vAfaPamMl6ij7fqAIHomnXQ2X2*2J94H0pixVjsjEkL0TbILtKInZ4hqPp3-lC1nQZcN9Fd*BGbTQp7WlZyzLvCXy0Z8yFVF*lIDlo75pemv7kW8wov4Hz5-uiVzBT5q*Nwaw='\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_article_detail(text, del_qqmusic=True, del_voice=True):\n",
      "        html_obj = BeautifulSoup(text, \"lxml\")\n",
      "        content_text = html_obj.find('div', {'class': 'rich_media_content', 'id': 'js_content'})\n",
      "        if del_qqmusic:\n",
      "            qqmusic = content_text.find_all('qqmusic') or []\n",
      "            for music in qqmusic:\n",
      "                music.parent.decompose()\n",
      "        if del_voice:\n",
      "            voices = content_text.find_all('mpvoice') or []\n",
      "            for voice in voices:\n",
      "                voice.parent.decompose()\n",
      "        all_img_set = set()\n",
      "        all_img_element = content_text.find_all('img') or []\n",
      "        for ele in all_img_element:\n",
      "            img_url = format_image_url(ele.attrs['data-src'])\n",
      "            del ele.attrs['data-src']\n",
      "            ele.attrs['src'] = img_url\n",
      "            if not img_url.startswith('http'):\n",
      "                raise WechatSogouException('img_url [{}] '.format(img_url))\n",
      "            all_img_set.add(img_url)\n",
      "        backgroud_image = content_text.find_all(style=re.compile(\"background-image\")) or []\n",
      "        for ele in backgroud_image:\n",
      "            if ele.attrs.get('data-src'):\n",
      "                del ele.attrs['data-src']\n",
      "            if ele.attrs.get('data-wxurl'):\n",
      "                del ele.attrs['data-wxurl']\n",
      "            img_url = re.findall(backgroud_image_p, str(ele))\n",
      "            if not img_url:\n",
      "                continue\n",
      "            all_img_set.add(img_url[0])\n",
      "        all_img_element = content_text.find_all('iframe') or []\n",
      "        for ele in all_img_element:\n",
      "            img_url = ele.attrs['data-src']\n",
      "            del ele.attrs['data-src']\n",
      "            ele.attrs['src'] = img_url\n",
      "        all_img_list = list(all_img_set)\n",
      "        content_html = content_text.prettify()\n",
      "        content_html = re.findall(js_content, content_html)[0][0]\n",
      "        return {\n",
      "            'content_html': content_html,\n",
      "            'content_img_list': all_img_list\n",
      "        }\n",
      "get_article_detail(text='<!DOCTYPE html>\\n<!--headTrap<body></body><head></head><html></html>-->\\n<html>\\n<head>\\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\\n    <meta name=\"viewport\"\\n          content=\"width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=0,viewport-fit=cover\">\\n    <meta name=\"apple-mobile-web-app-capable\" content=\"yes\">\\n    <meta name=\"apple-mobile-web-app-status-bar-style\" content=\"black\">\\n    <meta name=\"format-detection\" content=\"telephone=no\">\\n\\n\\n    <script nonce=\"1926782896\" type=\"text/javascript\">\\n        window.logs = {\\n            pagetime: {}\\n        };\\n        window.logs.pagetime[\\'html_begin\\'] = (+new Date());\\n    </script>\\n\\n    <script nonce=\"1926782896\" type=\"text/javascript\">\\n        var biz = \"\" || \"ODUzMjkwMzYx\";\\n        var sn = \"\" || \"\" || \"\";\\n        var mid = \"\" || \"\" || \"2654593305\";\\n        var idx = \"\" || \"\" || \"1\";\\n        window.__allowLoadResFromMp = true;\\n\\n    </script>\\n    <script nonce=\"1926782896\" type=\"text/javascript\">\\n        var page_begintime = +new Date, is_rumor = \"\", norumor = \"\";\\n        1 * is_rumor && !(1 * norumor) && biz && mid && (document.referrer && -1 != document.referrer.indexOf(\"mp.weixin.qq.com/mp/rumor\") || (location.href = \"http://mp.weixin.qq.com/mp/rumor?action=info&__biz=\" + biz + \"&mid=\" + mid + \"&idx=\" + idx + \"&sn=\" + sn + \"\n",
      "\n",
      "n:\n",
      "25\n",
      "Statement:\n",
      "html_obj = BeautifulSoup(text, \"lxml\")\n",
      "State:\n",
      "<!DOCTYPE html><!--headTrap<body></body><head></head><html></html>--><html><head><meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/><meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/><meta content=\"width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=0,viewport-fit=cover\" name=\"viewport\"/><meta content=\"yes\" name=\"apple-mobile-web-app-capable\"/><meta content=\"black\" name=\"apple-mobile-web-app-status-bar-style\"/><meta content=\"telephone=no\" name=\"format-detection\"/><script nonce=\"1926782896\" type=\"text/javascript\">        window.logs = {            pagetime: {}        };        window.logs.pagetime['html_begin'] = (+new Date());    </script><script nonce=\"1926782896\" type=\"text/javascript\">        var biz = \"\" || \"ODUzMjkwMzYx\";        var sn = \"\" || \"\" || \"\";        var mid = \"\" || \"\" || \"2654593305\";        var idx = \"\" || \"\" || \"1\";        window.__allowLoadResFromMp = true;    </script><script nonce=\"1926782896\" type=\"text/javascript\">        var page_begintime = +new Date, is_rumor = \"\", norumor = \"\";        1 * is_rumor && !(1 * norumor) && biz && mid && (document.referrer && -1 != document.referrer.indexOf(\"mp.weixin.qq.com/mp/rumor\") || (location.href = \"http://mp.weixin.qq.com/mp/rumor?action=info&__biz=\" + biz + \"&mid=\" + mid + \"&idx=\" + idx + \"&sn=\" + sn + \"#wechat_redirect\")),                document.domain = \"qq.com\";    </script><script nonce=\"1926782896\" type=\"text/javascript\">        var MutationObserver = window.WebKitMutationObserver || window.MutationObserver || window.MozMutationObserver, isDangerSrc = function (t) {            if (t) {                var e = t.match(/http(?:s)?:\\/\\/([^\\/]+?)(\\/|$)/);                if (e && !/qq\\.com(\\:8080)?$/.test(e[1]) && !/weishi\\.com$/.test(e[1]))return !0;            }            return !1;        }, ishttp = 0 == location.href.indexOf(\"http://\");        -1 == location.href.indexOf(\"safe=0\") && ishttp && \"function\" == typeof MutationObserver && \"mp.weixin.qq.com\" == location.host && (window.__observer_data = {            count: 0,            exec_time: 0,            list: []        }, window.__observer = new MutationObserver(function (t) {            window.__observer_data.count++;            var e = new Date, r = [];            t.forEach(function (t) {                for (var e = t.addedNodes, o = 0; o < e.length; o++) {                    var n = e[o];                    if (\"SCRIPT\" === n.tagName) {                        var i = n.src;                        isDangerSrc(i) && (window.__observer_data.list.push(i), r.push(n)), !i && window.__nonce_str && n.getAttribute(\"nonce\") != window.__nonce_str && (window.__observer_data.list.push(\"inlinescript_without_nonce\"),                                r.push(n));                    }                }            });            for (var o = 0; o < r.length; o++) {                var n = r[o];                n.parentNode && n.parentNode.removeChild(n);            }            window.__observer_data.exec_time += new Date - e;        }), window.__observer.observe(document, {            subtree: !0,            childList: !0        })), function () {            if (-1 == location.href.indexOf(\"safe=0\") && Math.random() < .01 && ishttp && HTMLScriptElement.prototype.__lookupSetter__ && \"undefined\" != typeof Object.defineProperty) {                window.__danger_src = {                    xmlhttprequest: [],                    script_src: [],                    script_setAttribute: []                };                var t = \"$\" + Math.random();                HTMLScriptElement.prototype.__old_method_script_src = HTMLScriptElement.prototype.__lookupSetter__(\"src\"),                        HTMLScriptElement.prototype.__defineSetter__(\"src\", function (t) {                            t && isDangerSrc(t) && window.__danger_src.script_src.push(t), this.__old_method_script_src(t);                        });                var e = \"element_setAttribute\" + t;                Object.defineProperty(Element.prototype, e, {                    value: Element.prototype.setAttribute,                   ...t/javascript\", a.async = !0, a.down_time = +new Date, a.onerror = function (s) {                                    t(i, \"status\", \"error\"), t(i, \"end\", +new Date);                                    var c = new Error(s);                                    if (n >= 0)if (u > n) {                                        var m = o.replace(\"res.wx.qq.com\", \"mp.weixin.qq.com\");                                        g.request(m, n, r);                                    } else g.request(o, n, r); else window.__moon_report && window.__moon_report([{                                        offset: d,                                        log: \"load_script_error: \" + o,                                        e: c                                    }], 1);                                    if (n == u - 1 && window.__moon_report([{                                                offset: _,                                                log: \"load_script_error: \" + o,                                                e: c                                            }], 1), -1 == n) {                                        var l = \"ua: \" + window.navigator.userAgent + \", time=\" + (+new Date - a.down_time) + \", load_script_error -1 : \" + o;                                        window.__moon_report([{                                            offset: w,                                            log: l                                        }], 1);                                    }                                    window.__moonclientlog.push(\"moon load js error : \" + o + \", error -> \" + c.toString()),                                            e(\"moon_request_error url:\" + o);                                }, \"undefined\" != typeof moon_crossorigin && moon_crossorigin && a.setAttribute(\"crossorigin\", !0),                                        a.onload = a.onreadystatechange = function () {                                            t(i, \"status\", \"loaded\"), t(i, \"end\", +new Date), !a || a.readyState && !/loaded|complete/.test(a.readyState) || (t(i, \"status\", \"200\"),                                                    a.onload = a.onreadystatechange = null, \"function\" == typeof r && r());                                        }, n--, c.appendChild(a), e(\"moon_request url:\" + o + \" retry:\" + n);                            }                        },                        setItem: function (e, o) {                            !!s && s.setItem(e, o);                        },                        clear: function () {                            s && (a(s, function (e, o) {                                ~o.indexOf(g.prefix) && s.removeItem(o);                            }), console.debug && console.debug(\"[moon] clear\"));                        },                        idkeyReport: function (e, o, t) {                            t = t || 1;                            var n = e + \"_\" + o + \"_\" + t;                            (new Image).src = \"/mp/jsmonitor?idkey=\" + n + \"&r=\" + Math.random();                        }                    };                    seajs && seajs.use && \"string\" == typeof window.__moon_mainjs && seajs.use(window.__moon_mainjs),                            window.moon = g;                }(window), function () {                    try {                        Math.random() < 1;                    } catch (e) {                    }                }(), window.moon.init();            };            e(), !!window.__moon_initcallback && window.__moon_initcallback(), window.__wxgspeeds && (window.__wxgspeeds.moonendtime = +new Date);        }    }    __moonf__();}, 25);</script><script nonce=\"1926782896\" type=\"text/javascript\">    var real_show_page_time = +new Date();    if (!!window.addEventListener) {        window.addEventListener(\"load\", function () {            window.onload_endtime = +new Date();        });    }</script></body><script nonce=\"1926782896\" type=\"text/javascript\">document.addEventListener(\"touchstart\", function () {}, false);</script></html><!--tailTrap<body></body><head></head><html></html>-->\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parser_dnb(data):\n",
      "    data = re.split('<tr>', data)\n",
      "    recs = {}\n",
      "    recs['Authors'] = []\n",
      "    try:\n",
      "        for line in data:\n",
      "            line = line.replace('\\n', ' ').replace('\\t', '')\n",
      "            if len(recs) == 4:\n",
      "                break\n",
      "            elif re.search(r\"<strong>Person.+</strong>\", line):\n",
      "                authors = re.findall('</td>(.*)</td', line)[0]\n",
      "                authors = authors.replace('<td >', '')\n",
      "                authors = re.split('<br/>', authors)\n",
      "                for auth in authors:\n",
      "                    if 'href' in auth:\n",
      "                        auth = re.findall(r'<a href=\".*\" >(.*)</a>', auth)[0]\n",
      "                    auth = u(re.sub(r'\\(.*?\\)', '', auth))\n",
      "                    recs['Authors'].append(auth)\n",
      "            elif re.search(r\"<strong>Verlag</strong>\", line):\n",
      "                publisher = re.findall('td .*>(.*)</td', line)[0]\n",
      "                recs['Publisher'] = u(publisher)\n",
      "            elif re.search(r\"<strong>Titel</strong\", line):\n",
      "                title = re.findall('td .*>(.*)/.*</td', line)[0]\n",
      "                title = u(title.replace('td >', '').replace('</td', ''))\n",
      "                recs['Title'] = u(title)\n",
      "            elif re.search(r\"<strong>Zeitliche Einordnung</strong\", line):\n",
      "                recs['Year'] = u(re.findall(r'\\d{4}', line)[0])\n",
      "            elif line == '':\n",
      "                continue\n",
      "    except IndexError:\n",
      "        LOGGER.debug('Check the parsing for German DNB (possible error!)')\n",
      "    try:\n",
      "        if not recs['Title'] and not recs['Authors']:\n",
      "            recs = {}\n",
      "    except KeyError:\n",
      "        recs = {}\n",
      "    return recs\n",
      "parser_dnb(data='<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"de\" lang=\"de\" dir=\"ltr\">\\n\\n\\t<head>\\n\\t\\t\\t\\t\\t\\n\\t\\t<!-- openSearch -->\\n      <link rel=\"search\"\\n\\t\\t\\ttype=\"application/opensearchdescription+xml\"\\n\\t\\t\\thref=\"/static/opensearch.osxml\" title=\"DNB Katalog\" />\\n\\n\\n\\t\\t<title>DNB, Fehler</title>\\t\\t\\n\\t\\t\\t\\t<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\" />\\n\\n\\t\\t<meta name=\"keywords\" content=\"Deutsche Nationalbibliothek, Die Deutsche Bibliothek, Suche, Katalogsuche\" />\\n\\t\\t<meta name=\"DC.subject\" content=\"Deutsche Nationalbibliothek\" />\\n\\t\\t<meta name=\"DC.subject\" content=\"Online-Katalog\" />\\n\\t\\t<meta name=\"DC.subject\" content=\"Recherche\" />\\n\\t\\t<meta name=\"DC.format\" content=\"text/html\" />\\n\\t\\t<meta name=\"DC.language\" content=\"ger\" />\\n\\t\\t<meta name=\"DC.publisher\" content=\"Deutsche Nationalbibliothek\" />\\n\\t\\t<meta name=\"DC.rights\" content=\"Copyright Deutsche Nationalbibliothek 2008\" />\\n\\t\\t<meta name=\"DC.type\" content=\"Text\" />\\n\\t\\t<meta name=\"DC.title\" content=\"Fehler\" />\\n\\t\\t<meta name=\"copyright\" content=\"Deutsche Nationalbibliothek\" />\\n\\t\\t<meta name=\"keywords\" content=\"search, Suche\" />\\n\\t\\t<meta name=\"generator\" content=\"\" />\\n\\t\\t<meta name=\"date\" content=\"\" />\\n\\t\\t<meta name=\"Robots\" content=\"index,follow\" />\\n\\t\\t\\n\\t\\t<link rel=\"chapter\" href=\"https://www.dnb.de/url/Aktuell/aktuell.html\" title=\"Aktuell\" />\\t\\t\\t\\n\\t\\t<link rel=\"chapter\" href=\"https://www.dnb.de/url/Netzpublikationen/hilfe/netzpublikationen.html\" title=\"Netzpublikationen\" />\\n\\t\\t<link rel=\"chapter\" href=\"https://www.dnb.de/url/Wir/sondersammlungen.html\" title=\"Kataloge und Sammlungen\" />\\n\\t\\t<link rel=\"chapter\" href=\"https://www.dnb.de/url/Service/service.html\" title=\"Service\" />\\n\\t\\t<link rel=\"chapter\" href=\"https://www.dnb.de/url/Standardisierung/standardisierung.html\" title=\"Standardisierung\" />\\n\\t\\t<link rel=\"chapter\" href=\"https://www.dnb.de/url/Wir/wir.html\" title=\"Wir ber uns\" />\\n\\t\\t<link rel=\"start\" href=\"https://www.dnb.de/DE/Home/home_node.html\" title=\"Startseite\" />\\n\\t\\t<link rel=\"shortcut icon\" href=\"/favicon.ico\" type=\"image/x-icon\" />\\n\\t\\t\\n\\t\\t<style type=\"text/css\" media=\"all\">\\n\\t\\t<!--\\n\\t\\t@import url(\"/static/css/layout_2col_73.css\");\\n   \\t\\t\\t\\t@import url(\"/static/css/webservice.css\");\\n\\t\\t@import url(\"/static/css/dnb.css\");\\t\\n\\t\\n\\t\\t //-->\\n\\t\\t\\n\\t\\t</style>\\n\\t\\t\\n<style type=\"text/css\">\\ninput.link {\\n\\tcolor:\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "data = re.split('<tr>', data)  # split rows in table into lines for loop\n",
      "State:\n",
      "['<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"de\" lang=\"de\" dir=\"ltr\">\\n\\n\\t<head>\\n\\t\\t\\t\\t\\t\\n\\t\\t<!-- openSearch -->\\n      <link rel=\"search\"\\n\\t\\t\\ttype=\"application/opensearchdescription+xml\"\\n\\t\\t\\thref=\"/static/opensearch.osxml\" title=\"DNB Katalog\" />\\n\\n\\n\\t\\t<title>DNB, Fehler</title>\\t\\t\\n\\t\\t\\t\\t<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\" />\\n\\n\\t\\t<meta name=\"keywords\" content=\"Deutsche Nationalbibliothek, Die Deutsche Bibliothek, Suche, Katalogsuche\" />\\n\\t\\t<meta name=\"DC.subject\" content=\"Deutsche Nationalbibliothek\" />\\n\\t\\t<meta name=\"DC.subject\" content=\"Online-Katalog\" />\\n\\t\\t<meta name=\"DC.subject\" content=\"Recherche\" />\\n\\t\\t<meta name=\"DC.format\" content=\"text/html\" />\\n\\t\\t<meta name=\"DC.language\" content=\"ger\" />\\n\\t\\t<meta name=\"DC.publisher\" content=\"Deutsche Nationalbibliothek\" />\\n\\t\\t<meta name=\"DC.rights\" content=\"Copyright Deutsche Nationalbibliothek 2008\" />\\n\\t\\t<meta name=\"DC.type\" content=\"Text\" />\\n\\t\\t<meta name=\"DC.title\" content=\"Fehler\" />\\n\\t\\t<meta name=\"copyright\" content=\"Deutsche Nationalbibliothek\" />\\n\\t\\t<meta name=\"keywords\" content=\"search, Suche\" />\\n\\t\\t<meta name=\"generator\" content=\"\" />\\n\\t\\t<meta name=\"date\" content=\"\" />\\n\\t\\t<meta name=\"Robots\" content=\"index,follow\" />\\n\\t\\t\\n\\t\\t<link rel=\"chapter\" href=\"https://www.dnb.de/url/Aktuell/aktuell.html\" title=\"Aktuell\" />\\t\\t\\t\\n\\t\\t<link rel=\"chapter\" href=\"https://www.dnb.de/url/Netzpublikationen/hilfe/netzpublikationen.html\" title=\"Netzpublikationen\" />\\n\\t\\t<link rel=\"chapter\" href=\"https://www.dnb.de/url/Wir/sondersammlungen.html\" title=\"Kataloge und Sammlungen\" />\\n\\t\\t<link rel=\"chapter\" href=\"https://www.dnb.de/url/Service/service.html\" title=\"Service\" />\\n\\t\\t<link rel=\"chapter\" href=\"https://www.dnb.de/url/Standardisierung/standardisierung.html\" title=\"Standardisierung\" />\\n\\t\\t<link rel=\"chapter\" href=\"https://www.dnb.de/url/Wir/wir.html\" title=\"Wir ber uns\" />\\n\\t\\t<link rel=\"start\" href=\"https://www.dnb.de/DE/Home/home_node.html\" title=\"Startseite\" />\\n\\t\\t<link rel=\"shortcut icon\" href=\"/favicon.ico\" type=\"image/x-icon\" />\\n\\t\\t\\n\\t\\t<style type=\"text/css\" media=\"all\">\\n\\t\\t<!--\\n\\t\\t@import url(\"/static/css/layout_2col_73.css\");\\n   \\t\\t\\t\\t@import url(\"/static/css/webservice.css\");\\n\\t\\t@import url(\"/static/css/dnb.css\");\\t\\n\\t\\n\\t\\t //-->\\n\\t\\t\\n\\t\\t</style>\\n\\t\\t\\n<style type=\"text/css\">\\ninput.link {\\n\\tcolor: #4160a5;\\n\\tbackground: transparent;\\n\\tborder: none;\\n\\tcursor: pointer;\\n\\tfont-size: 1.0em;\\n}\\n\\ninput.link:hover {\\n\\ttext-decoration: underline;\\n}\\n\\n.portalmessage_INFORMATION {\\n\\tcolor: #4160A5;\\n}\\n\\n.portalmessage_INFORMATION pre {\\n\\tfont-family: verdana,arial,sans-serif;\\n\\tfont-size: 1.0em !important;\\n\\tmargin: 0px;\\n}\\n\\n.portalmessage_WARNUNG {\\n\\tfont-weight:bold;\\n\\tcolor: red;\\n}\\n\\n.portalmessage_WARNUNG pre {\\n\\tfont-family: verdana,arial,sans-serif;\\n\\tfont-size: 1.0em !important;\\n\\tmargin: 0px;\\n}\\n\\n.portalmessage_HINWEIS {\\n\\tfont-weight:bold;\\n\\tcolor: #4160A5;\\n}\\n\\n.portalmessage_HINWEIS pre {\\n\\tfont-family: verdana,arial,sans-serif;\\n\\tfont-size: 1.0em !important;\\n\\tmargin: 0px;\\n}\\n\\n*[readonly]{\\n\\tbackground-color: #dddddd;\\n}\\n</style>\\n\\t\\n\\t\\t<style type=\"text/css\" media=\"screen\">\\n\\t\\t<!--\\n\\t\\t@import url(\"/static/css/style.css\");\\t\\t\\n\\t\\t //-->\\n\\t\\t</style>\\n\\t\\t\\n\\t\\t<link rel=\"stylesheet\" type=\"text/css\" media=\"print\" href=\"/static/css/print.css\" />\\n\\t\\t\\n\\t\\t<script type=\"text/javascript\" src=\"/static/js/onLoadBody.js\"></script>\\n\\t\\t<script type=\"text/javascript\" src=\"/static/js/rememberPosition.js\"></script>\\n\\t\\t<script type=\"text/javascript\" src=\"/static/js/popups.js\"></script>\\n\\t\\t<script type=\"text/javascript\" src=\"/static/js/sendForm.js\"></script>\\n\\t\\t<script type=\"text/javascript\" src=\"/static/js/help.js\"></script>\\n\\t\\t\\t\\t<script type=\"text/javascript\" src=\"/static/js/formFunctions.js\"></script>\\n\\t\\t<script type=\"text/javascript\" src=\"/static/js/ch...andler.java:255)\\n\\t\\t\\tio.undertow.servlet@2.2.19.Final//io.undertow.servlet.handlers.ServletInitialHandler.access$000(ServletInitialHandler.java:79)\\n\\t\\t\\tio.undertow.servlet@2.2.19.Final//io.undertow.servlet.handlers.ServletInitialHandler$1.handleRequest(ServletInitialHandler.java:100)\\n\\t\\t\\tio.undertow.core@2.2.19.Final//io.undertow.server.Connectors.executeRootHandler(Connectors.java:387)\\n\\t\\t\\tio.undertow.core@2.2.19.Final//io.undertow.server.HttpServerExchange$1.run(HttpServerExchange.java:852)\\n\\t\\t\\torg.jboss.threads@2.4.0.Final//org.jboss.threads.ContextClassLoaderSavingRunnable.run(ContextClassLoaderSavingRunnable.java:35)\\n\\t\\t\\torg.jboss.threads@2.4.0.Final//org.jboss.threads.EnhancedQueueExecutor.safeRun(EnhancedQueueExecutor.java:1990)\\n\\t\\t\\torg.jboss.threads@2.4.0.Final//org.jboss.threads.EnhancedQueueExecutor$ThreadBody.doRunTask(EnhancedQueueExecutor.java:1486)\\n\\t\\t\\torg.jboss.threads@2.4.0.Final//org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1377)\\n\\t\\t\\torg.jboss.xnio@3.8.7.Final//org.xnio.XnioWorker$WorkerThreadFactory$1$1.run(XnioWorker.java:1282)\\n\\t\\t\\tjava.base/java.lang.Thread.run(Thread.java:840)\\n\\t\\t\\t\\t\\t-->\\n\\n\\t\\t\\t\\t<div class=\"link\">\\n\\t\\t\\t\\t\\t<ul>\\n\\t\\t\\t\\t\\t\\t<li><a id=\"continueAfterErrorLink\" href=\"https://portal.dnb.de:443//\">Weiter</a></li>\\n\\t\\t\\t\\t\\t</ul>\\n\\t\\t\\t\\t</div>\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\n\\n\\t\\t\\t<div class=\"clear\"> </div>\\n\\t\\t\\t<a href=\"/emailMe/showEmail?context=DEFAULT\" title=\"E-Mail an Administration\"><img src=\"/static/bilder/icon_email_klein.gif\" height=\"10\" width=\"17\" alt=\"E-Mail-Icon\" />Administration</a>\\n\\t\\t</div> \\n\\t\\t<!-- Ende div id=\"content\" -->\\n\\n\\t\\t<div class=\"invisible\">\\n\\t\\t\\t<a href=\"https://portal.dnb.de:443/opac/showFullRecord#inhalt\" accesskey=\"i\" title=\"zum Inhalt springen, Acceskey: alt+i\"><img src=\"/static/bilder/transparent.gif\" title=\"zum Inhalt springen, Acceskey: alt+i\" alt=\"\" height=\"1\" width=\"1\" /></a>\\n\\t\\t\\t<a href=\"https://portal.dnb.de:443/opac/showFullRecord#hauptmenu\" accesskey=\"n\" title=\"zur Hauptnavigation springen, Acceskey: alt+n\"><img src=\"/static/bilder/transparent.gif\" title=\"zur Hauptnavigation springen, Acceskey: alt+n\" alt=\"\" height=\"1\" width=\"1\" /></a>\\n\\t\\t\\t<a href=\"https://portal.dnb.de:443/opac/showFullRecord#metamenu\" accesskey=\"m\" title=\"zur Metanavigation springen, Acceskey: alt+m\"><img src=\"/static/bilder/transparent.gif\" title=\"zur Metanavigation springen, Acceskey: alt+m\" alt=\"\" height=\"1\" width=\"1\" /></a>\\n\\t\\t\\t<a href=\"https://portal.dnb.de:443/opac/showFullRecord#pfadnavigation\" accesskey=\"p\" title=\"zur Pfadnavigation springen, Acceskey: alt+p\"><img src=\"/static/bilder/transparent.gif\" title=\"zur Pfadnavigation springen, Acceskey: alt+p\" alt=\"\" height=\"1\" width=\"1\" /></a>\\n\\t\\t\\t<a href=\"https://portal.dnb.de:443/opac/showFullRecord#suchformular\" accesskey=\"q\" title=\"zum Suchformular springen, Acceskey: alt+q\"><img src=\"/static/bilder/transparent.gif\" title=\"zum Suchformular springen, Acceskey: alt+q\" alt=\"\" height=\"1\" width=\"1\" /></a>\\n\\t\\t</div>\\t\\t<div id=\"footer\">\\t\\t\\t\\n\\t\\t\\t<div class=\"footer_banner\"><img src=\"/static/bilder/banner_fuss.gif\" alt=\"Schriftbanner mit Deutscher Nationalbibliothek Leipzig, Frankfurt am Main\" height=\"11\" width=\"461\" /></div>\\n\\t\\t\\t<div class=\"footer_jump\">Version 1.8.0.18 / 2024-02-13T15:58:54<a href=\"https://portal.dnb.de:443/opac/showFullRecord#header\" title=\"Sprungmarke zum Seitenbeginn\" tabindex=\"1000\" shape=\"rect\">Seitenanfang</a></div>\\t\\t\\t\\n\\t\\t</div>\\n\\n<!-- Zhlpixel fr Deutsche Bibliotheksstatistik -->\\n<script type=\"text/javascript\" src=\"https://data-8ec206415a.dnb.de/iomm/latest/manager/base/es5/bundle.js\"></script>\\n<script type=\"text/javascript\">\\nIOMm(\\'configure\\', { st: \\'ag292\\', dn: \\'data-8ec206415a.dnb.de\\', mh: 5, ct: \\'0000810000\\' });\\nIOMm(\\'pageview\\', { cp: \\'portal\\' });\\n</script>\\n\\n\\n<!--  Chat -->\\n<script async type=\"text/javascript\" src=\"https://userlike-cdn-widgets.s3-eu-west-1.amazonaws.com/4193b3db51c9445284434823108e8892e66a89d0c22c4343a8eae30c0127ec1e.js\"></script>\\n\\t</body>\\n</html>\\n\\n']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parser_dnb(data):\n",
      "    data = re.split('<tr>', data)\n",
      "    recs = {}\n",
      "    recs['Authors'] = []\n",
      "    try:\n",
      "        for line in data:\n",
      "            line = line.replace('\\n', ' ').replace('\\t', '')\n",
      "            if len(recs) == 4:\n",
      "                break\n",
      "            elif re.search(r\"<strong>Person.+</strong>\", line):\n",
      "                authors = re.findall('</td>(.*)</td', line)[0]\n",
      "                authors = authors.replace('<td >', '')\n",
      "                authors = re.split('<br/>', authors)\n",
      "                for auth in authors:\n",
      "                    if 'href' in auth:\n",
      "                        auth = re.findall(r'<a href=\".*\" >(.*)</a>', auth)[0]\n",
      "                    auth = u(re.sub(r'\\(.*?\\)', '', auth))\n",
      "                    recs['Authors'].append(auth)\n",
      "            elif re.search(r\"<strong>Verlag</strong>\", line):\n",
      "                publisher = re.findall('td .*>(.*)</td', line)[0]\n",
      "                recs['Publisher'] = u(publisher)\n",
      "            elif re.search(r\"<strong>Titel</strong\", line):\n",
      "                title = re.findall('td .*>(.*)/.*</td', line)[0]\n",
      "                title = u(title.replace('td >', '').replace('</td', ''))\n",
      "                recs['Title'] = u(title)\n",
      "            elif re.search(r\"<strong>Zeitliche Einordnung</strong\", line):\n",
      "                recs['Year'] = u(re.findall(r'\\d{4}', line)[0])\n",
      "            elif line == '':\n",
      "                continue\n",
      "    except IndexError:\n",
      "        LOGGER.debug('Check the parsing for German DNB (possible error!)')\n",
      "    try:\n",
      "        if not recs['Title'] and not recs['Authors']:\n",
      "            recs = {}\n",
      "    except KeyError:\n",
      "        recs = {}\n",
      "    return recs\n",
      "parser_dnb(data='<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"de\" lang=\"de\" dir=\"ltr\">\\n\\n\\t<head>\\n\\t\\t\\t\\t\\t\\n\\t\\t<!-- openSearch -->\\n      <link rel=\"search\"\\n\\t\\t\\ttype=\"application/opensearchdescription+xml\"\\n\\t\\t\\thref=\"/static/opensearch.osxml\" title=\"DNB Katalog\" />\\n\\n\\n\\t\\t<title>DNB, Fehler</title>\\t\\t\\n\\t\\t\\t\\t<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\" />\\n\\n\\t\\t<meta name=\"keywords\" content=\"Deutsche Nationalbibliothek, Die Deutsche Bibliothek, Suche, Katalogsuche\" />\\n\\t\\t<meta name=\"DC.subject\" content=\"Deutsche Nationalbibliothek\" />\\n\\t\\t<meta name=\"DC.subject\" content=\"Online-Katalog\" />\\n\\t\\t<meta name=\"DC.subject\" content=\"Recherche\" />\\n\\t\\t<meta name=\"DC.format\" content=\"text/html\" />\\n\\t\\t<meta name=\"DC.language\" content=\"ger\" />\\n\\t\\t<meta name=\"DC.publisher\" content=\"Deutsche Nationalbibliothek\" />\\n\\t\\t<meta name=\"DC.rights\" content=\"Copyright Deutsche Nationalbibliothek 2008\" />\\n\\t\\t<meta name=\"DC.type\" content=\"Text\" />\\n\\t\\t<meta name=\"DC.title\" content=\"Fehler\" />\\n\\t\\t<meta name=\"copyright\" content=\"Deutsche Nationalbibliothek\" />\\n\\t\\t<meta name=\"keywords\" content=\"search, Suche\" />\\n\\t\\t<meta name=\"generator\" content=\"\" />\\n\\t\\t<meta name=\"date\" content=\"\" />\\n\\t\\t<meta name=\"Robots\" content=\"index,follow\" />\\n\\t\\t\\n\\t\\t<link rel=\"chapter\" href=\"https://www.dnb.de/url/Aktuell/aktuell.html\" title=\"Aktuell\" />\\t\\t\\t\\n\\t\\t<link rel=\"chapter\" href=\"https://www.dnb.de/url/Netzpublikationen/hilfe/netzpublikationen.html\" title=\"Netzpublikationen\" />\\n\\t\\t<link rel=\"chapter\" href=\"https://www.dnb.de/url/Wir/sondersammlungen.html\" title=\"Kataloge und Sammlungen\" />\\n\\t\\t<link rel=\"chapter\" href=\"https://www.dnb.de/url/Service/service.html\" title=\"Service\" />\\n\\t\\t<link rel=\"chapter\" href=\"https://www.dnb.de/url/Standardisierung/standardisierung.html\" title=\"Standardisierung\" />\\n\\t\\t<link rel=\"chapter\" href=\"https://www.dnb.de/url/Wir/wir.html\" title=\"Wir ber uns\" />\\n\\t\\t<link rel=\"start\" href=\"https://www.dnb.de/DE/Home/home_node.html\" title=\"Startseite\" />\\n\\t\\t<link rel=\"shortcut icon\" href=\"/favicon.ico\" type=\"image/x-icon\" />\\n\\t\\t\\n\\t\\t<style type=\"text/css\" media=\"all\">\\n\\t\\t<!--\\n\\t\\t@import url(\"/static/css/layout_2col_73.css\");\\n   \\t\\t\\t\\t@import url(\"/static/css/webservice.css\");\\n\\t\\t@import url(\"/static/css/dnb.css\");\\t\\n\\t\\n\\t\\t //-->\\n\\t\\t\\n\\t\\t</style>\\n\\t\\t\\n<style type=\"text/css\">\\ninput.link {\\n\\tcolor:\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "recs = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_basic_h_tree():\n",
      "    result = HTree()\n",
      "    dot_node = HNode(hcontent=HMacro(value=Macro(macro_const=DOT_ELEM_TYPE)),\n",
      "                     htree=result)\n",
      "    result.root = dot_node\n",
      "    literal_node = HNode(hcontent=HMacro(value=Macro(macro_const=LITERAL_ELEM_TYPE)),\n",
      "                         htree=result)\n",
      "    iri_node = HNode(hcontent=HMacro(value=Macro(macro_const=IRI_ELEM_TYPE)),\n",
      "                     htree=result)\n",
      "    dot_node.add_child(literal_node)\n",
      "    dot_node.add_child(iri_node)\n",
      "    return result\n",
      "get_basic_h_tree()\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "result = HTree()\n",
      "State:\n",
      "{_root=None, _node_index={}}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _subscribe_element(self, hcontent):\n",
      "        str_value = hcontent.str_value\n",
      "        if str_value not in self._node_index:\n",
      "            self._node_index[str_value] = hcontent\n",
      "_subscribe_element(self=<shexer.model.hierarchy_tree.HTree object at 0x7f44690ade80>, hcontent={_hcontent=<shexer.model.hierarchy_tree.HMacro object at 0x7f44690adee0>, _parents={}, _children={}}, self._node_index={}, self._root=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "str_value = hcontent.str_value\n",
      "State:\n",
      "'.'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add_child(self, child):\n",
      "        str_child = child.str_value\n",
      "        if str_child not in self._children:\n",
      "            self._children[str_child] = child\n",
      "            child._parents[self.str_value] = self\n",
      "add_child(self=<shexer.model.hierarchy_tree.HNode object at 0x7f44690adf70>, child={_hcontent=<shexer.model.hierarchy_tree.HMacro object at 0x7f44690b5460>, _parents={}, _children={}}, self._children={}, self._hcontent=<shexer.model.hierarchy_tree.HMacro object at 0x7f44690adee0>, self._parents={})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "str_child = child.str_value\n",
      "State:\n",
      "'LITERAL'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def check_just_one_not_none(*value_refname):\n",
      "    nones=0\n",
      "    for a_tuple in value_refname:\n",
      "        if a_tuple[0] is not None:\n",
      "            nones += 1\n",
      "    if nones != 1:\n",
      "        raise ValueError(error_message_for_non_compatible_references([a_tuple[1] for a_tuple in value_refname]))\n",
      "check_just_one_not_none(value_refname=((None, 'graph_file_input'), (None, 'graph_list_of_files_input'), ('\\n@prefix : <http://example.org/> .\\n@prefix xsd: <http://www.w3.org/2001/XMLSchema\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "nones=0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def check_just_one_not_none(*value_refname):\n",
      "    nones=0\n",
      "    for a_tuple in value_refname:\n",
      "        if a_tuple[0] is not None:\n",
      "            nones += 1\n",
      "    if nones != 1:\n",
      "        raise ValueError(error_message_for_non_compatible_references([a_tuple[1] for a_tuple in value_refname]))\n",
      "check_just_one_not_none(value_refname=((None, 'graph_file_input'), (None, 'graph_list_of_files_input'), ('\\n@prefix : <http://example.org/> .\\n@prefix xsd: <http://www.w3.org/2001/XMLSchema\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "nones += 1\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def check_one_or_zero_not_none(*value_refname):\n",
      "    nones=0\n",
      "    for a_tuple in value_refname:\n",
      "        if a_tuple[0] is not None:\n",
      "            nones += 1\n",
      "    if nones > 1:\n",
      "        raise ValueError(error_message_for_non_compatible_references(\n",
      "            list_of_ref_names=[a_tuple[1] for a_tuple in value_refname],\n",
      "            one_mandatory=False))\n",
      "check_one_or_zero_not_none(value_refname=((None, 'namespaces_dict'), (None, 'namespaces_dict_file')))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "nones=0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_proper_strategy(self):\n",
      "        strategies_list = []\n",
      "        if self._all_classes_mode:\n",
      "            strategies_list.append(AllClasesMode(anotator_ref=self))\n",
      "        if self._target_classes is not None and len(self._target_classes) > 0:\n",
      "            strategies_list.append(TargetClassesMode(anotator_ref=self))\n",
      "        if self._shape_qualifiers_mode:\n",
      "            strategies_list.append(\n",
      "                ShapeQualifiersMode(anotator_ref=self,\n",
      "                                    namespaces_for_qualifiers_props=self._namespaces_for_qualifiers_props))\n",
      "        if len(strategies_list) == 0:\n",
      "            raise ValueError(\"Wrong combination of params when building the instance tracker. There are not target classes\")\n",
      "        if len(strategies_list) == 1:\n",
      "            return strategies_list[0]\n",
      "        else:\n",
      "            return CompoundStrategyMode(anotator_ref=self,\n",
      "                                        list_of_strategies=strategies_list)\n",
      "_get_proper_strategy(self=<shexer.core.instances.annotators.base_annotator.BaseAnnotator object at 0x7f44690cec70>, self._all_classes_mode=True, self._classes_considered_in_htree=None, self._htree=None, self._instance_tracker=<shexer.core.instances.instance_tracker.InstanceTracker object at 0x7f44690ceaf0>, self._instances_dict={}, self._instantiation_property=<shexer.model.property.Property object at 0x7f44690cec10>, self._namespaces_for_qualifiers_props=[], self._shape_qualifiers_mode=False, self._subclass_property=<shexer.model.property.Property object at 0x7f446903b280>, self._target_classes=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "strategies_list = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add_new_class_to_instances_dict(self, class_uri):\n",
      "        if class_uri not in self._instances_dict:\n",
      "            self._instances_dict[class_uri] = set()\n",
      "add_new_class_to_instances_dict(self=<shexer.core.instances.annotators.base_annotator.BaseAnnotator object at 0x7f44690cec70>, class_uri='http://example.org/person', self._all_classes_mode=True, self._classes_considered_in_htree=None, self._htree=None, self._instance_tracker=<shexer.core.instances.instance_tracker.InstanceTracker object at 0x7f44690ceaf0>, self._instances_dict={}, self._instantiation_property=<shexer.model.property.Property object at 0x7f44690cec10>, self._namespaces_for_qualifiers_props=[], self._shape_qualifiers_mode=False, self._strategy_mode=<shexer.core.instances.annotators.strategy_mode.all_classes_mode.AllClasesMode object at 0x7f44690ceeb0>, self._subclass_property=<shexer.model.property.Property object at 0x7f446903b280>, self._target_classes=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self._instances_dict[class_uri] = set()\n",
      "State:\n",
      "{'http://example.org/person': set()}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def glob_absolute_paths(file: Union[str, Path]) -> List[Path]:\n",
      "            path = Path(file)\n",
      "            if not path.is_absolute():\n",
      "                path = base / path\n",
      "            return sorted(path.parent.glob(path.name), key=lambda p: p.stem)\n",
      "glob_absolute_paths(file='aggrid.js', base=PosixPath('/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/zauberzeug+nicegui/zauberzeug+nicegui/nicegui/elements'))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "path = Path(file)\n",
      "State:\n",
      "PosixPath('aggrid.js')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def glob_absolute_paths(file: Union[str, Path]) -> List[Path]:\n",
      "            path = Path(file)\n",
      "            if not path.is_absolute():\n",
      "                path = base / path\n",
      "            return sorted(path.parent.glob(path.name), key=lambda p: p.stem)\n",
      "glob_absolute_paths(file='aggrid.js', base=PosixPath('/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/zauberzeug+nicegui/zauberzeug+nicegui/nicegui/elements'))\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "path = base / path\n",
      "State:\n",
      "PosixPath('/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/zauberzeug+nicegui/zauberzeug+nicegui/nicegui/elements/aggrid.js')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def fill_from_encoding(enc: str) -> List[str]:\n",
      "    lst: List[str] = []\n",
      "    for x in range(256):\n",
      "        try:\n",
      "            lst += (bytes((x,)).decode(enc),)\n",
      "        except Exception:\n",
      "            lst += (chr(x),)\n",
      "    return lst\n",
      "fill_from_encoding(enc='cp1252')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "lst: List[str] = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def rev_encoding(enc: List[str]) -> Dict[str, int]:\n",
      "    rev: Dict[str, int] = {}\n",
      "    for i in range(256):\n",
      "        char = enc[i]\n",
      "        if char == \"\\u0000\":\n",
      "            continue\n",
      "        assert char not in rev, f\"{char} at {i} already at {rev[char]}\"\n",
      "        rev[char] = i\n",
      "    return rev\n",
      "rev_encoding(enc=['\\x00', '\\x01', '\\x02', '\\x03', '\\x04', '\\x05', '\\x06', '\\x07', '\\x08', '\\t', '\\n', '\\x0b', '\\x0c', '\\r', '\\x0e', '\\x0f', '\\x10', '\\x11', '\\x12', '\\x13', '\\x14', '\\x15', '\\x16', '\\x17', '\\x18', '\\x19', '\\x1a', '\\x1b', '\\x1c', '\\x1d', '\\x1e', '\\x1f', ' ', '!', '\"', '\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "rev: Dict[str, int] = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __init__(self, dtype, nbit, group_size=-1):\n",
      "        super().__init__(dtype)\n",
      "        self.nbit = nbit\n",
      "        assert group_size in [-1, 64, 128], f\"Group size {group_size} is not supported.\"\n",
      "        self.group_size = group_size\n",
      "        if tvm.cuda(0).exist:\n",
      "            major, minor = parse_compute_version(tvm.cuda(0).compute_version)\n",
      "            if major == 8:\n",
      "                self.sm = 80\n",
      "            else:\n",
      "                self.sm = 10 * major + minor\n",
      "        else:\n",
      "            self.sm = None\n",
      "        self.do_preprocess = True\n",
      "__init__(self=REPR FAILED, dtype='float16', nbit=4, group_size=-1, __class__=<class 'mlc_llm.quantization.ft_quantization.FTQuantizationSpec'>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "super().__init__(dtype)\n",
      "State:\n",
      "'float16'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __init__(self, dtype, nbit, group_size=-1):\n",
      "        super().__init__(dtype)\n",
      "        self.nbit = nbit\n",
      "        assert group_size in [-1, 64, 128], f\"Group size {group_size} is not supported.\"\n",
      "        self.group_size = group_size\n",
      "        if tvm.cuda(0).exist:\n",
      "            major, minor = parse_compute_version(tvm.cuda(0).compute_version)\n",
      "            if major == 8:\n",
      "                self.sm = 80\n",
      "            else:\n",
      "                self.sm = 10 * major + minor\n",
      "        else:\n",
      "            self.sm = None\n",
      "        self.do_preprocess = True\n",
      "__init__(self=REPR FAILED, dtype='float16', nbit=4, group_size=-1, __class__=<class 'mlc_llm.quantization.ft_quantization.FTQuantizationSpec'>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.nbit = nbit\n",
      "State:\n",
      "4\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __init__(self, dtype, nbit, group_size=-1):\n",
      "        super().__init__(dtype)\n",
      "        self.nbit = nbit\n",
      "        assert group_size in [-1, 64, 128], f\"Group size {group_size} is not supported.\"\n",
      "        self.group_size = group_size\n",
      "        if tvm.cuda(0).exist:\n",
      "            major, minor = parse_compute_version(tvm.cuda(0).compute_version)\n",
      "            if major == 8:\n",
      "                self.sm = 80\n",
      "            else:\n",
      "                self.sm = 10 * major + minor\n",
      "        else:\n",
      "            self.sm = None\n",
      "        self.do_preprocess = True\n",
      "__init__(self=REPR FAILED, dtype='float16', nbit=4, group_size=-1, __class__=<class 'mlc_llm.quantization.ft_quantization.FTQuantizationSpec'>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "self.group_size = group_size\n",
      "State:\n",
      "-1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def show_frameworks(args: list = None) -> None:\n",
      "    api = state_connection.get_api()\n",
      "    frameworks = api.ios_bundles_get_frameworks()\n",
      "    if not _should_include_apple_bundles(args):\n",
      "        frameworks = [f for f in frameworks if not _is_apple_bundle(f['bundle'])]\n",
      "    click.secho(tabulate(\n",
      "        [[\n",
      "            entry['executable'],\n",
      "            entry['bundle'],\n",
      "            entry['version'],\n",
      "            entry['path'] if _should_print_full_path(args) else pretty_concat(entry['path'], 40, True),\n",
      "        ] for entry in frameworks\n",
      "        ], headers=['Executable', 'Bundle', 'Version', 'Path'],\n",
      "    ))\n",
      "show_frameworks(args=['--include-apple-frameworks'])\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "api = state_connection.get_api()\n",
      "State:\n",
      "<MagicMock name='get_api()' id='140041836427824'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def sizeof_fmt(num: float, suffix: str = 'B') -> str:\n",
      "    for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
      "        if abs(num) < 1024.0:\n",
      "            return '%3.1f %s%s' % (num, unit, suffix)\n",
      "        num /= 1024.0\n",
      "    return '%.1f %s%s' % (num, 'Yi', suffix)\n",
      "sizeof_fmt(num=249.0, suffix='B')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n",
      "State:\n",
      "''\n",
      "==================================================\n",
      "Clean Code:\n",
      "def dump_from_base(args: list) -> None:\n",
      "    if len(clean_argument_flags(args)) < 3:\n",
      "        click.secho('Usage: memory dump from_base <base_address> <size_to_dump> <local_destination>', bold=True)\n",
      "        return\n",
      "    base_address = args[0]\n",
      "    memory_size = args[1]\n",
      "    destination = args[2]\n",
      "    if os.path.exists(destination):\n",
      "        click.secho('Destination file {dest} already exists'.format(dest=destination), fg='yellow', bold=True)\n",
      "        if not click.confirm('Override?'):\n",
      "            return\n",
      "    click.secho('Dumping {0} from {1} to {2}'.format(sizeof_fmt(int(memory_size)), base_address, destination),\n",
      "                fg='green', dim=True)\n",
      "    api = state_connection.get_api()\n",
      "    dump = bytearray()\n",
      "    chunks = _get_chunks(int(base_address, 16), int(memory_size), BLOCK_SIZE)\n",
      "    for chunk in chunks:\n",
      "        dump.extend(bytearray(api.memory_dump(chunk[0], chunk[1])))\n",
      "    with open(destination, 'wb') as f:\n",
      "        f.write(dump)\n",
      "    click.secho('Memory dumped to file: {0}'.format(destination), fg='green')\n",
      "dump_from_base(args=['0x00008000', '200', '/foo'])\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "destination = args[2]\n",
      "State:\n",
      "'/foo'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def setUp(self):\n",
      "        self.base_url = u'base'\n",
      "        self.path = subprocess.check_output('mktemp -d',\n",
      "                                            shell=True).splitlines()[0]\n",
      "        self.path = self.path.decode()\n",
      "        print(\"Running tests in {}\".format(self.path))\n",
      "        self.gh_api = github.Api(api_url=self.base_url, user_token='')\n",
      "        self.ws = workspace.Workspace(self.path)\n",
      "        self.config = hs_conifg.Config()\n",
      "        self.syncer = sync.SyncHelper(self.gh_api, self.config)\n",
      "setUp(self=<tests.int.sanity_test.SanityTestCase testMethod=test_empty_org_already_synced_does_nothing>, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7f629f94cca0>, self._subtest=None, self._testMethodDoc='Test a user with a repo locally already synced makes no change', self._testMethodName='test_empty_org_already_synced_does_nothing', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_empty_org_already_synced_does_nothing=<bound method SanityTestCase.test_empty_org_already_synced_does_nothing of <tests.int.sanity_test.SanityTestCase testMethod=test_empty_org_already_synced_does_nothing>>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.base_url = u'base'\n",
      "State:\n",
      "'base'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def setUp(self):\n",
      "        self.base_url = u'base'\n",
      "        self.path = subprocess.check_output('mktemp -d',\n",
      "                                            shell=True).splitlines()[0]\n",
      "        self.path = self.path.decode()\n",
      "        print(\"Running tests in {}\".format(self.path))\n",
      "        self.gh_api = github.Api(api_url=self.base_url, user_token='')\n",
      "        self.ws = workspace.Workspace(self.path)\n",
      "        self.config = hs_conifg.Config()\n",
      "        self.syncer = sync.SyncHelper(self.gh_api, self.config)\n",
      "setUp(self=<tests.int.sanity_test.SanityTestCase testMethod=test_empty_org_already_synced_does_nothing>, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7f629f94cca0>, self._subtest=None, self._testMethodDoc='Test a user with a repo locally already synced makes no change', self._testMethodName='test_empty_org_already_synced_does_nothing', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_empty_org_already_synced_does_nothing=<bound method SanityTestCase.test_empty_org_already_synced_does_nothing of <tests.int.sanity_test.SanityTestCase testMethod=test_empty_org_already_synced_does_nothing>>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.path = subprocess.check_output('mktemp -d',\n",
      "State:\n",
      "b'/tmp/tmp.gCVlUIyvYh'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def setUp(self):\n",
      "        self.base_url = u'base'\n",
      "        self.path = subprocess.check_output('mktemp -d',\n",
      "                                            shell=True).splitlines()[0]\n",
      "        self.path = self.path.decode()\n",
      "        print(\"Running tests in {}\".format(self.path))\n",
      "        self.gh_api = github.Api(api_url=self.base_url, user_token='')\n",
      "        self.ws = workspace.Workspace(self.path)\n",
      "        self.config = hs_conifg.Config()\n",
      "        self.syncer = sync.SyncHelper(self.gh_api, self.config)\n",
      "setUp(self=<tests.int.sanity_test.SanityTestCase testMethod=test_empty_org_already_synced_does_nothing>, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7f629f94cca0>, self._subtest=None, self._testMethodDoc='Test a user with a repo locally already synced makes no change', self._testMethodName='test_empty_org_already_synced_does_nothing', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_empty_org_already_synced_does_nothing=<bound method SanityTestCase.test_empty_org_already_synced_does_nothing of <tests.int.sanity_test.SanityTestCase testMethod=test_empty_org_already_synced_does_nothing>>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "self.path = self.path.decode()\n",
      "State:\n",
      "'/tmp/tmp.gCVlUIyvYh'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def songs_url(self, ids):\n",
      "        quality = Config().get(\"music_quality\")\n",
      "        rate_map = {0: 320000, 1: 192000, 2: 128000}\n",
      "        path = \"/weapi/song/enhance/player/url\"\n",
      "        params = dict(ids=ids, br=rate_map[quality])\n",
      "        return self.request(\"POST\", path, params).get(\"data\", [])\n",
      "songs_url(self=<NEMbox.api.NetEase object at 0x7f925764df70>, ids=[347230, 496619464, 405998841, 28012031], self.header={'Accept': '*/*', 'Accept-Encoding': 'gzip,deflate,sdch', 'Accept-Language': 'zh-CN,zh;q=0.8,gl;q=0.6,zh-TW;q=0.4', 'Connection': 'keep-alive', 'Content-Type': 'application/x-www-form-urlencoded', 'Host': 'music.163.com', 'Referer': 'http://music.163.com', 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.87 Safari/537.36'}, self.session=<CachedSession(DbCache('/home/XXX/.netease-musicbox/nemcache', ...), expire_after=1:00:00, allowable_methods=('GET',))>, self.storage=<NEMbox.storage.Storage object at 0x7f925764deb0>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "quality = Config().get(\"music_quality\")\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def songs_url(self, ids):\n",
      "        quality = Config().get(\"music_quality\")\n",
      "        rate_map = {0: 320000, 1: 192000, 2: 128000}\n",
      "        path = \"/weapi/song/enhance/player/url\"\n",
      "        params = dict(ids=ids, br=rate_map[quality])\n",
      "        return self.request(\"POST\", path, params).get(\"data\", [])\n",
      "songs_url(self=<NEMbox.api.NetEase object at 0x7f925764df70>, ids=[347230, 496619464, 405998841, 28012031], self.header={'Accept': '*/*', 'Accept-Encoding': 'gzip,deflate,sdch', 'Accept-Language': 'zh-CN,zh;q=0.8,gl;q=0.6,zh-TW;q=0.4', 'Connection': 'keep-alive', 'Content-Type': 'application/x-www-form-urlencoded', 'Host': 'music.163.com', 'Referer': 'http://music.163.com', 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.87 Safari/537.36'}, self.session=<CachedSession(DbCache('/home/XXX/.netease-musicbox/nemcache', ...), expire_after=1:00:00, allowable_methods=('GET',))>, self.storage=<NEMbox.storage.Storage object at 0x7f925764deb0>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "rate_map = {0: 320000, 1: 192000, 2: 128000}\n",
      "State:\n",
      "{0: 320000, 1: 192000, 2: 128000}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def songs_url(self, ids):\n",
      "        quality = Config().get(\"music_quality\")\n",
      "        rate_map = {0: 320000, 1: 192000, 2: 128000}\n",
      "        path = \"/weapi/song/enhance/player/url\"\n",
      "        params = dict(ids=ids, br=rate_map[quality])\n",
      "        return self.request(\"POST\", path, params).get(\"data\", [])\n",
      "songs_url(self=<NEMbox.api.NetEase object at 0x7f925764df70>, ids=[347230, 496619464, 405998841, 28012031], self.header={'Accept': '*/*', 'Accept-Encoding': 'gzip,deflate,sdch', 'Accept-Language': 'zh-CN,zh;q=0.8,gl;q=0.6,zh-TW;q=0.4', 'Connection': 'keep-alive', 'Content-Type': 'application/x-www-form-urlencoded', 'Host': 'music.163.com', 'Referer': 'http://music.163.com', 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.87 Safari/537.36'}, self.session=<CachedSession(DbCache('/home/XXX/.netease-musicbox/nemcache', ...), expire_after=1:00:00, allowable_methods=('GET',))>, self.storage=<NEMbox.storage.Storage object at 0x7f925764deb0>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "path = \"/weapi/song/enhance/player/url\"\n",
      "State:\n",
      "'/weapi/song/enhance/player/url'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def songs_url(self, ids):\n",
      "        quality = Config().get(\"music_quality\")\n",
      "        rate_map = {0: 320000, 1: 192000, 2: 128000}\n",
      "        path = \"/weapi/song/enhance/player/url\"\n",
      "        params = dict(ids=ids, br=rate_map[quality])\n",
      "        return self.request(\"POST\", path, params).get(\"data\", [])\n",
      "songs_url(self=<NEMbox.api.NetEase object at 0x7f925764df70>, ids=[347230, 496619464, 405998841, 28012031], self.header={'Accept': '*/*', 'Accept-Encoding': 'gzip,deflate,sdch', 'Accept-Language': 'zh-CN,zh;q=0.8,gl;q=0.6,zh-TW;q=0.4', 'Connection': 'keep-alive', 'Content-Type': 'application/x-www-form-urlencoded', 'Host': 'music.163.com', 'Referer': 'http://music.163.com', 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.87 Safari/537.36'}, self.session=<CachedSession(DbCache('/home/XXX/.netease-musicbox/nemcache', ...), expire_after=1:00:00, allowable_methods=('GET',))>, self.storage=<NEMbox.storage.Storage object at 0x7f925764deb0>)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "params = dict(ids=ids, br=rate_map[quality])\n",
      "State:\n",
      "{'ids': [347230, 496619464, 405998841, 28012031], 'br': 320000}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def request(self, method, path, params={}, default={\"code\": -1}, custom_cookies={}):\n",
      "        endpoint = \"{}{}\".format(BASE_URL, path)\n",
      "        csrf_token = \"\"\n",
      "        for cookie in self.session.cookies:\n",
      "            if cookie.name == \"__csrf\":\n",
      "                csrf_token = cookie.value\n",
      "                break\n",
      "        params.update({\"csrf_token\": csrf_token})\n",
      "        data = default\n",
      "        for key, value in custom_cookies.items():\n",
      "            cookie = self.make_cookie(key, value)\n",
      "            self.session.cookies.set_cookie(cookie)\n",
      "        params = encrypted_request(params)\n",
      "        resp = None\n",
      "        try:\n",
      "            resp = self._raw_request(method, endpoint, params)\n",
      "            data = resp.json()\n",
      "        except requests.exceptions.RequestException as e:\n",
      "            log.error(e)\n",
      "        except ValueError:\n",
      "            log.error(\"Path: {}, response: {}\".format(path, resp.text[:200]))\n",
      "        finally:\n",
      "            return data\n",
      "request(self=<NEMbox.api.NetEase object at 0x7f925764df70>, method='POST', path='/weapi/song/enhance/player/url', params={'ids': [347230, 496619464, 405998841, 28012031], 'br': 320000}, default={'code': -1}, custom_cookies={}, self.header={'Accept': '*/*', 'Accept-Encoding': 'gzip,deflate,sdch', 'Accept-Language': 'zh-CN,zh;q=0.8,gl;q=0.6,zh-TW;q=0.4', 'Connection': 'keep-alive', 'Content-Type': 'application/x-www-form-urlencoded', 'Host': 'music.163.com', 'Referer': 'http://music.163.com', 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.87 Safari/537.36'}, self.session=<CachedSession(DbCache('/home/XXX/.netease-musicbox/nemcache', ...), expire_after=1:00:00, allowable_methods=('GET',))>, self.storage=<NEMbox.storage.Storage object at 0x7f925764deb0>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "endpoint = \"{}{}\".format(BASE_URL, path)\n",
      "State:\n",
      "'http://music.163.com/weapi/song/enhance/player/url'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def request(self, method, path, params={}, default={\"code\": -1}, custom_cookies={}):\n",
      "        endpoint = \"{}{}\".format(BASE_URL, path)\n",
      "        csrf_token = \"\"\n",
      "        for cookie in self.session.cookies:\n",
      "            if cookie.name == \"__csrf\":\n",
      "                csrf_token = cookie.value\n",
      "                break\n",
      "        params.update({\"csrf_token\": csrf_token})\n",
      "        data = default\n",
      "        for key, value in custom_cookies.items():\n",
      "            cookie = self.make_cookie(key, value)\n",
      "            self.session.cookies.set_cookie(cookie)\n",
      "        params = encrypted_request(params)\n",
      "        resp = None\n",
      "        try:\n",
      "            resp = self._raw_request(method, endpoint, params)\n",
      "            data = resp.json()\n",
      "        except requests.exceptions.RequestException as e:\n",
      "            log.error(e)\n",
      "        except ValueError:\n",
      "            log.error(\"Path: {}, response: {}\".format(path, resp.text[:200]))\n",
      "        finally:\n",
      "            return data\n",
      "request(self=<NEMbox.api.NetEase object at 0x7f925764df70>, method='POST', path='/weapi/song/enhance/player/url', params={'ids': [347230, 496619464, 405998841, 28012031], 'br': 320000}, default={'code': -1}, custom_cookies={}, self.header={'Accept': '*/*', 'Accept-Encoding': 'gzip,deflate,sdch', 'Accept-Language': 'zh-CN,zh;q=0.8,gl;q=0.6,zh-TW;q=0.4', 'Connection': 'keep-alive', 'Content-Type': 'application/x-www-form-urlencoded', 'Host': 'music.163.com', 'Referer': 'http://music.163.com', 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.87 Safari/537.36'}, self.session=<CachedSession(DbCache('/home/XXX/.netease-musicbox/nemcache', ...), expire_after=1:00:00, allowable_methods=('GET',))>, self.storage=<NEMbox.storage.Storage object at 0x7f925764deb0>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "csrf_token = \"\"\n",
      "State:\n",
      "''\n",
      "==================================================\n",
      "Clean Code:\n",
      "def request(self, method, path, params={}, default={\"code\": -1}, custom_cookies={}):\n",
      "        endpoint = \"{}{}\".format(BASE_URL, path)\n",
      "        csrf_token = \"\"\n",
      "        for cookie in self.session.cookies:\n",
      "            if cookie.name == \"__csrf\":\n",
      "                csrf_token = cookie.value\n",
      "                break\n",
      "        params.update({\"csrf_token\": csrf_token})\n",
      "        data = default\n",
      "        for key, value in custom_cookies.items():\n",
      "            cookie = self.make_cookie(key, value)\n",
      "            self.session.cookies.set_cookie(cookie)\n",
      "        params = encrypted_request(params)\n",
      "        resp = None\n",
      "        try:\n",
      "            resp = self._raw_request(method, endpoint, params)\n",
      "            data = resp.json()\n",
      "        except requests.exceptions.RequestException as e:\n",
      "            log.error(e)\n",
      "        except ValueError:\n",
      "            log.error(\"Path: {}, response: {}\".format(path, resp.text[:200]))\n",
      "        finally:\n",
      "            return data\n",
      "request(self=<NEMbox.api.NetEase object at 0x7f925764df70>, method='POST', path='/weapi/song/enhance/player/url', params={'ids': [347230, 496619464, 405998841, 28012031], 'br': 320000}, default={'code': -1}, custom_cookies={}, self.header={'Accept': '*/*', 'Accept-Encoding': 'gzip,deflate,sdch', 'Accept-Language': 'zh-CN,zh;q=0.8,gl;q=0.6,zh-TW;q=0.4', 'Connection': 'keep-alive', 'Content-Type': 'application/x-www-form-urlencoded', 'Host': 'music.163.com', 'Referer': 'http://music.163.com', 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.87 Safari/537.36'}, self.session=<CachedSession(DbCache('/home/XXX/.netease-musicbox/nemcache', ...), expire_after=1:00:00, allowable_methods=('GET',))>, self.storage=<NEMbox.storage.Storage object at 0x7f925764deb0>)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "resp = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def aes(text, key):\n",
      "    pad = 16 - len(text) % 16\n",
      "    text = text + bytearray([pad] * pad)\n",
      "    encryptor = AES.new(key, 2, b\"0102030405060708\")\n",
      "    ciphertext = encryptor.encrypt(text)\n",
      "    return base64.b64encode(ciphertext)\n",
      "aes(text=b'{\"ids\": [347230, 496619464, 405998841, 28012031], \"br\": 320000, \"csrf_token\": \"\"}', key=b'0CoJUm6Qyw8W8jud')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "pad = 16 - len(text) % 16\n",
      "State:\n",
      "15\n",
      "==================================================\n",
      "Clean Code:\n",
      "def aes(text, key):\n",
      "    pad = 16 - len(text) % 16\n",
      "    text = text + bytearray([pad] * pad)\n",
      "    encryptor = AES.new(key, 2, b\"0102030405060708\")\n",
      "    ciphertext = encryptor.encrypt(text)\n",
      "    return base64.b64encode(ciphertext)\n",
      "aes(text=b'{\"ids\": [347230, 496619464, 405998841, 28012031], \"br\": 320000, \"csrf_token\": \"\"}', key=b'0CoJUm6Qyw8W8jud')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "text = text + bytearray([pad] * pad)\n",
      "State:\n",
      "b'{\"ids\": [347230, 496619464, 405998841, 28012031], \"br\": 320000, \"csrf_token\": \"\"}\\x0f\\x0f\\x0f\\x0f\\x0f\\x0f\\x0f\\x0f\\x0f\\x0f\\x0f\\x0f\\x0f\\x0f\\x0f'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def aes(text, key):\n",
      "    pad = 16 - len(text) % 16\n",
      "    text = text + bytearray([pad] * pad)\n",
      "    encryptor = AES.new(key, 2, b\"0102030405060708\")\n",
      "    ciphertext = encryptor.encrypt(text)\n",
      "    return base64.b64encode(ciphertext)\n",
      "aes(text=b'{\"ids\": [347230, 496619464, 405998841, 28012031], \"br\": 320000, \"csrf_token\": \"\"}', key=b'0CoJUm6Qyw8W8jud')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "ciphertext = encryptor.encrypt(text)\n",
      "State:\n",
      "b'}8(\\n\\x91;\\xe9\\xca\\x03\\x0b+\\xc3\\xb6\\xb5\\xef\\xc2\\x06\\x0e\\xae?i\\xd3\\x1d\\xd2b\\x9d\\x1c\\xea\\x80\\xf9v\\x1c\\xac{\\x06\\xa2YS*\\xfd\\xbc\\xf6*\\x8fZS\\x12\\xec\\xe8yZ\\x06\\x19d\\x96\\xb6E`\\x11G\\x01V\\xcf\\xa7\\xcf\\xbd\\x90{VxT\\xe3\\x1c\\xc4!\\x96,p3\\xffK\\xc7g\\x17\\xfc\\x0c\\x11\\xdf\\\\\\xff\\xa8\\x84,\\x85j\\x12'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def rsa(text, pubkey, modulus):\n",
      "    text = text[::-1]\n",
      "    rs = pow(int(binascii.hexlify(text), 16), int(pubkey, 16), int(modulus, 16))\n",
      "    return format(rs, \"x\").zfill(256)\n",
      "rsa(text=b'8f0f5370d2e586d8', pubkey='010001', modulus='00e0b509f6259df8642dbc35662901477df22677ec152b5ff68ace615bb7b725152b3ab17a876aea8a5aa76d2e417629ec4ee341f56135fccf695280104e0312ecbda92557c93870114af6c9d05c4f7f0c3685b7a46bee255932575cce10b424d813cfe4875d3e82047b97ddef52741d546b8e289dc6935b3ece0462db0a22b8e7')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "text = text[::-1]\n",
      "State:\n",
      "b'8d685e2d0735f0f8'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def rsa(text, pubkey, modulus):\n",
      "    text = text[::-1]\n",
      "    rs = pow(int(binascii.hexlify(text), 16), int(pubkey, 16), int(modulus, 16))\n",
      "    return format(rs, \"x\").zfill(256)\n",
      "rsa(text=b'8f0f5370d2e586d8', pubkey='010001', modulus='00e0b509f6259df8642dbc35662901477df22677ec152b5ff68ace615bb7b725152b3ab17a876aea8a5aa76d2e417629ec4ee341f56135fccf695280104e0312ecbda92557c93870114af6c9d05c4f7f0c3685b7a46bee255932575cce10b424d813cfe4875d3e82047b97ddef52741d546b8e289dc6935b3ece0462db0a22b8e7')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "rs = pow(int(binascii.hexlify(text), 16), int(pubkey, 16), int(modulus, 16))\n",
      "State:\n",
      "41289025236364532763659893484694654271957746525301846225496173810914971221673940219469794803075175772641409601769538690249948382654879184221263545282891541468281379611086448019401240456345998524768441427254251530526498242614378789439179129682525095499524622736788988351523341804681052326293260583730641212123\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _raw_request(self, method, endpoint, data=None):\n",
      "        resp = None\n",
      "        if method == \"GET\":\n",
      "            resp = self.session.get(\n",
      "                endpoint, params=data, headers=self.header, timeout=DEFAULT_TIMEOUT\n",
      "            )\n",
      "        elif method == \"POST\":\n",
      "            resp = self.session.post(\n",
      "                endpoint, data=data, headers=self.header, timeout=DEFAULT_TIMEOUT\n",
      "            )\n",
      "        return resp\n",
      "_raw_request(self=<NEMbox.api.NetEase object at 0x7f925764df70>, method='POST', endpoint='http://music.163.com/weapi/song/enhance/player/url', data={'params': b'yHwUDrGuHBs0hovjij5z7lCGl7Imo682wv20BfPZeswAKHz1m/y2DhTvPaye6a2atqcWsLNzhBLbqT9R9ub1SU2Fk1s8XEP8ASc4Z+g1xrLssTSGyUi2E20KHUwZ9ORCQjDXtWXiMH8HmxqA7BXttlR0eZQQH5oTuW1LjPFQA4klDdCg/k8qaflXGn72xeLZ', 'encSecKey': '3acc2a8278d1470cb5a0c5c1ba1fb65331fa5b459568f896d2e348f416b871cb15f2ced68cdee0f4b799b74df42aa0591700ab387ec8082d7a177ddeaeec7f25b98db25dd2761bc6856db12bf30ff351f38b97a98db9ad19e9d10abf3909b6935630235c76269759f8811a7118de552f3450138c62e06e7ce592669aac5192db'}, self.header={'Accept': '*/*', 'Accept-Encoding': 'gzip,deflate,sdch', 'Accept-Language': 'zh-CN,zh;q=0.8,gl;q=0.6,zh-TW;q=0.4', 'Connection': 'keep-alive', 'Content-Type': 'application/x-www-form-urlencoded', 'Host': 'music.163.com', 'Referer': 'http://music.163.com', 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.87 Safari/537.36'}, self.session=<CachedSession(DbCache('/home/XXX/.netease-musicbox/nemcache', ...), expire_after=1:00:00, allowable_methods=('GET',))>, self.storage=<NEMbox.storage.Storage object at 0x7f925764deb0>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "resp = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def songs_detail(self, ids):\n",
      "        path = \"/weapi/v3/song/detail\"\n",
      "        params = dict(c=json.dumps([{\"id\": _id} for _id in ids]), ids=json.dumps(ids))\n",
      "        return self.request(\"POST\", path, params).get(\"songs\", [])\n",
      "songs_detail(self=<NEMbox.api.NetEase object at 0x7f925764df70>, ids=[347230, 496619464, 405998841, 28012031], self.header={'Accept': '*/*', 'Accept-Encoding': 'gzip,deflate,sdch', 'Accept-Language': 'zh-CN,zh;q=0.8,gl;q=0.6,zh-TW;q=0.4', 'Connection': 'keep-alive', 'Content-Type': 'application/x-www-form-urlencoded', 'Host': 'music.163.com', 'Referer': 'http://music.163.com', 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.87 Safari/537.36'}, self.session=<CachedSession(DbCache('/home/XXX/.netease-musicbox/nemcache', ...), expire_after=1:00:00, allowable_methods=('GET',))>, self.storage=<NEMbox.storage.Storage object at 0x7f925764deb0>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "path = \"/weapi/v3/song/detail\"\n",
      "State:\n",
      "'/weapi/v3/song/detail'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def songs_detail(self, ids):\n",
      "        path = \"/weapi/v3/song/detail\"\n",
      "        params = dict(c=json.dumps([{\"id\": _id} for _id in ids]), ids=json.dumps(ids))\n",
      "        return self.request(\"POST\", path, params).get(\"songs\", [])\n",
      "songs_detail(self=<NEMbox.api.NetEase object at 0x7f925764df70>, ids=[347230, 496619464, 405998841, 28012031], self.header={'Accept': '*/*', 'Accept-Encoding': 'gzip,deflate,sdch', 'Accept-Language': 'zh-CN,zh;q=0.8,gl;q=0.6,zh-TW;q=0.4', 'Connection': 'keep-alive', 'Content-Type': 'application/x-www-form-urlencoded', 'Host': 'music.163.com', 'Referer': 'http://music.163.com', 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.87 Safari/537.36'}, self.session=<CachedSession(DbCache('/home/XXX/.netease-musicbox/nemcache', ...), expire_after=1:00:00, allowable_methods=('GET',))>, self.storage=<NEMbox.storage.Storage object at 0x7f925764deb0>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "return self.request(\"POST\", path, params).get(\"songs\", [])\n",
      "State:\n",
      "{'c': '[{\"id\": 347230}, {\"id\": 496619464}, {\"id\": 405998841}, {\"id\": 28012031}]', 'ids': '[347230, 496619464, 405998841, 28012031]', 'csrf_token': ''}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __setitem__(self, key, val):\n",
      "        if key.startswith('path.') and not val.startswith('/'):\n",
      "            val = self._absolute_path(val)\n",
      "        dict.__setitem__(self, key, val)\n",
      "__setitem__(self={}, key='path.workdir', val='/home/XXX/.cheat.sh')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "dict.__setitem__(self, key, val)\n",
      "State:\n",
      "'/home/XXX/.cheat.sh'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _load_config_from_file(default_config, filename):\n",
      "    import yaml\n",
      "    update = {}\n",
      "    if not os.path.exists(filename):\n",
      "        return update\n",
      "    with open(filename) as f:\n",
      "        newconfig = yaml.load(f.read(), Loader=yaml.SafeLoader)\n",
      "    for key, val in default_config.items():\n",
      "        newval = _get_nested(newconfig, key)\n",
      "        if newval is None:\n",
      "            continue\n",
      "        if isinstance(val, int):\n",
      "            try:\n",
      "                newval = int(newval)\n",
      "            except (ValueError, TypeError):\n",
      "                continue\n",
      "        update[key] = newval\n",
      "    return update\n",
      "_load_config_from_file(default_config={'adapters.active': ['tldr', 'cheat', 'fosdem', 'translation', 'rosetta', 'late.nz', 'question', 'cheat.sheets', 'cheat.sheets dir', 'learnxiny', 'rfc', 'oeis', 'chmod'], 'adapters.mandatory': ['search'], 'cache.redis.db': 0, 'cache.redis.host': 'localhost', 'cache.redis.port': 6379, 'cache.redis.prefix': '', 'cache.type': 'redis', 'frontend.styles': ['abap', 'algol', 'algol_nu', 'arduino', 'autumn', 'borland', 'bw', 'colorful', 'default', 'dracula', 'emacs', 'friendly', 'friendly_grayscale', 'fruity', 'github-dark', 'gruvbox-dark', 'gruvbox-light', 'igor', 'inkpot', 'lightbulb', 'lilypond', 'lovelace', 'manni', 'material', 'monokai', 'murphy', 'native', 'nord', 'nord-darker', 'one-dark', 'paraiso-dark', 'paraiso-light', 'pastie', 'perldoc', 'rainbow_dash', 'rrt', 'sas', 'solarized-dark', 'solarized-light', 'staroffice', 'stata-dark', 'stata-light', 'tango', 'trac', 'vim', 'vs', 'xcode', 'zenburn'], 'log.level': 4, 'path.internal.ansi2html': '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/chubin+cheat.sh/chubin+cheat.sh/share/ansi2html.sh', 'path.internal.bin': '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/chubin+cheat.sh/chubin+cheat.sh/bin', 'path.internal.bin.upstream': '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/chubin+cheat.sh/chubin+cheat.sh/bin/upstream', 'path.internal.malformed': '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/chubin+cheat.sh/chubin+cheat.sh/share/static/malformed-response.html', 'path.internal.pages': '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/chubin+cheat.sh/chubin+cheat.sh/share', 'path.internal.static': '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/chubin+cheat.sh/chubin+cheat.sh/share/static', 'path.internal.templates': '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/chubin+cheat.sh/chubin+cheat.sh/share/templates', 'path.internal.vim': '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/chubin+cheat.sh/chubin+cheat.sh/share/vim', 'path.log.main': 'log/main.log', 'path.log.queries': 'log/queries.log', 'path.log.fetch': 'log/fetch.log', 'path.repositories': 'upstream', 'path.spool': 'spool', 'path.workdir': '/home/XXX/.cheat.sh', 'routing.pre': [('^$', 'search'), ('^[^/]*/rosetta(/|$)', 'rosetta'), ('^rfc/', 'rfc'), ('^oeis/', 'oeis'), ('^chmod/', 'chmod'), ('^:', 'internal'), ('/:list$', 'internal'), ('/$', 'cheat.sheets dir')], 'routing.main': [('', 'cheat.sheets'), ('', 'cheat'), ('', 'tldr'), ('', 'late.nz'), ('', 'fosdem'), ('', 'learnxiny')], 'routing.post': [('^[^/ +]*$', 'unknown'), ('^[a-z][a-z]-[a-z][a-z]$', 'translation')], 'routing.default': 'question', 'upstream.url': 'https://cheat.sh', 'upstream.timeout': 5, 'search.limit': 20, 'server.bind': '0.0.0.0', 'server.port': 8002}, filename='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/chubin+cheat.sh/chubin+cheat.sh/etc/config.yaml')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "import yaml\n",
      "State:\n",
      "<module 'yaml' from '/local/rcs/XXX/miniforge3/envs/chubin+cheat.sh/lib/python3.9/site-packages/yaml/__init__.py'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def num_parameters(module: nn.Module, requires_grad: Optional[bool] = None) -> int:\n",
      "    total = 0\n",
      "    for p in module.parameters():\n",
      "        if requires_grad is None or p.requires_grad == requires_grad:\n",
      "            if hasattr(p, \"quant_state\"):\n",
      "                total += math.prod(p.quant_state[1])\n",
      "            else:\n",
      "                total += p.numel()\n",
      "    return total\n",
      "num_parameters(module=GPT(  (lm_head): Linear(in_features=8, out_features=8, bias=False)  (transformer): ModuleDict(    (wte): Embedding(8, 8)    (h): ModuleList(      (0-1): 2 x Block(        (norm_1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)        (attn): CausalSelfAttention(          (attn): Linear(in_features=8, out_features=24, bias=True)          (proj): Linear(in_features=8, out_features=8, bias=True)          (adapter_wte): Embedding(10, 8)        )        (norm_2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)        (mlp): GptNeoxMLP(          (fc): Linear(in_features=8, out_features=32, bias=True)          (proj): Linear(in_features=32, out_features=8, bias=True)        )      )    )    (ln_f): LayerNorm((8,), eps=1e-05, elementwise_affine=True)  )), requires_grad=True)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "total = 0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def validate(fabric: L.Fabric, model: GPT, val_data: List[Dict], tokenizer: Tokenizer, max_iters: int) -> torch.Tensor:\n",
      "    fabric.print(\"Validating ...\")\n",
      "    model.eval()\n",
      "    losses = torch.zeros(max_iters)\n",
      "    for k in range(max_iters):\n",
      "        input_ids, targets = get_batch(fabric, val_data)\n",
      "        logits = model(input_ids)\n",
      "        losses[k] = chunked_cross_entropy(logits[..., :-1, :], targets[..., 1:], chunk_size=0)\n",
      "    val_loss = losses.mean()\n",
      "    instruction = \"Recommend a movie for me to watch during the weekend and explain the reason.\"\n",
      "    fabric.print(instruction)\n",
      "    sample = {\"instruction\": instruction, \"input\": \"\"}\n",
      "    prompt = generate_prompt(sample)\n",
      "    encoded = tokenizer.encode(prompt, device=fabric.device)\n",
      "    with fabric.init_tensor():\n",
      "        model.set_kv_cache(batch_size=1)\n",
      "    output = generate(\n",
      "        model, encoded, max_returned_tokens=len(encoded) + eval_max_new_tokens, temperature=0.8, eos_id=tokenizer.eos_id\n",
      "    )\n",
      "    model.clear_kv_cache()\n",
      "    output = tokenizer.decode(output)\n",
      "    fabric.print(output)\n",
      "    model.train()\n",
      "    return val_loss\n",
      "validate(fabric={_connector=<lightning.fabric.connector._Connector object at 0x7f8052456c70>, _strategy=<lightning.fabric.strategies.single_device.SingleDeviceStrategy object at 0x7f8052456640>, _accelerator=<lightning.fabric.accelerators.cuda.CUDAAccelerator object at 0x7f8052456580>, _precision=<lightning.fabric.plugins.precision.precision.Precision object at 0x7f80524568b0>, _callbacks=[], _loggers=[<lightning.fabric.loggers.csv_logs.CSVLogger object at 0x7f80524569d0>], _models_setup=1, _launched=True, run=functools.partial(<bound method Fabric._wrap_and_launch of <lightning.fabric.fabric.Fabric object at 0x7f8052480f40>>, <bound method Fabric.run of <lightning.fabric.fabric.Fabric object at 0x7f8052480f40>>)}, model=_FabricModule(  (_forward_module): GPT(    (lm_head): Linear(in_features=8, out_features=8, bias=False)    (transformer): ModuleDict(      (wte): Embedding(8, 8)      (h): ModuleList(        (0-1): 2 x Block(          (norm_1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)          (attn): CausalSelfAttention(            (attn): Linear(in_features=8, out_features=24, bias=True)            (proj): Linear(in_features=8, out_features=8, bias=True)            (adapter_wte): Embedding(10, 8)          )          (norm_2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)          (mlp): GptNeoxMLP(            (fc): Linear(in_features=8, out_features=32, bias=True)            (proj): Linear(in_features=32, out_features=8, bias=True)          )        )      )      (ln_f): LayerNorm((8,), eps=1e-05, elementwise_affine=True)    )  )  (_original_module): GPT(    (lm_head): Linear(in_features=8, out_features=8, bias=False)    (transformer): ModuleDict(      (wte): Embedding(8, 8)      (h): ModuleList(        (0-1): 2 x Block(          (norm_1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)          (attn): CausalSelfAttention(            (attn): Linear(in_features=8, out_features=24, bias=True)            (proj): Linear(in_features=8, out_features=8, bias=True)            (adapter_wte): Embedding(10, 8)          )          (norm_2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)          (mlp): GptNeoxMLP(            (fc): Linear(in_features=8, out_features=32, bias=True)            (proj): Linear(in_features=32, out_features=8, bias=True)          )        )      )      (ln_f): LayerNorm((8,), eps=1e-05, elementwise_affine=True)    )  )), val_data=[{'input_ids': tensor([0, 1, 2]), 'labels': tensor([1, 2, 3])}, {'input_ids': tensor([1, 2, 3]), 'labels': tensor([2, 3, 4])}], tokenizer=<Mock id='140189112821600'>, max_iters=2)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "losses = torch.zeros(max_iters)\n",
      "State:\n",
      "tensor([0., 0.])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def validate(fabric: L.Fabric, model: GPT, val_data: List[Dict], tokenizer: Tokenizer, max_iters: int) -> torch.Tensor:\n",
      "    fabric.print(\"Validating ...\")\n",
      "    model.eval()\n",
      "    losses = torch.zeros(max_iters)\n",
      "    for k in range(max_iters):\n",
      "        input_ids, targets = get_batch(fabric, val_data)\n",
      "        logits = model(input_ids)\n",
      "        losses[k] = chunked_cross_entropy(logits[..., :-1, :], targets[..., 1:], chunk_size=0)\n",
      "    val_loss = losses.mean()\n",
      "    instruction = \"Recommend a movie for me to watch during the weekend and explain the reason.\"\n",
      "    fabric.print(instruction)\n",
      "    sample = {\"instruction\": instruction, \"input\": \"\"}\n",
      "    prompt = generate_prompt(sample)\n",
      "    encoded = tokenizer.encode(prompt, device=fabric.device)\n",
      "    with fabric.init_tensor():\n",
      "        model.set_kv_cache(batch_size=1)\n",
      "    output = generate(\n",
      "        model, encoded, max_returned_tokens=len(encoded) + eval_max_new_tokens, temperature=0.8, eos_id=tokenizer.eos_id\n",
      "    )\n",
      "    model.clear_kv_cache()\n",
      "    output = tokenizer.decode(output)\n",
      "    fabric.print(output)\n",
      "    model.train()\n",
      "    return val_loss\n",
      "validate(fabric={_connector=<lightning.fabric.connector._Connector object at 0x7f8052456c70>, _strategy=<lightning.fabric.strategies.single_device.SingleDeviceStrategy object at 0x7f8052456640>, _accelerator=<lightning.fabric.accelerators.cuda.CUDAAccelerator object at 0x7f8052456580>, _precision=<lightning.fabric.plugins.precision.precision.Precision object at 0x7f80524568b0>, _callbacks=[], _loggers=[<lightning.fabric.loggers.csv_logs.CSVLogger object at 0x7f80524569d0>], _models_setup=1, _launched=True, run=functools.partial(<bound method Fabric._wrap_and_launch of <lightning.fabric.fabric.Fabric object at 0x7f8052480f40>>, <bound method Fabric.run of <lightning.fabric.fabric.Fabric object at 0x7f8052480f40>>)}, model=_FabricModule(  (_forward_module): GPT(    (lm_head): Linear(in_features=8, out_features=8, bias=False)    (transformer): ModuleDict(      (wte): Embedding(8, 8)      (h): ModuleList(        (0-1): 2 x Block(          (norm_1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)          (attn): CausalSelfAttention(            (attn): Linear(in_features=8, out_features=24, bias=True)            (proj): Linear(in_features=8, out_features=8, bias=True)            (adapter_wte): Embedding(10, 8)          )          (norm_2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)          (mlp): GptNeoxMLP(            (fc): Linear(in_features=8, out_features=32, bias=True)            (proj): Linear(in_features=32, out_features=8, bias=True)          )        )      )      (ln_f): LayerNorm((8,), eps=1e-05, elementwise_affine=True)    )  )  (_original_module): GPT(    (lm_head): Linear(in_features=8, out_features=8, bias=False)    (transformer): ModuleDict(      (wte): Embedding(8, 8)      (h): ModuleList(        (0-1): 2 x Block(          (norm_1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)          (attn): CausalSelfAttention(            (attn): Linear(in_features=8, out_features=24, bias=True)            (proj): Linear(in_features=8, out_features=8, bias=True)            (adapter_wte): Embedding(10, 8)          )          (norm_2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)          (mlp): GptNeoxMLP(            (fc): Linear(in_features=8, out_features=32, bias=True)            (proj): Linear(in_features=32, out_features=8, bias=True)          )        )      )      (ln_f): LayerNorm((8,), eps=1e-05, elementwise_affine=True)    )  )), val_data=[{'input_ids': tensor([0, 1, 2]), 'labels': tensor([1, 2, 3])}, {'input_ids': tensor([1, 2, 3]), 'labels': tensor([2, 3, 4])}], tokenizer=<Mock id='140189112821600'>, max_iters=2)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "encoded = tokenizer.encode(prompt, device=fabric.device)\n",
      "State:\n",
      "tensor([3, 2, 1], device='cuda:0')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def sample(logits: torch.Tensor, temperature: float = 1.0, top_k: Optional[int] = None) -> torch.Tensor:\n",
      "    logits = logits[0, -1]\n",
      "    if top_k is not None:\n",
      "        v, i = torch.topk(logits, min(top_k, logits.size(-1)))\n",
      "        logits = torch.full_like(logits, float(\"-inf\")).scatter_(-1, i, v)\n",
      "    if temperature > 0.0:\n",
      "        probs = torch.nn.functional.softmax(logits / temperature, dim=-1)\n",
      "        return multinomial_num_samples_1(probs)\n",
      "    return torch.argmax(logits, dim=-1, keepdim=True)\n",
      "sample(logits=tensor([[[ 0.2595,  0.0058,  0.6498, -0.5155,  0.0366, -0.2944,  0.0555,          -0.4013],         [ 0.5342, -0.9790, -0.0861,  0.2530,  0.1378, -0.4913,  0.1915,          -0.3248],         [ 0.5289, -0.3149,  0.4052,  0.2107, -0.8788, -0.9871,  0.6539,          -0.2213]]], device='cuda:0'), temperature=0.8, top_k=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "logits = logits[0, -1]\n",
      "State:\n",
      "tensor([ 0.5289, -0.3149,  0.4052,  0.2107, -0.8788, -0.9871,  0.6539, -0.2213],       device='cuda:0')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def clear_kv_cache(self) -> None:\n",
      "        self.mask_cache = None\n",
      "        for block in self.transformer.h:\n",
      "            block.attn.kv_cache = None\n",
      "clear_kv_cache(self=GPT(  (lm_head): Linear(in_features=8, out_features=8, bias=False)  (transformer): ModuleDict(    (wte): Embedding(8, 8)    (h): ModuleList(      (0-1): 2 x Block(        (norm_1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)        (attn): CausalSelfAttention(          (attn): Linear(in_features=8, out_features=24, bias=True)          (proj): Linear(in_features=8, out_features=8, bias=True)          (adapter_wte): Embedding(10, 8)          (kv_cache): KVCache()        )        (norm_2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)        (mlp): GptNeoxMLP(          (fc): Linear(in_features=8, out_features=32, bias=True)          (proj): Linear(in_features=32, out_features=8, bias=True)        )      )    )    (ln_f): LayerNorm((8,), eps=1e-05, elementwise_affine=True)  )), self._backward_hooks=OrderedDict(), self._backward_pre_hooks=OrderedDict(), self._buffers=OrderedDict([('cos', tensor([], device='cuda:0', size=(3, 0))), ('sin', tensor([], device='cuda:0', size=(3, 0)))]), self._forward_hooks=OrderedDict(), self._forward_hooks_always_called=OrderedDict(), self._forward_hooks_with_kwargs=OrderedDict(), self._forward_pre_hooks=OrderedDict(), self._forward_pre_hooks_with_kwargs=OrderedDict(), self._is_full_backward_hook=None, self._load_state_dict_post_hooks=OrderedDict(), self._load_state_dict_pre_hooks=OrderedDict(), self._max_seq_length=3, self._modules=OrderedDict([('lm_head', Linear(in_features=8, out_features=8, bias=False)), ('transformer', ModuleDict(  (wte): Embedding(8, 8)  (h): ModuleList(    (0-1): 2 x Block(      (norm_1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)      (attn): CausalSelfAttention(        (attn): Linear(in_features=8, out_features=24, bias=True)        (proj): Linear(in_features=8, out_features=8, bias=True)        (adapter_wte): Embedding(10, 8)        (kv_cache): KVCache()      )      (norm_2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)      (mlp): GptNeoxMLP(        (fc): Linear(in_features=8, out_features=32, bias=True)        (proj): Linear(in_features=32, out_features=8, bias=True)      )    )  )  (ln_f): LayerNorm((8,), eps=1e-05, elementwise_affine=True)))]), self._non_persistent_buffers_set={'cos', 'sin'}, self._parameters=OrderedDict(), self._state_dict_hooks=OrderedDict(), self._state_dict_pre_hooks=OrderedDict(), self.config=Config(name='', hf_config={}, block_size=128, vocab_size=8, padding_multiple=512, padded_vocab_size=8, n_layer=2, n_head=4, n_embd=8, rotary_percentage=0.25, parallel_residual=True, bias=True, lm_head_bias=False, n_query_groups=4, shared_attention_norm=False, _norm_class='LayerNorm', norm_eps=1e-05, _mlp_class='GptNeoxMLP', gelu_approximate='none', intermediate_size=32, rope_condense_ratio=1, rope_base=10000, n_expert=0, n_expert_per_token=0, adapter_prompt_length=10, adapter_start_layer=0), self.mask_cache=tensor([[[[ True, False, False],          [ True,  True, False],          [ True,  True,  True]]]], device='cuda:0'), self.training=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.mask_cache = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def reset_parameters(self) -> None:\n",
      "        torch.nn.init.zeros_(self.gating_factor)\n",
      "reset_parameters(self=CausalSelfAttention(  (attn): Linear(in_features=12, out_features=36, bias=True)  (proj): Linear(in_features=12, out_features=12, bias=True)  (adapter_wte): Embedding(10, 12)), self._backward_hooks=OrderedDict(), self._backward_pre_hooks=OrderedDict(), self._buffers=OrderedDict(), self._forward_hooks=OrderedDict(), self._forward_hooks_always_called=OrderedDict(), self._forward_hooks_with_kwargs=OrderedDict(), self._forward_pre_hooks=OrderedDict(), self._forward_pre_hooks_with_kwargs=OrderedDict(), self._is_full_backward_hook=None, self._load_state_dict_post_hooks=OrderedDict(), self._load_state_dict_pre_hooks=OrderedDict(), self._modules=OrderedDict([('attn', Linear(in_features=12, out_features=36, bias=True)), ('proj', Linear(in_features=12, out_features=12, bias=True)), ('adapter_wte', Embedding(10, 12))]), self._non_persistent_buffers_set=set(), self._parameters=OrderedDict([('gating_factor', Parameter containing:tensor([[[[1.2300],          [1.2300],          [1.2300],          [1.2300],          [1.2300],          [1.2300]]]], requires_grad=True))]), self._state_dict_hooks=OrderedDict(), self._state_dict_pre_hooks=OrderedDict(), self.adapter_kv_cache=None, self.block_idx=0, self.config=Config(name='', hf_config={}, block_size=1, vocab_size=1, padding_multiple=512, padded_vocab_size=512, n_layer=1, n_head=6, n_embd=12, rotary_percentage=0.25, parallel_residual=True, bias=True, lm_head_bias=False, n_query_groups=6, shared_attention_norm=False, _norm_class='LayerNorm', norm_eps=1e-05, _mlp_class='GptNeoxMLP', gelu_approximate='none', intermediate_size=48, rope_condense_ratio=1, rope_base=10000, n_expert=0, n_expert_per_token=0, adapter_prompt_length=10, adapter_start_layer=0), self.kv_cache=None, self.training=True)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "torch.nn.init.zeros_(self.gating_factor)\n",
      "State:\n",
      "OrderedDict([('gating_factor', Parameter containing:tensor([[[[0.],          [0.],          [0.],          [0.],          [0.],          [0.]]]], requires_grad=True))])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_base_model_can_be_adapter_v2_loaded(name):\n",
      "    from lit_gpt.adapter_v2 import GPT as AdapterV2GPT\n",
      "    from lit_gpt.adapter_v2 import adapter_filter\n",
      "    from lit_gpt.model import GPT as BaseGPT\n",
      "    kwargs = {\"n_layer\": 2, \"n_head\": 8, \"n_embd\": 16, \"padded_vocab_size\": 32}\n",
      "    base_model = BaseGPT.from_name(name, **kwargs)\n",
      "    base_model_state_dict = base_model.state_dict()\n",
      "    lora_model = AdapterV2GPT.from_name(name, **kwargs, adapter_start_layer=0)\n",
      "    keys = lora_model.load_state_dict(base_model_state_dict, strict=False)\n",
      "    assert not keys.unexpected_keys\n",
      "    for k in keys.missing_keys:\n",
      "        assert adapter_filter(k, None)\n",
      "test_base_model_can_be_adapter_v2_loaded(name='stablelm-base-alpha-3b')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "kwargs = {\"n_layer\": 2, \"n_head\": 8, \"n_embd\": 16, \"padded_vocab_size\": 32}\n",
      "State:\n",
      "{'n_layer': 2, 'n_head': 8, 'n_embd': 16, 'padded_vocab_size': 32}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def layer_template(layer_name: str, idx: int) -> Tuple[str, int]:\n",
      "    split = layer_name.split(\".\")\n",
      "    number = int(split[idx])\n",
      "    split[idx] = \"{}\"\n",
      "    from_name = \".\".join(split)\n",
      "    return from_name, number\n",
      "layer_template(layer_name='model.layers.0.self_attn.q_proj.weight', idx=2)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "split = layer_name.split(\".\")\n",
      "State:\n",
      "['model', 'layers', '0', 'self_attn', 'q_proj', 'weight']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def layer_template(layer_name: str, idx: int) -> Tuple[str, int]:\n",
      "    split = layer_name.split(\".\")\n",
      "    number = int(split[idx])\n",
      "    split[idx] = \"{}\"\n",
      "    from_name = \".\".join(split)\n",
      "    return from_name, number\n",
      "layer_template(layer_name='model.layers.0.self_attn.q_proj.weight', idx=2)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "number = int(split[idx])\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def layer_template(layer_name: str, idx: int) -> Tuple[str, int]:\n",
      "    split = layer_name.split(\".\")\n",
      "    number = int(split[idx])\n",
      "    split[idx] = \"{}\"\n",
      "    from_name = \".\".join(split)\n",
      "    return from_name, number\n",
      "layer_template(layer_name='model.layers.0.self_attn.q_proj.weight', idx=2)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "split[idx] = \"{}\"\n",
      "State:\n",
      "['model', 'layers', '{}', 'self_attn', 'q_proj', 'weight']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def layer_template(layer_name: str, idx: int) -> Tuple[str, int]:\n",
      "    split = layer_name.split(\".\")\n",
      "    number = int(split[idx])\n",
      "    split[idx] = \"{}\"\n",
      "    from_name = \".\".join(split)\n",
      "    return from_name, number\n",
      "layer_template(layer_name='model.layers.0.self_attn.q_proj.weight', idx=2)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "from_name = \".\".join(split)\n",
      "State:\n",
      "'model.layers.{}.self_attn.q_proj.weight'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_generate(monkeypatch, generated, stop_tokens, expected):\n",
      "    import chat.base as chat\n",
      "    import generate.base as generate\n",
      "    input_idx = torch.tensor([5, 3])\n",
      "    max_returned_tokens = len(input_idx) + 8\n",
      "    model = MagicMock()\n",
      "    model.config.block_size = 100\n",
      "    model.max_seq_length = 100\n",
      "    it = iter(generated)\n",
      "    def multinomial(*_, **__):\n",
      "        out = next(it)\n",
      "        return torch.tensor([out])\n",
      "    monkeypatch.setattr(generate, \"multinomial_num_samples_1\", multinomial)\n",
      "    actual = chat.generate(model, input_idx, max_returned_tokens, stop_tokens=stop_tokens)\n",
      "    actual = list(actual)\n",
      "    assert len(actual) == len(expected)\n",
      "    if not actual:\n",
      "        assert actual == expected\n",
      "    else:\n",
      "        for t in actual:\n",
      "            assert t.dtype == torch.long\n",
      "        assert torch.cat(actual).tolist() == expected\n",
      "test_generate(monkeypatch={_setattr=[], _setitem=[], _cwd=None, _savesyspath=None}, generated=repeat(1), stop_tokens=(), expected=[1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "import chat.base as chat\n",
      "State:\n",
      "<module 'chat.base' from '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/Lightning-AI+lit-gpt/Lightning-AI+lit-gpt/chat/base.py'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_generate(monkeypatch, generated, stop_tokens, expected):\n",
      "    import chat.base as chat\n",
      "    import generate.base as generate\n",
      "    input_idx = torch.tensor([5, 3])\n",
      "    max_returned_tokens = len(input_idx) + 8\n",
      "    model = MagicMock()\n",
      "    model.config.block_size = 100\n",
      "    model.max_seq_length = 100\n",
      "    it = iter(generated)\n",
      "    def multinomial(*_, **__):\n",
      "        out = next(it)\n",
      "        return torch.tensor([out])\n",
      "    monkeypatch.setattr(generate, \"multinomial_num_samples_1\", multinomial)\n",
      "    actual = chat.generate(model, input_idx, max_returned_tokens, stop_tokens=stop_tokens)\n",
      "    actual = list(actual)\n",
      "    assert len(actual) == len(expected)\n",
      "    if not actual:\n",
      "        assert actual == expected\n",
      "    else:\n",
      "        for t in actual:\n",
      "            assert t.dtype == torch.long\n",
      "        assert torch.cat(actual).tolist() == expected\n",
      "test_generate(monkeypatch={_setattr=[], _setitem=[], _cwd=None, _savesyspath=None}, generated=repeat(1), stop_tokens=(), expected=[1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "import generate.base as generate\n",
      "State:\n",
      "<module 'generate.base' from '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/Lightning-AI+lit-gpt/Lightning-AI+lit-gpt/generate/base.py'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_generate(monkeypatch, generated, stop_tokens, expected):\n",
      "    import chat.base as chat\n",
      "    import generate.base as generate\n",
      "    input_idx = torch.tensor([5, 3])\n",
      "    max_returned_tokens = len(input_idx) + 8\n",
      "    model = MagicMock()\n",
      "    model.config.block_size = 100\n",
      "    model.max_seq_length = 100\n",
      "    it = iter(generated)\n",
      "    def multinomial(*_, **__):\n",
      "        out = next(it)\n",
      "        return torch.tensor([out])\n",
      "    monkeypatch.setattr(generate, \"multinomial_num_samples_1\", multinomial)\n",
      "    actual = chat.generate(model, input_idx, max_returned_tokens, stop_tokens=stop_tokens)\n",
      "    actual = list(actual)\n",
      "    assert len(actual) == len(expected)\n",
      "    if not actual:\n",
      "        assert actual == expected\n",
      "    else:\n",
      "        for t in actual:\n",
      "            assert t.dtype == torch.long\n",
      "        assert torch.cat(actual).tolist() == expected\n",
      "test_generate(monkeypatch={_setattr=[], _setitem=[], _cwd=None, _savesyspath=None}, generated=repeat(1), stop_tokens=(), expected=[1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "input_idx = torch.tensor([5, 3])\n",
      "State:\n",
      "tensor([5, 3])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_generate(monkeypatch, generated, stop_tokens, expected):\n",
      "    import chat.base as chat\n",
      "    import generate.base as generate\n",
      "    input_idx = torch.tensor([5, 3])\n",
      "    max_returned_tokens = len(input_idx) + 8\n",
      "    model = MagicMock()\n",
      "    model.config.block_size = 100\n",
      "    model.max_seq_length = 100\n",
      "    it = iter(generated)\n",
      "    def multinomial(*_, **__):\n",
      "        out = next(it)\n",
      "        return torch.tensor([out])\n",
      "    monkeypatch.setattr(generate, \"multinomial_num_samples_1\", multinomial)\n",
      "    actual = chat.generate(model, input_idx, max_returned_tokens, stop_tokens=stop_tokens)\n",
      "    actual = list(actual)\n",
      "    assert len(actual) == len(expected)\n",
      "    if not actual:\n",
      "        assert actual == expected\n",
      "    else:\n",
      "        for t in actual:\n",
      "            assert t.dtype == torch.long\n",
      "        assert torch.cat(actual).tolist() == expected\n",
      "test_generate(monkeypatch={_setattr=[], _setitem=[], _cwd=None, _savesyspath=None}, generated=repeat(1), stop_tokens=(), expected=[1, 1, 1, 1, 1, 1, 1, 1])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "max_returned_tokens = len(input_idx) + 8\n",
      "State:\n",
      "10\n",
      "==================================================\n",
      "Clean Code:\n",
      "def decode(fabric: L.Fabric, tokenizer: Tokenizer, token_stream: Iterator[torch.Tensor]) -> int:\n",
      "    tokens_generated = 0\n",
      "    if tokenizer.backend == \"huggingface\":\n",
      "        try:\n",
      "            for token in token_stream:\n",
      "                fabric.print(tokenizer.decode(token), end=\"\", flush=True)\n",
      "                tokens_generated += 1\n",
      "        except KeyboardInterrupt:\n",
      "            return tokens_generated\n",
      "    elif tokenizer.backend == \"sentencepiece\":\n",
      "        so_far = torch.tensor([], dtype=torch.long, device=fabric.device)\n",
      "        decoded_so_far = \"\"\n",
      "        try:\n",
      "            for token in token_stream:\n",
      "                so_far = so_far.to(device=token.device)\n",
      "                so_far = torch.cat((so_far, token.view(-1)))\n",
      "                decoded_new = tokenizer.decode(so_far)\n",
      "                fabric.print(decoded_new[len(decoded_so_far) :], end=\"\", flush=True)\n",
      "                decoded_so_far = decoded_new\n",
      "                tokens_generated += 1\n",
      "        except KeyboardInterrupt:\n",
      "            return tokens_generated\n",
      "    else:\n",
      "        raise NotImplementedError(tokenizer.backend)\n",
      "    return tokens_generated\n",
      "decode(fabric={_connector=<lightning.fabric.connector._Connector object at 0x7f7fc9bfb9a0>, _strategy=<lightning.fabric.strategies.single_device.SingleDeviceStrategy object at 0x7f7fca64f0a0>, _accelerator=<lightning.fabric.accelerators.cpu.CPUAccelerator object at 0x7f7fc9bfb910>, _precision=<lightning.fabric.plugins.precision.precision.Precision object at 0x7f7fca64f7c0>, _callbacks=[], _loggers=[], _models_setup=0, _launched=False, run=functools.partial(<bound method Fabric._wrap_and_launch of <lightning.fabric.fabric.Fabric object at 0x7f7fc9bfb970>>, <bound method Fabric.run of <lightning.fabric.fabric.Fabric object at 0x7f7fc9bfb970>>)}, tokenizer={}, token_stream=tensor([3, 2, 1]))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "tokens_generated = 0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_main(mocked_input, stop_iteration, fake_checkpoint_dir, monkeypatch, tensor_like):\n",
      "    import chat.base as chat\n",
      "    mocked_input.side_effect = [\"Hello\", stop_iteration]\n",
      "    config_path = fake_checkpoint_dir / \"lit_config.json\"\n",
      "    config = {\"block_size\": 128, \"vocab_size\": 50, \"n_layer\": 2, \"n_head\": 4, \"n_embd\": 8, \"rotary_percentage\": 1}\n",
      "    config_path.write_text(json.dumps(config))\n",
      "    load_mock = Mock()\n",
      "    load_mock.return_value = load_mock\n",
      "    monkeypatch.setattr(chat, \"load_checkpoint\", load_mock)\n",
      "    tokenizer_mock = Mock()\n",
      "    tokenizer_mock.return_value.backend = \"sentencepiece\"\n",
      "    tokenizer_mock.return_value.encode.return_value = torch.tensor([1, 2, 3])\n",
      "    tokenizer_mock.return_value.decode.return_value = \"foo bar baz\"\n",
      "    monkeypatch.setattr(chat, \"Tokenizer\", tokenizer_mock)\n",
      "    generate_mock = Mock()\n",
      "    generate_mock.return_value = torch.tensor([3, 2, 1])\n",
      "    monkeypatch.setattr(chat, \"generate\", generate_mock)\n",
      "    out, err = StringIO(), StringIO()\n",
      "    with redirect_stdout(out), redirect_stderr(err):\n",
      "        chat.main(temperature=2.0, top_k=2, checkpoint_dir=fake_checkpoint_dir)\n",
      "    assert len(tokenizer_mock.return_value.decode.mock_calls) == generate_mock.return_value.numel()\n",
      "    assert torch.allclose(tokenizer_mock.return_value.decode.call_args[0][0], generate_mock.return_value)\n",
      "    assert generate_mock.mock_calls == [\n",
      "        call(ANY, tensor_like, 128, temperature=2.0, top_k=2, stop_tokens=([tokenizer_mock.return_value.eos_id],))\n",
      "    ]\n",
      "    assert out.getvalue() == \">> Reply: foo bar baz\\n\"\n",
      "    assert \"'padded_vocab_size': 512, 'n_layer': 2, 'n_head': 4\" in err.getvalue()\n",
      "test_main(mocked_input=<MagicMock name='input' id='140186822359552'>, stop_iteration=<class 'KeyboardInterrupt'>, fake_checkpoint_dir=PosixPath('/tmp/pytest-of-XXX/pytest-193/test_main_KeyboardInterrupt_0/checkpoints/tmp'), monkeypatch={_setattr=[], _setitem=[], _cwd=None, _savesyspath=None}, tensor_like={})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "import chat.base as chat\n",
      "State:\n",
      "<module 'chat.base' from '/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/Lightning-AI+lit-gpt/Lightning-AI+lit-gpt/chat/base.py'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_main(mocked_input, stop_iteration, fake_checkpoint_dir, monkeypatch, tensor_like):\n",
      "    import chat.base as chat\n",
      "    mocked_input.side_effect = [\"Hello\", stop_iteration]\n",
      "    config_path = fake_checkpoint_dir / \"lit_config.json\"\n",
      "    config = {\"block_size\": 128, \"vocab_size\": 50, \"n_layer\": 2, \"n_head\": 4, \"n_embd\": 8, \"rotary_percentage\": 1}\n",
      "    config_path.write_text(json.dumps(config))\n",
      "    load_mock = Mock()\n",
      "    load_mock.return_value = load_mock\n",
      "    monkeypatch.setattr(chat, \"load_checkpoint\", load_mock)\n",
      "    tokenizer_mock = Mock()\n",
      "    tokenizer_mock.return_value.backend = \"sentencepiece\"\n",
      "    tokenizer_mock.return_value.encode.return_value = torch.tensor([1, 2, 3])\n",
      "    tokenizer_mock.return_value.decode.return_value = \"foo bar baz\"\n",
      "    monkeypatch.setattr(chat, \"Tokenizer\", tokenizer_mock)\n",
      "    generate_mock = Mock()\n",
      "    generate_mock.return_value = torch.tensor([3, 2, 1])\n",
      "    monkeypatch.setattr(chat, \"generate\", generate_mock)\n",
      "    out, err = StringIO(), StringIO()\n",
      "    with redirect_stdout(out), redirect_stderr(err):\n",
      "        chat.main(temperature=2.0, top_k=2, checkpoint_dir=fake_checkpoint_dir)\n",
      "    assert len(tokenizer_mock.return_value.decode.mock_calls) == generate_mock.return_value.numel()\n",
      "    assert torch.allclose(tokenizer_mock.return_value.decode.call_args[0][0], generate_mock.return_value)\n",
      "    assert generate_mock.mock_calls == [\n",
      "        call(ANY, tensor_like, 128, temperature=2.0, top_k=2, stop_tokens=([tokenizer_mock.return_value.eos_id],))\n",
      "    ]\n",
      "    assert out.getvalue() == \">> Reply: foo bar baz\\n\"\n",
      "    assert \"'padded_vocab_size': 512, 'n_layer': 2, 'n_head': 4\" in err.getvalue()\n",
      "test_main(mocked_input=<MagicMock name='input' id='140186822359552'>, stop_iteration=<class 'KeyboardInterrupt'>, fake_checkpoint_dir=PosixPath('/tmp/pytest-of-XXX/pytest-193/test_main_KeyboardInterrupt_0/checkpoints/tmp'), monkeypatch={_setattr=[], _setitem=[], _cwd=None, _savesyspath=None}, tensor_like={})\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "load_mock = Mock()\n",
      "State:\n",
      "<Mock id='140188336799360'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def from_json(cls, path: Union[str, Path], **kwargs: Any) -> Self:\n",
      "        with open(path, encoding=\"utf-8\") as fp:\n",
      "            json_kwargs = json.load(fp)\n",
      "        if \"condense_ratio\" in json_kwargs:\n",
      "            json_kwargs[\"rope_condense_ratio\"] = json_kwargs.pop(\"condense_ratio\")\n",
      "        if \"condense_ratio\" in kwargs:\n",
      "            kwargs[\"rope_condense_ratio\"] = kwargs.pop(\"condense_ratio\")\n",
      "        if \"org\" in json_kwargs:\n",
      "            json_kwargs[\"hf_config\"] = {\"name\": json_kwargs[\"name\"], \"org\": json_kwargs.pop(\"org\")}\n",
      "        if \"org\" in kwargs:\n",
      "            kwargs[\"hf_config\"] = {\"name\": kwargs.get(\"name\", json_kwargs[\"name\"]), \"org\": kwargs.pop(\"org\")}\n",
      "        json_kwargs.update(kwargs)\n",
      "        return cls(**json_kwargs)\n",
      "from_json(cls=<class 'lit_gpt.config.Config'>, path=PosixPath('/tmp/pytest-of-XXX/pytest-193/test_main_KeyboardInterrupt_0/checkpoints/tmp/lit_config.json'), kwargs={})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "json_kwargs = json.load(fp)\n",
      "State:\n",
      "{'block_size': 128, 'vocab_size': 50, 'n_layer': 2, 'n_head': 4, 'n_embd': 8, 'rotary_percentage': 1}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def convert_lit_checkpoint(checkpoint_path: Path, output_path: Path, config_path: Path) -> None:\n",
      "    config = Config.from_json(config_path)\n",
      "    if \"falcon\" in config.name:\n",
      "        copy_fn = partial(copy_weights_falcon, config.name)\n",
      "    elif config._mlp_class in (\"LLaMAMLP\", \"LLaMAMoE\"):\n",
      "        copy_fn = partial(copy_weights_llama, config)\n",
      "    elif \"phi\" in config.name:\n",
      "        copy_fn = partial(copy_weights_phi, config)\n",
      "    else:\n",
      "        copy_fn = copy_weights_gpt_neox\n",
      "    sd = {}\n",
      "    with incremental_save(output_path) as saver:\n",
      "        lit_weights = lazy_load(checkpoint_path)\n",
      "        lit_weights = lit_weights.get(\"model\", lit_weights)\n",
      "        check_conversion_supported(lit_weights)\n",
      "        copy_fn(sd, lit_weights, saver=saver)\n",
      "        gc.collect()\n",
      "        saver.save(sd)\n",
      "convert_lit_checkpoint(checkpoint_path=PosixPath('/tmp/pytest-of-XXX/pytest-193/test_convert_lit_checkpoint0/foo.ckpt'), output_path=PosixPath('/tmp/pytest-of-XXX/pytest-193/test_convert_lit_checkpoint0/generated.bin'), config_path=PosixPath('/tmp/pytest-of-XXX/pytest-193/test_convert_lit_checkpoint0/foo.json'))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "config = Config.from_json(config_path)\n",
      "State:\n",
      "Config(name='Llama-2-7b-hf', hf_config={'org': 'meta-llama', 'name': 'Llama-2-7b-hf'}, block_size=8, vocab_size=32000, padding_multiple=128, padded_vocab_size=32000, n_layer=2, n_head=2, n_embd=32, rotary_percentage=1.0, parallel_residual=False, bias=False, lm_head_bias=False, n_query_groups=2, shared_attention_norm=False, _norm_class='RMSNorm', norm_eps=1e-05, _mlp_class='LLaMAMLP', gelu_approximate='none', intermediate_size=11008, rope_condense_ratio=1, rope_base=10000, n_expert=0, n_expert_per_token=0)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def convert_lit_checkpoint(checkpoint_path: Path, output_path: Path, config_path: Path) -> None:\n",
      "    config = Config.from_json(config_path)\n",
      "    if \"falcon\" in config.name:\n",
      "        copy_fn = partial(copy_weights_falcon, config.name)\n",
      "    elif config._mlp_class in (\"LLaMAMLP\", \"LLaMAMoE\"):\n",
      "        copy_fn = partial(copy_weights_llama, config)\n",
      "    elif \"phi\" in config.name:\n",
      "        copy_fn = partial(copy_weights_phi, config)\n",
      "    else:\n",
      "        copy_fn = copy_weights_gpt_neox\n",
      "    sd = {}\n",
      "    with incremental_save(output_path) as saver:\n",
      "        lit_weights = lazy_load(checkpoint_path)\n",
      "        lit_weights = lit_weights.get(\"model\", lit_weights)\n",
      "        check_conversion_supported(lit_weights)\n",
      "        copy_fn(sd, lit_weights, saver=saver)\n",
      "        gc.collect()\n",
      "        saver.save(sd)\n",
      "convert_lit_checkpoint(checkpoint_path=PosixPath('/tmp/pytest-of-XXX/pytest-193/test_convert_lit_checkpoint0/foo.ckpt'), output_path=PosixPath('/tmp/pytest-of-XXX/pytest-193/test_convert_lit_checkpoint0/generated.bin'), config_path=PosixPath('/tmp/pytest-of-XXX/pytest-193/test_convert_lit_checkpoint0/foo.json'))\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "lit_weights = lazy_load(checkpoint_path)\n",
      "State:\n",
      "OrderedDict([('lm_head.weight', _NotYetLoadedTensor(tensor(..., device='meta', size=(32000, 32)))), ('transformer.wte.weight', _NotYetLoadedTensor(tensor(..., device='meta', size=(32000, 32)))), ('transformer.h.0.norm_1.weight', _NotYetLoadedTensor(tensor(..., device='meta', size=(32,)))), ('transformer.h.0.attn.attn.weight', _NotYetLoadedTensor(tensor(..., device='meta', size=(96, 32)))), ('transformer.h.0.attn.proj.weight', _NotYetLoadedTensor(tensor(..., device='meta', size=(32, 32)))), ('transformer.h.0.norm_2.weight', _NotYetLoadedTensor(tensor(..., device='meta', size=(32,)))), ('transformer.h.0.mlp.fc_1.weight', _NotYetLoadedTensor(tensor(..., device='meta', size=(11008, 32)))), ('transformer.h.0.mlp.fc_2.weight', _NotYetLoadedTensor(tensor(..., device='meta', size=(11008, 32)))), ('transformer.h.0.mlp.proj.weight', _NotYetLoadedTensor(tensor(..., device='meta', size=(32, 11008)))), ('transformer.h.1.norm_1.weight', _NotYetLoadedTensor(tensor(..., device='meta', size=(32,)))), ('transformer.h.1.attn.attn.weight', _NotYetLoadedTensor(tensor(..., device='meta', size=(96, 32)))), ('transformer.h.1.attn.proj.weight', _NotYetLoadedTensor(tensor(..., device='meta', size=(32, 32)))), ('transformer.h.1.norm_2.weight', _NotYetLoadedTensor(tensor(..., device='meta', size=(32,)))), ('transformer.h.1.mlp.fc_1.weight', _NotYetLoadedTensor(tensor(..., device='meta', size=(11008, 32)))), ('transformer.h.1.mlp.fc_2.weight', _NotYetLoadedTensor(tensor(..., device='meta', size=(11008, 32)))), ('transformer.h.1.mlp.proj.weight', _NotYetLoadedTensor(tensor(..., device='meta', size=(32, 11008)))), ('transformer.ln_f.weight', _NotYetLoadedTensor(tensor(..., device='meta', size=(32,))))])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def store_early(self, tensor):\n",
      "        if isinstance(tensor, torch.Tensor):\n",
      "            return SavingProxyForTensor(tensor, self)\n",
      "        raise TypeError(f\"can only store tensors early, not {type(tensor)}\")\n",
      "store_early(self=<lit_gpt.utils.incremental_save object at 0x7f7fca4b0520>, tensor=tensor([[-0.1665, -0.0347, -0.0849,  ..., -0.1229,  0.0588, -0.0586],        [ 0.1023, -0.0631,  0.0087,  ...,  0.1013,  0.0567, -0.1307],        [-0.0531, -0.0416,  0.1076,  ..., -0.0947,  0.0802, -0.1348],        ...,        [ 0.0274,  0.1068,  0.1416,  ...,  0.0639,  0.0812,  0.1492],        [-0.0855, -0.1713, -0.0230,  ...,  0.0350,  0.0230,  0.1575],        [ 0.1431,  0.0215, -0.1665,  ..., -0.0446, -0.0326, -0.0599]]), self.has_saved=False, self.name=PosixPath('/tmp/pytest-of-XXX/pytest-193/test_convert_lit_checkpoint0/generated.bin'), self.next_key=0, self.zipfile=<torch.PyTorchFileWriter object at 0x7f8109872270>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "return SavingProxyForTensor(tensor, self)\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def fetch_sheet_metadata(self, params=None):\n",
      "        if params is None:\n",
      "            params = {\"includeGridData\": \"false\"}\n",
      "        url = SPREADSHEET_URL % self.id\n",
      "        r = self.client.request(\"get\", url, params=params)\n",
      "        return r.json()\n",
      "fetch_sheet_metadata(self=REPR FAILED, params=None, self._properties={'id': '1Sle_EDNRq8PsQGzmskqQWIGnUT4epR-d9A-9gaJsrrQ'}, self.client=<gspread.client.BackoffClient object at 0x7fd0d38cb490>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "params = {\"includeGridData\": \"false\"}\n",
      "State:\n",
      "{'includeGridData': 'false'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def fetch_sheet_metadata(self, params=None):\n",
      "        if params is None:\n",
      "            params = {\"includeGridData\": \"false\"}\n",
      "        url = SPREADSHEET_URL % self.id\n",
      "        r = self.client.request(\"get\", url, params=params)\n",
      "        return r.json()\n",
      "fetch_sheet_metadata(self=REPR FAILED, params=None, self._properties={'id': '1Sle_EDNRq8PsQGzmskqQWIGnUT4epR-d9A-9gaJsrrQ'}, self.client=<gspread.client.BackoffClient object at 0x7fd0d38cb490>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "url = SPREADSHEET_URL % self.id\n",
      "State:\n",
      "'https://sheets.googleapis.com/v4/spreadsheets/1Sle_EDNRq8PsQGzmskqQWIGnUT4epR-d9A-9gaJsrrQ'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def wrapper(self, *args, **kwargs):\n",
      "        try:\n",
      "            if len(args):\n",
      "                int(args[0])\n",
      "                range_start = rowcol_to_a1(*args[:2])\n",
      "                range_end = rowcol_to_a1(*args[-2:])\n",
      "                range_name = \":\".join((range_start, range_end))\n",
      "                args = (range_name,) + args[4:]\n",
      "        except ValueError:\n",
      "            pass\n",
      "        return method(self, *args, **kwargs)\n",
      "wrapper(self=<Worksheet 'Sheet1' id:0>, args=('A1:B2', 'TestDefineNamedRange'), kwargs={}, method=<function Worksheet.define_named_range at 0x7fd0d39cd5e0>, self._properties={'sheetId': 0, 'title': 'Sheet1', 'index': 0, 'sheetType': 'GRID', 'gridProperties': {'rowCount': 1000, 'columnCount': 26}}, self.client=<gspread.client.BackoffClient object at 0x7fd0d38cb490>, self.spreadsheet=<Spreadsheet 'Test CellTest test_define_named_range' id:1WADX3MU1axnVuqJivovc7d-H4TbTdrIGoxApTWZ4iVI>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "int(args[0])\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def update_title(self, title):\n",
      "        body = {\n",
      "            \"requests\": [\n",
      "                {\n",
      "                    \"updateSpreadsheetProperties\": {\n",
      "                        \"properties\": {\"title\": title},\n",
      "                        \"fields\": \"title\",\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "        res = self.batch_update(body)\n",
      "        self._properties[\"title\"] = title\n",
      "        return res\n",
      "update_title(self=<Spreadsheet 'Test SpreadsheetTest test_get_lastUpdateTime' id:1r-GMohxVV3K91rV1lZHqzHr9B48HLpYD2-x8koAVsuM>, title=' Updated Title\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "body = {\n",
      "State:\n",
      "{'requests': [{'updateSpreadsheetProperties': {'properties': {'title': ' Updated Title #123 '}, 'fields': 'title'}}]}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_lastUpdateTime(self) -> str:\n",
      "        metadata = self.client.get_file_drive_metadata(self.id)\n",
      "        self._properties[\"modifiedTime\"] = metadata[\"modifiedTime\"]\n",
      "        return metadata[\"modifiedTime\"]\n",
      "get_lastUpdateTime(self=<Spreadsheet 'Test SpreadsheetTest test_get_lastUpdateTime' id:1r-GMohxVV3K91rV1lZHqzHr9B48HLpYD2-x8koAVsuM>, self._properties={'id': '1r-GMohxVV3K91rV1lZHqzHr9B48HLpYD2-x8koAVsuM', 'title': 'Test SpreadsheetTest test_get_lastUpdateTime', 'locale': 'en_US', 'autoRecalc': 'ON_CHANGE', 'timeZone': 'Etc/GMT', 'defaultFormat': {'backgroundColor': {'red': 1, 'green': 1, 'blue': 1}, 'padding': {'top': 2, 'right': 3, 'bottom': 2, 'left': 3}, 'verticalAlignment': 'BOTTOM', 'wrapStrategy': 'OVERFLOW_CELL', 'textFormat': {'foregroundColor': {}, 'fontFamily': 'arial,sans,sans-serif', 'fontSize': 10, 'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'foregroundColorStyle': {'rgbColor': {}}}, 'backgroundColorStyle': {'rgbColor': {'red': 1, 'green': 1, 'blue': 1}}}, 'spreadsheetTheme': {'primaryFontFamily': 'Arial', 'themeColors': [{'colorType': 'TEXT', 'color': {'rgbColor': {}}}, {'colorType': 'BACKGROUND', 'color': {'rgbColor': {'red': 1, 'green': 1, 'blue': 1}}}, {'colorType': 'ACCENT1', 'color': {'rgbColor': {'red': 0.25882354, 'green': 0.52156866, 'blue': 0.95686275}}}, {'colorType': 'ACCENT2', 'color': {'rgbColor': {'red': 0.91764706, 'green': 0.2627451, 'blue': 0.20784314}}}, {'colorType': 'ACCENT3', 'color': {'rgbColor': {'red': 0.9843137, 'green': 0.7372549, 'blue': 0.015686275}}}, {'colorType': 'ACCENT4', 'color': {'rgbColor': {'red': 0.20392157, 'green': 0.65882355, 'blue': 0.3254902}}}, {'colorType': 'ACCENT5', 'color': {'rgbColor': {'red': 1, 'green': 0.42745098, 'blue': 0.003921569}}}, {'colorType': 'ACCENT6', 'color': {'rgbColor': {'red': 0.27450982, 'green': 0.7411765, 'blue': 0.7764706}}}, {'colorType': 'LINK', 'color': {'rgbColor': {'red': 0.06666667, 'green': 0.33333334, 'blue': 0.8}}}]}}, self.client=<gspread.client.BackoffClient object at 0x7fd0d3a77250>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "metadata = self.client.get_file_drive_metadata(self.id)\n",
      "State:\n",
      "{'id': '1r-GMohxVV3K91rV1lZHqzHr9B48HLpYD2-x8koAVsuM', 'name': 'Test SpreadsheetTest test_get_lastUpdateTime', 'createdTime': '2023-09-06T21:29:57.884Z', 'modifiedTime': '2023-09-06T21:29:57.905Z'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def hide(self):\n",
      "        return self._set_hidden_flag(True)\n",
      "hide(self=<Worksheet 'gacha' id:755875121>, self._properties={'sheetId': 755875121, 'title': 'gacha', 'index': 2, 'sheetType': 'GRID', 'gridProperties': {'rowCount': 100, 'columnCount': 100}}, self.client=<gspread.client.BackoffClient object at 0x7fd0d3a77250>, self.spreadsheet=<Spreadsheet 'Test SpreadsheetTest test_worksheets_exclude_hidden' id:19SsaJoNCUcVKkbPg7p_tE0GbuR_Nzb-9i-UxNemn1jw>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "return self._set_hidden_flag(True)\n",
      "State:\n",
      "{'sheetId': 755875121, 'title': 'gacha', 'index': 2, 'sheetType': 'GRID', 'gridProperties': {'rowCount': 100, 'columnCount': 100}, 'hidden': True}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _set_hidden_flag(self, hidden):\n",
      "        body = {\n",
      "            \"requests\": [\n",
      "                {\n",
      "                    \"updateSheetProperties\": {\n",
      "                        \"properties\": {\n",
      "                            \"sheetId\": self.id,\n",
      "                            \"hidden\": hidden,\n",
      "                        },\n",
      "                        \"fields\": \"hidden\",\n",
      "                    }\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "        res = self.spreadsheet.batch_update(body)\n",
      "        self._properties[\"hidden\"] = hidden\n",
      "        return res\n",
      "_set_hidden_flag(self=<Worksheet 'gacha' id:755875121>, hidden=True, self._properties={'sheetId': 755875121, 'title': 'gacha', 'index': 2, 'sheetType': 'GRID', 'gridProperties': {'rowCount': 100, 'columnCount': 100}}, self.client=<gspread.client.BackoffClient object at 0x7fd0d3a77250>, self.spreadsheet=<Spreadsheet 'Test SpreadsheetTest test_worksheets_exclude_hidden' id:19SsaJoNCUcVKkbPg7p_tE0GbuR_Nzb-9i-UxNemn1jw>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "body = {\n",
      "State:\n",
      "{'requests': [{'updateSheetProperties': {'properties': {'sheetId': 755875121, 'hidden': True}, 'fields': 'hidden'}}]}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_config():\n",
      "    cfg = VersioneerConfig()\n",
      "    cfg.VCS = \"git\"\n",
      "    cfg.style = \"pep440-auto\"\n",
      "    cfg.tag_prefix = \"\"\n",
      "    cfg.parentdir_prefix = \"\"\n",
      "    cfg.versionfile_source = \"milo/_version.py\"\n",
      "    cfg.verbose = False\n",
      "    return cfg\n",
      "get_config()\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "cfg = VersioneerConfig()\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_config():\n",
      "    cfg = VersioneerConfig()\n",
      "    cfg.VCS = \"git\"\n",
      "    cfg.style = \"pep440-auto\"\n",
      "    cfg.tag_prefix = \"\"\n",
      "    cfg.parentdir_prefix = \"\"\n",
      "    cfg.versionfile_source = \"milo/_version.py\"\n",
      "    cfg.verbose = False\n",
      "    return cfg\n",
      "get_config()\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "cfg.VCS = \"git\"\n",
      "State:\n",
      "{VCS='git'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n",
      "    pieces = _git_pieces_from_vcs(\n",
      "        tag_prefix, root, verbose, run_command=run_command)\n",
      "    GITS = [\"git\"]\n",
      "    if sys.platform == \"win32\":\n",
      "        GITS = [\"git.cmd\", \"git.exe\"]\n",
      "    (authors_raw, rc) = run_command(GITS, [\"log\", \"--pretty=%an\"], cwd=root)\n",
      "    authors = [author.strip() for author in authors_raw.split('\\n')]\n",
      "    (authors_unique, authors_indices) = np.unique(authors, return_index=True)\n",
      "    pieces[\"authors\"] = list(reversed(np.array(authors)[authors_indices]))\n",
      "    return pieces\n",
      "git_pieces_from_vcs(tag_prefix='', root='/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/berkeleylab+als.milo/berkeleylab+als.milo', verbose=False, run_command=<function run_command at 0x7fc331e3caf0>)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "pieces = _git_pieces_from_vcs(\n",
      "State:\n",
      "{'long': 'e60d005d389cb31b6e99f937e35adbe3fccb7aaf', 'short': 'e60d005', 'error': None, 'dirty': False, 'closest-tag': '0.8', 'distance': 3, 'branch': 'HEAD', 'distance-to-master': 0, 'develop': None, 'distance-to-develop': None, 'date': '2018-05-05T22:17:58-0700'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def gradient_check(f, *args):\n",
      "    out = f(*args)\n",
      "    out.backward()\n",
      "    eps = 1e-8\n",
      "    for arg in args:\n",
      "        if isinstance(arg, Tensor) and arg.require_grad:\n",
      "            shape = arg.data.shape\n",
      "            gradient = np.zeros(shape)\n",
      "            size = np.size(arg.data)\n",
      "            for idx in range(size):\n",
      "                loc = compute_loc(idx, shape)\n",
      "                arg.data[loc] += eps\n",
      "                out_eps = f(*args)\n",
      "                gradient[loc] = np.sum(out_eps.data - out.data) / eps\n",
      "                arg.data[loc] -= eps\n",
      "            assert np.allclose(gradient, arg.grad)\n",
      "gradient_check(f=<bound method Function.apply of <class 'flow.function.LogSoftmax'>>, args=(<flow.tensor.Tensor object at 0x7f47a8c2f490>,))\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "eps = 1e-8\n",
      "State:\n",
      "1e-08\n",
      "==================================================\n",
      "Clean Code:\n",
      "def apply(cls, *args):\n",
      "        func = cls()\n",
      "        func.inputs = args\n",
      "        output = cls.forward(func.ctx, *args)\n",
      "        register_backward(func, output)\n",
      "        return output\n",
      "apply(cls=<class 'flow.function.LogSoftmax'>, args=(<flow.tensor.Tensor object at 0x7f47a8c2f490>,))\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "output = cls.forward(func.ctx, *args)\n",
      "State:\n",
      "{data=array([[-3.4401897, -2.4401897, -1.4401897, -0.4401897],       [-3.4401897, -2.4401897, -1.4401897, -0.4401897]]), grad=None, grad_fn=None, is_leaf=True, require_grad=False}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def backward(self, grad=None):\n",
      "        from . import autograd\n",
      "        autograd.backward(self, grad)\n",
      "backward(self=<flow.tensor.Tensor object at 0x7f47a8c2f1c0>, grad=None, self.data=array([[-3.4401897, -2.4401897, -1.4401897, -0.4401897],       [-3.4401897, -2.4401897, -1.4401897, -0.4401897]]), self.grad=None, self.grad_fn=<flow.function.LogSoftmax object at 0x7f47a8c2f190>, self.is_leaf=False, self.require_grad=True)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "from . import autograd\n",
      "State:\n",
      "<module 'flow.autograd' from '/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/Cjkkkk+Pyflow/Cjkkkk+Pyflow/flow/autograd.py'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def backward(self, grad=None):\n",
      "        from . import autograd\n",
      "        autograd.backward(self, grad)\n",
      "backward(self=<flow.tensor.Tensor object at 0x7f47a8c2f1c0>, grad=None, self.data=array([[-3.4401897, -2.4401897, -1.4401897, -0.4401897],       [-3.4401897, -2.4401897, -1.4401897, -0.4401897]]), self.grad=None, self.grad_fn=<flow.function.LogSoftmax object at 0x7f47a8c2f190>, self.is_leaf=False, self.require_grad=True)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "autograd.backward(self, grad)\n",
      "State:\n",
      "array([[1., 1., 1., 1.],       [1., 1., 1., 1.]])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def compute_loc(idx, shape):\n",
      "    loc = [0] * len(shape)\n",
      "    for i in range(len(shape)):\n",
      "        prod = int(np.prod(shape[i + 1:]))\n",
      "        loc[i] = idx // prod\n",
      "        idx = idx % prod\n",
      "    return tuple(loc)\n",
      "compute_loc(idx=0, shape=(2, 4))\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "loc = [0] * len(shape)\n",
      "State:\n",
      "[0, 0]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse(self, version: str) -> Union[pv.Version, pv.LegacyVersion]:\n",
      "        try:\n",
      "            version = metadata.version(version)\n",
      "        except metadata.PackageNotFoundError:\n",
      "            pass\n",
      "        return pv.parse(version)\n",
      "_parse(self=<kats.compat.compat.Version object at 0x7f7e95758b80>, version='pandas')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "version = metadata.version(version)\n",
      "State:\n",
      "'1.3.5'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __call__(self, text):\n",
      "        atoms = self.toatoms(text)\n",
      "        j = 0\n",
      "        while j < len(atoms):\n",
      "            i = j\n",
      "            chunksize = 0\n",
      "            while j < len(atoms) and chunksize + len(atoms[j]) <= self.buffersize:\n",
      "                chunksize += len(atoms[j])\n",
      "                j += 1\n",
      "            self._juststuff(b''.join(atoms[i:j]))\n",
      "__call__(self=<screen.Stuff object at 0x7f6aa9661040>, text='xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx$', self.doublequoteexpr='${DUB_QUO}', self.session='tmp7zr1imyb', self.window='0')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "atoms = self.toatoms(text)\n",
      "State:\n",
      "[b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'x', b'\\\\$']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __call__(self, text):\n",
      "        atoms = self.toatoms(text)\n",
      "        j = 0\n",
      "        while j < len(atoms):\n",
      "            i = j\n",
      "            chunksize = 0\n",
      "            while j < len(atoms) and chunksize + len(atoms[j]) <= self.buffersize:\n",
      "                chunksize += len(atoms[j])\n",
      "                j += 1\n",
      "            self._juststuff(b''.join(atoms[i:j]))\n",
      "__call__(self=<screen.Stuff object at 0x7f6aa9661040>, text='xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx$', self.doublequoteexpr='${DUB_QUO}', self.session='tmp7zr1imyb', self.window='0')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "j = 0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def toatoms(self, text):\n",
      "        atoms = []\n",
      "        def byteatoms(characterstring):\n",
      "            binary = characterstring.encode()\n",
      "            atoms.extend(binary[i:i + 1] for i in range(len(binary)))\n",
      "        mark = 0\n",
      "        for m in self.replpattern.finditer(text):\n",
      "            byteatoms(text[mark:m.start()])\n",
      "            atoms.append(self._repl(m).encode())\n",
      "            mark = m.end()\n",
      "        byteatoms(text[mark:])\n",
      "        return atoms\n",
      "toatoms(self=<screen.Stuff object at 0x7f6aa9661040>, text='xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx$', self.doublequoteexpr='${DUB_QUO}', self.session='tmp7zr1imyb', self.window='0')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "atoms = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def toatoms(self, text):\n",
      "        atoms = []\n",
      "        def byteatoms(characterstring):\n",
      "            binary = characterstring.encode()\n",
      "            atoms.extend(binary[i:i + 1] for i in range(len(binary)))\n",
      "        mark = 0\n",
      "        for m in self.replpattern.finditer(text):\n",
      "            byteatoms(text[mark:m.start()])\n",
      "            atoms.append(self._repl(m).encode())\n",
      "            mark = m.end()\n",
      "        byteatoms(text[mark:])\n",
      "        return atoms\n",
      "toatoms(self=<screen.Stuff object at 0x7f6aa9661040>, text='xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx$', self.doublequoteexpr='${DUB_QUO}', self.session='tmp7zr1imyb', self.window='0')\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "mark = 0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def byteatoms(characterstring):\n",
      "            binary = characterstring.encode()\n",
      "            atoms.extend(binary[i:i + 1] for i in range(len(binary)))\n",
      "byteatoms(characterstring='xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx', atoms=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "binary = characterstring.encode()\n",
      "State:\n",
      "b'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __new__(mcs, name, bases, dct):\n",
      "        for verb in ['get', 'delete', 'update']:\n",
      "            if '__%s__' % verb in dct:\n",
      "                info = dct['__%s__' % verb]\n",
      "                args = info[1] if len(info) >= 2 else []\n",
      "                kwargs = info[2] if len(info) == 3 else {}\n",
      "                dct[verb] = Type.__build_func(\n",
      "                    verb,\n",
      "                    args,\n",
      "                    kwargs\n",
      "                )\n",
      "        for verb in ['create', 'list']:\n",
      "            if '__%s__' % verb in dct:\n",
      "                info = dct['__%s__' % verb]\n",
      "                args = info[1] if len(info) >= 2 else ()\n",
      "                args = ('connection',) + args\n",
      "                kwargs = info[2] if len(info) == 3 else {}\n",
      "                dct[verb] = classmethod(Type.__build_func(\n",
      "                    verb,\n",
      "                    args,\n",
      "                    kwargs\n",
      "                ))\n",
      "        return super(Type, mcs).__new__(mcs, name, (mcs.Base,) + bases, dct)\n",
      "__new__(mcs=<class 'pyutrack.util.Type'>, name='Issue', bases=(<class 'object'>,), dct={'__module__': 'pyutrack.resources', '__get__': ('issue/%(id)s',), '__create__': ('issue/', ('project',), {'summary': None, 'description': None}), '__delete__': ('issue/%(id)s',), '__update__': ('issue/%(id)s/', (), {'summary': None, 'description': None}), '__aliases__': {'project': 'projectShortName'}, '__doc__': None, '__qualname__': 'Issue'}, __class__=<class 'pyutrack.util.Type'>)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "args = ('connection',) + args\n",
      "State:\n",
      "('connection', 'project')\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __build_func(verb, args, kwargs={}):\n",
      "        params = ['self']\n",
      "        params += ['%s' % stringcase.snakecase(k) for k in args]\n",
      "        params += ['%s=%s' % (stringcase.snakecase(k), v) for k, v in kwargs.items()]\n",
      "        largs = list(args) + list(kwargs.keys())\n",
      "        return eval(\n",
      "            'lambda %s: self._%s(%s)' % (\n",
      "                ','.join(params), verb, ','.join(['%s=%s' % (k, stringcase.snakecase(k)) for k in largs])\n",
      "            )\n",
      "        )\n",
      "__build_func(verb='get', args=[], kwargs={})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "params = ['self']\n",
      "State:\n",
      "['self']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __build_func(verb, args, kwargs={}):\n",
      "        params = ['self']\n",
      "        params += ['%s' % stringcase.snakecase(k) for k in args]\n",
      "        params += ['%s=%s' % (stringcase.snakecase(k), v) for k, v in kwargs.items()]\n",
      "        largs = list(args) + list(kwargs.keys())\n",
      "        return eval(\n",
      "            'lambda %s: self._%s(%s)' % (\n",
      "                ','.join(params), verb, ','.join(['%s=%s' % (k, stringcase.snakecase(k)) for k in largs])\n",
      "            )\n",
      "        )\n",
      "__build_func(verb='get', args=[], kwargs={})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "largs = list(args) + list(kwargs.keys())\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def run_validator_for_test_file(filename: str) -> List:\n",
      "    test_file_path = os.path.join(\n",
      "        os.path.dirname(os.path.abspath(__file__)),\n",
      "        'test_files',\n",
      "        filename,\n",
      "    )\n",
      "    with open(test_file_path, 'r') as file_handler:\n",
      "        raw_content = file_handler.read()\n",
      "    tree = ast.parse(raw_content)\n",
      "    checker = SuperMarionChecker(tree=tree, filename=test_file_path)\n",
      "    return list(checker.run())\n",
      "run_validator_for_test_file(filename='ok_pipe.py')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "test_file_path = os.path.join(\n",
      "State:\n",
      "'/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/Melevir+flake8-super-mario/Melevir+flake8-super-mario/tests/test_files/ok_pipe.py'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def create_mock_pages(number_mock_pages):\n",
      "    mock_pages = []\n",
      "    for _ in range(0, number_mock_pages):\n",
      "        mock_pages.append(mock.create_autospec(PageScraper))\n",
      "    return mock_pages\n",
      "create_mock_pages(number_mock_pages=10)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "mock_pages = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def create_mock_page_scrapers(number_mock_scrapers):\n",
      "    mock_scrapers = []\n",
      "    for _ in range(0, number_mock_scrapers):\n",
      "        mock_scrapers.append(mock.create_autospec(PageScraper))\n",
      "    return mock_scrapers\n",
      "create_mock_page_scrapers(number_mock_scrapers=1)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "mock_scrapers = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def create_mock_page_scrapers(number_mock_scrapers):\n",
      "    mock_scrapers = []\n",
      "    for _ in range(0, number_mock_scrapers):\n",
      "        mock_scrapers.append(mock.create_autospec(PageScraper))\n",
      "    return mock_scrapers\n",
      "create_mock_page_scrapers(number_mock_scrapers=1)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "mock_scrapers.append(mock.create_autospec(PageScraper))\n",
      "State:\n",
      "[<MagicMock spec='PageScraper' id='140669237222752'>]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def write_item(self, item):\n",
      "        if self.__no_items_written_to_file:\n",
      "            self.write_headers(item)\n",
      "            self.__no_items_written_to_file = False   \n",
      "        self.object_writer.write_object(item)\n",
      "write_item(self=<xcrawler.files.writers.item_writer.ItemWriter object at 0x7ff01c432190>, item=<Mock id='140669235151920'>, self._ItemWriter__no_items_written_to_file=True, self.object_writer=<NonCallableMagicMock name='mock()' spec='ObjectWriter' id='140669242130096'>, self.output_file=None, self.output_file_name='')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.__no_items_written_to_file = False\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def mix_protocol_domain(self, url1, url2):\n",
      "        if self.url_info.is_relative(url2):\n",
      "            url2 = self.prepend_protocol_domain(url1, url2)\n",
      "        return url2\n",
      "mix_protocol_domain(self=<xcrawler.http.urls.url_mixer.UrlMixer object at 0x7ff01bed4e80>, url1='http://example.com/path/to/mock_url1.html', url2='link/to/example_page.html', self.url_info=<NonCallableMagicMock name='mock()' spec='UrlInfo' id='140669237206752'>, self.url_joiner=<NonCallableMagicMock name='mock()' spec='UrlJoiner' id='140669236431072'>, self.url_splitter=<NonCallableMagicMock name='mock()' spec='UrlSplitter' id='140669239257024'>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "url2 = self.prepend_protocol_domain(url1, url2)\n",
      "State:\n",
      "'http://example.com/link/to/example_page.html'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def spawn_item_queue_thread(self):\n",
      "            self.item_processor = ItemProcessor(self.config, self.item_queue)\n",
      "            self.item_processor.daemon = True\n",
      "            self.item_processor.start()\n",
      "spawn_item_queue_thread(self=<xcrawler.threads.work_executor.WorkExecutor object at 0x7ff01c239280>, self.config=<MagicMock spec='Config' id='140669237721216'>, self.item_queue=<queue.Queue object at 0x7ff01bf99730>, self.page_queue=<queue.Queue object at 0x7ff01bf99490>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.item_processor = ItemProcessor(self.config, self.item_queue)\n",
      "State:\n",
      "<ItemProcessor(Thread-19, initial)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def spawn_item_queue_thread(self):\n",
      "            self.item_processor = ItemProcessor(self.config, self.item_queue)\n",
      "            self.item_processor.daemon = True\n",
      "            self.item_processor.start()\n",
      "spawn_item_queue_thread(self=<xcrawler.threads.work_executor.WorkExecutor object at 0x7ff01c239280>, self.config=<MagicMock spec='Config' id='140669237721216'>, self.item_queue=<queue.Queue object at 0x7ff01bf99730>, self.page_queue=<queue.Queue object at 0x7ff01bf99490>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.item_processor.daemon = True\n",
      "State:\n",
      "<ItemProcessor(Thread-19, initial daemon)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def spawn_item_queue_thread(self):\n",
      "            self.item_processor = ItemProcessor(self.config, self.item_queue)\n",
      "            self.item_processor.daemon = True\n",
      "            self.item_processor.start()\n",
      "spawn_item_queue_thread(self=<xcrawler.threads.work_executor.WorkExecutor object at 0x7ff01c239280>, self.config=<MagicMock spec='Config' id='140669237721216'>, self.item_queue=<queue.Queue object at 0x7ff01bf99730>, self.page_queue=<queue.Queue object at 0x7ff01bf99490>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.item_processor.start()\n",
      "State:\n",
      "<ItemProcessor(Thread-19, started daemon 140669209658944)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def create_mock_object_with_str(string):\n",
      "    mock_object = mock.Mock()\n",
      "    mock_str = mock.Mock()\n",
      "    mock_str.return_value = string\n",
      "    mock_object.__str__ = mock_str\n",
      "    return mock_object\n",
      "create_mock_object_with_str(string='mock_object')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "mock_object = mock.Mock()\n",
      "State:\n",
      "<Mock id='140669239367232'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def create_mock_object_with_str(string):\n",
      "    mock_object = mock.Mock()\n",
      "    mock_str = mock.Mock()\n",
      "    mock_str.return_value = string\n",
      "    mock_object.__str__ = mock_str\n",
      "    return mock_object\n",
      "create_mock_object_with_str(string='mock_object')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "mock_str = mock.Mock()\n",
      "State:\n",
      "<Mock id='140669239366416'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def create_mock_object_with_str(string):\n",
      "    mock_object = mock.Mock()\n",
      "    mock_str = mock.Mock()\n",
      "    mock_str.return_value = string\n",
      "    mock_object.__str__ = mock_str\n",
      "    return mock_object\n",
      "create_mock_object_with_str(string='mock_object')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "mock_object.__str__ = mock_str\n",
      "State:\n",
      "<Mock name='mock.__str__' id='140669239366416'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_protocol_attrs(cls):\n",
      "    attrs = set()\n",
      "    for base in cls.__mro__[:-1]:\n",
      "        if base.__name__ in {'Protocol', 'Generic'}:\n",
      "            continue\n",
      "        annotations = getattr(base, '__annotations__', {})\n",
      "        for attr in (*base.__dict__, *annotations):\n",
      "            if (not attr.startswith('_abc_') and attr not in _EXCLUDED_ATTRS):\n",
      "                attrs.add(attr)\n",
      "    return attrs\n",
      "_get_protocol_attrs(cls=<class 'typing_extensions.Protocol'>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "attrs = set()\n",
      "State:\n",
      "set()\n",
      "==================================================\n",
      "Clean Code:\n",
      "def decode(self, word_idxs, join_words=True):\n",
      "        if isinstance(word_idxs, list) and len(word_idxs) == 0:\n",
      "            return self.decode([word_idxs, ], join_words)[0]\n",
      "        if isinstance(word_idxs, list) and isinstance(word_idxs[0], int):\n",
      "            return self.decode([word_idxs, ], join_words)[0]\n",
      "        elif isinstance(word_idxs, np.ndarray) and word_idxs.ndim == 1:\n",
      "            return self.decode(word_idxs.reshape((1, -1)), join_words)[0]\n",
      "        elif isinstance(word_idxs, torch.Tensor) and word_idxs.ndimension() == 1:\n",
      "            return self.decode(word_idxs.unsqueeze(0), join_words)[0]\n",
      "        captions = []\n",
      "        for wis in word_idxs:\n",
      "            caption = []\n",
      "            for wi in wis:\n",
      "                word = self.vocab.itos[int(wi)]\n",
      "                if word == self.eos_token:\n",
      "                    break\n",
      "                caption.append(word)\n",
      "            if join_words:\n",
      "                caption = ' '.join(caption)\n",
      "            captions.append(caption)\n",
      "        return captions\n",
      "decode(self=<speaksee.data.field.TextField object at 0x7f901f246a30>, word_idxs=array([], shape=(1, 0), dtype=float64), join_words=True, self.batch_first=True, self.dtype=torch.int64, self.eos_token=None, self.fix_length=None, self.include_lengths=False, self.init_token=None, self.lower=False, self.pad_first=False, self.pad_token='<pad>', self.postprocessing=None, self.preprocessing=None, self.remove_punctuation=False, self.tokenize=<function TextField.<lambda> at 0x7f901f294c10>, self.truncate_first=False, self.unk_token='<unk>', self.use_vocab=True, self.vocab=<test_field.TestTextField.test_decode.<locals>.MyVocab object at 0x7f901f2469d0>)\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "captions.append(caption)\n",
      "State:\n",
      "['']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def encode_number(n):\n",
      "    b128_digits = []\n",
      "    while n:\n",
      "        b128_digits.insert(0, (n & 0x7f) | 0x80)\n",
      "        n = n >> 7\n",
      "    if not b128_digits:\n",
      "        b128_digits.append(0)\n",
      "    b128_digits[-1] &= 0x7f\n",
      "    return b''.join([int.to_bytes(d, 1, 'big') for d in b128_digits])\n",
      "encode_number(n=840)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "b128_digits = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __set_name__(self, owner, name):\n",
      "        self.name = name\n",
      "__set_name__(self=<ccxt.base.types.Entry object at 0x7f3e3f9eec70>, owner=<class 'ccxt.abstract.ace.ImplicitAPI'>, name='public_get_oapi_v2_list_tradeprice', self.api='public', self.config={}, self.method='GET', self.name=None, self.path='oapi/v2/list/tradePrice', self.unbound_method=<function Entry.__init__.<locals>.unbound_method at 0x7f3e3f923040>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.name = name\n",
      "State:\n",
      "'public_get_oapi_v2_list_tradeprice'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def append(self, item):\n",
      "        self._deque.append(item)\n",
      "        if self._clear_all_updates:\n",
      "            self._clear_all_updates = False\n",
      "            self._clear_updates_by_symbol.clear()\n",
      "            self._all_new_updates = 0\n",
      "            self._new_updates_by_symbol.clear()\n",
      "        if self._clear_updates_by_symbol.get(item['symbol']):\n",
      "            self._clear_updates_by_symbol[item['symbol']] = False\n",
      "            self._new_updates_by_symbol[item['symbol']] = 0\n",
      "        self._new_updates_by_symbol[item['symbol']] = self._new_updates_by_symbol.get(item['symbol'], 0) + 1\n",
      "        self._all_new_updates = (self._all_new_updates or 0) + 1\n",
      "append(self=[], item={'symbol': 'BTC/USDT', 'data': 1})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self._deque.append(item)\n",
      "State:\n",
      "{'symbol': 'BTC/USDT', 'data': 1}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __get__(self, instance, owner):\n",
      "        deque = getattr(instance, self.delegated)\n",
      "        return getattr(deque, self.name)\n",
      "__get__(self=<ccxt.async_support.base.ws.cache.Delegate object at 0x7f3e2bff2d90>, instance=[{'symbol': 'BTC/USDT', 'data': 2}, {'symbol': 'BTC/USDT', 'data': 3}, {'symbol': 'BTC/USDT', 'data': 4}], owner=<class 'ccxt.async_support.base.ws.cache.ArrayCache'>, self.delegated='_deque', self.name='__len__')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "deque = getattr(instance, self.delegated)\n",
      "State:\n",
      "deque([{'symbol': 'BTC/USDT', 'data': 2}, {'symbol': 'BTC/USDT', 'data': 3}, {'symbol': 'BTC/USDT', 'data': 4}], maxlen=3)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def extend(*args):\n",
      "        if args is not None:\n",
      "            result = None\n",
      "            if type(args[0]) is collections.OrderedDict:\n",
      "                result = collections.OrderedDict()\n",
      "            else:\n",
      "                result = {}\n",
      "            for arg in args:\n",
      "                result.update(arg)\n",
      "            return result\n",
      "        return {}\n",
      "extend(args=({'ETH': 'ERC20', 'TRX': 'TRC20', 'BNB': 'BEP2', 'BSC': 'BEP20', 'OMNI': 'OMNI', 'EOS': 'EOS', 'SOL': 'SPL'}, {'tronscan.org': 'TRC20', 'etherscan.io': 'ERC20', 'bscscan.com': 'BSC', 'explorer.binance.org': 'BEP2', 'bithomp.com': 'XRP', 'bloks.io': 'EOS', 'stellar.expert': 'XLM', 'blockchair.com/bitcoin': 'BTC', 'blockchair.com/bitcoin-cash': 'BCH', 'blockchair.com/ecash': 'XEC', 'explorer.litecoin.net': 'LTC', 'explorer.avax.network': 'AVAX', 'solscan.io': 'SOL', 'polkadot.subscan.io': 'DOT', 'dashboard.internetcomputer.org': 'ICP', 'explorer.chiliz.com': 'CHZ', 'cardanoscan.io': 'ADA', 'mainnet.theoan.com': 'AION', 'algoexplorer.io': 'ALGO', 'explorer.ambrosus.com': 'AMB', 'viewblock.io/zilliqa': 'ZIL', 'viewblock.io/arweave': 'AR', 'explorer.ark.io': 'ARK', 'atomscan.com': 'ATOM', 'www.mintscan.io': 'CTK', 'explorer.bitcoindiamond.org': 'BCD', 'btgexplorer.com': 'BTG', 'bts.ai': 'BTS', 'explorer.celo.org': 'CELO', 'explorer.nervos.org': 'CKB', 'cerebro.cortexlabs.ai': 'CTXC', 'chainz.cryptoid.info': 'VIA', 'explorer.dcrdata.org': 'DCR', 'digiexplorer.info': 'DGB', 'dock.subscan.io': 'DOCK', 'dogechain.info': 'DOGE', 'explorer.elrond.com': 'EGLD', 'blockscout.com': 'ETC', 'explore-fetchhub.fetch.ai': 'FET', 'filfox.info': 'FIL', 'fio.bloks.io': 'FIO', 'explorer.firo.org': 'FIRO', 'neoscan.io': 'NEO', 'ftmscan.com': 'FTM', 'explorer.gochain.io': 'GO', 'block.gxb.io': 'GXS', 'hash-hash.info': 'HBAR', 'www.hiveblockexplorer.com': 'HIVE', 'explorer.helium.com': 'HNT', 'tracker.icon.foundation': 'ICX', 'www.iostabc.com': 'IOST', 'explorer.iota.org': 'IOTA', 'iotexscan.io': 'IOTX', 'irishub.iobscan.io': 'IRIS', 'kava.mintscan.io': 'KAVA', 'scope.klaytn.com': 'KLAY', 'kmdexplorer.io': 'KMD', 'kusama.subscan.io': 'KSM', 'explorer.lto.network': 'LTO', 'polygonscan.com': 'POLYGON', 'explorer.ont.io': 'ONT', 'minaexplorer.com': 'MINA', 'nanolooker.com': 'NANO', 'explorer.nebulas.io': 'NAS', 'explorer.nbs.plus': 'NBS', 'explorer.nebl.io': 'NEBL', 'nulscan.io': 'NULS', 'nxscan.com': 'NXS', 'explorer.harmony.one': 'ONE', 'explorer.poa.network': 'POA', 'qtum.info': 'QTUM', 'explorer.rsk.co': 'RSK', 'www.oasisscan.com': 'ROSE', 'ravencoin.network': 'RVN', 'sc.tokenview.com': 'SC', 'secretnodes.com': 'SCRT', 'explorer.skycoin.com': 'SKY', 'steemscan.com': 'STEEM', 'explorer.stacks.co': 'STX', 'www.thetascan.io': 'THETA', 'scan.tomochain.com': 'TOMO', 'explore.vechain.org': 'VET', 'explorer.vite.net': 'VITE', 'www.wanscan.org': 'WAN', 'wavesexplorer.com': 'WAVES', 'wax.eosx.io': 'WAXP', 'waltonchain.pro': 'WTC', 'chain.nem.ninja': 'XEM', 'verge-blockchain.info': 'XVG', 'explorer.yoyow.org': 'YOYOW', 'explorer.zcha.in': 'ZEC', 'explorer.zensystem.io': 'ZEN'}))\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "result = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def leftmost_bit(x):\n",
      "            assert x > 0\n",
      "            result = 1\n",
      "            while result <= x:\n",
      "                result = 2 * result\n",
      "            return result // 2\n",
      "leftmost_bit(x=35418756707884953894268771885010418872764936485960643117951731578891074948686)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "result = 1\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def generate_k(order, secexp, hash_func, data, retry_gen=0, extra_entropy=b''):\n",
      "    qlen = bit_length(order)\n",
      "    holen = hash_func().digest_size\n",
      "    rolen = (qlen + 7) / 8\n",
      "    bx = number_to_string(secexp, order) + bits2octets(data, order) + \\\n",
      "        extra_entropy\n",
      "    v = b'\\x01' * holen\n",
      "    k = b'\\x00' * holen\n",
      "    k = hmac.new(k, v + b'\\x00' + bx, hash_func).digest()\n",
      "    v = hmac.new(k, v, hash_func).digest()\n",
      "    k = hmac.new(k, v + b'\\x01' + bx, hash_func).digest()\n",
      "    v = hmac.new(k, v, hash_func).digest()\n",
      "    while True:\n",
      "        t = b''\n",
      "        while len(t) < rolen:\n",
      "            v = hmac.new(k, v, hash_func).digest()\n",
      "            t += v\n",
      "        secret = bits2int(t, qlen)\n",
      "        if secret >= 1 and secret < order:\n",
      "            if retry_gen <= 0:\n",
      "                return secret\n",
      "            else:\n",
      "                retry_gen -= 1\n",
      "        k = hmac.new(k, v + b'\\x00', hash_func).digest()\n",
      "        v = hmac.new(k, v, hash_func).digest()\n",
      "generate_k(order=115792089237316195423570985008687907852837564279074904382605163141518161494337, secexp=11806252235961651298089590628336806290921645495320214372650577192963691649562, hash_func=<built-in function openssl_sha256>, data=b'\\xa7?\\xcf3\\x96@\\x92\\x92\\x07(\\x1f\\xb8\\xe08\\x88H\\x06\\xe2\\xeb\\x08@\\xf2$V\\x94\\xdb\\xba\\x1d\\\\\\xc8\\x9ee', retry_gen=0, extra_entropy=b'')\n",
      "\n",
      "n:\n",
      "25\n",
      "Statement:\n",
      "k = hmac.new(k, v + b'\\x00' + bx, hash_func).digest()\n",
      "State:\n",
      "b\"\\x99\\xe9zo\\xd1\\xff\\xec\\xa9.\\x1c\\xf8\\x07j\\xb9\\xa7J\\x9e\\xa5\\xe2\\xb3\\xac*\\xc5\\xafm\\n'\\x87~\\x17L~\"\n",
      "==================================================\n",
      "Clean Code:\n",
      "def bit_length(num):\n",
      "    s = bin(num)\n",
      "    s = s.lstrip('-0b')\n",
      "    return len(s)\n",
      "bit_length(num=115792089237316195423570985008687907852837564279074904382605163141518161494337)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "s = bin(num)  # binary representation:  bin(-37) --> '-0b100101'\n",
      "State:\n",
      "'0b1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111010111010101011101101110011100110101011110100100010100000001110111011111111010010010111101000110011010000001101100100000101000001'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def bit_length(num):\n",
      "    s = bin(num)\n",
      "    s = s.lstrip('-0b')\n",
      "    return len(s)\n",
      "bit_length(num=115792089237316195423570985008687907852837564279074904382605163141518161494337)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "s = s.lstrip('-0b')  # remove leading zeros and minus sign\n",
      "State:\n",
      "'1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111010111010101011101101110011100110101011110100100010100000001110111011111111010010010111101000110011010000001101100100000101000001'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def bits2octets(data, order):\n",
      "    z1 = bits2int(data, bit_length(order))\n",
      "    z2 = z1 - order\n",
      "    if z2 < 0:\n",
      "        z2 = z1\n",
      "    return number_to_string_crop(z2, order)\n",
      "bits2octets(data=b'\\xa7?\\xcf3\\x96@\\x92\\x92\\x07(\\x1f\\xb8\\xe08\\x88H\\x06\\xe2\\xeb\\x08@\\xf2$V\\x94\\xdb\\xba\\x1d\\\\\\xc8\\x9ee', order=115792089237316195423570985008687907852837564279074904382605163141518161494337)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "z1 = bits2int(data, bit_length(order))\n",
      "State:\n",
      "75648987130760998095283026105289635390775519882394219481699348731002280779365\n",
      "==================================================\n",
      "Clean Code:\n",
      "def bits2octets(data, order):\n",
      "    z1 = bits2int(data, bit_length(order))\n",
      "    z2 = z1 - order\n",
      "    if z2 < 0:\n",
      "        z2 = z1\n",
      "    return number_to_string_crop(z2, order)\n",
      "bits2octets(data=b'\\xa7?\\xcf3\\x96@\\x92\\x92\\x07(\\x1f\\xb8\\xe08\\x88H\\x06\\xe2\\xeb\\x08@\\xf2$V\\x94\\xdb\\xba\\x1d\\\\\\xc8\\x9ee', order=115792089237316195423570985008687907852837564279074904382605163141518161494337)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "z2 = z1 - order\n",
      "State:\n",
      "-40143102106555197328287958903398272462062044396680684900905814410515880714972\n",
      "==================================================\n",
      "Clean Code:\n",
      "def bits2octets(data, order):\n",
      "    z1 = bits2int(data, bit_length(order))\n",
      "    z2 = z1 - order\n",
      "    if z2 < 0:\n",
      "        z2 = z1\n",
      "    return number_to_string_crop(z2, order)\n",
      "bits2octets(data=b'\\xa7?\\xcf3\\x96@\\x92\\x92\\x07(\\x1f\\xb8\\xe08\\x88H\\x06\\xe2\\xeb\\x08@\\xf2$V\\x94\\xdb\\xba\\x1d\\\\\\xc8\\x9ee', order=115792089237316195423570985008687907852837564279074904382605163141518161494337)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "z2 = z1\n",
      "State:\n",
      "75648987130760998095283026105289635390775519882394219481699348731002280779365\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __str__(self):\n",
      "        self.reduce()\n",
      "        sign = '-' if self.integer < 0 else ''\n",
      "        integer_array = list(str(abs(self.integer)).rjust(self.decimals, '0'))\n",
      "        index = len(integer_array) - self.decimals\n",
      "        if index == 0:\n",
      "            item = '0.'\n",
      "        elif self.decimals < 0:\n",
      "            item = '0' * (-self.decimals)\n",
      "        elif self.decimals == 0:\n",
      "            item = ''\n",
      "        else:\n",
      "            item = '.'\n",
      "        integer_array.insert(index, item)\n",
      "        return sign + ''.join(integer_array)\n",
      "__str__(self=Precise(1393.938), self.base=10, self.decimals=3, self.integer=1393938)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "sign = '-' if self.integer < 0 else ''\n",
      "State:\n",
      "''\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __str__(self):\n",
      "        self.reduce()\n",
      "        sign = '-' if self.integer < 0 else ''\n",
      "        integer_array = list(str(abs(self.integer)).rjust(self.decimals, '0'))\n",
      "        index = len(integer_array) - self.decimals\n",
      "        if index == 0:\n",
      "            item = '0.'\n",
      "        elif self.decimals < 0:\n",
      "            item = '0' * (-self.decimals)\n",
      "        elif self.decimals == 0:\n",
      "            item = ''\n",
      "        else:\n",
      "            item = '.'\n",
      "        integer_array.insert(index, item)\n",
      "        return sign + ''.join(integer_array)\n",
      "__str__(self=Precise(1393.938), self.base=10, self.decimals=3, self.integer=1393938)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "integer_array = list(str(abs(self.integer)).rjust(self.decimals, '0'))\n",
      "State:\n",
      "['1', '3', '9', '3', '9', '3', '8']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __str__(self):\n",
      "        self.reduce()\n",
      "        sign = '-' if self.integer < 0 else ''\n",
      "        integer_array = list(str(abs(self.integer)).rjust(self.decimals, '0'))\n",
      "        index = len(integer_array) - self.decimals\n",
      "        if index == 0:\n",
      "            item = '0.'\n",
      "        elif self.decimals < 0:\n",
      "            item = '0' * (-self.decimals)\n",
      "        elif self.decimals == 0:\n",
      "            item = ''\n",
      "        else:\n",
      "            item = '.'\n",
      "        integer_array.insert(index, item)\n",
      "        return sign + ''.join(integer_array)\n",
      "__str__(self=Precise(1393.938), self.base=10, self.decimals=3, self.integer=1393938)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "index = len(integer_array) - self.decimals\n",
      "State:\n",
      "4\n",
      "==================================================\n",
      "Clean Code:\n",
      "def div(self, other, precision=18):\n",
      "        distance = precision - self.decimals + other.decimals\n",
      "        if distance == 0:\n",
      "            numerator = self.integer\n",
      "        elif distance < 0:\n",
      "            exponent = self.base ** -distance\n",
      "            numerator = self.integer // exponent\n",
      "        else:\n",
      "            exponent = self.base ** distance\n",
      "            numerator = self.integer * exponent\n",
      "        result, mod = divmod(numerator, other.integer)\n",
      "        result = result + 1 if result < 0 and mod else result\n",
      "        return Precise(result, precision)\n",
      "div(self=Precise(0.00000002), other=Precise(69696900000), precision=1, self.base=10, self.decimals=8, self.integer=2)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "distance = precision - self.decimals + other.decimals\n",
      "State:\n",
      "-12\n",
      "==================================================\n",
      "Clean Code:\n",
      "def div(self, other, precision=18):\n",
      "        distance = precision - self.decimals + other.decimals\n",
      "        if distance == 0:\n",
      "            numerator = self.integer\n",
      "        elif distance < 0:\n",
      "            exponent = self.base ** -distance\n",
      "            numerator = self.integer // exponent\n",
      "        else:\n",
      "            exponent = self.base ** distance\n",
      "            numerator = self.integer * exponent\n",
      "        result, mod = divmod(numerator, other.integer)\n",
      "        result = result + 1 if result < 0 and mod else result\n",
      "        return Precise(result, precision)\n",
      "div(self=Precise(0.00000002), other=Precise(69696900000), precision=1, self.base=10, self.decimals=8, self.integer=2)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "exponent = self.base ** -distance\n",
      "State:\n",
      "1000000000000\n",
      "==================================================\n",
      "Clean Code:\n",
      "def div(self, other, precision=18):\n",
      "        distance = precision - self.decimals + other.decimals\n",
      "        if distance == 0:\n",
      "            numerator = self.integer\n",
      "        elif distance < 0:\n",
      "            exponent = self.base ** -distance\n",
      "            numerator = self.integer // exponent\n",
      "        else:\n",
      "            exponent = self.base ** distance\n",
      "            numerator = self.integer * exponent\n",
      "        result, mod = divmod(numerator, other.integer)\n",
      "        result = result + 1 if result < 0 and mod else result\n",
      "        return Precise(result, precision)\n",
      "div(self=Precise(0.00000002), other=Precise(69696900000), precision=1, self.base=10, self.decimals=8, self.integer=2)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "result, mod = divmod(numerator, other.integer)\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def make_retry_state(previous_attempt_number, delay_since_first_attempt, last_result=None, upcoming_sleep=0):\n",
      "    required_parameter_unset = previous_attempt_number is _unset or delay_since_first_attempt is _unset\n",
      "    if required_parameter_unset:\n",
      "        raise _make_unset_exception(\n",
      "            \"wait/stop\",\n",
      "            previous_attempt_number=previous_attempt_number,\n",
      "            delay_since_first_attempt=delay_since_first_attempt,\n",
      "        )\n",
      "    retry_state = RetryCallState(None, None, (), {})\n",
      "    retry_state.attempt_number = previous_attempt_number\n",
      "    if last_result is not None:\n",
      "        retry_state.outcome = last_result\n",
      "    else:\n",
      "        retry_state.set_result(None)\n",
      "    retry_state.upcoming_sleep = upcoming_sleep\n",
      "    _set_delay_since_start(retry_state, delay_since_first_attempt)\n",
      "    return retry_state\n",
      "make_retry_state(previous_attempt_number=476, delay_since_first_attempt=0.1, last_result=None, upcoming_sleep=0)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "required_parameter_unset = previous_attempt_number is _unset or delay_since_first_attempt is _unset\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_result(self, val: t.Any) -> None:\n",
      "        ts = time.monotonic()\n",
      "        fut = Future(self.attempt_number)\n",
      "        fut.set_result(val)\n",
      "        self.outcome, self.outcome_timestamp = fut, ts\n",
      "set_result(self=<RetryCallState 140359814131136: attempt\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "ts = time.monotonic()\n",
      "State:\n",
      "3660312.638143485\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_result(self, val: t.Any) -> None:\n",
      "        ts = time.monotonic()\n",
      "        fut = Future(self.attempt_number)\n",
      "        fut.set_result(val)\n",
      "        self.outcome, self.outcome_timestamp = fut, ts\n",
      "set_result(self=<RetryCallState 140359814131136: attempt\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "self.outcome, self.outcome_timestamp = fut, ts\n",
      "State:\n",
      "<RetryCallState 140359814131136: attempt #476; slept for 0.0; last result: returned None>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def prepare_for_next_attempt(self) -> None:\n",
      "        self.outcome = None\n",
      "        self.outcome_timestamp = None\n",
      "        self.attempt_number += 1\n",
      "        self.next_action = None\n",
      "prepare_for_next_attempt(self=<RetryCallState 140359806784704: attempt\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.outcome = None\n",
      "State:\n",
      "<RetryCallState 140359806784704: attempt #1; slept for 0.0; last result: none yet>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def prepare_for_next_attempt(self) -> None:\n",
      "        self.outcome = None\n",
      "        self.outcome_timestamp = None\n",
      "        self.attempt_number += 1\n",
      "        self.next_action = None\n",
      "prepare_for_next_attempt(self=<RetryCallState 140359806784704: attempt\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.outcome_timestamp = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def prepare_for_next_attempt(self) -> None:\n",
      "        self.outcome = None\n",
      "        self.outcome_timestamp = None\n",
      "        self.attempt_number += 1\n",
      "        self.next_action = None\n",
      "prepare_for_next_attempt(self=<RetryCallState 140359806784704: attempt\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.attempt_number += 1\n",
      "State:\n",
      "<RetryCallState 140359806784704: attempt #2; slept for 0.0; last result: none yet>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def prepare_for_next_attempt(self) -> None:\n",
      "        self.outcome = None\n",
      "        self.outcome_timestamp = None\n",
      "        self.attempt_number += 1\n",
      "        self.next_action = None\n",
      "prepare_for_next_attempt(self=<RetryCallState 140359806784704: attempt\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "self.next_action = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def iter(self, retry_state: \"RetryCallState\") -> t.Union[DoAttempt, DoSleep, t.Any]:\n",
      "        fut = retry_state.outcome\n",
      "        if fut is None:\n",
      "            if self.before is not None:\n",
      "                self.before(retry_state)\n",
      "            return DoAttempt()\n",
      "        is_explicit_retry = fut.failed and isinstance(fut.exception(), TryAgain)\n",
      "        if not (is_explicit_retry or self.retry(retry_state)):\n",
      "            return fut.result()\n",
      "        if self.after is not None:\n",
      "            self.after(retry_state)\n",
      "        if self.wait:\n",
      "            sleep = self.wait(retry_state)\n",
      "        else:\n",
      "            sleep = 0.0\n",
      "        retry_state.upcoming_sleep = sleep\n",
      "        self.statistics[\"delay_since_first_attempt\"] = retry_state.seconds_since_start\n",
      "        if self.stop(retry_state):\n",
      "            if self.retry_error_callback:\n",
      "                return self.retry_error_callback(retry_state)\n",
      "            retry_exc = self.retry_error_cls(fut)\n",
      "            if self.reraise:\n",
      "                raise retry_exc.reraise()\n",
      "            raise retry_exc from fut.exception()\n",
      "        retry_state.next_action = RetryAction(sleep)\n",
      "        retry_state.idle_for += sleep\n",
      "        self.statistics[\"idle_for\"] += sleep\n",
      "        self.statistics[\"attempt_number\"] += 1\n",
      "        if self.before_sleep is not None:\n",
      "            self.before_sleep(retry_state)\n",
      "        return DoSleep(sleep)\n",
      "iter(self=<AsyncRetrying object at 0x7fa81069c6a0 (stop=<tenacity.stop._stop_never object at 0x7fa811b0ff10>, wait=<tenacity.wait.wait_none object at 0x7fa811db1220>, sleep=<function sleep at 0x7fa81134a8b0>, retry=<tenacity.retry.retry_if_result object at 0x7fa81069cf70>, before=<function before_nothing at 0x7fa810d724c0>, after=<function after_nothing at 0x7fa810d72670>)>, retry_state=<RetryCallState 140359806600192: attempt\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "fut = retry_state.outcome\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _raise_try_again(self):\n",
      "        self._attempts += 1\n",
      "        if self._attempts < 3:\n",
      "            raise tenacity.TryAgain\n",
      "_raise_try_again(self=<tests.test_tenacity.TestRetryConditions testMethod=test_retry_try_again>, self._attempts=0, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fa8108bb640>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_retry_try_again', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_retry_try_again=<bound method TestRetryConditions.test_retry_try_again of <tests.test_tenacity.TestRetryConditions testMethod=test_retry_try_again>>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self._attempts += 1\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _raise_try_again(self):\n",
      "        self._attempts += 1\n",
      "        if self._attempts < 3:\n",
      "            raise tenacity.TryAgain\n",
      "_raise_try_again(self=<tests.test_tenacity.TestRetryConditions testMethod=test_retry_try_again>, self._attempts=0, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fa8108bb640>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_retry_try_again', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_retry_try_again=<bound method TestRetryConditions.test_retry_try_again of <tests.test_tenacity.TestRetryConditions testMethod=test_retry_try_again>>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "raise tenacity.TryAgain\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _retryable_test_not_exception_message_delay(thing):\n",
      "    return thing.go()\n",
      "_retryable_test_not_exception_message_delay(thing={counter=0, count=3})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "return thing.go()\n",
      "State:\n",
      "{counter=1, count=3}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _retryable_test_with_unless_exception_type_name(thing):\n",
      "    return thing.go()\n",
      "_retryable_test_with_unless_exception_type_name(thing={counter=0, count=5})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "return thing.go()\n",
      "State:\n",
      "{counter=1, count=5}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _retryable_test_with_unless_exception_type_no_input(thing):\n",
      "    return thing.go()\n",
      "_retryable_test_with_unless_exception_type_no_input(thing={counter=0, count=5})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "return thing.go()\n",
      "State:\n",
      "{counter=1, count=5}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _retryable_test_with_unless_exception_type_name_attempt_limit(thing):\n",
      "    return thing.go()\n",
      "_retryable_test_with_unless_exception_type_name_attempt_limit(thing={counter=0, count=2})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "return thing.go()\n",
      "State:\n",
      "{counter=1, count=2}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _retryable_test_with_stop(thing):\n",
      "    return thing.go()\n",
      "_retryable_test_with_stop(thing={counter=0, count=5})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "return thing.go()\n",
      "State:\n",
      "{counter=1, count=5}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _retryable_test_with_wait(thing):\n",
      "    return thing.go()\n",
      "_retryable_test_with_wait(thing={counter=0, count=5})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "return thing.go()\n",
      "State:\n",
      "{counter=1, count=5}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def emit(self, record):\n",
      "        self.records.append(record)\n",
      "emit(self=<CapturingHandler (NOTSET)>, record=<LogRecord: tests.test_tenacity.TestBeforeAfterAttempts.test_before_sleep_log_raises, 20, /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/jd+tenacity/jd+tenacity/tenacity/before_sleep.py, 65, \"Retrying tests.test_tenacity.NoIOErrorAfterCount.go in 0.01 seconds as it raised OSError: Hi there, I'm an IOError.\">, self._name=None, self.filters=[], self.formatter=None, self.level=0, self.lock=<locked _thread.RLock object owner=140359845672768 count=1 at 0x7fa80ffdb030>, self.records=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.records.append(record)\n",
      "State:\n",
      "[<LogRecord: tests.test_tenacity.TestBeforeAfterAttempts.test_before_sleep_log_raises, 20, /local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/jd+tenacity/jd+tenacity/tenacity/before_sleep.py, 65, \"Retrying tests.test_tenacity.NoIOErrorAfterCount.go in 0.01 seconds as it raised OSError: Hi there, I'm an IOError.\">]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_before_sleep_log_returns(self, exc_info=False):\n",
      "        thing = NoneReturnUntilAfterCount(2)\n",
      "        logger = logging.getLogger(self.id())\n",
      "        logger.propagate = False\n",
      "        logger.setLevel(logging.INFO)\n",
      "        handler = CapturingHandler()\n",
      "        logger.addHandler(handler)\n",
      "        try:\n",
      "            _before_sleep = tenacity.before_sleep_log(logger, logging.INFO, exc_info=exc_info)\n",
      "            _retry = tenacity.retry_if_result(lambda result: result is None)\n",
      "            retrying = Retrying(\n",
      "                wait=tenacity.wait_fixed(0.01),\n",
      "                stop=tenacity.stop_after_attempt(3),\n",
      "                retry=_retry,\n",
      "                before_sleep=_before_sleep,\n",
      "            )\n",
      "            retrying(thing.go)\n",
      "        finally:\n",
      "            logger.removeHandler(handler)\n",
      "        etalon_re = r\"^Retrying .* in 0\\.01 seconds as it returned None\\.$\"\n",
      "        self.assertEqual(len(handler.records), 2)\n",
      "        fmt = logging.Formatter().format\n",
      "        self.assertRegex(fmt(handler.records[0]), etalon_re)\n",
      "        self.assertRegex(fmt(handler.records[1]), etalon_re)\n",
      "test_before_sleep_log_returns(self=<tests.test_tenacity.TestBeforeAfterAttempts testMethod=test_before_sleep_log_returns>, exc_info=False, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fa80ffdbeb0>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_before_sleep_log_returns', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_before_sleep_log_returns=<bound method TestBeforeAfterAttempts.test_before_sleep_log_returns of <tests.test_tenacity.TestBeforeAfterAttempts testMethod=test_before_sleep_log_returns>>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "thing = NoneReturnUntilAfterCount(2)\n",
      "State:\n",
      "{counter=0, count=2}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_before_sleep_log_returns(self, exc_info=False):\n",
      "        thing = NoneReturnUntilAfterCount(2)\n",
      "        logger = logging.getLogger(self.id())\n",
      "        logger.propagate = False\n",
      "        logger.setLevel(logging.INFO)\n",
      "        handler = CapturingHandler()\n",
      "        logger.addHandler(handler)\n",
      "        try:\n",
      "            _before_sleep = tenacity.before_sleep_log(logger, logging.INFO, exc_info=exc_info)\n",
      "            _retry = tenacity.retry_if_result(lambda result: result is None)\n",
      "            retrying = Retrying(\n",
      "                wait=tenacity.wait_fixed(0.01),\n",
      "                stop=tenacity.stop_after_attempt(3),\n",
      "                retry=_retry,\n",
      "                before_sleep=_before_sleep,\n",
      "            )\n",
      "            retrying(thing.go)\n",
      "        finally:\n",
      "            logger.removeHandler(handler)\n",
      "        etalon_re = r\"^Retrying .* in 0\\.01 seconds as it returned None\\.$\"\n",
      "        self.assertEqual(len(handler.records), 2)\n",
      "        fmt = logging.Formatter().format\n",
      "        self.assertRegex(fmt(handler.records[0]), etalon_re)\n",
      "        self.assertRegex(fmt(handler.records[1]), etalon_re)\n",
      "test_before_sleep_log_returns(self=<tests.test_tenacity.TestBeforeAfterAttempts testMethod=test_before_sleep_log_returns>, exc_info=False, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fa80ffdbeb0>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_before_sleep_log_returns', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_before_sleep_log_returns=<bound method TestBeforeAfterAttempts.test_before_sleep_log_returns of <tests.test_tenacity.TestBeforeAfterAttempts testMethod=test_before_sleep_log_returns>>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "logger = logging.getLogger(self.id())\n",
      "State:\n",
      "<Logger tests.test_tenacity.TestBeforeAfterAttempts.test_before_sleep_log_returns (WARNING)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_before_sleep_log_returns(self, exc_info=False):\n",
      "        thing = NoneReturnUntilAfterCount(2)\n",
      "        logger = logging.getLogger(self.id())\n",
      "        logger.propagate = False\n",
      "        logger.setLevel(logging.INFO)\n",
      "        handler = CapturingHandler()\n",
      "        logger.addHandler(handler)\n",
      "        try:\n",
      "            _before_sleep = tenacity.before_sleep_log(logger, logging.INFO, exc_info=exc_info)\n",
      "            _retry = tenacity.retry_if_result(lambda result: result is None)\n",
      "            retrying = Retrying(\n",
      "                wait=tenacity.wait_fixed(0.01),\n",
      "                stop=tenacity.stop_after_attempt(3),\n",
      "                retry=_retry,\n",
      "                before_sleep=_before_sleep,\n",
      "            )\n",
      "            retrying(thing.go)\n",
      "        finally:\n",
      "            logger.removeHandler(handler)\n",
      "        etalon_re = r\"^Retrying .* in 0\\.01 seconds as it returned None\\.$\"\n",
      "        self.assertEqual(len(handler.records), 2)\n",
      "        fmt = logging.Formatter().format\n",
      "        self.assertRegex(fmt(handler.records[0]), etalon_re)\n",
      "        self.assertRegex(fmt(handler.records[1]), etalon_re)\n",
      "test_before_sleep_log_returns(self=<tests.test_tenacity.TestBeforeAfterAttempts testMethod=test_before_sleep_log_returns>, exc_info=False, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fa80ffdbeb0>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_before_sleep_log_returns', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_before_sleep_log_returns=<bound method TestBeforeAfterAttempts.test_before_sleep_log_returns of <tests.test_tenacity.TestBeforeAfterAttempts testMethod=test_before_sleep_log_returns>>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "logger.setLevel(logging.INFO)\n",
      "State:\n",
      "<Logger tests.test_tenacity.TestBeforeAfterAttempts.test_before_sleep_log_returns (INFO)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_before_sleep_log_returns(self, exc_info=False):\n",
      "        thing = NoneReturnUntilAfterCount(2)\n",
      "        logger = logging.getLogger(self.id())\n",
      "        logger.propagate = False\n",
      "        logger.setLevel(logging.INFO)\n",
      "        handler = CapturingHandler()\n",
      "        logger.addHandler(handler)\n",
      "        try:\n",
      "            _before_sleep = tenacity.before_sleep_log(logger, logging.INFO, exc_info=exc_info)\n",
      "            _retry = tenacity.retry_if_result(lambda result: result is None)\n",
      "            retrying = Retrying(\n",
      "                wait=tenacity.wait_fixed(0.01),\n",
      "                stop=tenacity.stop_after_attempt(3),\n",
      "                retry=_retry,\n",
      "                before_sleep=_before_sleep,\n",
      "            )\n",
      "            retrying(thing.go)\n",
      "        finally:\n",
      "            logger.removeHandler(handler)\n",
      "        etalon_re = r\"^Retrying .* in 0\\.01 seconds as it returned None\\.$\"\n",
      "        self.assertEqual(len(handler.records), 2)\n",
      "        fmt = logging.Formatter().format\n",
      "        self.assertRegex(fmt(handler.records[0]), etalon_re)\n",
      "        self.assertRegex(fmt(handler.records[1]), etalon_re)\n",
      "test_before_sleep_log_returns(self=<tests.test_tenacity.TestBeforeAfterAttempts testMethod=test_before_sleep_log_returns>, exc_info=False, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fa80ffdbeb0>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_before_sleep_log_returns', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_before_sleep_log_returns=<bound method TestBeforeAfterAttempts.test_before_sleep_log_returns of <tests.test_tenacity.TestBeforeAfterAttempts testMethod=test_before_sleep_log_returns>>)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "handler = CapturingHandler()\n",
      "State:\n",
      "<CapturingHandler (NOTSET)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_before_sleep_log_returns(self, exc_info=False):\n",
      "        thing = NoneReturnUntilAfterCount(2)\n",
      "        logger = logging.getLogger(self.id())\n",
      "        logger.propagate = False\n",
      "        logger.setLevel(logging.INFO)\n",
      "        handler = CapturingHandler()\n",
      "        logger.addHandler(handler)\n",
      "        try:\n",
      "            _before_sleep = tenacity.before_sleep_log(logger, logging.INFO, exc_info=exc_info)\n",
      "            _retry = tenacity.retry_if_result(lambda result: result is None)\n",
      "            retrying = Retrying(\n",
      "                wait=tenacity.wait_fixed(0.01),\n",
      "                stop=tenacity.stop_after_attempt(3),\n",
      "                retry=_retry,\n",
      "                before_sleep=_before_sleep,\n",
      "            )\n",
      "            retrying(thing.go)\n",
      "        finally:\n",
      "            logger.removeHandler(handler)\n",
      "        etalon_re = r\"^Retrying .* in 0\\.01 seconds as it returned None\\.$\"\n",
      "        self.assertEqual(len(handler.records), 2)\n",
      "        fmt = logging.Formatter().format\n",
      "        self.assertRegex(fmt(handler.records[0]), etalon_re)\n",
      "        self.assertRegex(fmt(handler.records[1]), etalon_re)\n",
      "test_before_sleep_log_returns(self=<tests.test_tenacity.TestBeforeAfterAttempts testMethod=test_before_sleep_log_returns>, exc_info=False, self._cleanups=[], self._outcome=<unittest.case._Outcome object at 0x7fa80ffdbeb0>, self._subtest=None, self._testMethodDoc=None, self._testMethodName='test_before_sleep_log_returns', self._type_equality_funcs={<class 'dict'>: 'assertDictEqual', <class 'list'>: 'assertListEqual', <class 'tuple'>: 'assertTupleEqual', <class 'set'>: 'assertSetEqual', <class 'frozenset'>: 'assertSetEqual', <class 'str'>: 'assertMultiLineEqual'}, self.test_before_sleep_log_returns=<bound method TestBeforeAfterAttempts.test_before_sleep_log_returns of <tests.test_tenacity.TestBeforeAfterAttempts testMethod=test_before_sleep_log_returns>>)\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "etalon_re = r\"^Retrying .* in 0\\.01 seconds as it returned None\\.$\"\n",
      "State:\n",
      "'^Retrying .* in 0\\\\.01 seconds as it returned None\\\\.$'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _reraised_by_default():\n",
      "            calls.append(\"x\")\n",
      "            raise KeyError(\"Bad key\")\n",
      "_reraised_by_default(calls=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "calls.append(\"x\")\n",
      "State:\n",
      "['x']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _reraised_by_default():\n",
      "            calls.append(\"x\")\n",
      "            raise KeyError(\"Bad key\")\n",
      "_reraised_by_default(calls=[])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "raise KeyError(\"Bad key\")\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _raise_key_error():\n",
      "            calls.append(\"x\")\n",
      "            raise KeyError(\"Bad key\")\n",
      "_raise_key_error(calls=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "calls.append(\"x\")\n",
      "State:\n",
      "['x']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _raise_key_error():\n",
      "            calls.append(\"x\")\n",
      "            raise KeyError(\"Bad key\")\n",
      "_raise_key_error(calls=[])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "raise KeyError(\"Bad key\")\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _import_plugins(plugin_names):\n",
      "    plugins = []\n",
      "    for name, path in plugin_names.items():\n",
      "        log.debug(\"Importing plugin %s: %s\", name, path)\n",
      "        if '.' not in path:\n",
      "            plugins.append(__import__(path))\n",
      "        else:\n",
      "            package, module = path.rsplit('.', 1)\n",
      "            module = __import__(path, fromlist=[module])\n",
      "            plugins.append(module)\n",
      "    return plugins\n",
      "_import_plugins(plugin_names={'__builtin__': 'awscli.handlers'})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "plugins = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _transform(self, schema, shapes, shape_name):\n",
      "        if 'type' not in schema:\n",
      "            raise ParameterRequiredError(\"Missing required key: 'type'\")\n",
      "        if schema['type'] == 'object':\n",
      "            shapes[shape_name] = self._transform_structure(schema, shapes)\n",
      "        elif schema['type'] == 'array':\n",
      "            shapes[shape_name] = self._transform_list(schema, shapes)\n",
      "        elif schema['type'] == 'map':\n",
      "            shapes[shape_name] = self._transform_map(schema, shapes)\n",
      "        else:\n",
      "            shapes[shape_name] = self._transform_scalar(schema)\n",
      "        return shapes\n",
      "_transform(self=<awscli.schema.SchemaTransformer object at 0x7f521c755a30>, schema={'type': 'string'}, shapes={}, shape_name='InputShape', self._shape_namer=<awscli.schema.ShapeNameGenerator object at 0x7f521c7551c0>)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "shapes[shape_name] = self._transform_scalar(schema)\n",
      "State:\n",
      "{'InputShape': {'type': 'string'}}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _populate_initial_shape(self, schema):\n",
      "        shape = {'type': self._json_schema_to_aws_type(schema)}\n",
      "        if 'description' in schema:\n",
      "            shape['documentation'] = schema['description']\n",
      "        if 'enum' in schema:\n",
      "            shape['enum'] = schema['enum']\n",
      "        return shape\n",
      "_populate_initial_shape(self=<awscli.schema.SchemaTransformer object at 0x7f521c755a30>, schema={'type': 'string'}, self._shape_namer=<awscli.schema.ShapeNameGenerator object at 0x7f521c7551c0>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "shape = {'type': self._json_schema_to_aws_type(schema)}\n",
      "State:\n",
      "{'type': 'string'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def create_help_command(self):\n",
      "        cli_data = self._get_cli_data()\n",
      "        return ProviderHelpCommand(self.session, self._get_command_table(),\n",
      "                                   self._get_argument_table(),\n",
      "                                   cli_data.get('description', None),\n",
      "                                   cli_data.get('synopsis', None),\n",
      "                                   cli_data.get('help_usage', None))\n",
      "create_help_command(self=<awscli.clidriver.CLIDriver object at 0x7f521cb6c220>, self._argument_table=None, self._cli_data=None, self._command_table=None, self.alias_loader=<awscli.alias.AliasLoader object at 0x7f521c7ca9a0>, self.session=<botocore.session.Session object at 0x7f521d1a98b0>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "cli_data = self._get_cli_data()\n",
      "State:\n",
      "OrderedDict([('description', 'The AWS Command Line Interface is a unified tool to manage your AWS services.'), ('synopsis', 'aws [options] <command> <subcommand> [parameters]'), ('help_usage', 'Use *aws command help* for information on a specific command. Use *aws help topics* to view a list of available help topics. The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets.'), ('options', OrderedDict([('debug', OrderedDict([('action', 'store_true'), ('help', '<p>Turn on debug logging.</p>')])), ('endpoint-url', OrderedDict([('help', \"<p>Override command's default URL with the given URL.</p>\")])), ('no-verify-ssl', OrderedDict([('action', 'store_false'), ('dest', 'verify_ssl'), ('help', '<p>By default, the AWS CLI uses SSL when communicating with AWS services.  For each SSL connection, the AWS CLI will verify SSL certificates.  This option overrides the default behavior of verifying SSL certificates.</p>')])), ('no-paginate', OrderedDict([('action', 'store_false'), ('help', '<p>Disable automatic pagination.</p>'), ('dest', 'paginate')])), ('output', OrderedDict([('choices', ['json', 'text', 'table']), ('help', '<p>The formatting style for command output.</p>')])), ('query', OrderedDict([('help', '<p>A JMESPath query to use in filtering the response data.</p>')])), ('profile', OrderedDict([('help', '<p>Use a specific profile from your credential file.</p>')])), ('region', OrderedDict([('help', '<p>The region to use.  Overrides config/env settings.</p>')])), ('version', OrderedDict([('action', 'version'), ('help', '<p>Display the version of this tool.</p>')])), ('color', OrderedDict([('choices', ['on', 'off', 'auto']), ('default', 'auto'), ('help', '<p>Turn on/off color output.</p>')])), ('no-sign-request', OrderedDict([('action', 'store_false'), ('dest', 'sign_request'), ('help', '<p>Do not sign requests.  Credentials will not be loaded if this argument is provided.</p>')])), ('ca-bundle', OrderedDict([('dest', 'ca_bundle'), ('help', '<p>The CA certificate bundle to use when verifying SSL certificates. Overrides config/env settings.</p>')])), ('cli-read-timeout', OrderedDict([('dest', 'read_timeout'), ('type', 'int'), ('help', '<p>The maximum socket read time in seconds. If the value is set to 0, the socket read will be blocking and not timeout. The default value is 60 seconds.</p>')])), ('cli-connect-timeout', OrderedDict([('dest', 'connect_timeout'), ('type', 'int'), ('help', '<p>The maximum socket connect time in seconds. If the value is set to 0, the socket connect will be blocking and not timeout. The default value is 60 seconds.</p>')]))]))])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _build_builtin_commands(self, session):\n",
      "        commands = OrderedDict()\n",
      "        services = session.get_available_services()\n",
      "        for service_name in services:\n",
      "            commands[service_name] = ServiceCommand(cli_name=service_name,\n",
      "                                                    session=self.session,\n",
      "                                                    service_name=service_name)\n",
      "        return commands\n",
      "_build_builtin_commands(self=<awscli.clidriver.CLIDriver object at 0x7f521cb6c220>, session={_original_handler=<botocore.hooks.HierarchicalEmitter object at 0x7f521d1a9820>, _events=<botocore.hooks.EventAliaser object at 0x7f521d1a9700>, user_agent_name='aws-cli', user_agent_version='1.32.28', user_agent_extra='botocore/1.34.77', _profile=None, _config={'profiles': {}}, _credentials=None, _auth_token=None, _profile_map=None, _session_instance_vars={}, _client_config=None, _last_client_region_used=None, _components=<botocore.session.ComponentLocator object at 0x7f521d199e20>, _internal_components=<botocore.session.ComponentLocator object at 0x7f521d1a9be0>, session_var_map=<botocore.session.SessionVarDict object at 0x7f521d161d90>}, self._argument_table=None, self._cli_data=OrderedDict([('description', 'The AWS Command Line Interface is a unified tool to manage your AWS services.'), ('synopsis', 'aws [options] <command> <subcommand> [parameters]'), ('help_usage', 'Use *aws command help* for information on a specific command. Use *aws help topics* to view a list of available help topics. The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets.'), ('options', OrderedDict([('debug', OrderedDict([('action', 'store_true'), ('help', '<p>Turn on debug logging.</p>')])), ('endpoint-url', OrderedDict([('help', \"<p>Override command's default URL with the given URL.</p>\")])), ('no-verify-ssl', OrderedDict([('action', 'store_false'), ('dest', 'verify_ssl'), ('help', '<p>By default, the AWS CLI uses SSL when communicating with AWS services.  For each SSL connection, the AWS CLI will verify SSL certificates.  This option overrides the default behavior of verifying SSL certificates.</p>')])), ('no-paginate', OrderedDict([('action', 'store_false'), ('help', '<p>Disable automatic pagination.</p>'), ('dest', 'paginate')])), ('output', OrderedDict([('choices', ['json', 'text', 'table']), ('help', '<p>The formatting style for command output.</p>')])), ('query', OrderedDict([('help', '<p>A JMESPath query to use in filtering the response data.</p>')])), ('profile', OrderedDict([('help', '<p>Use a specific profile from your credential file.</p>')])), ('region', OrderedDict([('help', '<p>The region to use.  Overrides config/env settings.</p>')])), ('version', OrderedDict([('action', 'version'), ('help', '<p>Display the version of this tool.</p>')])), ('color', OrderedDict([('choices', ['on', 'off', 'auto']), ('default', 'auto'), ('help', '<p>Turn on/off color output.</p>')])), ('no-sign-request', OrderedDict([('action', 'store_false'), ('dest', 'sign_request'), ('help', '<p>Do not sign requests.  Credentials will not be loaded if this argument is provided.</p>')])), ('ca-bundle', OrderedDict([('dest', 'ca_bundle'), ('help', '<p>The CA certificate bundle to use when verifying SSL certificates. Overrides config/env settings.</p>')])), ('cli-read-timeout', OrderedDict([('dest', 'read_timeout'), ('type', 'int'), ('help', '<p>The maximum socket read time in seconds. If the value is set to 0, the socket read will be blocking and not timeout. The default value is 60 seconds.</p>')])), ('cli-connect-timeout', OrderedDict([('dest', 'connect_timeout'), ('type', 'int'), ('help', '<p>The maximum socket connect time in seconds. If the value is set to 0, the socket connect will be blocking and not timeout. The default value is 60 seconds.</p>')]))]))]), self._command_table=None, self.alias_loader=<awscli.alias.AliasLoader object at 0x7f521c7ca9a0>, self.session=<botocore.session.Session object at 0x7f521d1a98b0>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "commands = OrderedDict()\n",
      "State:\n",
      "OrderedDict()\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _build_builtin_commands(self, session):\n",
      "        commands = OrderedDict()\n",
      "        services = session.get_available_services()\n",
      "        for service_name in services:\n",
      "            commands[service_name] = ServiceCommand(cli_name=service_name,\n",
      "                                                    session=self.session,\n",
      "                                                    service_name=service_name)\n",
      "        return commands\n",
      "_build_builtin_commands(self=<awscli.clidriver.CLIDriver object at 0x7f521cb6c220>, session={_original_handler=<botocore.hooks.HierarchicalEmitter object at 0x7f521d1a9820>, _events=<botocore.hooks.EventAliaser object at 0x7f521d1a9700>, user_agent_name='aws-cli', user_agent_version='1.32.28', user_agent_extra='botocore/1.34.77', _profile=None, _config={'profiles': {}}, _credentials=None, _auth_token=None, _profile_map=None, _session_instance_vars={}, _client_config=None, _last_client_region_used=None, _components=<botocore.session.ComponentLocator object at 0x7f521d199e20>, _internal_components=<botocore.session.ComponentLocator object at 0x7f521d1a9be0>, session_var_map=<botocore.session.SessionVarDict object at 0x7f521d161d90>}, self._argument_table=None, self._cli_data=OrderedDict([('description', 'The AWS Command Line Interface is a unified tool to manage your AWS services.'), ('synopsis', 'aws [options] <command> <subcommand> [parameters]'), ('help_usage', 'Use *aws command help* for information on a specific command. Use *aws help topics* to view a list of available help topics. The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets.'), ('options', OrderedDict([('debug', OrderedDict([('action', 'store_true'), ('help', '<p>Turn on debug logging.</p>')])), ('endpoint-url', OrderedDict([('help', \"<p>Override command's default URL with the given URL.</p>\")])), ('no-verify-ssl', OrderedDict([('action', 'store_false'), ('dest', 'verify_ssl'), ('help', '<p>By default, the AWS CLI uses SSL when communicating with AWS services.  For each SSL connection, the AWS CLI will verify SSL certificates.  This option overrides the default behavior of verifying SSL certificates.</p>')])), ('no-paginate', OrderedDict([('action', 'store_false'), ('help', '<p>Disable automatic pagination.</p>'), ('dest', 'paginate')])), ('output', OrderedDict([('choices', ['json', 'text', 'table']), ('help', '<p>The formatting style for command output.</p>')])), ('query', OrderedDict([('help', '<p>A JMESPath query to use in filtering the response data.</p>')])), ('profile', OrderedDict([('help', '<p>Use a specific profile from your credential file.</p>')])), ('region', OrderedDict([('help', '<p>The region to use.  Overrides config/env settings.</p>')])), ('version', OrderedDict([('action', 'version'), ('help', '<p>Display the version of this tool.</p>')])), ('color', OrderedDict([('choices', ['on', 'off', 'auto']), ('default', 'auto'), ('help', '<p>Turn on/off color output.</p>')])), ('no-sign-request', OrderedDict([('action', 'store_false'), ('dest', 'sign_request'), ('help', '<p>Do not sign requests.  Credentials will not be loaded if this argument is provided.</p>')])), ('ca-bundle', OrderedDict([('dest', 'ca_bundle'), ('help', '<p>The CA certificate bundle to use when verifying SSL certificates. Overrides config/env settings.</p>')])), ('cli-read-timeout', OrderedDict([('dest', 'read_timeout'), ('type', 'int'), ('help', '<p>The maximum socket read time in seconds. If the value is set to 0, the socket read will be blocking and not timeout. The default value is 60 seconds.</p>')])), ('cli-connect-timeout', OrderedDict([('dest', 'connect_timeout'), ('type', 'int'), ('help', '<p>The maximum socket connect time in seconds. If the value is set to 0, the socket connect will be blocking and not timeout. The default value is 60 seconds.</p>')]))]))]), self._command_table=None, self.alias_loader=<awscli.alias.AliasLoader object at 0x7f521c7ca9a0>, self.session=<botocore.session.Session object at 0x7f521d1a98b0>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "services = session.get_available_services()\n",
      "State:\n",
      "['accessanalyzer', 'account', 'acm', 'acm-pca', 'alexaforbusiness', 'amp', 'amplify', 'amplifybackend', 'amplifyuibuilder', 'apigateway', 'apigatewaymanagementapi', 'apigatewayv2', 'appconfig', 'appconfigdata', 'appfabric', 'appflow', 'appintegrations', 'application-autoscaling', 'application-insights', 'applicationcostprofiler', 'appmesh', 'apprunner', 'appstream', 'appsync', 'arc-zonal-shift', 'artifact', 'athena', 'auditmanager', 'autoscaling', 'autoscaling-plans', 'b2bi', 'backup', 'backup-gateway', 'backupstorage', 'batch', 'bcm-data-exports', 'bedrock', 'bedrock-agent', 'bedrock-agent-runtime', 'bedrock-runtime', 'billingconductor', 'braket', 'budgets', 'ce', 'chatbot', 'chime', 'chime-sdk-identity', 'chime-sdk-media-pipelines', 'chime-sdk-meetings', 'chime-sdk-messaging', 'chime-sdk-voice', 'cleanrooms', 'cleanroomsml', 'cloud9', 'cloudcontrol', 'clouddirectory', 'cloudformation', 'cloudfront', 'cloudfront-keyvaluestore', 'cloudhsm', 'cloudhsmv2', 'cloudsearch', 'cloudsearchdomain', 'cloudtrail', 'cloudtrail-data', 'cloudwatch', 'codeartifact', 'codebuild', 'codecatalyst', 'codecommit', 'codeconnections', 'codedeploy', 'codeguru-reviewer', 'codeguru-security', 'codeguruprofiler', 'codepipeline', 'codestar', 'codestar-connections', 'codestar-notifications', 'cognito-identity', 'cognito-idp', 'cognito-sync', 'comprehend', 'comprehendmedical', 'compute-optimizer', 'config', 'connect', 'connect-contact-lens', 'connectcampaigns', 'connectcases', 'connectparticipant', 'controltower', 'cost-optimization-hub', 'cur', 'customer-profiles', 'databrew', 'dataexchange', 'datapipeline', 'datasync', 'datazone', 'dax', 'deadline', 'detective', 'devicefarm', 'devops-guru', 'directconnect', 'discovery', 'dlm', 'dms', 'docdb', 'docdb-elastic', 'drs', 'ds', 'dynamodb', 'dynamodbstreams', 'ebs', 'ec2', 'ec2-instance-connect', 'ecr', 'ecr-public', 'ecs', 'efs', 'eks', 'eks-auth', 'elastic-inference', 'elasticache', 'elasticbeanstalk', 'elastictranscoder', 'elb', 'elbv2', 'emr', 'emr-containers', 'emr-serverless', 'entityresolution', 'es', 'events', 'evidently', 'finspace', 'finspace-data', 'firehose', 'fis', 'fms', 'forecast', 'forecastquery', 'frauddetector', 'freetier', 'fsx', 'gamelift', 'glacier', 'globalaccelerator', 'glue', 'grafana', 'greengrass', 'greengrassv2', 'groundstation', 'guardduty', 'health', 'healthlake', 'honeycode', 'iam', 'identitystore', 'imagebuilder', 'importexport', 'inspector', 'inspector-scan', 'inspector2', 'internetmonitor', 'iot', 'iot-data', 'iot-jobs-data', 'iot1click-devices', 'iot1click-projects', 'iotanalytics', 'iotdeviceadvisor', 'iotevents', 'iotevents-data', 'iotfleethub', 'iotfleetwise', 'iotsecuretunneling', 'iotsitewise', 'iotthingsgraph', 'iottwinmaker', 'iotwireless', 'ivs', 'ivs-realtime', 'ivschat', 'kafka', 'kafkaconnect', 'kendra', 'kendra-ranking', 'keyspaces', 'kinesis', 'kinesis-video-archived-media', 'kinesis-video-media', 'kinesis-video-signaling', 'kinesis-video-webrtc-storage', 'kinesisanalytics', 'kinesisanalyticsv2', 'kinesisvideo', 'kms', 'lakeformation', 'lambda', 'launch-wizard', 'lex-models', 'lex-runtime', 'lexv2-models', 'lexv2-runtime', 'license-manager', 'license-manager-linux-subscriptions', 'license-manager-user-subscriptions', 'lightsail', 'location', 'logs', 'lookoutequipment', 'lookoutmetrics', 'lookoutvision', 'm2', 'machinelearning', 'macie2', 'managedblockchain', 'managedblockchain-query', 'marketplace-agreement', 'marketplace-catalog', 'marketplace-deployment', 'marketplace-entitlement', 'marketplacecommerceanalytics', 'mediaconnect', 'mediaconvert', 'medialive', 'mediapackage', 'mediapackage-vod', 'mediapackagev2', 'mediastore', 'mediastore-data', 'mediatailor', 'medical-imaging', 'memorydb', 'meteringmarketplace', 'mgh', 'mgn', 'migration-hub-refactor-spaces', 'migrationhub-config', 'migrationhuborchestrator', 'migrationhubstrategy', 'mobile', 'mq', 'mturk', 'mwaa', 'neptune', 'neptune-graph', 'neptunedata', 'network-firewall', 'networkmanager', 'networkmonitor', 'nimble', 'oam', 'omics', 'opensearch', 'opensearchserverless', 'opsworks', 'opsworkscm', 'organizations', 'osis', 'outposts', 'panorama', 'payment-cryptography', 'payment-cryptography-data', 'pca-connector-ad', 'personalize', 'personalize-events', 'personalize-runtime', 'pi', 'pinpoint', 'pinpoint-email', 'pinpoint-sms-voice', 'pinpoint-sms-voice-v2', 'pipes', 'polly', 'pricing', 'privatenetworks', 'proton', 'qbusiness', 'qconnect', 'qldb', 'qldb-session', 'quicksight', 'ram', 'rbin', 'rds', 'rds-data', 'redshift', 'redshift-data', 'redshift-serverless', 'rekognition', 'repostspace', 'resiliencehub', 'resource-explorer-2', 'resource-groups', 'resourcegroupstaggingapi', 'robomaker', 'rolesanywhere', 'route53', 'route53-recovery-cluster', 'route53-recovery-control-config', 'route53-recovery-readiness', 'route53domains', 'route53resolver', 'rum', 's3', 's3control', 's3outposts', 'sagemaker', 'sagemaker-a2i-runtime', 'sagemaker-edge', 'sagemaker-featurestore-runtime', 'sagemaker-geospatial', 'sagemaker-metrics', 'sagemaker-runtime', 'savingsplans', 'scheduler', 'schemas', 'sdb', 'secretsmanager', 'securityhub', 'securitylake', 'serverlessrepo', 'service-quotas', 'servicecatalog', 'servicecatalog-appregistry', 'servicediscovery', 'ses', 'sesv2', 'shield', 'signer', 'simspaceweaver', 'sms', 'sms-voice', 'snow-device-management', 'snowball', 'sns', 'sqs', 'ssm', 'ssm-contacts', 'ssm-incidents', 'ssm-sap', 'sso', 'sso-admin', 'sso-oidc', 'stepfunctions', 'storagegateway', 'sts', 'supplychain', 'support', 'support-app', 'swf', 'synthetics', 'textract', 'timestream-influxdb', 'timestream-query', 'timestream-write', 'tnb', 'transcribe', 'transfer', 'translate', 'trustedadvisor', 'verifiedpermissions', 'voice-id', 'vpc-lattice', 'waf', 'waf-regional', 'wafv2', 'wellarchitected', 'wisdom', 'workdocs', 'worklink', 'workmail', 'workmailmessageflow', 'workspaces', 'workspaces-thin-client', 'workspaces-web', 'xray']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def service_model(self):\n",
      "        return self._get_service_model()\n",
      "service_model(self=<awscli.clidriver.ServiceCommand object at 0x7f521c12dac0>, self._command_table=None, self._lineage=[<awscli.clidriver.ServiceCommand object at 0x7f521c12dac0>], self._name='sdb', self._service_model=None, self._service_name='sdb', self.session=<botocore.session.Session object at 0x7f521d1a98b0>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "return self._get_service_model()\n",
      "State:\n",
      "ServiceModel(sdb)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_service_model(self):\n",
      "        if self._service_model is None:\n",
      "            try:\n",
      "                api_version = self.session.get_config_variable(\n",
      "                    'api_versions').get(self._service_name, None)\n",
      "            except ProfileNotFound:\n",
      "                api_version = None\n",
      "            self._service_model = self.session.get_service_model(\n",
      "                self._service_name, api_version=api_version)\n",
      "        return self._service_model\n",
      "_get_service_model(self=<awscli.clidriver.ServiceCommand object at 0x7f521c12dac0>, self._command_table=None, self._lineage=[<awscli.clidriver.ServiceCommand object at 0x7f521c12dac0>], self._name='sdb', self._service_model=None, self._service_name='sdb', self.session=<botocore.session.Session object at 0x7f521d1a98b0>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "api_version = self.session.get_config_variable(\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def reset(self):\n",
      "        six.moves.html_parser.HTMLParser.reset(self)\n",
      "        self.tree = HTMLTree(self.doc)\n",
      "reset(self=<awscli.bcdoc.docstringparser.DocStringParser object at 0x7f521bdd35e0>, self.convert_charrefs=True, self.doc=<awscli.bcdoc.restdoc.ReSTDocument object at 0x7f521bdd3070>, self.tree=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "six.moves.html_parser.HTMLParser.reset(self)\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _create_command_table(self):\n",
      "        command_table = OrderedDict()\n",
      "        service_model = self._get_service_model()\n",
      "        for operation_name in service_model.operation_names:\n",
      "            cli_name = xform_name(operation_name, '-')\n",
      "            operation_model = service_model.operation_model(operation_name)\n",
      "            command_table[cli_name] = ServiceOperation(\n",
      "                name=cli_name,\n",
      "                parent_name=self._name,\n",
      "                session=self.session,\n",
      "                operation_model=operation_model,\n",
      "                operation_caller=CLIOperationCaller(self.session),\n",
      "            )\n",
      "        self.session.emit('building-command-table.%s' % self._name,\n",
      "                          command_table=command_table,\n",
      "                          session=self.session,\n",
      "                          command_object=self)\n",
      "        self._add_lineage(command_table)\n",
      "        return command_table\n",
      "_create_command_table(self=<awscli.clidriver.ServiceCommand object at 0x7f521c7ca760>, self._command_table=None, self._lineage=[<awscli.clidriver.ServiceCommand object at 0x7f521c7ca760>], self._name='accessanalyzer', self._service_model=None, self._service_name='accessanalyzer', self.session=<botocore.session.Session object at 0x7f521d1a98b0>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "command_table = OrderedDict()\n",
      "State:\n",
      "OrderedDict()\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _create_command_table(self):\n",
      "        command_table = OrderedDict()\n",
      "        service_model = self._get_service_model()\n",
      "        for operation_name in service_model.operation_names:\n",
      "            cli_name = xform_name(operation_name, '-')\n",
      "            operation_model = service_model.operation_model(operation_name)\n",
      "            command_table[cli_name] = ServiceOperation(\n",
      "                name=cli_name,\n",
      "                parent_name=self._name,\n",
      "                session=self.session,\n",
      "                operation_model=operation_model,\n",
      "                operation_caller=CLIOperationCaller(self.session),\n",
      "            )\n",
      "        self.session.emit('building-command-table.%s' % self._name,\n",
      "                          command_table=command_table,\n",
      "                          session=self.session,\n",
      "                          command_object=self)\n",
      "        self._add_lineage(command_table)\n",
      "        return command_table\n",
      "_create_command_table(self=<awscli.clidriver.ServiceCommand object at 0x7f521c7ca760>, self._command_table=None, self._lineage=[<awscli.clidriver.ServiceCommand object at 0x7f521c7ca760>], self._name='accessanalyzer', self._service_model=None, self._service_name='accessanalyzer', self.session=<botocore.session.Session object at 0x7f521d1a98b0>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "service_model = self._get_service_model()\n",
      "State:\n",
      "ServiceModel(accessanalyzer)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def create_help_command_table(self):\n",
      "        commands = {}\n",
      "        for command in self.SUBCOMMANDS:\n",
      "            commands[command['name']] = command['command_class'](self._session)\n",
      "        self._add_lineage(commands)\n",
      "        return commands\n",
      "create_help_command_table(self=<awscli.customizations.s3.s3.S3 object at 0x7f521c000700>, self._arg_table=None, self._lineage=[<awscli.customizations.s3.s3.S3 object at 0x7f521c000700>], self._session=<botocore.session.Session object at 0x7f521d1a98b0>, self._subcommand_table=None)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "commands = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def arg_table(self):\n",
      "        if self._arg_table is None:\n",
      "            self._arg_table = self._build_arg_table()\n",
      "        return self._arg_table\n",
      "arg_table(self=<awscli.customizations.s3.s3.S3 object at 0x7f521c000700>, self._arg_table=None, self._lineage=[<awscli.customizations.s3.s3.S3 object at 0x7f521c000700>], self._session=<botocore.session.Session object at 0x7f521d1a98b0>, self._subcommand_table=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self._arg_table = self._build_arg_table()\n",
      "State:\n",
      "OrderedDict()\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _build_arg_table(self):\n",
      "        arg_table = OrderedDict()\n",
      "        self._session.emit('building-arg-table.%s' % self.NAME,\n",
      "                           arg_table=self.ARG_TABLE)\n",
      "        for arg_data in self.ARG_TABLE:\n",
      "            if 'schema' in arg_data:\n",
      "                argument_model = create_argument_model_from_schema(\n",
      "                    arg_data.pop('schema'))\n",
      "                arg_data['argument_model'] = argument_model\n",
      "            custom_argument = CustomArgument(**arg_data)\n",
      "            arg_table[arg_data['name']] = custom_argument\n",
      "        return arg_table\n",
      "_build_arg_table(self=<awscli.customizations.s3.s3.S3 object at 0x7f521c000700>, self._arg_table=None, self._lineage=[<awscli.customizations.s3.s3.S3 object at 0x7f521c000700>], self._session=<botocore.session.Session object at 0x7f521d1a98b0>, self._subcommand_table=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "arg_table = OrderedDict()\n",
      "State:\n",
      "OrderedDict()\n",
      "==================================================\n",
      "Clean Code:\n",
      "def dict_to_ini_section(ini_dict, section_header):\n",
      "    section_str = f'[{section_header}]\\n'\n",
      "    for key, value in ini_dict.items():\n",
      "        if isinstance(value, dict):\n",
      "            section_str += f\"{key} =\\n\"\n",
      "            for new_key, new_value in value.items():\n",
      "                section_str += f\"  {new_key}={new_value}\\n\"\n",
      "        else:\n",
      "            section_str += f\"{key}={value}\\n\"\n",
      "    return section_str + \"\\n\"\n",
      "dict_to_ini_section(ini_dict={'aws_access_key_id': '123', 'aws_secret_access_key': '456', 'region': 'fake-region-10', 'endpoint_url': 'https://global.endpoint.aws'}, section_header='profile service_global_only')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "section_str = f'[{section_header}]\\n'\n",
      "State:\n",
      "'[profile service_global_only]\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def requires_crt(reason=None):\n",
      "    if reason is None:\n",
      "        reason = \"Test requires awscrt to be installed\"\n",
      "    def decorator(func):\n",
      "        return unittest.skipIf(not HAS_CRT, reason)(func)\n",
      "    return decorator\n",
      "requires_crt(reason=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "reason = \"Test requires awscrt to be installed\"\n",
      "State:\n",
      "'Test requires awscrt to be installed'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _create_argument_table(self):\n",
      "        argument_table = OrderedDict()\n",
      "        input_shape = self._operation_model.input_shape\n",
      "        required_arguments = []\n",
      "        arg_dict = {}\n",
      "        if input_shape is not None:\n",
      "            required_arguments = input_shape.required_members\n",
      "            arg_dict = input_shape.members\n",
      "        for arg_name, arg_shape in arg_dict.items():\n",
      "            cli_arg_name = xform_name(arg_name, '-')\n",
      "            arg_class = self.ARG_TYPES.get(arg_shape.type_name,\n",
      "                                           self.DEFAULT_ARG_CLASS)\n",
      "            is_token = arg_shape.metadata.get('idempotencyToken', False)\n",
      "            is_required = arg_name in required_arguments and not is_token\n",
      "            event_emitter = self._session.get_component('event_emitter')\n",
      "            arg_object = arg_class(\n",
      "                name=cli_arg_name,\n",
      "                argument_model=arg_shape,\n",
      "                is_required=is_required,\n",
      "                operation_model=self._operation_model,\n",
      "                serialized_name=arg_name,\n",
      "                event_emitter=event_emitter)\n",
      "            arg_object.add_to_arg_table(argument_table)\n",
      "        LOG.debug(argument_table)\n",
      "        self._emit('building-argument-table.%s.%s' % (self._parent_name,\n",
      "                                                      self._name),\n",
      "                   operation_model=self._operation_model,\n",
      "                   session=self._session,\n",
      "                   command=self,\n",
      "                   argument_table=argument_table)\n",
      "        return argument_table\n",
      "_create_argument_table(self=<awscli.clidriver.ServiceOperation object at 0x7f51d7514100>, self._arg_table=None, self._lineage=[<awscli.clidriver.ServiceCommand object at 0x7f51d7cb2e20>, <awscli.clidriver.ServiceOperation object at 0x7f51d7514100>], self._name='apply-archive-rule', self._operation_caller=<awscli.clidriver.CLIOperationCaller object at 0x7f51d75148b0>, self._operation_model=OperationModel(name=ApplyArchiveRule), self._parent_name='accessanalyzer', self._session=<botocore.session.Session object at 0x7f51d7ea6160>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "argument_table = OrderedDict()\n",
      "State:\n",
      "OrderedDict()\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _create_argument_table(self):\n",
      "        argument_table = OrderedDict()\n",
      "        input_shape = self._operation_model.input_shape\n",
      "        required_arguments = []\n",
      "        arg_dict = {}\n",
      "        if input_shape is not None:\n",
      "            required_arguments = input_shape.required_members\n",
      "            arg_dict = input_shape.members\n",
      "        for arg_name, arg_shape in arg_dict.items():\n",
      "            cli_arg_name = xform_name(arg_name, '-')\n",
      "            arg_class = self.ARG_TYPES.get(arg_shape.type_name,\n",
      "                                           self.DEFAULT_ARG_CLASS)\n",
      "            is_token = arg_shape.metadata.get('idempotencyToken', False)\n",
      "            is_required = arg_name in required_arguments and not is_token\n",
      "            event_emitter = self._session.get_component('event_emitter')\n",
      "            arg_object = arg_class(\n",
      "                name=cli_arg_name,\n",
      "                argument_model=arg_shape,\n",
      "                is_required=is_required,\n",
      "                operation_model=self._operation_model,\n",
      "                serialized_name=arg_name,\n",
      "                event_emitter=event_emitter)\n",
      "            arg_object.add_to_arg_table(argument_table)\n",
      "        LOG.debug(argument_table)\n",
      "        self._emit('building-argument-table.%s.%s' % (self._parent_name,\n",
      "                                                      self._name),\n",
      "                   operation_model=self._operation_model,\n",
      "                   session=self._session,\n",
      "                   command=self,\n",
      "                   argument_table=argument_table)\n",
      "        return argument_table\n",
      "_create_argument_table(self=<awscli.clidriver.ServiceOperation object at 0x7f51d7514100>, self._arg_table=None, self._lineage=[<awscli.clidriver.ServiceCommand object at 0x7f51d7cb2e20>, <awscli.clidriver.ServiceOperation object at 0x7f51d7514100>], self._name='apply-archive-rule', self._operation_caller=<awscli.clidriver.CLIOperationCaller object at 0x7f51d75148b0>, self._operation_model=OperationModel(name=ApplyArchiveRule), self._parent_name='accessanalyzer', self._session=<botocore.session.Session object at 0x7f51d7ea6160>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "input_shape = self._operation_model.input_shape\n",
      "State:\n",
      "<StructureShape(ApplyArchiveRuleRequest)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _create_argument_table(self):\n",
      "        argument_table = OrderedDict()\n",
      "        input_shape = self._operation_model.input_shape\n",
      "        required_arguments = []\n",
      "        arg_dict = {}\n",
      "        if input_shape is not None:\n",
      "            required_arguments = input_shape.required_members\n",
      "            arg_dict = input_shape.members\n",
      "        for arg_name, arg_shape in arg_dict.items():\n",
      "            cli_arg_name = xform_name(arg_name, '-')\n",
      "            arg_class = self.ARG_TYPES.get(arg_shape.type_name,\n",
      "                                           self.DEFAULT_ARG_CLASS)\n",
      "            is_token = arg_shape.metadata.get('idempotencyToken', False)\n",
      "            is_required = arg_name in required_arguments and not is_token\n",
      "            event_emitter = self._session.get_component('event_emitter')\n",
      "            arg_object = arg_class(\n",
      "                name=cli_arg_name,\n",
      "                argument_model=arg_shape,\n",
      "                is_required=is_required,\n",
      "                operation_model=self._operation_model,\n",
      "                serialized_name=arg_name,\n",
      "                event_emitter=event_emitter)\n",
      "            arg_object.add_to_arg_table(argument_table)\n",
      "        LOG.debug(argument_table)\n",
      "        self._emit('building-argument-table.%s.%s' % (self._parent_name,\n",
      "                                                      self._name),\n",
      "                   operation_model=self._operation_model,\n",
      "                   session=self._session,\n",
      "                   command=self,\n",
      "                   argument_table=argument_table)\n",
      "        return argument_table\n",
      "_create_argument_table(self=<awscli.clidriver.ServiceOperation object at 0x7f51d7514100>, self._arg_table=None, self._lineage=[<awscli.clidriver.ServiceCommand object at 0x7f51d7cb2e20>, <awscli.clidriver.ServiceOperation object at 0x7f51d7514100>], self._name='apply-archive-rule', self._operation_caller=<awscli.clidriver.CLIOperationCaller object at 0x7f51d75148b0>, self._operation_model=OperationModel(name=ApplyArchiveRule), self._parent_name='accessanalyzer', self._session=<botocore.session.Session object at 0x7f51d7ea6160>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "required_arguments = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _create_argument_table(self):\n",
      "        argument_table = OrderedDict()\n",
      "        input_shape = self._operation_model.input_shape\n",
      "        required_arguments = []\n",
      "        arg_dict = {}\n",
      "        if input_shape is not None:\n",
      "            required_arguments = input_shape.required_members\n",
      "            arg_dict = input_shape.members\n",
      "        for arg_name, arg_shape in arg_dict.items():\n",
      "            cli_arg_name = xform_name(arg_name, '-')\n",
      "            arg_class = self.ARG_TYPES.get(arg_shape.type_name,\n",
      "                                           self.DEFAULT_ARG_CLASS)\n",
      "            is_token = arg_shape.metadata.get('idempotencyToken', False)\n",
      "            is_required = arg_name in required_arguments and not is_token\n",
      "            event_emitter = self._session.get_component('event_emitter')\n",
      "            arg_object = arg_class(\n",
      "                name=cli_arg_name,\n",
      "                argument_model=arg_shape,\n",
      "                is_required=is_required,\n",
      "                operation_model=self._operation_model,\n",
      "                serialized_name=arg_name,\n",
      "                event_emitter=event_emitter)\n",
      "            arg_object.add_to_arg_table(argument_table)\n",
      "        LOG.debug(argument_table)\n",
      "        self._emit('building-argument-table.%s.%s' % (self._parent_name,\n",
      "                                                      self._name),\n",
      "                   operation_model=self._operation_model,\n",
      "                   session=self._session,\n",
      "                   command=self,\n",
      "                   argument_table=argument_table)\n",
      "        return argument_table\n",
      "_create_argument_table(self=<awscli.clidriver.ServiceOperation object at 0x7f51d7514100>, self._arg_table=None, self._lineage=[<awscli.clidriver.ServiceCommand object at 0x7f51d7cb2e20>, <awscli.clidriver.ServiceOperation object at 0x7f51d7514100>], self._name='apply-archive-rule', self._operation_caller=<awscli.clidriver.CLIOperationCaller object at 0x7f51d75148b0>, self._operation_model=OperationModel(name=ApplyArchiveRule), self._parent_name='accessanalyzer', self._session=<botocore.session.Session object at 0x7f51d7ea6160>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "arg_dict = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _build_subcommand_table(self):\n",
      "        subcommand_table = super(WaitCommand, self)._build_subcommand_table()\n",
      "        self.waiter_cmd_builder.build_all_waiter_state_cmds(subcommand_table)\n",
      "        self._add_lineage(subcommand_table)\n",
      "        return subcommand_table\n",
      "_build_subcommand_table(self=<awscli.customizations.waiters.WaitCommand object at 0x7f51d5a7e430>, __class__=<class 'awscli.customizations.waiters.WaitCommand'>, self._arg_table=None, self._lineage=[<awscli.clidriver.ServiceCommand object at 0x7f51d7cb28b0>, <awscli.customizations.waiters.WaitCommand object at 0x7f51d5a7e430>], self._model=<botocore.waiter.WaiterModel object at 0x7f51d5a7e550>, self._service_model=ServiceModel(acm), self._session=<botocore.session.Session object at 0x7f51d7ea6160>, self._subcommand_table=None, self.waiter_cmd_builder=<awscli.customizations.waiters.WaiterStateCommandBuilder object at 0x7f51d5a7e1f0>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "subcommand_table = super(WaitCommand, self)._build_subcommand_table()\n",
      "State:\n",
      "OrderedDict()\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _build_waiter_state_cmd(self, waiter_name):\n",
      "        waiter_config = self._model.get_waiter(waiter_name)\n",
      "        waiter_cli_name = xform_name(waiter_name, '-')\n",
      "        operation_name = waiter_config.operation\n",
      "        operation_model = self._service_model.operation_model(operation_name)\n",
      "        waiter_state_command = WaiterStateCommand(\n",
      "            name=waiter_cli_name, parent_name='wait',\n",
      "            operation_caller=WaiterCaller(self._session, waiter_name),\n",
      "            session=self._session,\n",
      "            operation_model=operation_model,\n",
      "        )\n",
      "        waiter_state_doc_builder = WaiterStateDocBuilder(waiter_config)\n",
      "        description = waiter_state_doc_builder.build_waiter_state_description()\n",
      "        waiter_state_command.DESCRIPTION = description\n",
      "        return waiter_state_command\n",
      "_build_waiter_state_cmd(self=<awscli.customizations.waiters.WaiterStateCommandBuilder object at 0x7f51d5a7e1f0>, waiter_name='CertificateValidated', self._model=<botocore.waiter.WaiterModel object at 0x7f51d5a7e550>, self._service_model=ServiceModel(acm), self._session=<botocore.session.Session object at 0x7f51d7ea6160>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "waiter_config = self._model.get_waiter(waiter_name)\n",
      "State:\n",
      "{_config=OrderedDict([('delay', 60), ('maxAttempts', 40), ('operation', 'DescribeCertificate'), ('acceptors', [OrderedDict([('matcher', 'pathAll'), ('expected', 'SUCCESS'), ('argument', 'Certificate.DomainValidationOptions[].ValidationStatus'), ('state', 'success')]), OrderedDict([('matcher', 'pathAny'), ('expected', 'PENDING_VALIDATION'), ('argument', 'Certificate.DomainValidationOptions[].ValidationStatus'), ('state', 'retry')]), OrderedDict([('matcher', 'path'), ('expected', 'FAILED'), ('argument', 'Certificate.Status'), ('state', 'failure')]), OrderedDict([('matcher', 'error'), ('expected', 'ResourceNotFoundException'), ('state', 'failure')])])]), description='', operation='DescribeCertificate', delay=60, max_attempts=40}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _build_waiter_state_cmd(self, waiter_name):\n",
      "        waiter_config = self._model.get_waiter(waiter_name)\n",
      "        waiter_cli_name = xform_name(waiter_name, '-')\n",
      "        operation_name = waiter_config.operation\n",
      "        operation_model = self._service_model.operation_model(operation_name)\n",
      "        waiter_state_command = WaiterStateCommand(\n",
      "            name=waiter_cli_name, parent_name='wait',\n",
      "            operation_caller=WaiterCaller(self._session, waiter_name),\n",
      "            session=self._session,\n",
      "            operation_model=operation_model,\n",
      "        )\n",
      "        waiter_state_doc_builder = WaiterStateDocBuilder(waiter_config)\n",
      "        description = waiter_state_doc_builder.build_waiter_state_description()\n",
      "        waiter_state_command.DESCRIPTION = description\n",
      "        return waiter_state_command\n",
      "_build_waiter_state_cmd(self=<awscli.customizations.waiters.WaiterStateCommandBuilder object at 0x7f51d5a7e1f0>, waiter_name='CertificateValidated', self._model=<botocore.waiter.WaiterModel object at 0x7f51d5a7e550>, self._service_model=ServiceModel(acm), self._session=<botocore.session.Session object at 0x7f51d7ea6160>)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "waiter_cli_name = xform_name(waiter_name, '-')\n",
      "State:\n",
      "'certificate-validated'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def build_waiter_state_description(self):\n",
      "        description = self._waiter_config.description\n",
      "        if not description:\n",
      "            description = u'Wait until '\n",
      "            for acceptor in self._waiter_config.acceptors:\n",
      "                if acceptor.state == 'success':\n",
      "                    description += self._build_success_description(acceptor)\n",
      "                    break\n",
      "            description += self._build_operation_description(\n",
      "                self._waiter_config.operation)\n",
      "        description += self._build_polling_description(\n",
      "            self._waiter_config.delay, self._waiter_config.max_attempts)\n",
      "        return description\n",
      "build_waiter_state_description(self=<awscli.customizations.waiters.WaiterStateDocBuilder object at 0x7f51d553ebb0>, self._waiter_config=<botocore.waiter.SingleWaiterConfig object at 0x7f51d553ec40>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "description = self._waiter_config.description\n",
      "State:\n",
      "''\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _build_success_description(self, acceptor):\n",
      "        matcher = acceptor.matcher\n",
      "        success_description = self.SUCCESS_DESCRIPTIONS[matcher]\n",
      "        resource_description = None\n",
      "        if matcher in ['path', 'pathAny', 'pathAll']:\n",
      "            resource_description = u'JMESPath query %s returns ' % \\\n",
      "                acceptor.argument\n",
      "            success_description = resource_description + success_description\n",
      "        full_success_description = success_description % acceptor.expected\n",
      "        return full_success_description\n",
      "_build_success_description(self=<awscli.customizations.waiters.WaiterStateDocBuilder object at 0x7f51d553ebb0>, acceptor={state='success', matcher='pathAll', expected='SUCCESS', argument='Certificate.DomainValidationOptions[].ValidationStatus'}, self._waiter_config=<botocore.waiter.SingleWaiterConfig object at 0x7f51d553ec40>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "matcher = acceptor.matcher\n",
      "State:\n",
      "'pathAll'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _build_success_description(self, acceptor):\n",
      "        matcher = acceptor.matcher\n",
      "        success_description = self.SUCCESS_DESCRIPTIONS[matcher]\n",
      "        resource_description = None\n",
      "        if matcher in ['path', 'pathAny', 'pathAll']:\n",
      "            resource_description = u'JMESPath query %s returns ' % \\\n",
      "                acceptor.argument\n",
      "            success_description = resource_description + success_description\n",
      "        full_success_description = success_description % acceptor.expected\n",
      "        return full_success_description\n",
      "_build_success_description(self=<awscli.customizations.waiters.WaiterStateDocBuilder object at 0x7f51d553ebb0>, acceptor={state='success', matcher='pathAll', expected='SUCCESS', argument='Certificate.DomainValidationOptions[].ValidationStatus'}, self._waiter_config=<botocore.waiter.SingleWaiterConfig object at 0x7f51d553ec40>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "success_description = self.SUCCESS_DESCRIPTIONS[matcher]\n",
      "State:\n",
      "'%s for all elements '\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _transform_map(self, schema, shapes):\n",
      "        structure_shape = self._populate_initial_shape(schema)\n",
      "        for attribute in ['key', 'value']:\n",
      "            type_name = self._json_schema_to_aws_type(schema[attribute])\n",
      "            shape_name = self._shape_namer.new_shape_name(type_name)\n",
      "            structure_shape[attribute] = {'shape': shape_name}\n",
      "            self._transform(schema[attribute], shapes, shape_name)\n",
      "        return structure_shape\n",
      "_transform_map(self=<awscli.schema.SchemaTransformer object at 0x7f518738fca0>, schema={'type': 'map', 'key': {'type': 'string'}, 'value': {'type': 'string'}}, shapes={}, self._shape_namer=<awscli.schema.ShapeNameGenerator object at 0x7f518738fd30>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "structure_shape = self._populate_initial_shape(schema)\n",
      "State:\n",
      "{'type': 'map'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def new_shape_name(self, type_name):\n",
      "        self._name_cache[type_name] += 1\n",
      "        current_index = self._name_cache[type_name]\n",
      "        return '%sType%s' % (type_name.capitalize(), current_index)\n",
      "new_shape_name(self=<awscli.schema.ShapeNameGenerator object at 0x7f518738fd30>, type_name='string', self._name_cache=defaultdict(<class 'int'>, {}))\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "current_index = self._name_cache[type_name]\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _transform_list(self, schema, shapes):\n",
      "        list_shape = self._populate_initial_shape(schema)\n",
      "        member_type = self._json_schema_to_aws_type(schema['items'])\n",
      "        member_shape_name = self._shape_namer.new_shape_name(member_type)\n",
      "        list_shape['member'] = {'shape': member_shape_name}\n",
      "        self._transform(schema['items'], shapes, member_shape_name)\n",
      "        return list_shape\n",
      "_transform_list(self=<awscli.schema.SchemaTransformer object at 0x7f518738fe80>, schema={'type': 'array', 'items': {'type': 'string'}}, shapes={}, self._shape_namer=<awscli.schema.ShapeNameGenerator object at 0x7f518738f100>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "list_shape = self._populate_initial_shape(schema)\n",
      "State:\n",
      "{'type': 'list'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _transform_list(self, schema, shapes):\n",
      "        list_shape = self._populate_initial_shape(schema)\n",
      "        member_type = self._json_schema_to_aws_type(schema['items'])\n",
      "        member_shape_name = self._shape_namer.new_shape_name(member_type)\n",
      "        list_shape['member'] = {'shape': member_shape_name}\n",
      "        self._transform(schema['items'], shapes, member_shape_name)\n",
      "        return list_shape\n",
      "_transform_list(self=<awscli.schema.SchemaTransformer object at 0x7f518738fe80>, schema={'type': 'array', 'items': {'type': 'string'}}, shapes={}, self._shape_namer=<awscli.schema.ShapeNameGenerator object at 0x7f518738f100>)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "member_type = self._json_schema_to_aws_type(schema['items'])\n",
      "State:\n",
      "'string'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def flatten_args(self, command, argument_table, **kwargs):\n",
      "        for name, argument in self.configs[command.name].items():\n",
      "            argument_from_table = argument_table[name]\n",
      "            overwritten = False\n",
      "            LOG.debug('Flattening {0} argument {1} into {2}'.format(\n",
      "                command.name, name,\n",
      "                ', '.join([v['name'] for k, v in argument['flatten'].items()])\n",
      "            ))\n",
      "            for sub_argument, new_config in argument['flatten'].items():\n",
      "                config = new_config.copy()\n",
      "                config['container'] = argument_from_table\n",
      "                config['prop'] = sub_argument\n",
      "                _arg = self._find_nested_arg(\n",
      "                    argument_from_table.argument_model, sub_argument\n",
      "                )\n",
      "                self._merge_member_config(_arg, sub_argument, config)\n",
      "                new_arg = FlattenedArgument(**config)\n",
      "                argument_table[new_config['name']] = new_arg\n",
      "                if name == new_config['name']:\n",
      "                    overwritten = True\n",
      "            if not overwritten and ('keep' not in argument or\n",
      "                                    not argument['keep']):\n",
      "                del argument_table[name]\n",
      "flatten_args(self=<awscli.customizations.flatten.FlattenArguments object at 0x7f51d7cb2730>, command={_arg_table=None, _name='define-expression', _parent_name='cloudsearch', _operation_caller=<awscli.clidriver.CLIOperationCaller object at 0x7f5181bfc430>, _lineage=[<awscli.clidriver.ServiceCommand object at 0x7f51d7c11940>, <awscli.clidriver.ServiceOperation object at 0x7f5181bfc1f0>], _operation_model=OperationModel(name=DefineExpression), _session=<botocore.session.Session object at 0x7f51d7ea6160>}, argument_table=OrderedDict([('domain-name', <awscli.arguments.CLIArgument object at 0x7f5181b0dcd0>), ('expression', <awscli.arguments.CLIArgument object at 0x7f5181b0dd00>)]), kwargs={'operation_model': OperationModel(name=DefineExpression), 'session': <botocore.session.Session object at 0x7f51d7ea6160>, 'event_name': 'building-argument-table.cloudsearch.define-expression'}, self.configs={'define-expression': {'expression': {'keep': False, 'flatten': OrderedDict([('ExpressionName', {'name': 'name'}), ('ExpressionValue', {'name': 'expression'})])}}, 'define-index-field': {'index-field': {'keep': False, 'flatten': OrderedDict([('IndexFieldName', {'name': 'name'}), ('IndexFieldType', {'name': 'type'}), ('IntOptions.DefaultValue', {'name': 'default-value', 'type': 'string', 'hydrate': <function index_hydrate at 0x7f521cddd820>}), ('IntOptions.FacetEnabled', {'name': 'facet-enabled', 'hydrate': <function index_hydrate at 0x7f521cddd820>}), ('IntOptions.SearchEnabled', {'name': 'search-enabled', 'hydrate': <function index_hydrate at 0x7f521cddd820>}), ('IntOptions.ReturnEnabled', {'name': 'return-enabled', 'hydrate': <function index_hydrate at 0x7f521cddd820>}), ('IntOptions.SortEnabled', {'name': 'sort-enabled', 'hydrate': <function index_hydrate at 0x7f521cddd820>}), ('IntOptions.SourceField', {'name': 'source-field', 'type': 'string', 'hydrate': <function index_hydrate at 0x7f521cddd820>}), ('TextOptions.HighlightEnabled', {'name': 'highlight-enabled', 'hydrate': <function index_hydrate at 0x7f521cddd820>}), ('TextOptions.AnalysisScheme', {'name': 'analysis-scheme', 'hydrate': <function index_hydrate at 0x7f521cddd820>})])}}}, self.service_name='cloudsearch')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "overwritten = False\n",
      "State:\n",
      "False\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _detect_shape_structure(param, stack):\n",
      "    if param.name in stack:\n",
      "        return 'recursive'\n",
      "    else:\n",
      "        stack.append(param.name)\n",
      "    try:\n",
      "        if param.type_name in SCALAR_TYPES:\n",
      "            return 'scalar'\n",
      "        elif param.type_name == 'structure':\n",
      "            sub_types = [_detect_shape_structure(p, stack)\n",
      "                        for p in param.members.values()]\n",
      "            if len(sub_types) == 1 and all(p == 'scalar' for p in sub_types):\n",
      "                return 'structure(scalar)'\n",
      "            elif len(sub_types) > 1 and all(p == 'scalar' for p in sub_types):\n",
      "                return 'structure(scalars)'\n",
      "            else:\n",
      "                return 'structure(%s)' % ', '.join(sorted(set(sub_types)))\n",
      "        elif param.type_name == 'list':\n",
      "            return 'list-%s' % _detect_shape_structure(param.member, stack)\n",
      "        elif param.type_name == 'map':\n",
      "            if param.value.type_name in SCALAR_TYPES:\n",
      "                return 'map-scalar'\n",
      "            else:\n",
      "                return 'map-%s' % _detect_shape_structure(param.value, stack)\n",
      "    finally:\n",
      "        stack.pop()\n",
      "_detect_shape_structure(param=<StringShape(String)>, stack=[])\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "stack.append(param.name)\n",
      "State:\n",
      "['String']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def hoist(self, session, argument_table, **kwargs):\n",
      "        help_text = ZIP_DOCSTRING.format(param_type=self._name)\n",
      "        argument_table['zip-file'] = ZipFileArgument(\n",
      "            'zip-file', help_text=help_text, cli_type_name='blob',\n",
      "            serialized_name=self._serialized_name\n",
      "        )\n",
      "        argument = argument_table[self._name]\n",
      "        model = copy.deepcopy(argument.argument_model)\n",
      "        del model.members['ZipFile']\n",
      "        argument_table[self._name] = ReplacedZipFileArgument(\n",
      "            name=self._name,\n",
      "            argument_model=model,\n",
      "            operation_model=argument._operation_model,\n",
      "            is_required=False,\n",
      "            event_emitter=session.get_component('event_emitter'),\n",
      "            serialized_name=self._serialized_name,\n",
      "        )\n",
      "hoist(self=<awscli.customizations.awslambda.ZipFileArgumentHoister object at 0x7f51d7cb26a0>, session={_original_handler=<botocore.hooks.HierarchicalEmitter object at 0x7f51d7ea6250>, _events=<botocore.hooks.EventAliaser object at 0x7f51d7ea6220>, user_agent_name='aws-cli', user_agent_version='1.32.28', user_agent_extra='botocore/1.34.77', _profile=None, _config={'profiles': {}}, _credentials=None, _auth_token=None, _profile_map={}, _session_instance_vars={}, _client_config=None, _last_client_region_used=None, _components=<botocore.session.ComponentLocator object at 0x7f51d7ea6310>, _internal_components=<botocore.session.ComponentLocator object at 0x7f51d7ea63a0>, session_var_map=<botocore.session.SessionVarDict object at 0x7f51d7e4b5b0>}, argument_table=OrderedDict([('function-name', <awscli.arguments.CLIArgument object at 0x7f5090604790>), ('runtime', <awscli.arguments.CLIArgument object at 0x7f50906047c0>), ('role', <awscli.arguments.CLIArgument object at 0x7f50906048e0>), ('handler', <awscli.arguments.CLIArgument object at 0x7f5090604580>), ('code', <awscli.arguments.CLIArgument object at 0x7f50906049d0>), ('description', <awscli.arguments.CLIArgument object at 0x7f5090604880>), ('timeout', <awscli.arguments.CLIArgument object at 0x7f5090604ac0>), ('memory-size', <awscli.arguments.CLIArgument object at 0x7f5090604bb0>), ('publish', <awscli.arguments.BooleanArgument object at 0x7f5090604a60>), ('no-publish', <awscli.arguments.BooleanArgument object at 0x7f5090604b20>), ('vpc-config', <awscli.arguments.CLIArgument object at 0x7f5090604a00>), ('package-type', <awscli.arguments.CLIArgument object at 0x7f5090604a90>), ('dead-letter-config', <awscli.arguments.CLIArgument object at 0x7f5090604b80>), ('environment', <awscli.arguments.CLIArgument object at 0x7f5090604c70>), ('kms-key-arn', <awscli.arguments.CLIArgument object at 0x7f5090604c40>), ('tracing-config', <awscli.arguments.CLIArgument object at 0x7f50906049a0>), ('tags', <awscli.arguments.CLIArgument object at 0x7f5090604d60>), ('layers', <awscli.arguments.ListArgument object at 0x7f5090604970>), ('file-system-configs', <awscli.arguments.ListArgument object at 0x7f5090604d00>), ('image-config', <awscli.arguments.CLIArgument object at 0x7f5090604df0>), ('code-signing-config-arn', <awscli.arguments.CLIArgument object at 0x7f5090604e20>), ('architectures', <awscli.arguments.ListArgument object at 0x7f5090604e50>), ('ephemeral-storage', <awscli.arguments.CLIArgument object at 0x7f5090604c10>), ('snap-start', <awscli.arguments.CLIArgument object at 0x7f5090604d30>), ('logging-config', <awscli.arguments.CLIArgument object at 0x7f5090604d90>)]), kwargs={'operation_model': OperationModel(name=CreateFunction), 'command': <awscli.clidriver.ServiceOperation object at 0x7f5090e6c700>, 'event_name': 'building-argument-table.lambda.create-function'}, self._name='code', self._serialized_name='Code')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "help_text = ZIP_DOCSTRING.format(param_type=self._name)\n",
      "State:\n",
      "'<p>The path to the zip file of the code you are uploading. Specify --zip-file or --code, but not both. Example: fileb://code.zip</p>'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def modify_revision_arguments(argument_table, session, **kwargs):\n",
      "    s3_model = create_argument_model_from_schema(S3_LOCATION_SCHEMA)\n",
      "    argument_table[S3_LOCATION_ARG_DESCRIPTION['name']] = (\n",
      "        S3LocationArgument(\n",
      "            argument_model=s3_model,\n",
      "            session=session,\n",
      "            **S3_LOCATION_ARG_DESCRIPTION\n",
      "        )\n",
      "    )\n",
      "    github_model = create_argument_model_from_schema(GITHUB_LOCATION_SCHEMA)\n",
      "    argument_table[GITHUB_LOCATION_ARG_DESCRIPTION['name']] = (\n",
      "        GitHubLocationArgument(\n",
      "            argument_model=github_model,\n",
      "            session=session,\n",
      "            **GITHUB_LOCATION_ARG_DESCRIPTION\n",
      "        )\n",
      "    )\n",
      "    argument_table['revision'].required = False\n",
      "modify_revision_arguments(argument_table=OrderedDict([('application-name', <awscli.arguments.CLIArgument object at 0x7f4f8a5bf070>), ('deployment-group-name', <awscli.arguments.CLIArgument object at 0x7f4f8a5bf2b0>), ('revision', <awscli.arguments.CLIArgument object at 0x7f4f8a5bfac0>), ('deployment-config-name', <awscli.arguments.CLIArgument object at 0x7f4f8a5bf130>), ('description', <awscli.arguments.CLIArgument object at 0x7f4f8a5bf400>), ('ignore-application-stop-failures', <awscli.arguments.BooleanArgument object at 0x7f4f8a5bf910>), ('no-ignore-application-stop-failures', <awscli.arguments.BooleanArgument object at 0x7f4f8a5bfa00>), ('target-instances', <awscli.arguments.CLIArgument object at 0x7f4f8a5bfb50>), ('auto-rollback-configuration', <awscli.arguments.CLIArgument object at 0x7f4f8a5bfb20>), ('update-outdated-instances-only', <awscli.arguments.BooleanArgument object at 0x7f4f8a5bfa90>), ('no-update-outdated-instances-only', <awscli.arguments.BooleanArgument object at 0x7f4f8a5bfc70>), ('file-exists-behavior', <awscli.arguments.CLIArgument object at 0x7f4f8a5bf040>), ('override-alarm-configuration', <awscli.arguments.CLIArgument object at 0x7f4f8a5bf8b0>)]), session={_original_handler=<botocore.hooks.HierarchicalEmitter object at 0x7f51d7ea6250>, _events=<botocore.hooks.EventAliaser object at 0x7f51d7ea6220>, user_agent_name='aws-cli', user_agent_version='1.32.28', user_agent_extra='botocore/1.34.77', _profile=None, _config={'profiles': {}}, _credentials=None, _auth_token=None, _profile_map={}, _session_instance_vars={}, _client_config=None, _last_client_region_used=None, _components=<botocore.session.ComponentLocator object at 0x7f51d7ea6310>, _internal_components=<botocore.session.ComponentLocator object at 0x7f51d7ea63a0>, session_var_map=<botocore.session.SessionVarDict object at 0x7f51d7e4b5b0>}, kwargs={'operation_model': OperationModel(name=CreateDeployment), 'command': <awscli.clidriver.ServiceOperation object at 0x7f4f8aac5550>, 'event_name': 'building-argument-table.deploy.create-deployment'})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "s3_model = create_argument_model_from_schema(S3_LOCATION_SCHEMA)\n",
      "State:\n",
      "<StructureShape(InputShape)>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_soft_jaccard_score(y_true, y_pred, expected, eps):\n",
      "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "    actual = F.soft_jaccard_score(y_pred, y_true, eps=eps)\n",
      "    assert float(actual) == pytest.approx(expected, eps)\n",
      "test_soft_jaccard_score(y_true=[1, 1, 1, 1], y_pred=[1, 1, 1, 1], expected=1.0, eps=1e-05)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "State:\n",
      "tensor([1., 1., 1., 1.])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_soft_jaccard_score(y_true, y_pred, expected, eps):\n",
      "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "    actual = F.soft_jaccard_score(y_pred, y_true, eps=eps)\n",
      "    assert float(actual) == pytest.approx(expected, eps)\n",
      "test_soft_jaccard_score(y_true=[1, 1, 1, 1], y_pred=[1, 1, 1, 1], expected=1.0, eps=1e-05)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "State:\n",
      "tensor([1., 1., 1., 1.])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_soft_jaccard_score(y_true, y_pred, expected, eps):\n",
      "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "    actual = F.soft_jaccard_score(y_pred, y_true, eps=eps)\n",
      "    assert float(actual) == pytest.approx(expected, eps)\n",
      "test_soft_jaccard_score(y_true=[1, 1, 1, 1], y_pred=[1, 1, 1, 1], expected=1.0, eps=1e-05)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "actual = F.soft_jaccard_score(y_pred, y_true, eps=eps)\n",
      "State:\n",
      "tensor(1.)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_soft_jaccard_score(y_true, y_pred, expected, eps):\n",
      "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "    actual = F.soft_jaccard_score(y_pred, y_true, eps=eps)\n",
      "    assert float(actual) == pytest.approx(expected, eps)\n",
      "test_soft_jaccard_score(y_true=[1, 1, 1, 1], y_pred=[1, 1, 1, 1], expected=1.0, eps=1e-05)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "assert float(actual) == pytest.approx(expected, eps)\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_soft_jaccard_score_2(y_true, y_pred, expected, eps):\n",
      "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "    actual = F.soft_jaccard_score(y_pred, y_true, dims=[1], eps=eps)\n",
      "    actual = actual.mean()\n",
      "    assert float(actual) == pytest.approx(expected, eps)\n",
      "test_soft_jaccard_score_2(y_true=[[1, 1, 0, 0], [0, 0, 1, 1]], y_pred=[[1, 1, 0, 0], [0, 0, 1, 1]], expected=1.0, eps=1e-05)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "State:\n",
      "tensor([[1., 1., 0., 0.],        [0., 0., 1., 1.]])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_soft_jaccard_score_2(y_true, y_pred, expected, eps):\n",
      "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "    actual = F.soft_jaccard_score(y_pred, y_true, dims=[1], eps=eps)\n",
      "    actual = actual.mean()\n",
      "    assert float(actual) == pytest.approx(expected, eps)\n",
      "test_soft_jaccard_score_2(y_true=[[1, 1, 0, 0], [0, 0, 1, 1]], y_pred=[[1, 1, 0, 0], [0, 0, 1, 1]], expected=1.0, eps=1e-05)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "State:\n",
      "tensor([[1., 1., 0., 0.],        [0., 0., 1., 1.]])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_soft_jaccard_score_2(y_true, y_pred, expected, eps):\n",
      "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "    actual = F.soft_jaccard_score(y_pred, y_true, dims=[1], eps=eps)\n",
      "    actual = actual.mean()\n",
      "    assert float(actual) == pytest.approx(expected, eps)\n",
      "test_soft_jaccard_score_2(y_true=[[1, 1, 0, 0], [0, 0, 1, 1]], y_pred=[[1, 1, 0, 0], [0, 0, 1, 1]], expected=1.0, eps=1e-05)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "actual = F.soft_jaccard_score(y_pred, y_true, dims=[1], eps=eps)\n",
      "State:\n",
      "tensor([1., 1.])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_soft_jaccard_score_2(y_true, y_pred, expected, eps):\n",
      "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "    actual = F.soft_jaccard_score(y_pred, y_true, dims=[1], eps=eps)\n",
      "    actual = actual.mean()\n",
      "    assert float(actual) == pytest.approx(expected, eps)\n",
      "test_soft_jaccard_score_2(y_true=[[1, 1, 0, 0], [0, 0, 1, 1]], y_pred=[[1, 1, 0, 0], [0, 0, 1, 1]], expected=1.0, eps=1e-05)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "actual = actual.mean()\n",
      "State:\n",
      "tensor(1.)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_soft_jaccard_score_2(y_true, y_pred, expected, eps):\n",
      "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "    actual = F.soft_jaccard_score(y_pred, y_true, dims=[1], eps=eps)\n",
      "    actual = actual.mean()\n",
      "    assert float(actual) == pytest.approx(expected, eps)\n",
      "test_soft_jaccard_score_2(y_true=[[1, 1, 0, 0], [0, 0, 1, 1]], y_pred=[[1, 1, 0, 0], [0, 0, 1, 1]], expected=1.0, eps=1e-05)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "assert float(actual) == pytest.approx(expected, eps)\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_soft_dice_score(y_true, y_pred, expected, eps):\n",
      "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "    actual = F.soft_dice_score(y_pred, y_true, eps=eps)\n",
      "    assert float(actual) == pytest.approx(expected, eps)\n",
      "test_soft_dice_score(y_true=[1, 1, 1, 1], y_pred=[1, 1, 1, 1], expected=1.0, eps=1e-05)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "State:\n",
      "tensor([1., 1., 1., 1.])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_soft_dice_score(y_true, y_pred, expected, eps):\n",
      "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "    actual = F.soft_dice_score(y_pred, y_true, eps=eps)\n",
      "    assert float(actual) == pytest.approx(expected, eps)\n",
      "test_soft_dice_score(y_true=[1, 1, 1, 1], y_pred=[1, 1, 1, 1], expected=1.0, eps=1e-05)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "State:\n",
      "tensor([1., 1., 1., 1.])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_soft_dice_score(y_true, y_pred, expected, eps):\n",
      "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "    actual = F.soft_dice_score(y_pred, y_true, eps=eps)\n",
      "    assert float(actual) == pytest.approx(expected, eps)\n",
      "test_soft_dice_score(y_true=[1, 1, 1, 1], y_pred=[1, 1, 1, 1], expected=1.0, eps=1e-05)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "actual = F.soft_dice_score(y_pred, y_true, eps=eps)\n",
      "State:\n",
      "tensor(1.)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_soft_dice_score(y_true, y_pred, expected, eps):\n",
      "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "    actual = F.soft_dice_score(y_pred, y_true, eps=eps)\n",
      "    assert float(actual) == pytest.approx(expected, eps)\n",
      "test_soft_dice_score(y_true=[1, 1, 1, 1], y_pred=[1, 1, 1, 1], expected=1.0, eps=1e-05)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "assert float(actual) == pytest.approx(expected, eps)\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_soft_tversky_score(y_true, y_pred, expected, eps, alpha, beta):\n",
      "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "    actual = F.soft_tversky_score(y_pred, y_true, eps=eps, alpha=alpha, beta=beta)\n",
      "    assert float(actual) == pytest.approx(expected, eps)\n",
      "test_soft_tversky_score(y_true=[1, 1, 1, 1], y_pred=[1, 1, 1, 1], expected=1.0, eps=1e-05, alpha=0.5, beta=0.5)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "State:\n",
      "tensor([1., 1., 1., 1.])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_soft_tversky_score(y_true, y_pred, expected, eps, alpha, beta):\n",
      "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "    actual = F.soft_tversky_score(y_pred, y_true, eps=eps, alpha=alpha, beta=beta)\n",
      "    assert float(actual) == pytest.approx(expected, eps)\n",
      "test_soft_tversky_score(y_true=[1, 1, 1, 1], y_pred=[1, 1, 1, 1], expected=1.0, eps=1e-05, alpha=0.5, beta=0.5)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "State:\n",
      "tensor([1., 1., 1., 1.])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_soft_tversky_score(y_true, y_pred, expected, eps, alpha, beta):\n",
      "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "    actual = F.soft_tversky_score(y_pred, y_true, eps=eps, alpha=alpha, beta=beta)\n",
      "    assert float(actual) == pytest.approx(expected, eps)\n",
      "test_soft_tversky_score(y_true=[1, 1, 1, 1], y_pred=[1, 1, 1, 1], expected=1.0, eps=1e-05, alpha=0.5, beta=0.5)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "actual = F.soft_tversky_score(y_pred, y_true, eps=eps, alpha=alpha, beta=beta)\n",
      "State:\n",
      "tensor(1.)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_soft_tversky_score(y_true, y_pred, expected, eps, alpha, beta):\n",
      "    y_true = torch.tensor(y_true, dtype=torch.float32)\n",
      "    y_pred = torch.tensor(y_pred, dtype=torch.float32)\n",
      "    actual = F.soft_tversky_score(y_pred, y_true, eps=eps, alpha=alpha, beta=beta)\n",
      "    assert float(actual) == pytest.approx(expected, eps)\n",
      "test_soft_tversky_score(y_true=[1, 1, 1, 1], y_pred=[1, 1, 1, 1], expected=1.0, eps=1e-05, alpha=0.5, beta=0.5)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "assert float(actual) == pytest.approx(expected, eps)\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
      "        assert y_true.size(0) == y_pred.size(0)\n",
      "        if self.from_logits:\n",
      "            if self.mode == MULTICLASS_MODE:\n",
      "                y_pred = y_pred.log_softmax(dim=1).exp()\n",
      "            else:\n",
      "                y_pred = F.logsigmoid(y_pred).exp()\n",
      "        bs = y_true.size(0)\n",
      "        num_classes = y_pred.size(1)\n",
      "        dims = (0, 2)\n",
      "        if self.mode == BINARY_MODE:\n",
      "            y_true = y_true.view(bs, 1, -1)\n",
      "            y_pred = y_pred.view(bs, 1, -1)\n",
      "            if self.ignore_index is not None:\n",
      "                mask = y_true != self.ignore_index\n",
      "                y_pred = y_pred * mask\n",
      "                y_true = y_true * mask\n",
      "        if self.mode == MULTICLASS_MODE:\n",
      "            y_true = y_true.view(bs, -1)\n",
      "            y_pred = y_pred.view(bs, num_classes, -1)\n",
      "            if self.ignore_index is not None:\n",
      "                mask = y_true != self.ignore_index\n",
      "                y_pred = y_pred * mask.unsqueeze(1)\n",
      "                y_true = F.one_hot((y_true * mask).to(torch.long), num_classes)\n",
      "                y_true = y_true.permute(0, 2, 1) * mask.unsqueeze(1)\n",
      "            else:\n",
      "                y_true = F.one_hot(y_true, num_classes)\n",
      "                y_true = y_true.permute(0, 2, 1)\n",
      "        if self.mode == MULTILABEL_MODE:\n",
      "            y_true = y_true.view(bs, num_classes, -1)\n",
      "            y_pred = y_pred.view(bs, num_classes, -1)\n",
      "            if self.ignore_index is not None:\n",
      "                mask = y_true != self.ignore_index\n",
      "                y_pred = y_pred * mask\n",
      "                y_true = y_true * mask\n",
      "        scores = self.compute_score(y_pred, y_true.type_as(y_pred), smooth=self.smooth, eps=self.eps, dims=dims)\n",
      "        if self.log_loss:\n",
      "            loss = -torch.log(scores.clamp_min(self.eps))\n",
      "        else:\n",
      "            loss = 1.0 - scores\n",
      "        mask = y_true.sum(dims) > 0\n",
      "        loss *= mask.to(loss.dtype)\n",
      "        if self.classes is not None:\n",
      "            loss = loss[self.classes]\n",
      "        return self.aggregate_loss(loss)\n",
      "forward(self=DiceLoss(), y_pred=tensor([[[[1., 1., 1.]]]]), y_true=tensor([[[[1, 1, 1]]]]), self._backward_hooks=OrderedDict(), self._backward_pre_hooks=OrderedDict(), self._buffers=OrderedDict(), self._forward_hooks=OrderedDict(), self._forward_hooks_always_called=OrderedDict(), self._forward_hooks_with_kwargs=OrderedDict(), self._forward_pre_hooks=OrderedDict(), self._forward_pre_hooks_with_kwargs=OrderedDict(), self._is_full_backward_hook=None, self._load_state_dict_post_hooks=OrderedDict(), self._load_state_dict_pre_hooks=OrderedDict(), self._modules=OrderedDict(), self._non_persistent_buffers_set=set(), self._parameters=OrderedDict(), self._state_dict_hooks=OrderedDict(), self._state_dict_pre_hooks=OrderedDict(), self.classes=None, self.eps=1e-07, self.from_logits=False, self.ignore_index=None, self.log_loss=False, self.mode='binary', self.reduction='mean', self.smooth=0.0, self.training=True)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "dims = (0, 2)\n",
      "State:\n",
      "(0, 2)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_stages(self):\n",
      "        stages = []\n",
      "        stage_modules = []\n",
      "        for module in self.features:\n",
      "            if isinstance(module, nn.MaxPool2d):\n",
      "                stages.append(nn.Sequential(*stage_modules))\n",
      "                stage_modules = []\n",
      "            stage_modules.append(module)\n",
      "        stages.append(nn.Sequential(*stage_modules))\n",
      "        return stages\n",
      "get_stages(self=VGGEncoder(  (features): Sequential(    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (1): ReLU(inplace=True)    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (4): ReLU(inplace=True)    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (7): ReLU(inplace=True)    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (9): ReLU(inplace=True)    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (12): ReLU(inplace=True)    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (14): ReLU(inplace=True)    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (17): ReLU(inplace=True)    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (19): ReLU(inplace=True)    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))), self._backward_hooks=OrderedDict(), self._backward_pre_hooks=OrderedDict(), self._buffers=OrderedDict(), self._depth=3, self._forward_hooks=OrderedDict(), self._forward_hooks_always_called=OrderedDict(), self._forward_hooks_with_kwargs=OrderedDict(), self._forward_pre_hooks=OrderedDict(), self._forward_pre_hooks_with_kwargs=OrderedDict(), self._in_channels=3, self._is_full_backward_hook=None, self._load_state_dict_post_hooks=OrderedDict(), self._load_state_dict_pre_hooks=OrderedDict(), self._modules=OrderedDict([('features', Sequential(  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  (1): ReLU(inplace=True)  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  (4): ReLU(inplace=True)  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  (7): ReLU(inplace=True)  (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  (9): ReLU(inplace=True)  (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  (12): ReLU(inplace=True)  (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  (14): ReLU(inplace=True)  (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  (17): ReLU(inplace=True)  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  (19): ReLU(inplace=True)  (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False))), ('avgpool', AdaptiveAvgPool2d(output_size=(7, 7)))]), self._non_persistent_buffers_set=set(), self._out_channels=(64, 128, 256, 512, 512, 512), self._parameters=OrderedDict(), self._state_dict_hooks=OrderedDict(), self._state_dict_pre_hooks=OrderedDict(), self.training=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "stages = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_stages(self):\n",
      "        stages = []\n",
      "        stage_modules = []\n",
      "        for module in self.features:\n",
      "            if isinstance(module, nn.MaxPool2d):\n",
      "                stages.append(nn.Sequential(*stage_modules))\n",
      "                stage_modules = []\n",
      "            stage_modules.append(module)\n",
      "        stages.append(nn.Sequential(*stage_modules))\n",
      "        return stages\n",
      "get_stages(self=VGGEncoder(  (features): Sequential(    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (1): ReLU(inplace=True)    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (4): ReLU(inplace=True)    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (7): ReLU(inplace=True)    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (9): ReLU(inplace=True)    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (12): ReLU(inplace=True)    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (14): ReLU(inplace=True)    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (17): ReLU(inplace=True)    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (19): ReLU(inplace=True)    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))), self._backward_hooks=OrderedDict(), self._backward_pre_hooks=OrderedDict(), self._buffers=OrderedDict(), self._depth=3, self._forward_hooks=OrderedDict(), self._forward_hooks_always_called=OrderedDict(), self._forward_hooks_with_kwargs=OrderedDict(), self._forward_pre_hooks=OrderedDict(), self._forward_pre_hooks_with_kwargs=OrderedDict(), self._in_channels=3, self._is_full_backward_hook=None, self._load_state_dict_post_hooks=OrderedDict(), self._load_state_dict_pre_hooks=OrderedDict(), self._modules=OrderedDict([('features', Sequential(  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  (1): ReLU(inplace=True)  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  (4): ReLU(inplace=True)  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  (7): ReLU(inplace=True)  (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  (9): ReLU(inplace=True)  (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  (12): ReLU(inplace=True)  (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  (14): ReLU(inplace=True)  (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)  (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  (17): ReLU(inplace=True)  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  (19): ReLU(inplace=True)  (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False))), ('avgpool', AdaptiveAvgPool2d(output_size=(7, 7)))]), self._non_persistent_buffers_set=set(), self._out_channels=(64, 128, 256, 512, 512, 512), self._parameters=OrderedDict(), self._state_dict_hooks=OrderedDict(), self._state_dict_pre_hooks=OrderedDict(), self.training=False)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "stage_modules = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_channels(self, mode, width_mult):\n",
      "        if mode == \"small\":\n",
      "            channels = [16, 16, 24, 48, 576]\n",
      "        else:\n",
      "            channels = [16, 24, 40, 112, 960]\n",
      "        channels = [\n",
      "            3,\n",
      "        ] + [_make_divisible(x * width_mult) for x in channels]\n",
      "        return tuple(channels)\n",
      "_get_channels(self=MobileNetV3Encoder(), mode='large', width_mult=0.75, self._backward_hooks=OrderedDict(), self._backward_pre_hooks=OrderedDict(), self._buffers=OrderedDict(), self._depth=3, self._forward_hooks=OrderedDict(), self._forward_hooks_always_called=OrderedDict(), self._forward_hooks_with_kwargs=OrderedDict(), self._forward_pre_hooks=OrderedDict(), self._forward_pre_hooks_with_kwargs=OrderedDict(), self._is_full_backward_hook=None, self._load_state_dict_post_hooks=OrderedDict(), self._load_state_dict_pre_hooks=OrderedDict(), self._mode='large', self._modules=OrderedDict(), self._non_persistent_buffers_set=set(), self._parameters=OrderedDict(), self._state_dict_hooks=OrderedDict(), self._state_dict_pre_hooks=OrderedDict(), self.training=True)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "channels = [16, 24, 40, 112, 960]\n",
      "State:\n",
      "[16, 24, 40, 112, 960]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _get_channels(self, mode, width_mult):\n",
      "        if mode == \"small\":\n",
      "            channels = [16, 16, 24, 48, 576]\n",
      "        else:\n",
      "            channels = [16, 24, 40, 112, 960]\n",
      "        channels = [\n",
      "            3,\n",
      "        ] + [_make_divisible(x * width_mult) for x in channels]\n",
      "        return tuple(channels)\n",
      "_get_channels(self=MobileNetV3Encoder(), mode='large', width_mult=0.75, self._backward_hooks=OrderedDict(), self._backward_pre_hooks=OrderedDict(), self._buffers=OrderedDict(), self._depth=3, self._forward_hooks=OrderedDict(), self._forward_hooks_always_called=OrderedDict(), self._forward_hooks_with_kwargs=OrderedDict(), self._forward_pre_hooks=OrderedDict(), self._forward_pre_hooks_with_kwargs=OrderedDict(), self._is_full_backward_hook=None, self._load_state_dict_post_hooks=OrderedDict(), self._load_state_dict_pre_hooks=OrderedDict(), self._mode='large', self._modules=OrderedDict(), self._non_persistent_buffers_set=set(), self._parameters=OrderedDict(), self._state_dict_hooks=OrderedDict(), self._state_dict_pre_hooks=OrderedDict(), self.training=True)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "channels = [\n",
      "State:\n",
      "[3, 16, 24, 32, 88, 720]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _make_stage(self, planes: int, num_blocks: int, num_se_blocks: int) -> nn.Sequential:\n",
      "        strides = [2] + [1] * (num_blocks - 1)\n",
      "        blocks = []\n",
      "        for ix, stride in enumerate(strides):\n",
      "            use_se = False\n",
      "            if num_se_blocks > num_blocks:\n",
      "                raise ValueError(\"Number of SE blocks cannot \" \"exceed number of layers.\")\n",
      "            if ix >= (num_blocks - num_se_blocks):\n",
      "                use_se = True\n",
      "            blocks.append(\n",
      "                MobileOneBlock(\n",
      "                    in_channels=self.in_planes,\n",
      "                    out_channels=self.in_planes,\n",
      "                    kernel_size=3,\n",
      "                    stride=stride,\n",
      "                    padding=1,\n",
      "                    groups=self.in_planes,\n",
      "                    inference_mode=self.inference_mode,\n",
      "                    use_se=use_se,\n",
      "                    num_conv_branches=self.num_conv_branches,\n",
      "                )\n",
      "            )\n",
      "            blocks.append(\n",
      "                MobileOneBlock(\n",
      "                    in_channels=self.in_planes,\n",
      "                    out_channels=planes,\n",
      "                    kernel_size=1,\n",
      "                    stride=1,\n",
      "                    padding=0,\n",
      "                    groups=1,\n",
      "                    inference_mode=self.inference_mode,\n",
      "                    use_se=use_se,\n",
      "                    num_conv_branches=self.num_conv_branches,\n",
      "                )\n",
      "            )\n",
      "            self.in_planes = planes\n",
      "            self.cur_layer_idx += 1\n",
      "        return nn.Sequential(*blocks)\n",
      "_make_stage(self=MobileOne(  (stage0): MobileOneBlock(    (se): Identity()    (activation): ReLU()    (rbr_conv): ModuleList(      (0): Sequential(        (conv): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)        (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)      )    )    (rbr_scale): Sequential(      (conv): Conv2d(3, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)      (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )  )), planes=48, num_blocks=2, num_se_blocks=0, self._backward_hooks=OrderedDict(), self._backward_pre_hooks=OrderedDict(), self._buffers=OrderedDict(), self._depth=3, self._forward_hooks=OrderedDict(), self._forward_hooks_always_called=OrderedDict(), self._forward_hooks_with_kwargs=OrderedDict(), self._forward_pre_hooks=OrderedDict(), self._forward_pre_hooks_with_kwargs=OrderedDict(), self._in_channels=3, self._is_full_backward_hook=None, self._load_state_dict_post_hooks=OrderedDict(), self._load_state_dict_pre_hooks=OrderedDict(), self._modules=OrderedDict([('stage0', MobileOneBlock(  (se): Identity()  (activation): ReLU()  (rbr_conv): ModuleList(    (0): Sequential(      (conv): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)      (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)    )  )  (rbr_scale): Sequential(    (conv): Conv2d(3, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)    (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  )))]), self._non_persistent_buffers_set=set(), self._out_channels=(3, 48, 48, 128, 256, 1024), self._parameters=OrderedDict(), self._state_dict_hooks=OrderedDict(), self._state_dict_pre_hooks=OrderedDict(), self.cur_layer_idx=1, self.in_planes=48, self.inference_mode=False, self.num_conv_branches=4, self.training=True, self.use_se=False)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "blocks = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def overloaded_constants(type_):\n",
      "    typename = type.__name__\n",
      "    if not typename.endswith('s'):\n",
      "        typename += 's'\n",
      "    return type(\n",
      "        \"overloaded_\" + typename,\n",
      "        (_ConstantTransformerBase,),\n",
      "        {'_type': type_, '__doc__': _format_constant_docstring(type_)},\n",
      "    )\n",
      "overloaded_constants(type_=<class 'bytes'>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "typename = type.__name__\n",
      "State:\n",
      "'type'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def load_translation(self) -> \"I18N\":\n",
      "        try:\n",
      "            dir_path = os.path.dirname(os.path.realpath(__file__))\n",
      "            prompts_path = os.path.join(\n",
      "                dir_path, f\"../translations/{self.language}.json\"\n",
      "            )\n",
      "            with open(prompts_path, \"r\") as f:\n",
      "                self._translations = json.load(f)\n",
      "        except FileNotFoundError:\n",
      "            raise ValidationError(\n",
      "                f\"Trasnlation file for language '{self.language}' not found.\"\n",
      "            )\n",
      "        except json.JSONDecodeError:\n",
      "            raise ValidationError(f\"Error decoding JSON from the prompts file.\")\n",
      "        return self\n",
      "load_translation(self=I18N(language='en'), self.__dict__={'language': 'en'}, self.__pydantic_extra__=None, self.__pydantic_fields_set__=set(), self.__pydantic_private__={}, self.language='en')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "dir_path = os.path.dirname(os.path.realpath(__file__))\n",
      "State:\n",
      "'/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/joaomdmoura+crewAI/joaomdmoura+crewAI/src/crewai/utilities'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def load_translation(self) -> \"I18N\":\n",
      "        try:\n",
      "            dir_path = os.path.dirname(os.path.realpath(__file__))\n",
      "            prompts_path = os.path.join(\n",
      "                dir_path, f\"../translations/{self.language}.json\"\n",
      "            )\n",
      "            with open(prompts_path, \"r\") as f:\n",
      "                self._translations = json.load(f)\n",
      "        except FileNotFoundError:\n",
      "            raise ValidationError(\n",
      "                f\"Trasnlation file for language '{self.language}' not found.\"\n",
      "            )\n",
      "        except json.JSONDecodeError:\n",
      "            raise ValidationError(f\"Error decoding JSON from the prompts file.\")\n",
      "        return self\n",
      "load_translation(self=I18N(language='en'), self.__dict__={'language': 'en'}, self.__pydantic_extra__=None, self.__pydantic_fields_set__=set(), self.__pydantic_private__={}, self.language='en')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "prompts_path = os.path.join(\n",
      "State:\n",
      "'/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/joaomdmoura+crewAI/joaomdmoura+crewAI/src/crewai/utilities/../translations/en.json'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parse(lines):\n",
      "    info = []\n",
      "    for line in lines:\n",
      "        if not line.strip():\n",
      "            continue\n",
      "        raw_info = parse_line(line)\n",
      "        if raw_info is not None:\n",
      "            info.append(raw_info)\n",
      "    return info\n",
      "parse(lines=['lrwxrwxrwx    1 0        0              19 Jan 18  2006 debian -> ./pub/mirror/debian', 'drwxr-xr-x   10 0        0            4096 Aug 03 09:21 debian-archive', 'lrwxrwxrwx    1 0        0              27 Nov 30  2015 debian-backports -> pub/mirror/debian-backports', 'drwxr-xr-x   12 0        0            4096 Sep 29 13:13 pub', '-rw-r--r--    1 0        0              26 Mar 04  2010 robots.txt', 'drwxr-xr-x   8 foo      bar          4096 Oct  4 09:05 test', 'drwxr-xr-x   2 foo-user foo-group         0 Jan  5 11:59 240485'])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "info = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def decode_linux(line, match):\n",
      "    perms, links, uid, gid, size, mtime, name = match.groups()\n",
      "    is_link = perms.startswith(\"l\")\n",
      "    is_dir = perms.startswith(\"d\") or is_link\n",
      "    if is_link:\n",
      "        name, _, _link_name = name.partition(\"->\")\n",
      "        name = name.strip()\n",
      "        _link_name = _link_name.strip()\n",
      "    permissions = Permissions.parse(perms[1:])\n",
      "    mtime_epoch = _parse_time(mtime)\n",
      "    name = unicodedata.normalize(\"NFC\", name)\n",
      "    raw_info = {\n",
      "        \"basic\": {\"name\": name, \"is_dir\": is_dir},\n",
      "        \"details\": {\n",
      "            \"size\": int(size),\n",
      "            \"type\": int(ResourceType.directory if is_dir else ResourceType.file),\n",
      "        },\n",
      "        \"access\": {\"permissions\": permissions.dump()},\n",
      "        \"ftp\": {\"ls\": line},\n",
      "    }\n",
      "    access = raw_info[\"access\"]\n",
      "    details = raw_info[\"details\"]\n",
      "    if mtime_epoch is not None:\n",
      "        details[\"modified\"] = mtime_epoch\n",
      "    access[\"user\"] = uid\n",
      "    access[\"group\"] = gid\n",
      "    return raw_info\n",
      "decode_linux(line='lrwxrwxrwx    1 0        0              19 Jan 18  2006 debian -> ./pub/mirror/debian', match=<re.Match object; span=(0, 85), match='lrwxrwxrwx    1 0        0              19 Jan 18>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "perms, links, uid, gid, size, mtime, name = match.groups()\n",
      "State:\n",
      "'lrwxrwxrwx'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def decode_linux(line, match):\n",
      "    perms, links, uid, gid, size, mtime, name = match.groups()\n",
      "    is_link = perms.startswith(\"l\")\n",
      "    is_dir = perms.startswith(\"d\") or is_link\n",
      "    if is_link:\n",
      "        name, _, _link_name = name.partition(\"->\")\n",
      "        name = name.strip()\n",
      "        _link_name = _link_name.strip()\n",
      "    permissions = Permissions.parse(perms[1:])\n",
      "    mtime_epoch = _parse_time(mtime)\n",
      "    name = unicodedata.normalize(\"NFC\", name)\n",
      "    raw_info = {\n",
      "        \"basic\": {\"name\": name, \"is_dir\": is_dir},\n",
      "        \"details\": {\n",
      "            \"size\": int(size),\n",
      "            \"type\": int(ResourceType.directory if is_dir else ResourceType.file),\n",
      "        },\n",
      "        \"access\": {\"permissions\": permissions.dump()},\n",
      "        \"ftp\": {\"ls\": line},\n",
      "    }\n",
      "    access = raw_info[\"access\"]\n",
      "    details = raw_info[\"details\"]\n",
      "    if mtime_epoch is not None:\n",
      "        details[\"modified\"] = mtime_epoch\n",
      "    access[\"user\"] = uid\n",
      "    access[\"group\"] = gid\n",
      "    return raw_info\n",
      "decode_linux(line='lrwxrwxrwx    1 0        0              19 Jan 18  2006 debian -> ./pub/mirror/debian', match=<re.Match object; span=(0, 85), match='lrwxrwxrwx    1 0        0              19 Jan 18>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "is_link = perms.startswith(\"l\")\n",
      "State:\n",
      "True\n",
      "==================================================\n",
      "Clean Code:\n",
      "def decode_linux(line, match):\n",
      "    perms, links, uid, gid, size, mtime, name = match.groups()\n",
      "    is_link = perms.startswith(\"l\")\n",
      "    is_dir = perms.startswith(\"d\") or is_link\n",
      "    if is_link:\n",
      "        name, _, _link_name = name.partition(\"->\")\n",
      "        name = name.strip()\n",
      "        _link_name = _link_name.strip()\n",
      "    permissions = Permissions.parse(perms[1:])\n",
      "    mtime_epoch = _parse_time(mtime)\n",
      "    name = unicodedata.normalize(\"NFC\", name)\n",
      "    raw_info = {\n",
      "        \"basic\": {\"name\": name, \"is_dir\": is_dir},\n",
      "        \"details\": {\n",
      "            \"size\": int(size),\n",
      "            \"type\": int(ResourceType.directory if is_dir else ResourceType.file),\n",
      "        },\n",
      "        \"access\": {\"permissions\": permissions.dump()},\n",
      "        \"ftp\": {\"ls\": line},\n",
      "    }\n",
      "    access = raw_info[\"access\"]\n",
      "    details = raw_info[\"details\"]\n",
      "    if mtime_epoch is not None:\n",
      "        details[\"modified\"] = mtime_epoch\n",
      "    access[\"user\"] = uid\n",
      "    access[\"group\"] = gid\n",
      "    return raw_info\n",
      "decode_linux(line='lrwxrwxrwx    1 0        0              19 Jan 18  2006 debian -> ./pub/mirror/debian', match=<re.Match object; span=(0, 85), match='lrwxrwxrwx    1 0        0              19 Jan 18>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "is_dir = perms.startswith(\"d\") or is_link\n",
      "State:\n",
      "True\n",
      "==================================================\n",
      "Clean Code:\n",
      "def decode_linux(line, match):\n",
      "    perms, links, uid, gid, size, mtime, name = match.groups()\n",
      "    is_link = perms.startswith(\"l\")\n",
      "    is_dir = perms.startswith(\"d\") or is_link\n",
      "    if is_link:\n",
      "        name, _, _link_name = name.partition(\"->\")\n",
      "        name = name.strip()\n",
      "        _link_name = _link_name.strip()\n",
      "    permissions = Permissions.parse(perms[1:])\n",
      "    mtime_epoch = _parse_time(mtime)\n",
      "    name = unicodedata.normalize(\"NFC\", name)\n",
      "    raw_info = {\n",
      "        \"basic\": {\"name\": name, \"is_dir\": is_dir},\n",
      "        \"details\": {\n",
      "            \"size\": int(size),\n",
      "            \"type\": int(ResourceType.directory if is_dir else ResourceType.file),\n",
      "        },\n",
      "        \"access\": {\"permissions\": permissions.dump()},\n",
      "        \"ftp\": {\"ls\": line},\n",
      "    }\n",
      "    access = raw_info[\"access\"]\n",
      "    details = raw_info[\"details\"]\n",
      "    if mtime_epoch is not None:\n",
      "        details[\"modified\"] = mtime_epoch\n",
      "    access[\"user\"] = uid\n",
      "    access[\"group\"] = gid\n",
      "    return raw_info\n",
      "decode_linux(line='lrwxrwxrwx    1 0        0              19 Jan 18  2006 debian -> ./pub/mirror/debian', match=<re.Match object; span=(0, 85), match='lrwxrwxrwx    1 0        0              19 Jan 18>)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "name, _, _link_name = name.partition(\"->\")\n",
      "State:\n",
      "'->'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def decode_linux(line, match):\n",
      "    perms, links, uid, gid, size, mtime, name = match.groups()\n",
      "    is_link = perms.startswith(\"l\")\n",
      "    is_dir = perms.startswith(\"d\") or is_link\n",
      "    if is_link:\n",
      "        name, _, _link_name = name.partition(\"->\")\n",
      "        name = name.strip()\n",
      "        _link_name = _link_name.strip()\n",
      "    permissions = Permissions.parse(perms[1:])\n",
      "    mtime_epoch = _parse_time(mtime)\n",
      "    name = unicodedata.normalize(\"NFC\", name)\n",
      "    raw_info = {\n",
      "        \"basic\": {\"name\": name, \"is_dir\": is_dir},\n",
      "        \"details\": {\n",
      "            \"size\": int(size),\n",
      "            \"type\": int(ResourceType.directory if is_dir else ResourceType.file),\n",
      "        },\n",
      "        \"access\": {\"permissions\": permissions.dump()},\n",
      "        \"ftp\": {\"ls\": line},\n",
      "    }\n",
      "    access = raw_info[\"access\"]\n",
      "    details = raw_info[\"details\"]\n",
      "    if mtime_epoch is not None:\n",
      "        details[\"modified\"] = mtime_epoch\n",
      "    access[\"user\"] = uid\n",
      "    access[\"group\"] = gid\n",
      "    return raw_info\n",
      "decode_linux(line='lrwxrwxrwx    1 0        0              19 Jan 18  2006 debian -> ./pub/mirror/debian', match=<re.Match object; span=(0, 85), match='lrwxrwxrwx    1 0        0              19 Jan 18>)\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "mtime_epoch = _parse_time(mtime)\n",
      "State:\n",
      "1137542400.0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_time(t):\n",
      "    t = \" \".join(token.strip() for token in t.lower().split(\" \"))\n",
      "    try:\n",
      "        try:\n",
      "            _t = time.strptime(t, \"%b %d %Y\")\n",
      "        except ValueError:\n",
      "            _t = time.strptime(t, \"%b %d %H:%M\")\n",
      "    except ValueError:\n",
      "        return None\n",
      "    year = _t.tm_year if _t.tm_year != 1900 else time.localtime().tm_year\n",
      "    month = _t.tm_mon\n",
      "    day = _t.tm_mday\n",
      "    hour = _t.tm_hour\n",
      "    minutes = _t.tm_min\n",
      "    dt = datetime.datetime(year, month, day, hour, minutes, tzinfo=UTC)\n",
      "    epoch_time = (dt - epoch_dt).total_seconds()\n",
      "    return epoch_time\n",
      "_parse_time(t='Jan 18  2006')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "_t = time.strptime(t, \"%b %d %Y\")\n",
      "State:\n",
      "time.struct_time(tm_year=2006, tm_mon=1, tm_mday=18, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=2, tm_yday=18, tm_isdst=-1)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_time(t):\n",
      "    t = \" \".join(token.strip() for token in t.lower().split(\" \"))\n",
      "    try:\n",
      "        try:\n",
      "            _t = time.strptime(t, \"%b %d %Y\")\n",
      "        except ValueError:\n",
      "            _t = time.strptime(t, \"%b %d %H:%M\")\n",
      "    except ValueError:\n",
      "        return None\n",
      "    year = _t.tm_year if _t.tm_year != 1900 else time.localtime().tm_year\n",
      "    month = _t.tm_mon\n",
      "    day = _t.tm_mday\n",
      "    hour = _t.tm_hour\n",
      "    minutes = _t.tm_min\n",
      "    dt = datetime.datetime(year, month, day, hour, minutes, tzinfo=UTC)\n",
      "    epoch_time = (dt - epoch_dt).total_seconds()\n",
      "    return epoch_time\n",
      "_parse_time(t='Jan 18  2006')\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "minutes = _t.tm_min\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _parse_facts(cls, line):\n",
      "        name = None\n",
      "        facts = {}\n",
      "        for fact in line.split(\";\"):\n",
      "            key, sep, value = fact.partition(\"=\")\n",
      "            if sep:\n",
      "                key = key.strip().lower()\n",
      "                value = value.strip()\n",
      "                facts[key] = value\n",
      "            else:\n",
      "                name = basename(fact.rstrip(\"/\").strip())\n",
      "        return name if name not in (\".\", \"..\") else None, facts\n",
      "_parse_facts(cls=<class 'fs.ftpfs.FTPFS'>, line='create=19740705000000;modify=19740705000000; /foo')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "name = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _transform_column(col_name, col_obj):\n",
      "            column_dict[col_name] = col_obj\n",
      "            if col_obj.primary_key:\n",
      "                primary_keys[col_name] = col_obj\n",
      "            col_obj.set_column_name(col_name)\n",
      "            attrs[col_name] = ColumnDescriptor(col_obj)\n",
      "_transform_column(col_name='partition', col_obj={partition_key=True, primary_key=True, index=False, db_field=None, required=False, clustering_order=None, discriminator_column=False, column_name=None, static=False, value=None, position=0}, attrs={'__module__': 'tests.integration.cqlengine.columns.test_container_columns', '__qualname__': 'TestSetModel', 'partition': <cassandra.cqlengine.columns.UUID object at 0x7f392b9a02b0>, 'int_set': <cassandra.cqlengine.columns.Set object at 0x7f392b9a0670>, 'text_set': <cassandra.cqlengine.columns.Set object at 0x7f392b9a0460>, '__abstract__': False, '__discriminator_value__': None, '__default_ttl__': None}, column_dict=OrderedDict(), primary_keys=OrderedDict())\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "col_obj.set_column_name(col_name)\n",
      "State:\n",
      "{partition_key=True, primary_key=True, index=False, db_field=None, required=False, clustering_order=None, discriminator_column=False, column_name='partition', static=False, value=None, position=0}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _insert(self, key, value):\n",
      "        flat_key = self._serialize_key(key)\n",
      "        i = self._index.get(flat_key, -1)\n",
      "        if i >= 0:\n",
      "            self._items[i] = (key, value)\n",
      "        else:\n",
      "            self._items.append((key, value))\n",
      "            self._index[flat_key] = len(self._items) - 1\n",
      "_insert(self=OrderedMapSerializedKey([]), key='bob', value=199)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "flat_key = self._serialize_key(key)\n",
      "State:\n",
      "b'\\xe3\\x81\\xbfbob'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _insert(self, key, value):\n",
      "        flat_key = self._serialize_key(key)\n",
      "        i = self._index.get(flat_key, -1)\n",
      "        if i >= 0:\n",
      "            self._items[i] = (key, value)\n",
      "        else:\n",
      "            self._items.append((key, value))\n",
      "            self._index[flat_key] = len(self._items) - 1\n",
      "_insert(self=OrderedMapSerializedKey([]), key='bob', value=199)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "i = self._index.get(flat_key, -1)\n",
      "State:\n",
      "-1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def setval(self, val):\n",
      "        self.value = val\n",
      "setval(self=<cassandra.cqlengine.columns.BaseValueManager object at 0x7f3928258d00>, val=UUID('075f961f-ee63-45f5-ae73-f5a8f0ec8072'), self.column=<cassandra.cqlengine.columns.UUID object at 0x7f392baa7580>, self.explicit=False, self.instance=IntegerTest(test_id=None, value=None), self.previous_value=None, self.value=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.value = val\n",
      "State:\n",
      "IntegerTest(test_id=UUID('075f961f-ee63-45f5-ae73-f5a8f0ec8072'), value=None)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def filter(self, *args, **kwargs):\n",
      "        if len([x for x in kwargs.values() if x is None]):\n",
      "            raise CQLEngineException(\"None values on filter are not allowed\")\n",
      "        clone = copy.deepcopy(self)\n",
      "        for operator in args:\n",
      "            if not isinstance(operator, WhereClause):\n",
      "                raise QueryException('{0} is not a valid query operator'.format(operator))\n",
      "            clone._where.append(operator)\n",
      "        for arg, val in kwargs.items():\n",
      "            col_name, col_op = self._parse_filter_arg(arg)\n",
      "            quote_field = True\n",
      "            try:\n",
      "                column = self.model._get_column(col_name)\n",
      "            except KeyError:\n",
      "                if col_name == 'pk__token':\n",
      "                    if not isinstance(val, Token):\n",
      "                        raise QueryException(\"Virtual column 'pk__token' may only be compared to Token() values\")\n",
      "                    column = columns._PartitionKeysToken(self.model)\n",
      "                    quote_field = False\n",
      "                else:\n",
      "                    raise QueryException(\"Can't resolve column name: '{0}'\".format(col_name))\n",
      "            if isinstance(val, Token):\n",
      "                if col_name != 'pk__token':\n",
      "                    raise QueryException(\"Token() values may only be compared to the 'pk__token' virtual column\")\n",
      "                partition_columns = column.partition_columns\n",
      "                if len(partition_columns) != len(val.value):\n",
      "                    raise QueryException(\n",
      "                        'Token() received {0} arguments but model has {1} partition keys'.format(\n",
      "                            len(val.value), len(partition_columns)))\n",
      "                val.set_columns(partition_columns)\n",
      "            operator_class = BaseWhereOperator.get_operator(col_op or 'EQ')\n",
      "            operator = operator_class()\n",
      "            if isinstance(operator, InOperator):\n",
      "                if not isinstance(val, (list, tuple)):\n",
      "                    raise QueryException('IN queries must use a list/tuple value')\n",
      "                query_val = [column.to_database(v) for v in val]\n",
      "            elif isinstance(val, BaseQueryFunction):\n",
      "                query_val = val\n",
      "            else:\n",
      "                query_val = column.to_database(val)\n",
      "            clone._where.append(WhereClause(column.db_field_name, operator, query_val, quote_field=quote_field))\n",
      "        return clone\n",
      "filter(self=<cassandra.cqlengine.query.SimpleQuerySet object at 0x7f39281a7940>, args=(), kwargs={'test_id': 5}, self._allow_filtering=False, self._batch=None, self._consistency=None, self._defer_fields=[], self._flat_values_list=False, self._if_not_exists=False, self._limit=10000, self._only_fields=[], self._order=[], self._result_cache=None, self._result_idx=None, self._timeout=<object object at 0x7f392d2d9b80>, self._timestamp=None, self._transaction=[], self._ttl=None, self._values_list=False, self._where=[], self.model=<cassandra.cqlengine.named.NamedTable object at 0x7f39281a75b0>)\n",
      "\n",
      "n:\n",
      "20\n",
      "Statement:\n",
      "quote_field = True\n",
      "State:\n",
      "True\n",
      "==================================================\n",
      "Clean Code:\n",
      "def print_status(s):\n",
      "            len_s = disp_len(s)\n",
      "            fp_write('\\r' + s + (' ' * max(last_len[0] - len_s, 0)))\n",
      "            last_len[0] = len_s\n",
      "print_status(s='0it [00:00, ?it/s]', fp_write=<function tqdm.status_printer.<locals>.fp_write at 0x7fde10963040>, last_len=[0])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "len_s = disp_len(s)\n",
      "State:\n",
      "18\n",
      "==================================================\n",
      "Clean Code:\n",
      "def print_status(s):\n",
      "            len_s = disp_len(s)\n",
      "            fp_write('\\r' + s + (' ' * max(last_len[0] - len_s, 0)))\n",
      "            last_len[0] = len_s\n",
      "print_status(s='0it [00:00, ?it/s]', fp_write=<function tqdm.status_printer.<locals>.fp_write at 0x7fde10963040>, last_len=[0])\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "last_len[0] = len_s\n",
      "State:\n",
      "[18]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __format__(self, _):\n",
      "        self.format_called += 1\n",
      "        return self.replace\n",
      "__format__(self=<tqdm.utils.FormatReplace object at 0x7fdd6e77bcd0>, _='', self.format_called=0, self.replace='')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.format_called += 1\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def main(fp=sys.stderr, argv=None):\n",
      "    if argv is None:\n",
      "        argv = sys.argv[1:]\n",
      "    try:\n",
      "        log_idx = argv.index('--log')\n",
      "    except ValueError:\n",
      "        for i in argv:\n",
      "            if i.startswith('--log='):\n",
      "                logLevel = i[len('--log='):]\n",
      "                break\n",
      "        else:\n",
      "            logLevel = 'INFO'\n",
      "    else:\n",
      "        logLevel = argv[log_idx + 1]\n",
      "    logging.basicConfig(level=getattr(logging, logLevel),\n",
      "                        format=\"%(levelname)s:%(module)s:%(lineno)d:%(message)s\")\n",
      "    d = tqdm.__doc__ + CLI_EXTRA_DOC\n",
      "    opt_types = dict(RE_OPTS.findall(d))\n",
      "    for o in UNSUPPORTED_OPTS:\n",
      "        opt_types.pop(o)\n",
      "    log.debug(sorted(opt_types.items()))\n",
      "    split = RE_OPTS.split(d)\n",
      "    opt_types_desc = zip(split[1::3], split[2::3], split[3::3])\n",
      "    d = ''.join(('\\n  --{0}  : {2}{3}' if otd[1] == 'bool' else\n",
      "                 '\\n  --{0}=<{1}>  : {2}{3}').format(\n",
      "                     otd[0].replace('_', '-'), otd[0], *otd[1:])\n",
      "                for otd in opt_types_desc if otd[0] not in UNSUPPORTED_OPTS)\n",
      "    help_short = \"Usage:\\n  tqdm [--help | options]\\n\"\n",
      "    d = help_short + \"\"\"\n",
      "Options:\n",
      "  -h, --help     Print this help and exit.\n",
      "  -v, --version  Print version and exit.\n",
      "                if hasattr(resources, 'files'):\n",
      "                    copyfile(str(resources.files('tqdm') / name), dst)\n",
      "                else:\n",
      "                    with resources.path('tqdm', name) as src:\n",
      "                        copyfile(str(src), dst)\n",
      "                log.info(\"written:%s\", dst)\n",
      "            if manpath is not None:\n",
      "                cp('tqdm.1', path.join(manpath, 'tqdm.1'))\n",
      "            if comppath is not None:\n",
      "                cp('completion.sh', path.join(comppath, 'tqdm_completion.sh'))\n",
      "            sys.exit(0)\n",
      "        if tee:\n",
      "            stdout_write = stdout.write\n",
      "            fp_write = getattr(fp, 'buffer', fp).write\n",
      "            class stdout(object):\n",
      "                @staticmethod\n",
      "                def write(x):\n",
      "                    with tqdm.external_write_mode(file=fp):\n",
      "                        fp_write(x)\n",
      "                    stdout_write(x)\n",
      "        if delim_per_char:\n",
      "            tqdm_args.setdefault('unit', 'B')\n",
      "            tqdm_args.setdefault('unit_scale', True)\n",
      "            tqdm_args.setdefault('unit_divisor', 1024)\n",
      "            log.debug(tqdm_args)\n",
      "            with tqdm(**tqdm_args) as t:\n",
      "                posix_pipe(stdin, stdout, '', buf_size, t.update)\n",
      "        elif delim == b'\\\\n':\n",
      "            log.debug(tqdm_args)\n",
      "            write = stdout.write\n",
      "            if update or update_to:\n",
      "                with tqdm(**tqdm_args) as t:\n",
      "                    if update:\n",
      "                        def callback(i):\n",
      "                            t.update(numeric(i.decode()))\n",
      "                    else:\n",
      "                        def callback(i):\n",
      "                            t.update(numeric(i.decode()) - t.n)\n",
      "                    for i in stdin:\n",
      "                        write(i)\n",
      "                        callback(i)\n",
      "            else:\n",
      "                for i in tqdm(stdin, **tqdm_args):\n",
      "                    write(i)\n",
      "        else:\n",
      "            log.debug(tqdm_args)\n",
      "            with tqdm(**tqdm_args) as t:\n",
      "                callback_len = False\n",
      "                if update:\n",
      "                    def callback(i):\n",
      "                        t.update(numeric(i.decode()))\n",
      "                elif update_to:\n",
      "                    def callback(i):\n",
      "                        t.update(numeric(i.decode()) - t.n)\n",
      "                else:\n",
      "                    callback = t.update\n",
      "                    callback_len = True\n",
      "                posix_pipe(stdin, stdout, delim, buf_size, callback, callback_len)\n",
      "main(fp=<_io.TextIOWrapper name='<stderr>' mode='w' encoding='utf-8'>, argv=None)\n",
      "\n",
      "n:\n",
      "25\n",
      "Statement:\n",
      "d = tqdm.__doc__ + CLI_EXTRA_DOC\n",
      "State:\n",
      "'\\n    Decorate an iterable object, returning an iterator which acts exactly\\n    like the original iterable, but prints a dynamically updating\\n    progressbar every time a value is requested.\\n\\n    Parameters\\n    ----------\\n    iterable  : iterable, optional\\n        Iterable to decorate with a progressbar.\\n        Leave blank to manually manage the updates.\\n    desc  : str, optional\\n        Prefix for the progressbar.\\n    total  : int or float, optional\\n        The number of expected iterations. If unspecified,\\n        len(iterable) is used if possible. If float(\"inf\") or as a last\\n        resort, only basic progress statistics are displayed\\n        (no ETA, no progressbar).\\n        If `gui` is True and this parameter needs subsequent updating,\\n        specify an initial arbitrary large positive number,\\n        e.g. 9e9.\\n    leave  : bool, optional\\n        If [default: True], keeps all traces of the progressbar\\n        upon termination of iteration.\\n        If `None`, will leave only if `position` is `0`.\\n    file  : `io.TextIOWrapper` or `io.StringIO`, optional\\n        Specifies where to output the progress messages\\n        (default: sys.stderr). Uses `file.write(str)` and `file.flush()`\\n        methods.  For encoding, see `write_bytes`.\\n    ncols  : int, optional\\n        The width of the entire output message. If specified,\\n        dynamically resizes the progressbar to stay within this bound.\\n        If unspecified, attempts to use environment width. The\\n        fallback is a meter width of 10 and no limit for the counter and\\n        statistics. If 0, will not print any meter (only stats).\\n    mininterval  : float, optional\\n        Minimum progress display update interval [default: 0.1] seconds.\\n    maxinterval  : float, optional\\n        Maximum progress display update interval [default: 10] seconds.\\n        Automatically adjusts `miniters` to correspond to `mininterval`\\n        after long display update lag. Only works if `dynamic_miniters`\\n        or monitor thread is enabled.\\n    miniters  : int or float, optional\\n        Minimum progress display update interval, in iterations.\\n        If 0 and `dynamic_miniters`, will automatically adjust to equal\\n        `mininterval` (more CPU efficient, good for tight loops).\\n        If > 0, will skip display of specified number of iterations.\\n        Tweak this and `mininterval` to get very efficient loops.\\n        If your progress is erratic with both fast and slow iterations\\n        (network, skipping items, etc) you should set miniters=1.\\n    ascii  : bool or str, optional\\n        If unspecified or False, use unicode (smooth blocks) to fill\\n        the meter. The fallback is to use ASCII characters \" 123456789#\".\\n    disable  : bool, optional\\n        Whether to disable the entire progressbar wrapper\\n        [default: False]. If set to None, disable on non-TTY.\\n    unit  : str, optional\\n        String that will be used to define the unit of each iteration\\n        [default: it].\\n    unit_scale  : bool or int or float, optional\\n        If 1 or True, the number of iterations will be reduced/scaled\\n        automatically and a metric prefix following the\\n        International System of Units standard will be added\\n        (kilo, mega, etc.) [default: False]. If any other non-zero\\n        number, will scale `total` and `n`.\\n    dynamic_ncols  : bool, optional\\n        If set, constantly alters `ncols` and `nrows` to the\\n        environment (allowing for window resizes) [default: False].\\n    smoothing  : float, optional\\n        Exponential moving average smoothing factor for speed estimates\\n        (ignored in GUI mode). Ranges from 0 (average speed) to 1\\n        (current/instantaneous speed) [default: 0.3].\\n    bar_format  : str, optional\\n        Specify a custom bar string formatting. May impact performance.\\n        [default: \\'{l_bar}{bar}{r_bar}\\'], where\\n        l_bar=\\'{desc}: {percentage:3.0f}%|\\' and\\n        r_bar=\\'| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, \\'\\n            \\'{rate_fmt}{postfix}]\\'\\n        Possible vars: l_bar, bar, r_bar, n, n_fmt, total, total_fmt,\\n            percentage, elapsed, elapsed_s, ncols, nrows, desc, unit,\\n            rate, rate_fmt, rate_noinv, rate_noinv_fmt,\\n            rate_inv, rate_inv_fmt, postfix, unit_divisor,\\n            remaining, remaining_s, eta.\\n        Note that a trailing \": \" is automatically removed after {desc}\\n        if the latter is empty.\\n    initial  : int or float, optional\\n        The initial counter value. Useful when restarting a progress\\n        bar [default: 0]. If using float, consider specifying `{n:.3f}`\\n        or similar in `bar_format`, or specifying `unit_scale`.\\n    position  : int, optional\\n        Specify the line offset to print this bar (starting from 0)\\n        Automatic if unspecified.\\n        Useful to manage multiple bars at once (eg, from threads).\\n    postfix  : dict or *, optional\\n        Specify additional stats to display at the end of the bar.\\n        Calls `set_postfix(**postfix)` if possible (dict).\\n    unit_divisor  : float, optional\\n        [default: 1000], ignored unless `unit_scale` is True.\\n    write_bytes  : bool, optional\\n        Whether to write bytes. If (default: False) will write unicode.\\n    lock_args  : tuple, optional\\n        Passed to `refresh` for intermediate output\\n        (initialisation, iterating, and updating).\\n    nrows  : int, optional\\n        The screen height. If specified, hides nested bars outside this\\n        bound. If unspecified, attempts to use environment height.\\n        The fallback is 20.\\n    colour  : str, optional\\n        Bar colour (e.g. \\'green\\', \\'#00ff00\\').\\n    delay  : float, optional\\n        Don\\'t display until [default: 0] seconds have elapsed.\\n    gui  : bool, optional\\n        WARNING: internal parameter - do not use.\\n        Use tqdm.gui.tqdm(...) instead. If set, will attempt to use\\n        matplotlib animations for a graphical output [default: False].\\n\\n    Returns\\n    -------\\n    out  : decorated iterator.\\n    \\n    Extra CLI Options\\n    -----------------\\n    name  : type, optional\\n        TODO: find out why this is needed.\\n    delim  : chr, optional\\n        Delimiting character [default: \\'\\\\n\\']. Use \\'\\\\0\\' for null.\\n        N.B.: on Windows systems, Python converts \\'\\\\n\\' to \\'\\\\r\\\\n\\'.\\n    buf_size  : int, optional\\n        String buffer size in bytes [default: 256]\\n        used when `delim` is specified.\\n    bytes  : bool, optional\\n        If true, will count bytes, ignore `delim`, and default\\n        `unit_scale` to True, `unit_divisor` to 1024, and `unit` to \\'B\\'.\\n    tee  : bool, optional\\n        If true, passes `stdin` to both `stderr` and `stdout`.\\n    update  : bool, optional\\n        If true, will treat input as newly elapsed iterations,\\n        i.e. numbers to pass to `update()`. Note that this is slow\\n        (~2e5 it/s) since every input must be decoded as a number.\\n    update_to  : bool, optional\\n        If true, will treat input as total elapsed iterations,\\n        i.e. numbers to assign to `self.n`. Note that this is slow\\n        (~2e5 it/s) since every input must be decoded as a number.\\n    null  : bool, optional\\n        If true, will discard input (no stdout).\\n    manpath  : str, optional\\n        Directory in which to install tqdm man pages.\\n    comppath  : str, optional\\n        Directory in which to place tqdm completion.\\n    log  : str, optional\\n        CRITICAL|FATAL|ERROR|WARN(ING)|[default: \\'INFO\\']|DEBUG|NOTSET.\\n'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def make_context(headers):\n",
      "    headers = {k.lower(): v for k, v in headers.items()}\n",
      "    if not all(h in headers for h in (\n",
      "            TRACE_ID_HEADER.lower(), SPAN_ID_HEADER.lower())):\n",
      "        return None\n",
      "    context = TraceContext(\n",
      "        trace_id=headers.get(TRACE_ID_HEADER.lower()),\n",
      "        parent_id=headers.get(PARENT_ID_HEADER.lower(), None),\n",
      "        span_id=headers.get(SPAN_ID_HEADER.lower()),\n",
      "        sampled=parse_sampled(headers),\n",
      "        shared=False,\n",
      "        debug=parse_debug(headers),\n",
      "    )\n",
      "    return context\n",
      "make_context(headers={'X-B3-Flags': '0', 'X-B3-ParentSpanId': None, 'X-B3-Sampled': '1', 'X-B3-SpanId': '41baf1be2fb9bfc5', 'X-B3-TraceId': '6f9a20b5092fa5e144fd15cc31141cd4'})\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "headers = {k.lower(): v for k, v in headers.items()}\n",
      "State:\n",
      "{'x-b3-flags': '0', 'x-b3-parentspanid': None, 'x-b3-sampled': '1', 'x-b3-spanid': '41baf1be2fb9bfc5', 'x-b3-traceid': '6f9a20b5092fa5e144fd15cc31141cd4'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def make_timestamp(ts=None):\n",
      "    ts = ts if ts is not None else time.time()\n",
      "    return int(ts * 1000 * 1000)\n",
      "make_timestamp(ts=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "ts = ts if ts is not None else time.time()\n",
      "State:\n",
      "1712184656.066319\n",
      "==================================================\n",
      "Clean Code:\n",
      "def start(self, ts):\n",
      "        self._timestamp = ts\n",
      "        return self\n",
      "start(self=<aiozipkin.record.Record object at 0x7f2c661f09a0>, ts=0, self._annotations=[], self._context=TraceContext(trace_id='string', parent_id='string', span_id='string', sampled=True, debug=True, shared=True), self._duration=None, self._finished=False, self._kind=None, self._local_endpoint={'serviceName': 'string', 'ipv4': 'string', 'ipv6': 'string', 'port': 0}, self._name='unknown', self._remote_endpoint=None, self._tags={}, self._timestamp=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self._timestamp = ts\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def set_tag(self, key, value):\n",
      "        self._tags[key] = value\n",
      "        return self\n",
      "set_tag(self=<aiozipkin.record.Record object at 0x7f2c661f09a0>, key='additionalProp1', value='string', self._annotations=[], self._context=TraceContext(trace_id='string', parent_id='string', span_id='string', sampled=True, debug=True, shared=True), self._duration=None, self._finished=False, self._kind=None, self._local_endpoint={'serviceName': 'string', 'ipv4': 'string', 'ipv6': 'string', 'port': 0}, self._name='string', self._remote_endpoint=None, self._tags={}, self._timestamp=0)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self._tags[key] = value\n",
      "State:\n",
      "{'additionalProp1': 'string'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def kind(self, kind):\n",
      "        self._kind = kind\n",
      "        return self\n",
      "kind(self=<aiozipkin.record.Record object at 0x7f2c661f09a0>, kind='CLIENT', self._annotations=[], self._context=TraceContext(trace_id='string', parent_id='string', span_id='string', sampled=True, debug=True, shared=True), self._duration=None, self._finished=False, self._kind=None, self._local_endpoint={'serviceName': 'string', 'ipv4': 'string', 'ipv6': 'string', 'port': 0}, self._name='string', self._remote_endpoint=None, self._tags={'additionalProp1': 'string', 'additionalProp2': 'string', 'additionalProp3': 'string'}, self._timestamp=0)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self._kind = kind\n",
      "State:\n",
      "'CLIENT'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def annotate(self, value, ts):\n",
      "        if value == CLIENT_SEND:\n",
      "            self.kind(CLIENT)\n",
      "            self._timestamp = ts\n",
      "        elif value == SERVER_RECEIVED:\n",
      "            self.kind(SERVER)\n",
      "            self._timestamp = ts\n",
      "        elif value == CLIENT_RECEIVED:\n",
      "            self.kind(CLIENT)\n",
      "            self.finish(ts)\n",
      "        elif value == SERVER_SEND:\n",
      "            self.kind(SERVER)\n",
      "            self.finish(ts)\n",
      "        else:\n",
      "            v = {\"value\": value, \"timestamp\": int(ts)}\n",
      "            self._annotations.append(v)\n",
      "        return self\n",
      "annotate(self=<aiozipkin.record.Record object at 0x7f2c661f09a0>, value='string', ts=0, self._annotations=[], self._context=TraceContext(trace_id='string', parent_id='string', span_id='string', sampled=True, debug=True, shared=True), self._duration=None, self._finished=False, self._kind='CLIENT', self._local_endpoint={'serviceName': 'string', 'ipv4': 'string', 'ipv6': 'string', 'port': 0}, self._name='string', self._remote_endpoint=None, self._tags={'additionalProp1': 'string', 'additionalProp2': 'string', 'additionalProp3': 'string'}, self._timestamp=0)\n",
      "\n",
      "n:\n",
      "15\n",
      "Statement:\n",
      "self._annotations.append(v)\n",
      "State:\n",
      "[{'value': 'string', 'timestamp': 0}]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def remote_endpoint(self, endpoint):\n",
      "        self._remote_endpoint = endpoint._asdict()\n",
      "        return self\n",
      "remote_endpoint(self=<aiozipkin.record.Record object at 0x7f2c661f09a0>, endpoint=Endpoint(serviceName='string', ipv4='string', ipv6='string', port=0), self._annotations=[{'value': 'string', 'timestamp': 0}], self._context=TraceContext(trace_id='string', parent_id='string', span_id='string', sampled=True, debug=True, shared=True), self._duration=None, self._finished=False, self._kind='CLIENT', self._local_endpoint={'serviceName': 'string', 'ipv4': 'string', 'ipv6': 'string', 'port': 0}, self._name='string', self._remote_endpoint=None, self._tags={'additionalProp1': 'string', 'additionalProp2': 'string', 'additionalProp3': 'string'}, self._timestamp=0)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self._remote_endpoint = endpoint._asdict()\n",
      "State:\n",
      "{'serviceName': 'string', 'ipv4': 'string', 'ipv6': 'string', 'port': 0}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def finish(self, ts):\n",
      "        if self._finished:\n",
      "            return self\n",
      "        if ts is not None and self._kind not in (PRODUCER, CONSUMER):\n",
      "            self._duration = max(ts - self._timestamp, 1)\n",
      "        return self\n",
      "finish(self=<aiozipkin.record.Record object at 0x7f2c661f09a0>, ts=0, self._annotations=[{'value': 'string', 'timestamp': 0}], self._context=TraceContext(trace_id='string', parent_id='string', span_id='string', sampled=True, debug=True, shared=True), self._duration=None, self._finished=False, self._kind='CLIENT', self._local_endpoint={'serviceName': 'string', 'ipv4': 'string', 'ipv6': 'string', 'port': 0}, self._name='string', self._remote_endpoint={'serviceName': 'string', 'ipv4': 'string', 'ipv6': 'string', 'port': 0}, self._tags={'additionalProp1': 'string', 'additionalProp2': 'string', 'additionalProp3': 'string'}, self._timestamp=0)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "self._duration = max(ts - self._timestamp, 1)\n",
      "State:\n",
      "1\n",
      "==================================================\n",
      "Clean Code:\n",
      "def new_trace(self, sampled=None, debug=False):\n",
      "        context = self._next_context(None, sampled=None, debug=False)\n",
      "        return self.to_span(context)\n",
      "new_trace(self=<aiozipkin.tracer.Tracer object at 0x7f2c66315370>, sampled=None, debug=False, self._local_endpoint=Endpoint(serviceName='test_service', ipv4='127.0.0.1', ipv6=None, port=8080), self._records={}, self._sampler=<aiozipkin.sampler.Sampler object at 0x7f2c66315970>, self._transport=<conftest.FakeTransport object at 0x7f2c661f0c70>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "context = self._next_context(None, sampled=None, debug=False)\n",
      "State:\n",
      "TraceContext(trace_id='15cbcc98ae50640b07868c479d9bbfc4', parent_id=None, span_id='3660667ac5c682ed', sampled=True, debug=False, shared=False)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _next_context(self, context=None, sampled=None, debug=False):\n",
      "        span_id = generate_random_64bit_string()\n",
      "        if context is not None:\n",
      "            new_context = context._replace(\n",
      "                span_id=span_id,\n",
      "                parent_id=context.span_id,\n",
      "                shared=False)\n",
      "            return new_context\n",
      "        trace_id = generate_random_128bit_string()\n",
      "        if sampled is None:\n",
      "            sampled = self._sampler.is_sampled(trace_id)\n",
      "        new_context = TraceContext(\n",
      "            trace_id=trace_id,\n",
      "            parent_id=None,\n",
      "            span_id=span_id,\n",
      "            sampled=sampled,\n",
      "            debug=debug,\n",
      "            shared=False)\n",
      "        return new_context\n",
      "_next_context(self=<aiozipkin.tracer.Tracer object at 0x7f2c66315370>, context=None, sampled=None, debug=False, self._local_endpoint=Endpoint(serviceName='test_service', ipv4='127.0.0.1', ipv6=None, port=8080), self._records={}, self._sampler=<aiozipkin.sampler.Sampler object at 0x7f2c66315970>, self._transport=<conftest.FakeTransport object at 0x7f2c661f0c70>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "span_id = generate_random_64bit_string()\n",
      "State:\n",
      "'3660667ac5c682ed'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def to_span(self, context):\n",
      "        if not context.sampled:\n",
      "            return NoopSpan(self, context)\n",
      "        record = Record(context, self._local_endpoint)\n",
      "        self._records[context] = record\n",
      "        return Span(self, context, record)\n",
      "to_span(self=<aiozipkin.tracer.Tracer object at 0x7f2c66315370>, context=TraceContext(trace_id='15cbcc98ae50640b07868c479d9bbfc4', parent_id=None, span_id='3660667ac5c682ed', sampled=True, debug=False, shared=False), self._local_endpoint=Endpoint(serviceName='test_service', ipv4='127.0.0.1', ipv6=None, port=8080), self._records={}, self._sampler=<aiozipkin.sampler.Sampler object at 0x7f2c66315970>, self._transport=<conftest.FakeTransport object at 0x7f2c661f0c70>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "record = Record(context, self._local_endpoint)\n",
      "State:\n",
      "{_context=TraceContext(trace_id='15cbcc98ae50640b07868c479d9bbfc4', parent_id=None, span_id='3660667ac5c682ed', sampled=True, debug=False, shared=False), _local_endpoint={'serviceName': 'test_service', 'ipv4': '127.0.0.1', 'ipv6': None, 'port': 8080}, _finished=False, _name='unknown', _kind=None, _timestamp=None, _duration=None, _remote_endpoint=None, _annotations=[], _tags={}}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _send(self, record):\n",
      "        self._records.pop(record._context, None)\n",
      "        self._transport.send(record)\n",
      "_send(self=<aiozipkin.tracer.Tracer object at 0x7f2c66315370>, record={_context=TraceContext(trace_id='15cbcc98ae50640b07868c479d9bbfc4', parent_id=None, span_id='3660667ac5c682ed', sampled=True, debug=False, shared=False), _local_endpoint={'serviceName': 'test_service', 'ipv4': '127.0.0.1', 'ipv6': None, 'port': 8080}, _finished=False, _name='root_span', _kind='CLIENT', _timestamp=1712184656174123, _duration=20563, _remote_endpoint={'serviceName': 'service_a', 'ipv4': '127.0.0.1', 'ipv6': None, 'port': 8080}, _annotations=[{'value': 'start:sql', 'timestamp': 1506970524000000}, {'value': 'end:sql', 'timestamp': 1506970524000000}], _tags={'span_type': 'root'}}, self._local_endpoint=Endpoint(serviceName='test_service', ipv4='127.0.0.1', ipv6=None, port=8080), self._records={TraceContext(trace_id='15cbcc98ae50640b07868c479d9bbfc4', parent_id=None, span_id='3660667ac5c682ed', sampled=True, debug=False, shared=False): <aiozipkin.record.Record object at 0x7f2c661f04c0>}, self._sampler=<aiozipkin.sampler.Sampler object at 0x7f2c66315970>, self._transport=<conftest.FakeTransport object at 0x7f2c661f0c70>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self._records.pop(record._context, None)\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def new_child(self, context):\n",
      "        new_context = self._next_context(context)\n",
      "        if not context.sampled:\n",
      "            return NoopSpan(self, new_context)\n",
      "        return self.to_span(new_context)\n",
      "new_child(self=<aiozipkin.tracer.Tracer object at 0x7f2c661f0850>, context=TraceContext(trace_id='6f9a20b5092fa5e144fd15cc31141cd4', parent_id=None, span_id='41baf1be2fb9bfc5', sampled=False, debug=False, shared=True), self._local_endpoint=Endpoint(serviceName='test_service', ipv4='127.0.0.1', ipv6=None, port=8080), self._records={}, self._sampler=<aiozipkin.sampler.Sampler object at 0x7f2c661f0070>, self._transport=<conftest.FakeTransport object at 0x7f2c661f0970>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "new_context = self._next_context(context)\n",
      "State:\n",
      "TraceContext(trace_id='6f9a20b5092fa5e144fd15cc31141cd4', parent_id='41baf1be2fb9bfc5', span_id='800adeecfc3c86e8', sampled=False, debug=False, shared=False)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def join_span(self, context):\n",
      "        new_context = context\n",
      "        if context.sampled is None:\n",
      "            sampled = self._sampler.is_sampled(context.trace_id)\n",
      "            new_context = new_context._replace(sampled=sampled)\n",
      "        else:\n",
      "            new_context = new_context._replace(shared=True)\n",
      "        return self.to_span(new_context)\n",
      "join_span(self=<aiozipkin.tracer.Tracer object at 0x7f2c661f0b50>, context=TraceContext(trace_id='6f9a20b5092fa5e144fd15cc31141cd4', parent_id=None, span_id='41baf1be2fb9bfc5', sampled=True, debug=False, shared=True), self._local_endpoint=Endpoint(serviceName='test_service', ipv4='127.0.0.1', ipv6=None, port=8080), self._records={}, self._sampler=<aiozipkin.sampler.Sampler object at 0x7f2c661f04c0>, self._transport=<conftest.FakeTransport object at 0x7f2c661f0430>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "new_context = context\n",
      "State:\n",
      "TraceContext(trace_id='6f9a20b5092fa5e144fd15cc31141cd4', parent_id=None, span_id='41baf1be2fb9bfc5', sampled=True, debug=False, shared=True)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def pytest_generate_tests(metafunc):\n",
      "    rule_id = metafunc.config.getoption(\"rule_id\")\n",
      "    ids, test_cases = load_test_cases(\n",
      "        test_cases_path=os.path.join(\n",
      "            \"test/fixtures/rules/std_rule_cases\", f\"{rule_id}.yml\"\n",
      "        )\n",
      "    )\n",
      "    if \"test_case\" in metafunc.fixturenames:\n",
      "        metafunc.parametrize(\"test_case\", test_cases, ids=ids)\n",
      "pytest_generate_tests(metafunc={definition=<FunctionDefinition test__rule_test_case>, config=<_pytest.config.Config object at 0x7fd1895e3e20>, fixturenames=['fail_on_parse_error_after_fix', 'test_verbosity_level', 'test_case', 'caplog', 'monkeypatch', 'request'], cls=None, _arg2fixturedefs={'fail_on_parse_error_after_fix': (<FixtureDef argname='fail_on_parse_error_after_fix' scope='function' baseid='test'>,), 'test_verbosity_level': (<FixtureDef argname='test_verbosity_level' scope='function' baseid='test'>,), 'caplog': (<FixtureDef argname='caplog' scope='function' baseid=''>,), 'monkeypatch': (<FixtureDef argname='monkeypatch' scope='function' baseid=''>,)}, _calls=[]})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "rule_id = metafunc.config.getoption(\"rule_id\")\n",
      "State:\n",
      "'*'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def pytest_generate_tests(metafunc):\n",
      "    rule_id = metafunc.config.getoption(\"rule_id\")\n",
      "    ids, test_cases = load_test_cases(\n",
      "        test_cases_path=os.path.join(\n",
      "            \"test/fixtures/rules/std_rule_cases\", f\"{rule_id}.yml\"\n",
      "        )\n",
      "    )\n",
      "    if \"test_case\" in metafunc.fixturenames:\n",
      "        metafunc.parametrize(\"test_case\", test_cases, ids=ids)\n",
      "pytest_generate_tests(metafunc={definition=<FunctionDefinition test__rule_test_case>, config=<_pytest.config.Config object at 0x7fd1895e3e20>, fixturenames=['fail_on_parse_error_after_fix', 'test_verbosity_level', 'test_case', 'caplog', 'monkeypatch', 'request'], cls=None, _arg2fixturedefs={'fail_on_parse_error_after_fix': (<FixtureDef argname='fail_on_parse_error_after_fix' scope='function' baseid='test'>,), 'test_verbosity_level': (<FixtureDef argname='test_verbosity_level' scope='function' baseid='test'>,), 'caplog': (<FixtureDef argname='caplog' scope='function' baseid=''>,), 'monkeypatch': (<FixtureDef argname='monkeypatch' scope='function' baseid=''>,)}, _calls=[]})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "ids, test_cases = load_test_cases(\n",
      "State:\n",
      "['AL01_test_fail_default_explicit', 'AL01_test_fail_explicit', 'AL01_test_fail_implicit', 'AL01_test_fail_implicit_alias', 'AL01_test_fail_implicit_alias_space', 'AL01_test_fail_implicit_alias_explicit', 'AL01_test_fail_implicit_alias_implicit', 'AL01_test_fail_implicit_alias_implicit_multiple', 'AL01_test_fail_implicit_alias_implicit_newline', 'AL01_test_fail_default_explicit_alias_merge', 'AL01_test_fail_explicit_alias_merge', 'AL01_test_pass_implicit_alias_merge', 'AL01_test_alias_expression_4492', 'AL01_test_alias_expression_4089', 'AL01_test_pass_alias_expression_oracle_tables', 'AL02_issue_561', 'AL02_test_fail_explicit_column_default', 'AL02_test_fail_explicit_column_explicit', 'AL02_test_fail_explicit_column_implicit', 'AL02_test_pass_tsql_alternative_alias', 'AL02_test_fail_alias_ending_equals', 'AL02_test_fail_alias_ending_raw_equals', 'AL02_test_alias_expression_align_4515_1', 'AL02_test_alias_expression_align_4515_2', 'AL02_test_alias_expression_align_4515_3', 'AL02_test_fail_alias_expression_oracle_columns', 'AL03_test_pass_column_exp_without_alias_1', 'AL03_test_pass_column_exp_without_alias_2', 'AL03_test_pass_column_exp_without_alias_allow_scalar_true', 'AL03_test_fail_column_exp_without_alias', 'AL03_test_pass_column_exp_without_alias_if_only_cast', 'AL03_test_pass_column_exp_without_alias_if_only_cast_inc_double_cast', 'AL03_test_pass_column_exp_without_alias_if_bracketed', 'AL03_test_fail_column_exp_without_alias_and_cast_fn', 'AL03_test_fail_column_exp_without_alias_allow_scalar_false', 'AL03_test_pass_column_exp_with_alias', 'AL03_test_pass_function_emits', 'AL03_test_fail_cte_no_column_list', 'AL03_test_pass_cte_column_list', 'AL04_test_fail_exactly_once_duplicated_aliases', 'AL04_test_fail_two_duplicated_aliases', 'AL04_test_fail_subquery', 'AL04_test_pass_subquery', 'AL04_test_pass_bigquery_function', 'AL04_test_pass_tsql_table_variable', 'AL05_test_fail_table_alias_not_referenced_1', 'AL05_test_fail_table_alias_not_referenced_1_subquery', 'AL05_test_pass_table_alias_referenced_subquery', 'AL05_test_pass_table_alias_referenced', 'AL05_test_pass_unaliased_table_referenced', 'AL05_test_ignore_bigquery_value_table_functions', 'AL05_test_ignore_postgres_value_table_functions', 'AL05_test_ignore_postgres_value_table_functions_generate_series', 'AL05_test_fail_table_alias_not_referenced_2', 'AL05_test_fail_table_alias_not_referenced_2_subquery', 'AL05_test_pass_subquery_alias_not_referenced', 'AL05_test_pass_bigquery_unaliased_table_with_hyphens', 'AL05_test_pass_bigquery_aliased_table_with_ticks_referenced', 'AL05_test_pass_tsql_object_reference_override', 'AL05_test_pass_subselect_uses_alias_1', 'AL05_test_pass_subselect_uses_alias_2', 'AL05_test_pass_subselect_uses_alias_3', 'AL05_test_ansi_function_not_table_parameter', 'AL05_test_bigquery_function_takes_tablealias_parameter', 'AL05_test_bigquery_function_takes_tablealias_column_parameter', 'AL05_test_bigquery_function_takes_tablealias_column_struct_parameter', 'AL05_test_snowflake_delete_cte', 'AL05_test_pass_exasol_values_clause', 'AL05_test_fail_exasol_values_clause', 'AL05_test_pass_sparksql_values_clause', 'AL05_test_fail_sparksql_values_clause', 'AL05_test_pass_snowflake_values', 'AL05_test_pass_tsql_values_clause_in_parentheses', 'AL05_test_pass_join_on_expression_in_parentheses', 'AL05_test_pass_bigquery_qualify_clause', 'AL05_test_pass_bigquery_nested_inner_join', 'AL05_test_fail_snowflake_flatten_function', 'AL05_test_pass_derived_query_requires_alias_1', 'AL05_test_pass_derived_query_requires_alias_2', 'AL05_test_pass_derived_query_requires_alias_3', 'AL05_test_pass_redshift_semi_structured_op', 'AL06_test_pass_no_config', 'AL06_test_fail_alias_too_short', 'AL06_test_fail_alias_too_long', 'AL06_test_fail_alias_min_and_max', 'AL06_test_pass_with_config', 'AL07_test_pass_allow_self_join_alias', 'AL07_test_fail_avoid_aliases_1', 'AL07_test_fail_avoid_aliases_2', 'AL07_test_fail_avoid_aliases_3', 'AL07_alias_single_char_identifiers', 'AL07_alias_with_wildcard_identifier', 'AL07_select_from_values', 'AL07_select_from_table_generator', 'AL07_..._nested_query_in_from_clause', 'ST04_test_pass_1', 'ST04_test_pass_2', 'ST04_test_fail_1', 'ST04_test_fail_2', 'ST04_test_fail_3', 'ST04_test_fail_4', 'ST04_test_fail_5', 'ST04_test_double_nesting_1', 'ST04_test_double_nesting_2', 'ST04_test_fail_no_copy_code_out_of_template', 'ST05_select_fail', 'ST05_cte_select_fail', 'ST05_cte_with_clashing_name', 'ST05_double_nested_fail', 'ST05_double_nested_fail_2', 'ST05_unfixable_cte_clash', 'ST05_with_recursive_fail_no_fix', 'ST05_select_multijoin_fail', 'ST05_with_fail', 'ST05_set_fail', 'ST05_simple_pass', 'ST05_from_clause_pass', 'ST05_from_clause_fail', 'ST05_both_clause_fail', 'ST05_no_inner_from_pass', 'ST05_uses_templating', 'ST05_issue_2898_redshift_attribute_error', 'ST05_issue_3623_internal_error_multiple_templated_files', 'ST05_issue_3622_no_space_after_from', 'ST05_issue_3617_parentheses_around_ctas_select', 'ST05_issue_3572_correlated_subquery_1', 'ST05_issue_3572_correlated_subquery_2', 'ST05_issue_3572_correlated_subquery_3', 'ST05_issue_3598_avoid_looping_1', 'ST05_issue_3598_avoid_looping_2', 'ST05_test_fail_subquery_in_cte', 'ST05_test_fail_subquery_in_cte_2', 'ST05_test_fail_subquery_in_cte_3', 'ST06_test_pass_select_statement_order', 'ST06_test_fail_select_statement_order_1', 'ST06_test_fail_select_statement_order_2', 'ST06_test_fail_select_statement_order_3', 'ST06_test_fail_select_statement_order_4', 'ST06_test_fail_select_statement_order_5', 'ST06_test_union_statements_ignored', 'ST06_test_insert_statements_ignored', 'ST06_test_insert_statement_with_cte_ignored', 'ST06_test_merge_statements_ignored', 'ST06_test_merge_statement_with_cte_ignored', 'ST06_test_create_table_as_select_statements_ignored', 'ST06_test_create_table_as_select_with_cte_ignored', 'ST06_test_fail_fix_explicit_column_references_1', 'ST06_test_fail_fix_explicit_column_references_2', 'ST06_test_fail_no_fix_implicit_column_references', 'ST07_test_pass_specify_join_keys', 'ST07_test_fail_specify_join_keys_1', 'ST07_test_fail_specify_join_keys_1_with_alias', 'ST07_test_fail_specify_join_keys_1_with_subquery', 'ST07_test_fail_specify_join_keys_1_with_multi_using', 'ST07_test_fail_specify_join_keys_2', 'ST07_test_partial_fixed_up_to_2nd_join', 'ST07_select_using_fail', 'ST07_test_fail_parent_child_positioning', 'ST07_fail_but_dont_fix_templated_table_names', 'ST07_test_pass_clickhouse', 'ST08_test_fail_distinct_with_parenthesis_1', 'ST08_test_fail_distinct_with_parenthesis_2', 'ST08_test_fail_distinct_with_parenthesis_3', 'ST08_test_fail_distinct_with_parenthesis_4', 'ST08_test_fail_distinct_with_parenthesis_5', 'ST08_test_fail_distinct_with_parenthesis_6', 'ST08_test_fail_distinct_with_parenthesis_7', 'ST08_test_pass_no_distinct', 'ST08_test_fail_distinct_column_inside_count', 'ST08_test_fail_distinct_concat_inside_count', 'ST09_test_pass_no_join_clauses', 'ST09_test_pass_no_join_on_conditions', 'ST09_test_pass_ignored_subconditions', 'ST09_test_pass_unqualified_column_reference', 'ST09_test_pass_earlier_table_first', 'ST09_test_pass_later_table_first', 'ST09_test_fail_earlier_table_first', 'ST09_test_fail_later_table_first', 'ST09_test_fail_later_table_first_left_outer', 'ST09_test_fail_later_table_first_inner', 'ST09_test_fail_later_table_first_right', 'ST09_test_fail_later_table_first_right_outer', 'ST09_test_fail_later_table_first_full_outer', 'ST09_test_pass_later_table_first_cross', 'ST09_test_fail_later_table_first_multiple_subconditions', 'ST09_test_fail_later_table_first_multiple_comparison_operators', 'ST09_test_fail_later_table_first_subquery', 'ST09_test_fail_later_table_first_cte', 'ST09_test_fail_later_table_no_join_clause_in_cte', 'ST09_test_fail_later_table_no_join_clause_in_main_query', 'ST09_test_fail_later_table_first_brackets_after_on', 'ST09_test_fail_later_table_first_brackets_after_from', 'ST09_test_fail_later_table_first_quoted_table_and_column', 'TQ01_test_fail_sp_prefix_1', 'TQ01_test_fail_sp_prefix_2', 'TQ01_test_fail_sp_prefix_3', 'TQ01_test_pass_non_sp_prefix_1', 'TQ01_test_pass_non_sp_prefix_2', 'TQ01_test_pass_non_sp_prefix_3', 'TQ01_test_pass_non_sp_prefix_4']\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_project_generation(cookies, context, context_override):\n",
      "    result = cookies.bake(extra_context={**context, **context_override})\n",
      "    assert result.exit_code == 0\n",
      "    assert result.exception is None\n",
      "    assert result.project_path.name == context[\"project_slug\"]\n",
      "    assert result.project_path.is_dir()\n",
      "    paths = build_files_list(str(result.project_path))\n",
      "    assert paths\n",
      "    check_paths(paths)\n",
      "test_project_generation(cookies={_default_template='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/cookiecutter+cookiecutter-django/cookiecutter+cookiecutter-django', _config_file=local('/tmp/pytest-of-XXX/pytest-202/user_dir0/config'), _counter=0}, context={'project_name': 'My Test Project', 'project_slug': 'my_test_project', 'author_name': 'Test Author', 'email': 'test@example.com', 'description': 'A short description of the project.', 'domain_name': 'example.com', 'version': '0.1.0', 'timezone': 'UTC'}, context_override={'username_type': 'username'})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "result = cookies.bake(extra_context={**context, **context_override})\n",
      "State:\n",
      "<Result /tmp/pytest-of-XXX/pytest-202/test_project_generation_userna0/cookies/bake00/my_test_project>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_project_generation(cookies, context, context_override):\n",
      "    result = cookies.bake(extra_context={**context, **context_override})\n",
      "    assert result.exit_code == 0\n",
      "    assert result.exception is None\n",
      "    assert result.project_path.name == context[\"project_slug\"]\n",
      "    assert result.project_path.is_dir()\n",
      "    paths = build_files_list(str(result.project_path))\n",
      "    assert paths\n",
      "    check_paths(paths)\n",
      "test_project_generation(cookies={_default_template='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/cookiecutter+cookiecutter-django/cookiecutter+cookiecutter-django', _config_file=local('/tmp/pytest-of-XXX/pytest-202/user_dir0/config'), _counter=0}, context={'project_name': 'My Test Project', 'project_slug': 'my_test_project', 'author_name': 'Test Author', 'email': 'test@example.com', 'description': 'A short description of the project.', 'domain_name': 'example.com', 'version': '0.1.0', 'timezone': 'UTC'}, context_override={'username_type': 'username'})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "assert result.exit_code == 0\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _process(self, event_data):\n",
      "        self.machine.callbacks(self.machine.prepare_event, event_data)\n",
      "        _LOGGER.debug(\"%sExecuted machine preparation callbacks before conditions.\", self.machine.name)\n",
      "        for trans in self.transitions[event_data.state.name]:\n",
      "            event_data.transition = trans\n",
      "            if trans.execute(event_data):\n",
      "                event_data.result = True\n",
      "                break\n",
      "_process(self=<Event('advance')@140243575260352>, event_data=<EventData(<Event('advance')@140243575260352>, <State('A')@140243613102032>, None)@140243617257984>, self.machine=<transitions.core.Machine object at 0x7f8d024908b0>, self.name='advance', self.transitions=defaultdict(<class 'list'>, {'A': [<Transition('A', 'B')@140243575459744>], 'B': [<Transition('B', 'C')@140243606888304>], 'C': [<Transition('C', 'D')@140243617258896>]}))\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "event_data.transition = trans\n",
      "State:\n",
      "<EventData(<Event('advance')@140243575260352>, <State('A')@140243613102032>, <Transition('A', 'B')@140243575459744>)@140243617257984>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _change_state(self, event_data):\n",
      "        event_data.machine.get_state(self.source).exit(event_data)\n",
      "        event_data.machine.set_state(self.dest, event_data.model)\n",
      "        event_data.update(getattr(event_data.model, event_data.machine.model_attribute))\n",
      "        event_data.machine.get_state(self.dest).enter(event_data)\n",
      "_change_state(self=<Transition('A', 'B')@140243575459744>, event_data=<EventData(<Event('advance')@140243575260352>, <State('A')@140243613102032>, <Transition('A', 'B')@140243575459744>)@140243617257984>, self.after=[], self.before=[], self.conditions=[], self.dest='B', self.prepare=[], self.source='A')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "event_data.update(getattr(event_data.model, event_data.machine.model_attribute))\n",
      "State:\n",
      "<EventData(<Event('advance')@140243575260352>, <State('B')@140243613097936>, <Transition('A', 'B')@140243575459744>)@140243617257984>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _identify_callback(self, name):\n",
      "        for callback in itertools.chain(self.state_cls.dynamic_methods, self.transition_cls.dynamic_methods):\n",
      "            if name.startswith(callback):\n",
      "                callback_type = callback\n",
      "                break\n",
      "        else:\n",
      "            return None, None\n",
      "        target = name[len(callback_type) + len(self.separator):]\n",
      "        if target == '' or name[len(callback_type)] != self.separator:\n",
      "            return None, None\n",
      "        return callback_type, target\n",
      "_identify_callback(self=<transitions.core.Machine object at 0x7f8d02925ca0>, name='before_move', self._after_state_change=[], self._before_state_change=[], self._finalize_event=[], self._initial='A', self._on_exception=[], self._prepare_event=[], self._queued=False, self._transition_queue=deque([]), self.auto_transitions=True, self.events=OrderedDict([('to_A', <Event('to_A')@140243613196192>), ('to_B', <Event('to_B')@140243610265152>), ('to_C', <Event('to_C')@140243610264480>), ('move', <Event('move')@140243575459744>)]), self.ignore_invalid_triggers=None, self.model_attribute='state', self.models=[<tests.utils.Stuff object at 0x7f8d00826f10>], self.name='', self.send_event=False, self.states=OrderedDict([('A', <State('A')@140243613077264>), ('B', <State('B')@140243613804816>), ('C', <State('C')@140243609784864>)]))\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "callback_type = callback\n",
      "State:\n",
      "'before'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _identify_callback(self, name):\n",
      "        for callback in itertools.chain(self.state_cls.dynamic_methods, self.transition_cls.dynamic_methods):\n",
      "            if name.startswith(callback):\n",
      "                callback_type = callback\n",
      "                break\n",
      "        else:\n",
      "            return None, None\n",
      "        target = name[len(callback_type) + len(self.separator):]\n",
      "        if target == '' or name[len(callback_type)] != self.separator:\n",
      "            return None, None\n",
      "        return callback_type, target\n",
      "_identify_callback(self=<transitions.core.Machine object at 0x7f8d02925ca0>, name='before_move', self._after_state_change=[], self._before_state_change=[], self._finalize_event=[], self._initial='A', self._on_exception=[], self._prepare_event=[], self._queued=False, self._transition_queue=deque([]), self.auto_transitions=True, self.events=OrderedDict([('to_A', <Event('to_A')@140243613196192>), ('to_B', <Event('to_B')@140243610265152>), ('to_C', <Event('to_C')@140243610264480>), ('move', <Event('move')@140243575459744>)]), self.ignore_invalid_triggers=None, self.model_attribute='state', self.models=[<tests.utils.Stuff object at 0x7f8d00826f10>], self.name='', self.send_event=False, self.states=OrderedDict([('A', <State('A')@140243613077264>), ('B', <State('B')@140243613804816>), ('C', <State('C')@140243609784864>)]))\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "target = name[len(callback_type) + len(self.separator):]\n",
      "State:\n",
      "'move'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add_state(self, states, on_enter=None, on_exit=None, ignore_invalid_triggers=None, **kwargs):\n",
      "        self.add_states(states=states, on_enter=on_enter, on_exit=on_exit,\n",
      "                        ignore_invalid_triggers=ignore_invalid_triggers, **kwargs)\n",
      "add_state(self=<transitions.core.Machine object at 0x7f8d000d6040>, states='X', on_enter=None, on_exit=None, ignore_invalid_triggers=None, kwargs={}, self._after_state_change=[], self._before_state_change=[], self._finalize_event=[], self._initial='A', self._on_exception=[], self._prepare_event=[], self._queued=False, self._transition_queue=deque([]), self.auto_transitions=True, self.events=OrderedDict([('to_A', <Event('to_A')@140243617470976>), ('to_B', <Event('to_B')@140243617471216>), ('to_C', <Event('to_C')@140243617469488>), ('to_D', <Event('to_D')@140243617469008>), ('to_E', <Event('to_E')@140243617469776>), ('to_F', <Event('to_F')@140243617471264>)]), self.ignore_invalid_triggers=None, self.model_attribute='state', self.models=[<tests.utils.Stuff object at 0x7f8d000d64c0>], self.name='Test Machine: ', self.send_event=False, self.states=OrderedDict([('A', <State('A')@140243617471600>), ('B', <State('B')@140243617470448>), ('C', <State('C')@140243617471072>), ('D', <State('D')@140243617470400>), ('E', <State('E')@140243617471024>), ('F', <State('F')@140243617469248>)]))\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "self.add_states(states=states, on_enter=on_enter, on_exit=on_exit,\n",
      "State:\n",
      "OrderedDict([('to_A', <Event('to_A')@140243617470976>), ('to_B', <Event('to_B')@140243617471216>), ('to_C', <Event('to_C')@140243617469488>), ('to_D', <Event('to_D')@140243617469008>), ('to_E', <Event('to_E')@140243617469776>), ('to_F', <Event('to_F')@140243617471264>), ('to_X', <Event('to_X')@140243617257696>)])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __new__(cls, name, bases, attrs):\n",
      "        cron_jobs = []\n",
      "        min_tick = 0\n",
      "        for each in attrs.values():\n",
      "            if inspect.isfunction(each) and getattr(each, 'is_cronjob', False):\n",
      "                cron_jobs.append(each)\n",
      "                min_tick = fractions.gcd(min_tick, each.tick)\n",
      "        newcls = type.__new__(cls, name, bases, attrs)\n",
      "        newcls._cron_jobs = cron_jobs\n",
      "        newcls._min_tick = min_tick\n",
      "        return newcls\n",
      "__new__(cls=<class 'pyspider.libs.base_handler.BaseHandlerMeta'>, name='Handler', bases=(<class 'pyspider.libs.base_handler.BaseHandler'>,), attrs={'__module__': 'tests.data_fetcher_processor_handler', '__qualname__': 'Handler', 'not_send_status': <function Handler.not_send_status at 0x7f6a0ec3f160>, 'url_deduplicated': <function Handler.url_deduplicated at 0x7f6a0ec3f1f0>, 'catch_http_error': <function Handler.catch_http_error at 0x7f6a0ec3f040>, 'json': <function Handler.json at 0x7f6a0ec3f310>, 'html': <function Handler.html at 0x7f6a0ec3f3a0>, 'links': <function Handler.links at 0x7f6a0ec3f430>, 'cookies': <function Handler.cookies at 0x7f6a0ec3f4c0>, 'get_save': <function Handler.get_save at 0x7f6a0ec3f550>, 'get_process_save': <function Handler.get_process_save at 0x7f6a0ec3f5e0>, 'set_process_save': <function Handler.set_process_save at 0x7f6a0ec3f670>})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "cron_jobs = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def quat_from_axis_angle(axis, angle):\n",
      "        axis_ = np.array(axis, dtype=np.float64)\n",
      "        half_angle = angle * 0.5\n",
      "        ret = np.empty(4)\n",
      "        ret[0] = math.cos(half_angle)\n",
      "        ret[1:4] = math.sin(half_angle) * axis_\n",
      "        return ret\n",
      "quat_from_axis_angle(axis=[1.0, 0.0, 0.0], angle=6.1086523819801535)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "axis_ = np.array(axis, dtype=np.float64)\n",
      "State:\n",
      "array([1., 0., 0.])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def quat_from_axis_angle(axis, angle):\n",
      "        axis_ = np.array(axis, dtype=np.float64)\n",
      "        half_angle = angle * 0.5\n",
      "        ret = np.empty(4)\n",
      "        ret[0] = math.cos(half_angle)\n",
      "        ret[1:4] = math.sin(half_angle) * axis_\n",
      "        return ret\n",
      "quat_from_axis_angle(axis=[1.0, 0.0, 0.0], angle=6.1086523819801535)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "half_angle = angle * 0.5\n",
      "State:\n",
      "3.0543261909900767\n",
      "==================================================\n",
      "Clean Code:\n",
      "def quat_from_axis_angle(axis, angle):\n",
      "        axis_ = np.array(axis, dtype=np.float64)\n",
      "        half_angle = angle * 0.5\n",
      "        ret = np.empty(4)\n",
      "        ret[0] = math.cos(half_angle)\n",
      "        ret[1:4] = math.sin(half_angle) * axis_\n",
      "        return ret\n",
      "quat_from_axis_angle(axis=[1.0, 0.0, 0.0], angle=6.1086523819801535)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "ret = np.empty(4)\n",
      "State:\n",
      "array([4.65265954e-310, 0.00000000e+000, 1.58101007e-322, 6.90572610e-310])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def quat_from_axis_angle(axis, angle):\n",
      "        axis_ = np.array(axis, dtype=np.float64)\n",
      "        half_angle = angle * 0.5\n",
      "        ret = np.empty(4)\n",
      "        ret[0] = math.cos(half_angle)\n",
      "        ret[1:4] = math.sin(half_angle) * axis_\n",
      "        return ret\n",
      "quat_from_axis_angle(axis=[1.0, 0.0, 0.0], angle=6.1086523819801535)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "ret[0] = math.cos(half_angle)\n",
      "State:\n",
      "array([-9.96194698e-001,  0.00000000e+000,  1.58101007e-322,        6.90572610e-310])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def quat_from_axis_angle(axis, angle):\n",
      "        axis_ = np.array(axis, dtype=np.float64)\n",
      "        half_angle = angle * 0.5\n",
      "        ret = np.empty(4)\n",
      "        ret[0] = math.cos(half_angle)\n",
      "        ret[1:4] = math.sin(half_angle) * axis_\n",
      "        return ret\n",
      "quat_from_axis_angle(axis=[1.0, 0.0, 0.0], angle=6.1086523819801535)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "ret[1:4] = math.sin(half_angle) * axis_\n",
      "State:\n",
      "array([-0.9961947 ,  0.08715574,  0.        ,  0.        ])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add_implicit_resolver(cls, tag, regexp, first):\n",
      "        if not 'yaml_implicit_resolvers' in cls.__dict__:\n",
      "            implicit_resolvers = {}\n",
      "            for key in cls.yaml_implicit_resolvers:\n",
      "                implicit_resolvers[key] = cls.yaml_implicit_resolvers[key][:]\n",
      "            cls.yaml_implicit_resolvers = implicit_resolvers\n",
      "        if first is None:\n",
      "            first = [None]\n",
      "        for ch in first:\n",
      "            cls.yaml_implicit_resolvers.setdefault(ch, []).append((tag, regexp))\n",
      "add_implicit_resolver(cls=<class 'invoke.vendor.yaml.resolver.Resolver'>, tag='tag:yaml.org,2002:bool', regexp=re.compile('^(?:yes|Yes|YES|no|No|NO\\n                    |true|True|TRUE|false|False|FALSE\\n                    |on|On|ON|off|Off|OFF)$', re.VERBOSE), first=['y', 'Y', 'n', 'N', 't', 'T', 'f', 'F', 'o', 'O'])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "implicit_resolvers = {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _add_object(self, obj: Any, name: Optional[str] = None) -> None:\n",
      "        method: Callable\n",
      "        if isinstance(obj, Task):\n",
      "            method = self.add_task\n",
      "        elif isinstance(obj, (Collection, ModuleType)):\n",
      "            method = self.add_collection\n",
      "        else:\n",
      "            raise TypeError(\"No idea how to insert {!r}!\".format(type(obj)))\n",
      "        method(obj, name=name)\n",
      "_add_object(self=<Collection None: >, obj=<Task 'make_sudouser'>, name=None, self._configuration={}, self.auto_dash_names=True, self.collections={}, self.default=None, self.loaded_from=None, self.name=None, self.tasks={})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "method = self.add_task\n",
      "State:\n",
      "<bound method Collection.add_task of <Collection None: >>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def unaliased(d, key, value):\n",
      "            super(AliasDict, d).__setitem__(key, value)\n",
      "unaliased(d={}, key='make-sudouser', value=<Task 'make_sudouser'>, __class__=<class 'invoke.vendor.lexicon.alias_dict.AliasDict'>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "super(AliasDict, d).__setitem__(key, value)\n",
      "State:\n",
      "{'make-sudouser': <Task 'make_sudouser'>}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def base64url_decode(input: Union[bytes, str]) -> bytes:\n",
      "    input_bytes = force_bytes(input)\n",
      "    rem = len(input_bytes) % 4\n",
      "    if rem > 0:\n",
      "        input_bytes += b\"=\" * (4 - rem)\n",
      "    return base64.urlsafe_b64decode(input_bytes)\n",
      "base64url_decode(input='hJtXIZ2uSN5kbQfbtTNWbpdmhkV8FJG-Onbc6mxCcYg')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "input_bytes = force_bytes(input)\n",
      "State:\n",
      "b'hJtXIZ2uSN5kbQfbtTNWbpdmhkV8FJG-Onbc6mxCcYg'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_hmac_to_jwk_returns_correct_values(self, as_dict):\n",
      "        algo = HMACAlgorithm(HMACAlgorithm.SHA256)\n",
      "        key: Any = algo.to_jwk(\"secret\", as_dict=as_dict)\n",
      "        if not as_dict:\n",
      "            key = json.loads(key)\n",
      "        assert key == {\"kty\": \"oct\", \"k\": \"c2VjcmV0\"}\n",
      "test_hmac_to_jwk_returns_correct_values(self=<tests.test_algorithms.TestAlgorithms object at 0x7fd0ce2894c0>, as_dict=False)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "algo = HMACAlgorithm(HMACAlgorithm.SHA256)\n",
      "State:\n",
      "{hash_alg=<built-in function openssl_sha256>}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_hmac_to_jwk_returns_correct_values(self, as_dict):\n",
      "        algo = HMACAlgorithm(HMACAlgorithm.SHA256)\n",
      "        key: Any = algo.to_jwk(\"secret\", as_dict=as_dict)\n",
      "        if not as_dict:\n",
      "            key = json.loads(key)\n",
      "        assert key == {\"kty\": \"oct\", \"k\": \"c2VjcmV0\"}\n",
      "test_hmac_to_jwk_returns_correct_values(self=<tests.test_algorithms.TestAlgorithms object at 0x7fd0ce2894c0>, as_dict=False)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "key: Any = algo.to_jwk(\"secret\", as_dict=as_dict)\n",
      "State:\n",
      "'{\"k\": \"c2VjcmV0\", \"kty\": \"oct\"}'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_hmac_to_jwk_returns_correct_values(self, as_dict):\n",
      "        algo = HMACAlgorithm(HMACAlgorithm.SHA256)\n",
      "        key: Any = algo.to_jwk(\"secret\", as_dict=as_dict)\n",
      "        if not as_dict:\n",
      "            key = json.loads(key)\n",
      "        assert key == {\"kty\": \"oct\", \"k\": \"c2VjcmV0\"}\n",
      "test_hmac_to_jwk_returns_correct_values(self=<tests.test_algorithms.TestAlgorithms object at 0x7fd0ce2894c0>, as_dict=False)\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "key = json.loads(key)\n",
      "State:\n",
      "{'k': 'c2VjcmV0', 'kty': 'oct'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _load(self, jwt: str | bytes) -> tuple[bytes, bytes, dict[str, Any], bytes]:\n",
      "        if isinstance(jwt, str):\n",
      "            jwt = jwt.encode(\"utf-8\")\n",
      "        if not isinstance(jwt, bytes):\n",
      "            raise DecodeError(f\"Invalid token type. Token must be a {bytes}\")\n",
      "        try:\n",
      "            signing_input, crypto_segment = jwt.rsplit(b\".\", 1)\n",
      "            header_segment, payload_segment = signing_input.split(b\".\", 1)\n",
      "        except ValueError as err:\n",
      "            raise DecodeError(\"Not enough segments\") from err\n",
      "        try:\n",
      "            header_data = base64url_decode(header_segment)\n",
      "        except (TypeError, binascii.Error) as err:\n",
      "            raise DecodeError(\"Invalid header padding\") from err\n",
      "        try:\n",
      "            header = json.loads(header_data)\n",
      "        except ValueError as e:\n",
      "            raise DecodeError(f\"Invalid header string: {e}\") from e\n",
      "        if not isinstance(header, dict):\n",
      "            raise DecodeError(\"Invalid header string: must be a json object\")\n",
      "        try:\n",
      "            payload = base64url_decode(payload_segment)\n",
      "        except (TypeError, binascii.Error) as err:\n",
      "            raise DecodeError(\"Invalid payload padding\") from err\n",
      "        try:\n",
      "            signature = base64url_decode(crypto_segment)\n",
      "        except (TypeError, binascii.Error) as err:\n",
      "            raise DecodeError(\"Invalid crypto padding\") from err\n",
      "        return (payload, signing_input, header, signature)\n",
      "_load(self=<jwt.api_jws.PyJWS object at 0x7fd0cdf477c0>, jwt='eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.aGVsbG8gd29ybGQ.SIr03zM64awWRdPrAM_61QWsZchAtgDV3pphfHPPWkI', self._algorithms={'none': <jwt.algorithms.NoneAlgorithm object at 0x7fd0cdf479a0>, 'HS256': <jwt.algorithms.HMACAlgorithm object at 0x7fd0cdf474f0>, 'HS384': <jwt.algorithms.HMACAlgorithm object at 0x7fd0cdf47dc0>, 'HS512': <jwt.algorithms.HMACAlgorithm object at 0x7fd0cdf47190>}, self._valid_algs={'HS256', 'HS512', 'none', 'HS384'}, self.options={'verify_signature': True})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "jwt = jwt.encode(\"utf-8\")\n",
      "State:\n",
      "b'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.aGVsbG8gd29ybGQ.SIr03zM64awWRdPrAM_61QWsZchAtgDV3pphfHPPWkI'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def bytes_from_int(val: int) -> bytes:\n",
      "    remaining = val\n",
      "    byte_length = 0\n",
      "    while remaining != 0:\n",
      "        remaining >>= 8\n",
      "        byte_length += 1\n",
      "    return val.to_bytes(byte_length, \"big\", signed=False)\n",
      "bytes_from_int(val=0)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "remaining = val\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def bytes_from_int(val: int) -> bytes:\n",
      "    remaining = val\n",
      "    byte_length = 0\n",
      "    while remaining != 0:\n",
      "        remaining >>= 8\n",
      "        byte_length += 1\n",
      "    return val.to_bytes(byte_length, \"big\", signed=False)\n",
      "bytes_from_int(val=0)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "byte_length = 0\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_targets_from_csv(csv_filename):\n",
      "        targets = []\n",
      "        import csv\n",
      "        with open(csv_filename, 'r') as csvopen:\n",
      "            lines = []\n",
      "            for line in csvopen:\n",
      "                line = line.replace('\\0', '')\n",
      "                lines.append(line)\n",
      "            csv_reader = csv.reader(lines,\n",
      "                    delimiter=',',\n",
      "                    quoting=csv.QUOTE_ALL,\n",
      "                    skipinitialspace=True,\n",
      "                    escapechar='\\\\')\n",
      "            hit_clients = False\n",
      "            for row in csv_reader:\n",
      "                if len(row) == 0: continue\n",
      "                if row[0].strip() == 'BSSID':\n",
      "                    hit_clients = False\n",
      "                    continue\n",
      "                elif row[0].strip() == 'Station MAC':\n",
      "                    hit_clients = True\n",
      "                    continue\n",
      "                if hit_clients:\n",
      "                    try:\n",
      "                        client = Client(row)\n",
      "                    except (IndexError, ValueError) as e:\n",
      "                        continue\n",
      "                    if 'not associated' in client.bssid:\n",
      "                        continue\n",
      "                    for t in targets:\n",
      "                        if t.bssid == client.bssid:\n",
      "                            t.clients.append(client)\n",
      "                            break\n",
      "                else:\n",
      "                    try:\n",
      "                        target = Target(row)\n",
      "                        targets.append(target)\n",
      "                    except Exception:\n",
      "                        continue\n",
      "        return targets\n",
      "get_targets_from_csv(csv_filename='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/derv82+wifite2/derv82+wifite2/tests/files/airodump-weird-ssids.csv')\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "targets = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_targets_from_csv(csv_filename):\n",
      "        targets = []\n",
      "        import csv\n",
      "        with open(csv_filename, 'r') as csvopen:\n",
      "            lines = []\n",
      "            for line in csvopen:\n",
      "                line = line.replace('\\0', '')\n",
      "                lines.append(line)\n",
      "            csv_reader = csv.reader(lines,\n",
      "                    delimiter=',',\n",
      "                    quoting=csv.QUOTE_ALL,\n",
      "                    skipinitialspace=True,\n",
      "                    escapechar='\\\\')\n",
      "            hit_clients = False\n",
      "            for row in csv_reader:\n",
      "                if len(row) == 0: continue\n",
      "                if row[0].strip() == 'BSSID':\n",
      "                    hit_clients = False\n",
      "                    continue\n",
      "                elif row[0].strip() == 'Station MAC':\n",
      "                    hit_clients = True\n",
      "                    continue\n",
      "                if hit_clients:\n",
      "                    try:\n",
      "                        client = Client(row)\n",
      "                    except (IndexError, ValueError) as e:\n",
      "                        continue\n",
      "                    if 'not associated' in client.bssid:\n",
      "                        continue\n",
      "                    for t in targets:\n",
      "                        if t.bssid == client.bssid:\n",
      "                            t.clients.append(client)\n",
      "                            break\n",
      "                else:\n",
      "                    try:\n",
      "                        target = Target(row)\n",
      "                        targets.append(target)\n",
      "                    except Exception:\n",
      "                        continue\n",
      "        return targets\n",
      "get_targets_from_csv(csv_filename='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/derv82+wifite2/derv82+wifite2/tests/files/airodump-weird-ssids.csv')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "with open(csv_filename, 'r') as csvopen:\n",
      "State:\n",
      "<_io.TextIOWrapper name='/local/rcs/XXX/code/pytrace-collector/logs/self_collected/tried/derv82+wifite2/derv82+wifite2/tests/files/airodump-weird-ssids.csv' mode='r' encoding='UTF-8'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def apply(self, transform):\n",
      "        self.transformations.append(transform)\n",
      "apply(self=Lineage: sequence, transform=Transformation(name='map(<lambda>)', function=functools.partial(<class 'map'>, <function TestPipeline.test_add.<locals>.<lambda> at 0x7f52e7bf7280>), execution_strategies=None), self.transformations=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "self.transformations.append(transform)\n",
      "State:\n",
      "Lineage: sequence -> map(<lambda>)\n",
      "==================================================\n",
      "Clean Code:\n",
      "def f(e):\n",
      "            result.append(e)\n",
      "f(e=1, result=[])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "result.append(e)\n",
      "State:\n",
      "[1]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_headers(self) -> HTTPHeaderDict:\n",
      "        headers = HTTPHeaderDict()\n",
      "        if self.content_type and (\n",
      "            not self.headers or \"Content-Type\" not in self.headers\n",
      "        ):\n",
      "            headers[\"Content-Type\"] = self.content_type\n",
      "        if self.headers:\n",
      "            headers.extend(self.headers)\n",
      "        return headers\n",
      "get_headers(self=<Response(url='http://example.com/?test=1&foo=bar' status=200 content_type='text/plain' headers='null')>, self._calls=<responses.CallList object at 0x7f6849b51b80>, self.auto_calculate_content_length=False, self.body=b'test', self.content_type='text/plain', self.headers=None, self.match=(<function query_string_matcher.<locals>.match at 0x7f6849b3d940>, <function query_string_matcher.<locals>.match at 0x7f6849b3de50>), self.method='GET', self.passthrough=False, self.status=200, self.stream=None, self.url='http://example.com/?test=1&foo=bar')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "headers = HTTPHeaderDict()  # Duplicate headers are legal\n",
      "State:\n",
      "HTTPHeaderDict({})\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __exit__(self, type: Any, value: Any, traceback: Any) -> bool:\n",
      "        success = type is None\n",
      "        try:\n",
      "            self.stop(allow_assert=success)\n",
      "        finally:\n",
      "            self.reset()\n",
      "        return success\n",
      "__exit__(self=<responses.RequestsMock object at 0x7f6849d999d0>, type=None, value=None, traceback=None, self._calls=<responses.CallList object at 0x7f6849d99dc0>, self._patcher=<unittest.mock._patch object at 0x7f6849b51580>, self._real_send=<function HTTPAdapter.send at 0x7f684b33c550>, self._registry=<responses.registries.FirstMatchRegistry object at 0x7f6849d99ca0>, self._thread_lock=<unlocked _thread.lock object at 0x7f6849d99f60>, self.assert_all_requests_are_fired=None, self.passthru_prefixes=(), self.response_callback=None, self.target='requests.adapters.HTTPAdapter.send')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "success = type is None\n",
      "State:\n",
      "True\n",
      "==================================================\n",
      "Clean Code:\n",
      "def __exit__(self, type: Any, value: Any, traceback: Any) -> bool:\n",
      "        success = type is None\n",
      "        try:\n",
      "            self.stop(allow_assert=success)\n",
      "        finally:\n",
      "            self.reset()\n",
      "        return success\n",
      "__exit__(self=<responses.RequestsMock object at 0x7f6849d999d0>, type=None, value=None, traceback=None, self._calls=<responses.CallList object at 0x7f6849d99dc0>, self._patcher=<unittest.mock._patch object at 0x7f6849b51580>, self._real_send=<function HTTPAdapter.send at 0x7f684b33c550>, self._registry=<responses.registries.FirstMatchRegistry object at 0x7f6849d99ca0>, self._thread_lock=<unlocked _thread.lock object at 0x7f6849d99f60>, self.assert_all_requests_are_fired=None, self.passthru_prefixes=(), self.response_callback=None, self.target='requests.adapters.HTTPAdapter.send')\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.stop(allow_assert=success)\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _add_from_file(self, file_path: \"Union[str, bytes, os.PathLike[Any]]\") -> None:\n",
      "        data = self._parse_response_file(file_path)\n",
      "        for rsp in data[\"responses\"]:\n",
      "            rsp = rsp[\"response\"]\n",
      "            self.add(\n",
      "                method=rsp[\"method\"],\n",
      "                url=rsp[\"url\"],\n",
      "                body=rsp[\"body\"],\n",
      "                status=rsp[\"status\"],\n",
      "                content_type=rsp[\"content_type\"],\n",
      "                auto_calculate_content_length=rsp[\"auto_calculate_content_length\"],\n",
      "            )\n",
      "_add_from_file(self=<responses.RequestsMock object at 0x7f6849d999d0>, file_path=PosixPath('response_record'), self._calls=<responses.CallList object at 0x7f6849d99dc0>, self._patcher=<unittest.mock._patch object at 0x7f67afb7efd0>, self._real_send=<function HTTPAdapter.send at 0x7f684b33c550>, self._registry=<responses.registries.FirstMatchRegistry object at 0x7f6849031f70>, self._thread_lock=<unlocked _thread.lock object at 0x7f6849d99f60>, self.assert_all_requests_are_fired=None, self.passthru_prefixes=(), self.response_callback=None, self.target='requests.adapters.HTTPAdapter.send')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "data = self._parse_response_file(file_path)\n",
      "State:\n",
      "{'responses': [{'response': {'auto_calculate_content_length': False, 'body': '404 Not Found', 'content_type': 'text/plain', 'method': 'GET', 'status': 404, 'url': 'http://example.com:8080/404'}}, {'response': {'auto_calculate_content_length': False, 'body': 'Invalid status code', 'content_type': 'text/plain', 'method': 'GET', 'status': 400, 'url': 'http://example.com:8080/status/wrong'}}, {'response': {'auto_calculate_content_length': False, 'body': '500 Internal Server Error', 'content_type': 'text/plain', 'method': 'GET', 'status': 500, 'url': 'http://example.com:8080/500'}}, {'response': {'auto_calculate_content_length': False, 'body': 'OK', 'content_type': 'text/plain', 'method': 'PUT', 'status': 202, 'url': 'http://example.com:8080/202'}}]}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def remove(self, response: \"BaseResponse\") -> List[\"BaseResponse\"]:\n",
      "        removed_responses = []\n",
      "        while response in self.registered:\n",
      "            self.registered.remove(response)\n",
      "            removed_responses.append(response)\n",
      "        return removed_responses\n",
      "remove(self=<responses.registries.FirstMatchRegistry object at 0x7f67afb4d460>, response={method='GET', url='http://example.com/two', match=(), _calls=<responses.CallList object at 0x7f67afb54700>, passthrough=False, status=200, body=''}, self._responses=[<Response(url='http://example.com/zero' status=200 content_type='text/plain' headers='null')>, <Response(url='http://example.com/one' status=200 content_type='text/plain' headers='null')>, <Response(url='http://example.com/two' status=200 content_type='text/plain' headers='null')>, <Response(url='re.compile('http://example\\\\.com/three')' status=200 content_type='text/plain' headers='null')>, <Response(url='re.compile('http://example\\\\.com/four')' status=200 content_type='text/plain' headers='null')>])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "removed_responses = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def remove(self, response: \"BaseResponse\") -> List[\"BaseResponse\"]:\n",
      "        removed_responses = []\n",
      "        while response in self.registered:\n",
      "            self.registered.remove(response)\n",
      "            removed_responses.append(response)\n",
      "        return removed_responses\n",
      "remove(self=<responses.registries.FirstMatchRegistry object at 0x7f67afb4d460>, response={method='GET', url='http://example.com/two', match=(), _calls=<responses.CallList object at 0x7f67afb54700>, passthrough=False, status=200, body=''}, self._responses=[<Response(url='http://example.com/zero' status=200 content_type='text/plain' headers='null')>, <Response(url='http://example.com/one' status=200 content_type='text/plain' headers='null')>, <Response(url='http://example.com/two' status=200 content_type='text/plain' headers='null')>, <Response(url='re.compile('http://example\\\\.com/three')' status=200 content_type='text/plain' headers='null')>, <Response(url='re.compile('http://example\\\\.com/four')' status=200 content_type='text/plain' headers='null')>])\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self.registered.remove(response)\n",
      "State:\n",
      "[<Response(url='http://example.com/zero' status=200 content_type='text/plain' headers='null')>, <Response(url='http://example.com/one' status=200 content_type='text/plain' headers='null')>, <Response(url='re.compile('http://example\\\\.com/three')' status=200 content_type='text/plain' headers='null')>, <Response(url='re.compile('http://example\\\\.com/four')' status=200 content_type='text/plain' headers='null')>]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def patched_init(self, *args, **kwargs):\n",
      "        kwargs[\"enforce_content_length\"] = True\n",
      "        original_init(self, *args, **kwargs)\n",
      "patched_init(self=<urllib3.response.HTTPResponse object at 0x7f67afb4d580>, args=(), kwargs={'body': <_io.BytesIO object at 0x7f679c171a90>, 'msg': HTTPHeaderDict({'Content-Type': 'application/json', 'content-length': '2'}), 'preload_content': False}, original_init=<function HTTPResponse.__init__ at 0x7f684a1deee0>)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "original_init(self, *args, **kwargs)\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def class_to_tg(sub_class: str):\n",
      "    trans = {\"Online\": \"_online\", \"Offline\": \"_offline\"}\n",
      "    for upper, lower in trans.items():\n",
      "        sub_class = sub_class.replace(upper, lower)\n",
      "    return sub_class.lower()\n",
      "class_to_tg(sub_class='YYeTsOffline')\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "trans = {\"Online\": \"_online\", \"Offline\": \"_offline\"}\n",
      "State:\n",
      "{'Online': '_online', 'Offline': '_offline'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def class_to_tg(sub_class: str):\n",
      "    trans = {\"Online\": \"_online\", \"Offline\": \"_offline\"}\n",
      "    for upper, lower in trans.items():\n",
      "        sub_class = sub_class.replace(upper, lower)\n",
      "    return sub_class.lower()\n",
      "class_to_tg(sub_class='YYeTsOffline')\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "sub_class = sub_class.replace(upper, lower)\n",
      "State:\n",
      "'YYeTs_offline'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def test_repo():\n",
      "    def log(arguments):\n",
      "        return test_logs\n",
      "    repo = Mock(log=log)\n",
      "    githeat = Githeat(repo)\n",
      "    githeat.parse_commits()\n",
      "    githeat.init_daily_contribution_map()\n",
      "    githeat.compute_daily_contribution_map()\n",
      "    githeat.normalize_daily_contribution_map()\n",
      "    return githeat\n",
      "test_repo()\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "repo = Mock(log=log)\n",
      "State:\n",
      "<Mock id='139779364003552'>\n",
      "==================================================\n",
      "Clean Code:\n",
      "def patch_terminal_size(monkeypatch):\n",
      "    term_width = '250'\n",
      "    term_height = '60'\n",
      "    monkeypatch.setitem(os.environ, 'COLUMNS', term_width)\n",
      "    monkeypatch.setitem(os.environ, 'LINES', term_height)\n",
      "patch_terminal_size(monkeypatch={_setattr=[], _setitem=[], _cwd=None, _savesyspath=None})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "term_width = '250'\n",
      "State:\n",
      "'250'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def patch_terminal_size(monkeypatch):\n",
      "    term_width = '250'\n",
      "    term_height = '60'\n",
      "    monkeypatch.setitem(os.environ, 'COLUMNS', term_width)\n",
      "    monkeypatch.setitem(os.environ, 'LINES', term_height)\n",
      "patch_terminal_size(monkeypatch={_setattr=[], _setitem=[], _cwd=None, _savesyspath=None})\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "term_height = '60'\n",
      "State:\n",
      "'60'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def patch_terminal_size(monkeypatch):\n",
      "    term_width = '250'\n",
      "    term_height = '60'\n",
      "    monkeypatch.setitem(os.environ, 'COLUMNS', term_width)\n",
      "    monkeypatch.setitem(os.environ, 'LINES', term_height)\n",
      "patch_terminal_size(monkeypatch={_setattr=[], _setitem=[], _cwd=None, _savesyspath=None})\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "monkeypatch.setitem(os.environ, 'COLUMNS', term_width)\n",
      "State:\n",
      "{_setattr=[], _setitem=[(environ({'SHELL': '/bin/bash', 'LSCOLORS': 'Gxfxcxdxbxegedabagacad', 'USER_ZDOTDIR': '/home/XXX', 'COLORTERM': 'truecolor', 'LESS': '-R', 'TERM_PROGRAM_VERSION': '3.2a', 'GVM_VERSION': '1.0.22', 'CONDA_EXE': '/local/rcs/XXX/miniforge3/bin/conda', '_CE_M': '', 'TMUX': '/tmp/tmux-19200/default,59951,3', 'PKG_CONFIG_PATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib/pkgconfig:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib/pkgconfig:', '_P9K_TTY': '/dev/pts/20', 'GVM_PATH_BACKUP': '/home/XXX/.gvm/bin:/local/rcs/XXX/miniforge3/envs/mal/bin:/local/rcs/XXX/miniforge3/condabin:/home/XXX/.gdrive-downloader:/local/arise/XXX/miniforge3/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/bin:/home/XXX/.gvm/gos/go1.19.1/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/bin:/home/XXX/.gvm/bin:/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/XXX/.local/bin:/home/XXX/.local/bin:/home/XXX/.local/bin', 'P9K_TTY': 'old', 'LC_FIG_SET_PARENT': '4c022497-5122-4b80-b325-c89bab32302a', 'PWD': '/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/AmmsA+Githeat/AmmsA+Githeat', 'LOGNAME': 'XXX', 'XDG_SESSION_TYPE': 'tty', 'CONDA_PREFIX': '/local/rcs/XXX/miniforge3/envs/AmmsA+Githeat', 'VSCODE_GIT_ASKPASS_NODE': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/node', 'MOTD_SHOWN': 'pam', 'VSCODE_INJECTION': '1', 'GVM_OVERLAY_PREFIX': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay', 'HOME': '/home/XXX', 'LANG': 'en_US.UTF-8', 'DYLD_LIBRARY_PATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'gvm_pkgset_name': 'global', 'SSL_CERT_DIR': '/usr/lib/ssl/certs', 'CONDA_PROMPT_MODIFIER': '(AmmsA+Githeat) ', 'GIT_ASKPASS': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/extensions/git/dist/askpass.sh', 'GVM_ROOT': '/home/XXX/.gvm', 'SSH_CONNECTION': '127.0.0.1 39996 127.0.0.1 22', 'GOROOT': '/home/XXX/.gvm/gos/go1.19.1', 'NVM_DIR': '/local/rcs/XXX/.nvm', 'VSCODE_GIT_ASKPASS_EXTRA_ARGS': '', 'XDG_SESSION_CLASS': 'user', 'PYTHONPATH': ':/local/rcs/XXX/code/pytrace-collector:/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/AmmsA+Githeat/AmmsA+Githeat', 'TERM': 'screen', 'ZSH': '/home/XXX/.oh-my-zsh', '_CE_CONDA': '', 'VSCODE_NONCE': 'd0bc7031-48a3-4719-8bb5-ef236ddd0016', 'ZDOTDIR': '/home/XXX', 'USER': 'XXX', 'TMUX_PANE': '%3', 'VSCODE_GIT_IPC_HANDLE': '/run/user/19200/vscode-git-13d67c6199.sock', 'CONDA_SHLVL': '3', 'SHLVL': '3', 'PAGER': 'less', '_P9K_SSH_TTY': '/dev/pts/20', 'XDG_SESSION_ID': '43', 'CONDA_PYTHON_EXE': '/local/rcs/XXX/miniforge3/bin/python', 'LD_LIBRARY_PATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib', 'XDG_RUNTIME_DIR': '/run/user/19200', 'SSL_CERT_FILE': '/usr/lib/ssl/certs/ca-certificates.crt', 'SSH_CLIENT': '127.0.0.1 46946 22', 'CONDA_DEFAULT_ENV': 'AmmsA+Githeat', 'P9K_SSH': '1', 'LC_ALL': 'en_US.UTF-8', 'VSCODE_GIT_ASKPASS_MAIN': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/extensions/git/dist/askpass-main.js', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'BROWSER': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/helpers/browser.sh', 'PATH': '/home/XXX/.gdrive-downloader:/local/arise/XXX/miniforge3/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/bin:/home/XXX/.gvm/gos/go1.19.1/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/bin:/home/XXX/.gvm/bin:/local/rcs/XXX/miniforge3/envs/AmmsA+Githeat/bin:/local/rcs/XXX/miniforge3/condabin:/home/XXX/.gdrive-downloader:/local/arise/XXX/miniforge3/bin:/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/XXX/.local/bin:/home/XXX/.local/bin', 'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/19200/bus', 'gvm_go_name': 'go1.19.1', 'CONDA_PREFIX_1': '/local/rcs/XXX/miniforge3', 'CONDA_PREFIX_2': '/local/rcs/XXX/miniforge3/envs/mal', 'OLDPWD': '/local/rcs/XXX/code/pytrace-collector', 'GOPATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global', 'TERM_PROGRAM': 'tmux', 'VSCODE_IPC_HOOK_CLI': '/run/user/19200/vscode-ipc-518d6355-acaf-4714-a359-be3fe9f21e09.sock', '_': '/local/rcs/XXX/miniforge3/envs/AmmsA+Githeat/bin/python', 'PYTEST_CURRENT_TEST': 'test/test_interactive.py::test_print_left_header (setup)', 'COLUMNS': '250'}), 'COLUMNS', <notset>)], _cwd=None, _savesyspath=None}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def patch_terminal_size(monkeypatch):\n",
      "    term_width = '250'\n",
      "    term_height = '60'\n",
      "    monkeypatch.setitem(os.environ, 'COLUMNS', term_width)\n",
      "    monkeypatch.setitem(os.environ, 'LINES', term_height)\n",
      "patch_terminal_size(monkeypatch={_setattr=[], _setitem=[], _cwd=None, _savesyspath=None})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "monkeypatch.setitem(os.environ, 'LINES', term_height)\n",
      "State:\n",
      "{_setattr=[], _setitem=[(environ({'SHELL': '/bin/bash', 'LSCOLORS': 'Gxfxcxdxbxegedabagacad', 'USER_ZDOTDIR': '/home/XXX', 'COLORTERM': 'truecolor', 'LESS': '-R', 'TERM_PROGRAM_VERSION': '3.2a', 'GVM_VERSION': '1.0.22', 'CONDA_EXE': '/local/rcs/XXX/miniforge3/bin/conda', '_CE_M': '', 'TMUX': '/tmp/tmux-19200/default,59951,3', 'PKG_CONFIG_PATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib/pkgconfig:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib/pkgconfig:', '_P9K_TTY': '/dev/pts/20', 'GVM_PATH_BACKUP': '/home/XXX/.gvm/bin:/local/rcs/XXX/miniforge3/envs/mal/bin:/local/rcs/XXX/miniforge3/condabin:/home/XXX/.gdrive-downloader:/local/arise/XXX/miniforge3/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/bin:/home/XXX/.gvm/gos/go1.19.1/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/bin:/home/XXX/.gvm/bin:/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/XXX/.local/bin:/home/XXX/.local/bin:/home/XXX/.local/bin', 'P9K_TTY': 'old', 'LC_FIG_SET_PARENT': '4c022497-5122-4b80-b325-c89bab32302a', 'PWD': '/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/AmmsA+Githeat/AmmsA+Githeat', 'LOGNAME': 'XXX', 'XDG_SESSION_TYPE': 'tty', 'CONDA_PREFIX': '/local/rcs/XXX/miniforge3/envs/AmmsA+Githeat', 'VSCODE_GIT_ASKPASS_NODE': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/node', 'MOTD_SHOWN': 'pam', 'VSCODE_INJECTION': '1', 'GVM_OVERLAY_PREFIX': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay', 'HOME': '/home/XXX', 'LANG': 'en_US.UTF-8', 'DYLD_LIBRARY_PATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'gvm_pkgset_name': 'global', 'SSL_CERT_DIR': '/usr/lib/ssl/certs', 'CONDA_PROMPT_MODIFIER': '(AmmsA+Githeat) ', 'GIT_ASKPASS': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/extensions/git/dist/askpass.sh', 'GVM_ROOT': '/home/XXX/.gvm', 'SSH_CONNECTION': '127.0.0.1 39996 127.0.0.1 22', 'GOROOT': '/home/XXX/.gvm/gos/go1.19.1', 'NVM_DIR': '/local/rcs/XXX/.nvm', 'VSCODE_GIT_ASKPASS_EXTRA_ARGS': '', 'XDG_SESSION_CLASS': 'user', 'PYTHONPATH': ':/local/rcs/XXX/code/pytrace-collector:/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/AmmsA+Githeat/AmmsA+Githeat', 'TERM': 'screen', 'ZSH': '/home/XXX/.oh-my-zsh', '_CE_CONDA': '', 'V...deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'gvm_pkgset_name': 'global', 'SSL_CERT_DIR': '/usr/lib/ssl/certs', 'CONDA_PROMPT_MODIFIER': '(AmmsA+Githeat) ', 'GIT_ASKPASS': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/extensions/git/dist/askpass.sh', 'GVM_ROOT': '/home/XXX/.gvm', 'SSH_CONNECTION': '127.0.0.1 39996 127.0.0.1 22', 'GOROOT': '/home/XXX/.gvm/gos/go1.19.1', 'NVM_DIR': '/local/rcs/XXX/.nvm', 'VSCODE_GIT_ASKPASS_EXTRA_ARGS': '', 'XDG_SESSION_CLASS': 'user', 'PYTHONPATH': ':/local/rcs/XXX/code/pytrace-collector:/local/rcs/XXX/code/pytrace-collector/logs/pypibugs/tried/AmmsA+Githeat/AmmsA+Githeat', 'TERM': 'screen', 'ZSH': '/home/XXX/.oh-my-zsh', '_CE_CONDA': '', 'VSCODE_NONCE': 'd0bc7031-48a3-4719-8bb5-ef236ddd0016', 'ZDOTDIR': '/home/XXX', 'USER': 'XXX', 'TMUX_PANE': '%3', 'VSCODE_GIT_IPC_HANDLE': '/run/user/19200/vscode-git-13d67c6199.sock', 'CONDA_SHLVL': '3', 'SHLVL': '3', 'PAGER': 'less', '_P9K_SSH_TTY': '/dev/pts/20', 'XDG_SESSION_ID': '43', 'CONDA_PYTHON_EXE': '/local/rcs/XXX/miniforge3/bin/python', 'LD_LIBRARY_PATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/lib', 'XDG_RUNTIME_DIR': '/run/user/19200', 'SSL_CERT_FILE': '/usr/lib/ssl/certs/ca-certificates.crt', 'SSH_CLIENT': '127.0.0.1 46946 22', 'CONDA_DEFAULT_ENV': 'AmmsA+Githeat', 'P9K_SSH': '1', 'LC_ALL': 'en_US.UTF-8', 'VSCODE_GIT_ASKPASS_MAIN': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/extensions/git/dist/askpass-main.js', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'BROWSER': '/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/helpers/browser.sh', 'PATH': '/home/XXX/.gdrive-downloader:/local/arise/XXX/miniforge3/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/bin:/home/XXX/.gvm/gos/go1.19.1/bin:/home/XXX/.gvm/pkgsets/go1.19.1/global/overlay/bin:/home/XXX/.gvm/bin:/local/rcs/XXX/miniforge3/envs/AmmsA+Githeat/bin:/local/rcs/XXX/miniforge3/condabin:/home/XXX/.gdrive-downloader:/local/arise/XXX/miniforge3/bin:/home/XXX/.vscode-server/cli/servers/Stable-31c37ee8f63491495ac49e43b8544550fbae4533/server/bin/remote-cli:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/XXX/.local/bin:/home/XXX/.local/bin', 'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/19200/bus', 'gvm_go_name': 'go1.19.1', 'CONDA_PREFIX_1': '/local/rcs/XXX/miniforge3', 'CONDA_PREFIX_2': '/local/rcs/XXX/miniforge3/envs/mal', 'OLDPWD': '/local/rcs/XXX/code/pytrace-collector', 'GOPATH': '/home/XXX/.gvm/pkgsets/go1.19.1/global', 'TERM_PROGRAM': 'tmux', 'VSCODE_IPC_HOOK_CLI': '/run/user/19200/vscode-ipc-518d6355-acaf-4714-a359-be3fe9f21e09.sock', '_': '/local/rcs/XXX/miniforge3/envs/AmmsA+Githeat/bin/python', 'PYTEST_CURRENT_TEST': 'test/test_interactive.py::test_print_left_header (setup)', 'COLUMNS': '250', 'LINES': '60'}), 'LINES', <notset>)], _cwd=None, _savesyspath=None}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def generate_samples_loguniform(low, high, step, base, size=1):\n",
      "    samples = base ** (random.uniform(low=logb(low, base), high=logb(high, base), size=size))\n",
      "    if step:\n",
      "        samples = step * np.floor(samples / step)\n",
      "    return samples\n",
      "generate_samples_loguniform(low=5.8884365535558836e-08, high=2.6977394324449206e-07, step=None, base=10, size=100000)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "samples = base ** (random.uniform(low=logb(low, base), high=logb(high, base), size=size))\n",
      "State:\n",
      "array([8.36400364e-08, 2.64090744e-07, 2.64351674e-07, ...,       6.02217873e-08, 1.13953435e-07, 8.52185827e-08])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def generate_samples_uniform(low, high, step, size=1):\n",
      "    samples = random.uniform(low=low,\n",
      "                             high=high,\n",
      "                             size=size)\n",
      "    if step:\n",
      "        samples = step * np.floor(samples / step)\n",
      "    return samples\n",
      "generate_samples_uniform(low=0, high=1, step=None, size=10000)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "samples = random.uniform(low=low,\n",
      "State:\n",
      "array([0.40589487, 0.11308253, 0.31209565, ..., 0.14268188, 0.68390556,       0.14199635])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def from_list(cls, parameters_data):\n",
      "        parameters = []\n",
      "        if type(parameters_data) != list:\n",
      "            raise ValueError(\"parameters_data must be a list of dict\")\n",
      "        for parameter_data in parameters_data:\n",
      "            parameters.append(Parameter.from_dict(parameter_data))\n",
      "        return cls(parameters)\n",
      "from_list(cls=<class 'benderopt.base.optimization_problem.OptimizationProblem'>, parameters_data=[{'name': 'x', 'category': 'uniform', 'search_space': {'low': 0, 'high': 3.141592653589793}}])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "parameters = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def suggest(self):\n",
      "        results = None\n",
      "        if self.batch:\n",
      "            if self.authorize_duplicate:\n",
      "                results = self._generate_sample(self.batch)\n",
      "            else:\n",
      "                results = self._generate_unique_sample(self.batch)\n",
      "        else:\n",
      "            if self.authorize_duplicate:\n",
      "                results = self._generate_samples(1)[0]\n",
      "            else:\n",
      "                results = self._generate_unique_samples(1)[0]\n",
      "        return results\n",
      "suggest(self=<benderopt.optimizer.parzen_estimator.ParzenEstimator object at 0x7fa9329df910>, self.authorize_duplicate=True, self.batch=None, self.gamma=0.15, self.max_retry=50, self.number_of_candidates=100, self.optimization_problem=<benderopt.base.optimization_problem.OptimizationProblem object at 0x7fa9329dfb20>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "results = None\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def observations_quantile(self, quantile):\n",
      "        size = int(len(self.observations) * quantile)\n",
      "        return self.sorted_observations[:size], self.sorted_observations[size:]\n",
      "observations_quantile(self=<benderopt.base.optimization_problem.OptimizationProblem object at 0x7fa9329dfb20>, quantile=0.15, self._finite=True, self.observations=[], self.parameters=[x])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "size = int(len(self.observations) * quantile)\n",
      "State:\n",
      "0\n",
      "==================================================\n",
      "Clean Code:\n",
      "def observations_quantile(self, quantile):\n",
      "        size = int(len(self.observations) * quantile)\n",
      "        return self.sorted_observations[:size], self.sorted_observations[size:]\n",
      "observations_quantile(self=<benderopt.base.optimization_problem.OptimizationProblem object at 0x7fa9329dfb20>, quantile=0.15, self._finite=True, self.observations=[], self.parameters=[x])\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "return self.sorted_observations[:size], self.sorted_observations[size:]\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def sorted_observations(self):\n",
      "        if (not hasattr(self, \"_sorted_observations\") or\n",
      "                len(self._sorted_observations) != len(self.observations)):\n",
      "            self._sorted_observations = sorted(self.observations, key=lambda x: x.loss)\n",
      "        return self._sorted_observations\n",
      "sorted_observations(self=<benderopt.base.optimization_problem.OptimizationProblem object at 0x7fa9329dfb20>, self._finite=True, self.observations=[], self.parameters=[x])\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "self._sorted_observations = sorted(self.observations, key=lambda x: x.loss)\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def parzen_estimator_build_posterior_parameter(parameter, observations):\n",
      "    posterior_parameter = None\n",
      "    parameter_values = [observation.sample[parameter.name] for observation in observations]\n",
      "    search_space = parameter.search_space\n",
      "    if parameter.category == \"categorical\":\n",
      "        prior_probabilities = np.array(search_space[\"probabilities\"])\n",
      "        posterior_probabilities = prior_probabilities\n",
      "        if len(parameter_values) != 0:\n",
      "            observed_probabilities = np.array([parameter_values.count(value)\n",
      "                                               for value in search_space[\"values\"]])\n",
      "            observed_probabilities = observed_probabilities / np.sum(observed_probabilities)\n",
      "            posterior_probabilities += observed_probabilities\n",
      "        posterior_probabilities /= sum(posterior_probabilities)\n",
      "        posterior_parameter = Parameter.from_dict(\n",
      "            {\n",
      "                \"name\": parameter.name,\n",
      "                \"category\": \"categorical\",\n",
      "                \"search_space\": {\n",
      "                    \"values\": search_space[\"values\"],\n",
      "                    \"probabilities\": list(posterior_probabilities),\n",
      "                }\n",
      "            }\n",
      "        )\n",
      "    if parameter.category in (\"uniform\", \"normal\", \"loguniform\", \"lognormal\"):\n",
      "        if parameter.category in (\"uniform\", \"loguniform\"):\n",
      "            prior_mu = 0.5 * (search_space[\"high\"] + search_space[\"low\"])\n",
      "            prior_sigma = (search_space[\"high\"] - search_space[\"low\"])\n",
      "        elif parameter.category in (\"normal\", \"lognormal\"):\n",
      "            prior_mu = search_space[\"mu\"]\n",
      "            prior_sigma = search_space[\"sigma\"]\n",
      "        mus = np.sort(parameter_values + [prior_mu])\n",
      "        tmp = np.concatenate(\n",
      "            (\n",
      "                [search_space.get(\"low\", np.inf)],\n",
      "                mus,\n",
      "                [search_space.get(\"high\", -np.inf)],\n",
      "            )\n",
      "        )\n",
      "        sigmas = np.maximum(tmp[1:-1] - tmp[0:-2], tmp[2:] - tmp[1:-1])\n",
      "        sigma_max_value = prior_sigma\n",
      "        sigma_min_value = prior_sigma / min(100.0, (1.0 + len(mus)))\n",
      "        sigmas = np.clip(sigmas, sigma_min_value, sigma_max_value)\n",
      "        sigmas[np.where(mus == prior_mu)[0]] = prior_sigma\n",
      "        posterior_parameter = Parameter.from_dict(\n",
      "            {\n",
      "                \"name\": parameter.name,\n",
      "                \"category\": \"mixture\",\n",
      "                \"search_space\": {\n",
      "                    \"parameters\": [\n",
      "                        {\n",
      "                            \"category\": \"normal\",\n",
      "                            \"search_space\": {\n",
      "                                \"mu\": mu.tolist(),\n",
      "                                \"sigma\": sigma.tolist(),\n",
      "                                \"low\": search_space[\"low\"],\n",
      "                                \"high\": search_space[\"high\"],\n",
      "                                \"step\": search_space.get(\"step\", None)\n",
      "                            }\n",
      "                        } if parameter.category[:3] != \"log\" else\n",
      "                        {\n",
      "                            \"category\": \"lognormal\",\n",
      "                            \"search_space\": {\n",
      "                                \"mu\": mu.tolist(),\n",
      "                                \"sigma\": sigma.tolist(),\n",
      "                                \"low\": search_space[\"low\"],\n",
      "                                \"high\": search_space[\"high\"],\n",
      "                                \"step\": search_space[\"step\"],\n",
      "                                \"base\": search_space[\"base\"],\n",
      "                            }\n",
      "                        } for mu, sigma in zip(mus, sigmas)\n",
      "                    ],\n",
      "                    \"weights\": [1 / len(mus) for _ in range(len(mus))]\n",
      "                }\n",
      "            }\n",
      "        )\n",
      "    return posterior_parameter\n",
      "parzen_estimator_build_posterior_parameter(parameter=x, observations=[])\n",
      "\n",
      "n:\n",
      "5\n",
      "Statement:\n",
      "parameter_values = [observation.sample[parameter.name] for observation in observations]\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add_observation(self, observation, raise_exception=True):\n",
      "        valid, reason = self._check_observation(observation)\n",
      "        if valid:\n",
      "            self.observations.append(observation)\n",
      "        elif raise_exception:\n",
      "            raise ValueError(reason)\n",
      "add_observation(self=<benderopt.base.optimization_problem.OptimizationProblem object at 0x7fa9329dfb20>, observation={loss=0.7543416204755612, sample={'x': 0.1318532970007551}}, raise_exception=True, self._finite=True, self._sorted_observations=[], self.observations=[], self.parameters=[x])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "valid, reason = self._check_observation(observation)\n",
      "State:\n",
      "True\n",
      "==================================================\n",
      "Clean Code:\n",
      "def detect_json1(conn=None):\n",
      "    if conn is None:\n",
      "        conn = sqlite3.connect(\":memory:\")\n",
      "    try:\n",
      "        conn.execute(\"SELECT json('{}')\")\n",
      "        return True\n",
      "    except Exception:\n",
      "        return False\n",
      "detect_json1(conn=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "conn = sqlite3.connect(\":memory:\")\n",
      "State:\n",
      "REPR FAILED\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add_database(self, db, name=None, route=None):\n",
      "        new_databases = self.databases.copy()\n",
      "        if name is None:\n",
      "            suggestion = db.suggest_name()\n",
      "            name = suggestion\n",
      "        else:\n",
      "            suggestion = name\n",
      "        i = 2\n",
      "        while name in self.databases:\n",
      "            name = \"{}_{}\".format(suggestion, i)\n",
      "            i += 1\n",
      "        db.name = name\n",
      "        db.route = route or name\n",
      "        new_databases[name] = db\n",
      "        self.databases = new_databases\n",
      "        return db\n",
      "add_database(self=<datasette.app.Datasette object at 0x7f478cd789a0>, db=<Database: None (mutable, size=262144)>, name=None, route=None, self._refresh_schemas_lock=<asyncio.locks.Lock object at 0x7f478fc49730 [unlocked]>, self._secret='d4d33894b58516013202e93fd364233ca791a527fa0c07ebde96d1cbd0d5e7ce', self._startup_invoked=False, self.config_dir=None, self.crossdb=False, self.databases=OrderedDict(), self.files=('/tmp/tmp93timthg/fixtures.db',), self.immutables=set(), self.inspect_data=None, self.nolock=False, self.pdb=False, self.permissions={})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "new_databases = self.databases.copy()\n",
      "State:\n",
      "OrderedDict()\n",
      "==================================================\n",
      "Clean Code:\n",
      "def add_database(self, db, name=None, route=None):\n",
      "        new_databases = self.databases.copy()\n",
      "        if name is None:\n",
      "            suggestion = db.suggest_name()\n",
      "            name = suggestion\n",
      "        else:\n",
      "            suggestion = name\n",
      "        i = 2\n",
      "        while name in self.databases:\n",
      "            name = \"{}_{}\".format(suggestion, i)\n",
      "            i += 1\n",
      "        db.name = name\n",
      "        db.route = route or name\n",
      "        new_databases[name] = db\n",
      "        self.databases = new_databases\n",
      "        return db\n",
      "add_database(self=<datasette.app.Datasette object at 0x7f478cd789a0>, db=<Database: None (mutable, size=262144)>, name=None, route=None, self._refresh_schemas_lock=<asyncio.locks.Lock object at 0x7f478fc49730 [unlocked]>, self._secret='d4d33894b58516013202e93fd364233ca791a527fa0c07ebde96d1cbd0d5e7ce', self._startup_invoked=False, self.config_dir=None, self.crossdb=False, self.databases=OrderedDict(), self.files=('/tmp/tmp93timthg/fixtures.db',), self.immutables=set(), self.inspect_data=None, self.nolock=False, self.pdb=False, self.permissions={})\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "suggestion = db.suggest_name()\n",
      "State:\n",
      "'fixtures'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_plugins():\n",
      "    plugins = []\n",
      "    plugin_to_distinfo = dict(pm.list_plugin_distinfo())\n",
      "    for plugin in pm.get_plugins():\n",
      "        static_path = None\n",
      "        templates_path = None\n",
      "        if plugin.__name__ not in DEFAULT_PLUGINS:\n",
      "            try:\n",
      "                if (importlib_resources.files(plugin.__name__) / \"static\").is_dir():\n",
      "                    static_path = str(\n",
      "                        importlib_resources.files(plugin.__name__) / \"static\"\n",
      "                    )\n",
      "                if (importlib_resources.files(plugin.__name__) / \"templates\").is_dir():\n",
      "                    templates_path = str(\n",
      "                        importlib_resources.files(plugin.__name__) / \"templates\"\n",
      "                    )\n",
      "            except (TypeError, ModuleNotFoundError):\n",
      "                pass\n",
      "        plugin_info = {\n",
      "            \"name\": plugin.__name__,\n",
      "            \"static_path\": static_path,\n",
      "            \"templates_path\": templates_path,\n",
      "            \"hooks\": [h.name for h in pm.get_hookcallers(plugin)],\n",
      "        }\n",
      "        distinfo = plugin_to_distinfo.get(plugin)\n",
      "        if distinfo:\n",
      "            plugin_info[\"version\"] = distinfo.version\n",
      "            plugin_info[\"name\"] = distinfo.name or distinfo.project_name\n",
      "        plugins.append(plugin_info)\n",
      "    return plugins\n",
      "get_plugins()\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "plugins = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def get_plugins():\n",
      "    plugins = []\n",
      "    plugin_to_distinfo = dict(pm.list_plugin_distinfo())\n",
      "    for plugin in pm.get_plugins():\n",
      "        static_path = None\n",
      "        templates_path = None\n",
      "        if plugin.__name__ not in DEFAULT_PLUGINS:\n",
      "            try:\n",
      "                if (importlib_resources.files(plugin.__name__) / \"static\").is_dir():\n",
      "                    static_path = str(\n",
      "                        importlib_resources.files(plugin.__name__) / \"static\"\n",
      "                    )\n",
      "                if (importlib_resources.files(plugin.__name__) / \"templates\").is_dir():\n",
      "                    templates_path = str(\n",
      "                        importlib_resources.files(plugin.__name__) / \"templates\"\n",
      "                    )\n",
      "            except (TypeError, ModuleNotFoundError):\n",
      "                pass\n",
      "        plugin_info = {\n",
      "            \"name\": plugin.__name__,\n",
      "            \"static_path\": static_path,\n",
      "            \"templates_path\": templates_path,\n",
      "            \"hooks\": [h.name for h in pm.get_hookcallers(plugin)],\n",
      "        }\n",
      "        distinfo = plugin_to_distinfo.get(plugin)\n",
      "        if distinfo:\n",
      "            plugin_info[\"version\"] = distinfo.version\n",
      "            plugin_info[\"name\"] = distinfo.name or distinfo.project_name\n",
      "        plugins.append(plugin_info)\n",
      "    return plugins\n",
      "get_plugins()\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "plugin_to_distinfo = dict(pm.list_plugin_distinfo())\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _fix(self, path, avoid_path_rewrites=False):\n",
      "        if not isinstance(path, PrefixedUrlString) and not avoid_path_rewrites:\n",
      "            path = self.ds.urls.path(path)\n",
      "        if path.startswith(\"/\"):\n",
      "            path = f\"http://localhost{path}\"\n",
      "        return path\n",
      "_fix(self=<datasette.app.DatasetteClient object at 0x7f47792f0ca0>, path='/.json', avoid_path_rewrites=False, self.app=<function asgi_wrapper.<locals>.wrap.<locals>.maybe_set_actor_in_scope at 0x7f4778fcaaf0>, self.ds=<datasette.app.Datasette object at 0x7f4779ba6a90>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "path = f\"http://localhost{path}\"\n",
      "State:\n",
      "'http://localhost/.json'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def path(self, path, format=None):\n",
      "        if not isinstance(path, PrefixedUrlString):\n",
      "            if path.startswith(\"/\"):\n",
      "                path = path[1:]\n",
      "            path = self.ds.setting(\"base_url\") + path\n",
      "        if format is not None:\n",
      "            path = path_with_format(path=path, format=format)\n",
      "        return PrefixedUrlString(path)\n",
      "path(self=<datasette.url_builder.Urls object at 0x7f478ac844c0>, path='/.json', format=None, self.ds=<datasette.app.Datasette object at 0x7f4779ba6a90>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "path = path[1:]\n",
      "State:\n",
      "'.json'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def path(self, path, format=None):\n",
      "        if not isinstance(path, PrefixedUrlString):\n",
      "            if path.startswith(\"/\"):\n",
      "                path = path[1:]\n",
      "            path = self.ds.setting(\"base_url\") + path\n",
      "        if format is not None:\n",
      "            path = path_with_format(path=path, format=format)\n",
      "        return PrefixedUrlString(path)\n",
      "path(self=<datasette.url_builder.Urls object at 0x7f478ac844c0>, path='/.json', format=None, self.ds=<datasette.app.Datasette object at 0x7f4779ba6a90>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "path = self.ds.setting(\"base_url\") + path\n",
      "State:\n",
      "'/.json'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def _gather_arguments(fn, kwargs):\n",
      "    parameters = inspect.signature(fn).parameters.keys()\n",
      "    call_with = []\n",
      "    for parameter in parameters:\n",
      "        if parameter not in kwargs:\n",
      "            raise TypeError(\n",
      "                \"{} requires parameters {}, missing: {}\".format(\n",
      "                    fn, tuple(parameters), set(parameters) - set(kwargs.keys())\n",
      "                )\n",
      "            )\n",
      "        call_with.append(kwargs[parameter])\n",
      "    return call_with\n",
      "_gather_arguments(fn=<bound method View.__call__ of <datasette.views.database.DatabaseView object at 0x7f478f1be220>>, kwargs={'scope': {'type': 'http', 'asgi': {'version': '3.0'}, 'http_version': '1.1', 'method': 'GET', 'headers': [(b'host', b'localhost'), (b'accept', b'*/*'), (b'accept-encoding', b'gzip, deflate'), (b'connection', b'keep-alive'), (b'user-agent', b'python-httpx/0.27.0')], 'scheme': 'http', 'path': '/fixtures.json', 'raw_path': b'/fixtures.json', 'query_string': b'', 'server': ('localhost', None), 'client': ('127.0.0.1', 123), 'root_path': '', 'csrftoken': <function asgi_csrf_decorator.<locals>._asgi_csrf_decorator.<locals>.app_wrapped_with_csrf.<locals>.get_csrftoken at 0x7f4778eedb80>, 'actor': None, 'url_route': {'kwargs': {'database': 'fixtures', 'format': 'json'}}}, 'receive': <function ASGITransport.handle_async_request.<locals>.receive at 0x7f4778f5c0d0>, 'send': <function asgi_csrf_decorator.<locals>._asgi_csrf_decorator.<locals>.app_wrapped_with_csrf.<locals>.wrapped_send at 0x7f4752c8e5e0>, 'request': <asgi.Request method=\"GET\" url=\"http://localhost/fixtures.json\">, 'datasette': <datasette.app.Datasette object at 0x7f4779ba6a90>})\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "parameters = inspect.signature(fn).parameters.keys()\n",
      "State:\n",
      "odict_keys(['request', 'datasette'])\n",
      "==================================================\n",
      "Clean Code:\n",
      "def json_renderer(request, args, data, error, truncated=None):\n",
      "    status_code = 200\n",
      "    json_cols = []\n",
      "    if \"_json\" in args:\n",
      "        json_cols = args.getlist(\"_json\")\n",
      "    if json_cols and \"rows\" in data and \"columns\" in data:\n",
      "        data[\"rows\"] = convert_specific_columns_to_json(\n",
      "            data[\"rows\"], data[\"columns\"], json_cols\n",
      "        )\n",
      "    if \"rows\" in data and not value_as_boolean(args.get(\"_json_infinity\", \"0\")):\n",
      "        data[\"rows\"] = [remove_infinites(row) for row in data[\"rows\"]]\n",
      "    shape = args.get(\"_shape\", \"objects\")\n",
      "    data[\"ok\"] = True\n",
      "    if error:\n",
      "        shape = \"objects\"\n",
      "        status_code = 400\n",
      "        data[\"error\"] = error\n",
      "        data[\"ok\"] = False\n",
      "    if truncated is not None:\n",
      "        data[\"truncated\"] = truncated\n",
      "    if shape == \"arrayfirst\":\n",
      "        if not data[\"rows\"]:\n",
      "            data = []\n",
      "        elif isinstance(data[\"rows\"][0], sqlite3.Row):\n",
      "            data = [row[0] for row in data[\"rows\"]]\n",
      "        else:\n",
      "            assert isinstance(data[\"rows\"][0], dict)\n",
      "            data = [next(iter(row.values())) for row in data[\"rows\"]]\n",
      "    elif shape in (\"objects\", \"object\", \"array\"):\n",
      "        columns = data.get(\"columns\")\n",
      "        rows = data.get(\"rows\")\n",
      "        if rows and columns and not isinstance(rows[0], dict):\n",
      "            data[\"rows\"] = [dict(zip(columns, row)) for row in rows]\n",
      "        if shape == \"object\":\n",
      "            shape_error = None\n",
      "            if \"primary_keys\" not in data:\n",
      "                shape_error = \"_shape=object is only available on tables\"\n",
      "            else:\n",
      "                pks = data[\"primary_keys\"]\n",
      "                if not pks:\n",
      "                    shape_error = (\n",
      "                        \"_shape=object not available for tables with no primary keys\"\n",
      "                    )\n",
      "                else:\n",
      "                    object_rows = {}\n",
      "                    for row in data[\"rows\"]:\n",
      "                        pk_string = path_from_row_pks(row, pks, not pks)\n",
      "                        object_rows[pk_string] = row\n",
      "                    data = object_rows\n",
      "            if shape_error:\n",
      "                data = {\"ok\": False, \"error\": shape_error}\n",
      "        elif shape == \"array\":\n",
      "            data = data[\"rows\"]\n",
      "    elif shape == \"arrays\":\n",
      "        if not data[\"rows\"]:\n",
      "            pass\n",
      "        elif isinstance(data[\"rows\"][0], sqlite3.Row):\n",
      "            data[\"rows\"] = [list(row) for row in data[\"rows\"]]\n",
      "        else:\n",
      "            data[\"rows\"] = [list(row.values()) for row in data[\"rows\"]]\n",
      "    else:\n",
      "        status_code = 400\n",
      "        data = {\n",
      "            \"ok\": False,\n",
      "            \"error\": f\"Invalid _shape: {shape}\",\n",
      "            \"status\": 400,\n",
      "            \"title\": None,\n",
      "        }\n",
      "    if isinstance(data, dict) and \"columns\" not in request.args.getlist(\"_extra\"):\n",
      "        data.pop(\"columns\", None)\n",
      "    nl = args.get(\"_nl\", \"\")\n",
      "    if nl and shape == \"array\":\n",
      "        body = \"\\n\".join(json.dumps(item, cls=CustomJSONEncoder) for item in data)\n",
      "        content_type = \"text/plain\"\n",
      "    else:\n",
      "        body = json.dumps(data, cls=CustomJSONEncoder)\n",
      "        content_type = \"application/json; charset=utf-8\"\n",
      "    headers = {}\n",
      "    return Response(\n",
      "        body, status=status_code, headers=headers, content_type=content_type\n",
      "    )\n",
      "json_renderer(request=<asgi.Request method=\"GET\" url=\"http://localhost/fixtures.json?sql=select+content+from+simple_primary_key\">, args=<MultiParams: {'sql': ['select content from simple_primary_key']}>, data={'ok': True, 'rows': [<sqlite3.Row object at 0x7f4748d9e570>, <sqlite3.Row object at 0x7f4748ca6fb0>, <sqlite3.Row object at 0x7f4748c42f50>, <sqlite3.Row object at 0x7f4748c42ad0>, <sqlite3.Row object at 0x7f4792e62c70>], 'columns': ['content']}, error=None, truncated=False)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "status_code = 200\n",
      "State:\n",
      "200\n",
      "==================================================\n",
      "Clean Code:\n",
      "def remove_infinites(row):\n",
      "    to_check = row\n",
      "    if isinstance(row, dict):\n",
      "        to_check = row.values()\n",
      "    if not any((c in _infinities) if isinstance(c, float) else 0 for c in to_check):\n",
      "        return row\n",
      "    if isinstance(row, dict):\n",
      "        return {\n",
      "            k: (None if (isinstance(v, float) and v in _infinities) else v)\n",
      "            for k, v in row.items()\n",
      "        }\n",
      "    else:\n",
      "        return [None if (isinstance(c, float) and c in _infinities) else c for c in row]\n",
      "remove_infinites(row=REPR FAILED)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "to_check = row\n",
      "State:\n",
      "REPR FAILED\n",
      "==================================================\n",
      "Clean Code:\n",
      "def table(self, database, table, format=None):\n",
      "        path = f\"{self.database(database)}/{tilde_encode(table)}\"\n",
      "        if format is not None:\n",
      "            path = path_with_format(path=path, format=format)\n",
      "        return PrefixedUrlString(path)\n",
      "table(self=<datasette.url_builder.Urls object at 0x7f4748a86790>, database='fixtures', table='foreign_key_references', format=None, self.ds=<datasette.app.Datasette object at 0x7f4779ba6a90>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "path = f\"{self.database(database)}/{tilde_encode(table)}\"\n",
      "State:\n",
      "'/fixtures/foreign_key_references'\n",
      "==================================================\n",
      "Clean Code:\n",
      "def redirect(cls, path, status=302, headers=None):\n",
      "        headers = headers or {}\n",
      "        headers[\"Location\"] = path\n",
      "        return cls(\"\", status=status, headers=headers)\n",
      "redirect(cls=<class 'datasette.utils.asgi.Response'>, path='/-/settings.json', status=301, headers=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "headers = headers or {}\n",
      "State:\n",
      "{}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def redirect(cls, path, status=302, headers=None):\n",
      "        headers = headers or {}\n",
      "        headers[\"Location\"] = path\n",
      "        return cls(\"\", status=status, headers=headers)\n",
      "redirect(cls=<class 'datasette.utils.asgi.Response'>, path='/-/settings.json', status=301, headers=None)\n",
      "\n",
      "n:\n",
      "2\n",
      "Statement:\n",
      "headers[\"Location\"] = path\n",
      "State:\n",
      "{'Location': '/-/settings.json'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def convert_specific_columns_to_json(rows, columns, json_cols):\n",
      "    json_cols = set(json_cols)\n",
      "    if not json_cols.intersection(columns):\n",
      "        return rows\n",
      "    new_rows = []\n",
      "    for row in rows:\n",
      "        new_row = []\n",
      "        for value, column in zip(row, columns):\n",
      "            if column in json_cols:\n",
      "                try:\n",
      "                    value = json.loads(value)\n",
      "                except (TypeError, ValueError) as e:\n",
      "                    pass\n",
      "            new_row.append(value)\n",
      "        new_rows.append(new_row)\n",
      "    return new_rows\n",
      "convert_specific_columns_to_json(rows=[<sqlite3.Row object at 0x7f474e0730b0>], columns=['intval', 'strval', 'floatval', 'jsonval'], json_cols=['intval'])\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "json_cols = set(json_cols)\n",
      "State:\n",
      "{'intval'}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def convert_specific_columns_to_json(rows, columns, json_cols):\n",
      "    json_cols = set(json_cols)\n",
      "    if not json_cols.intersection(columns):\n",
      "        return rows\n",
      "    new_rows = []\n",
      "    for row in rows:\n",
      "        new_row = []\n",
      "        for value, column in zip(row, columns):\n",
      "            if column in json_cols:\n",
      "                try:\n",
      "                    value = json.loads(value)\n",
      "                except (TypeError, ValueError) as e:\n",
      "                    pass\n",
      "            new_row.append(value)\n",
      "        new_rows.append(new_row)\n",
      "    return new_rows\n",
      "convert_specific_columns_to_json(rows=[<sqlite3.Row object at 0x7f474e0730b0>], columns=['intval', 'strval', 'floatval', 'jsonval'], json_cols=['intval'])\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "new_rows = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def convert_specific_columns_to_json(rows, columns, json_cols):\n",
      "    json_cols = set(json_cols)\n",
      "    if not json_cols.intersection(columns):\n",
      "        return rows\n",
      "    new_rows = []\n",
      "    for row in rows:\n",
      "        new_row = []\n",
      "        for value, column in zip(row, columns):\n",
      "            if column in json_cols:\n",
      "                try:\n",
      "                    value = json.loads(value)\n",
      "                except (TypeError, ValueError) as e:\n",
      "                    pass\n",
      "            new_row.append(value)\n",
      "        new_rows.append(new_row)\n",
      "    return new_rows\n",
      "convert_specific_columns_to_json(rows=[<sqlite3.Row object at 0x7f474e0730b0>], columns=['intval', 'strval', 'floatval', 'jsonval'], json_cols=['intval'])\n",
      "\n",
      "n:\n",
      "10\n",
      "Statement:\n",
      "value = json.loads(value)\n",
      "State:\n",
      "E\n",
      "==================================================\n",
      "Clean Code:\n",
      "def write_token(ds, actor_id=\"root\", permissions=None):\n",
      "    to_sign = {\"a\": actor_id, \"token\": \"dstok\", \"t\": int(time.time())}\n",
      "    if permissions:\n",
      "        to_sign[\"_r\"] = {\"a\": permissions}\n",
      "    return \"dstok_{}\".format(ds.sign(to_sign, namespace=\"token\"))\n",
      "write_token(ds={_startup_invoked=False, config_dir=None, pdb=False, _secret='3f2832bf019ce9b81ffdd7a282ab68964a21175d02e64361e0fb59b7251e9f77', files=('/tmp/pytest-of-XXX/pytest-225/dbs0/data.db', '/tmp/pytest-of-XXX/pytest-225/dbs0/immutable.db'), inspect_data=None, immutables={'/tmp/pytest-of-XXX/pytest-225/dbs0/immutable.db'}, databases=OrderedDict([('data', <Database: data (mutable, size=8192)>), ('immutable', <Database: immutable (hash=eac8229db4cd84df6658915e7fcb90761d1b1ee45d9edb7cb7b92f8e21bb9418, size=8192)>)]), permissions={}, _refresh_schemas_lock=<asyncio.locks.Lock object at 0x7f4700306250 [unlocked]>, crossdb=False, nolock=False, internal_db_created=False, _internal_database=<Database: __INTERNAL__ (mutable, memory, size=0)>, cache_headers=True, cors=False, _metadata_local={}, sqlite_extensions=[], template_dir=None, plugins_dir=None, static_mounts=[], config={}, _settings={'default_page_size': 100, 'max_returned_rows': 1000, 'max_insert_rows': 100, 'num_sql_threads': 3, 'sql_time_limit_ms': 1000, 'default_facet_size': 30, 'facet_time_limit_ms': 200, 'facet_suggest_time_limit_ms': 50, 'allow_facet': True, 'allow_download': True, 'allow_signed_tokens': True, 'default_allow_sql': True, 'max_signed_tokens_ttl': 0, 'suggest_facets': True, 'default_cache_ttl': 5, 'cache_size_kb': 0, 'allow_csv_stream': True, 'max_csv_mb': 100, 'truncate_cells_html': 2048, 'force_https_urls': False, 'template_debug': False, 'trace_debug': False, 'base_url': '/'}, renderers={'json': (<function json_renderer at 0x7f47917a53a0>, <function Datasette._register_renderers.<locals>.<lambda> at 0x7f46ce3ba4c0>), 'message': (<function render_message_debug at 0x7f478cdb1f70>, <function register_output_renderer.<locals>.<lambda> at 0x7f46be8d50d0>), 'testall': (<function render_test_all_parameters at 0x7f478cdb1dc0>, <function can_render at 0x7f478cdb1af0>), 'testnone': (<function render_test_no_parameters at 0x7f478cdb1e50>, <function Datasette._register_renderers.<locals>.<lambda> at 0x7f46cc110280>), 'testresponse': (<function render_response at 0x7f478cdb1ee0>, <function Datasette._register_renderers.<locals>.<lambda> at 0x7f46be2db310>), 'blob': (<function render_blob at 0x7f4791860ca0>, <function register_output_renderer.<locals>.<lambda> at 0x7f46cc0ab9d0>)}, version_note=None, executor=<concurrent.futures.thread.ThreadPoolExecutor object at 0x7f47003068e0>, max_returned_rows=1000, sql_time_limit_ms=1000, page_size=100, _jinja_env=<jinja2.environment.Environment object at 0x7f47002fc640>, _permission_checks=deque([], maxlen=200), _root_token='174815f98e61a422c5af3f355aeb6ccebb0d800ac2ae8d6ea8e4e2bad3230a70', client=<datasette.app.DatasetteClient object at 0x7f47002fce80>}, actor_id='root', permissions=None)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "to_sign = {\"a\": actor_id, \"token\": \"dstok\", \"t\": int(time.time())}\n",
      "State:\n",
      "{'a': 'root', 'token': 'dstok', 't': 1712232548}\n",
      "==================================================\n",
      "Clean Code:\n",
      "def filters_should_redirect(special_args):\n",
      "    redirect_params = []\n",
      "    filter_column = special_args.get(\"_filter_column\")\n",
      "    filter_op = special_args.get(\"_filter_op\") or \"\"\n",
      "    filter_value = special_args.get(\"_filter_value\") or \"\"\n",
      "    if \"__\" in filter_op:\n",
      "        filter_op, filter_value = filter_op.split(\"__\", 1)\n",
      "    if filter_column:\n",
      "        redirect_params.append((f\"{filter_column}__{filter_op}\", filter_value))\n",
      "    for key in (\"_filter_column\", \"_filter_op\", \"_filter_value\"):\n",
      "        if key in special_args:\n",
      "            redirect_params.append((key, None))\n",
      "    column_keys = [k for k in special_args if filter_column_re.match(k)]\n",
      "    for column_key in column_keys:\n",
      "        number = column_key.split(\"_\")[-1]\n",
      "        column = special_args[column_key]\n",
      "        op = special_args.get(f\"_filter_op_{number}\") or \"exact\"\n",
      "        value = special_args.get(f\"_filter_value_{number}\") or \"\"\n",
      "        if \"__\" in op:\n",
      "            op, value = op.split(\"__\", 1)\n",
      "        if column:\n",
      "            redirect_params.append((f\"{column}__{op}\", value))\n",
      "        redirect_params.extend(\n",
      "            [\n",
      "                (f\"_filter_column_{number}\", None),\n",
      "                (f\"_filter_op_{number}\", None),\n",
      "                (f\"_filter_value_{number}\", None),\n",
      "            ]\n",
      "        )\n",
      "    return redirect_params\n",
      "filters_should_redirect(special_args=<MultiParams: {'_shape': ['array']}>)\n",
      "\n",
      "n:\n",
      "1\n",
      "Statement:\n",
      "redirect_params = []\n",
      "State:\n",
      "[]\n",
      "==================================================\n",
      "Clean Code:\n",
      "def filters_should_redirect(special_args):\n",
      "    redirect_params = []\n",
      "    filter_column = special_args.get(\"_filter_column\")\n",
      "    filter_op = special_args.get(\"_filter_op\") or \"\"\n",
      "    filter_value = special_args.get(\"_filter_value\") or \"\"\n",
      "    if \"__\" in filter_op:\n",
      "        filter_op, filter_value = filter_op.split(\"__\", 1)\n",
      "    if filter_column:\n",
      "        redirect_params.append((f\"{filter_column}__{filter_op}\", filter_value))\n",
      "    for key in (\"_filter_column\", \"_filter_op\", \"_filter_value\"):\n",
      "        if key in special_args:\n",
      "            redirect_params.append((key, None))\n",
      "    column_keys = [k for k in special_args if filter_column_re.match(k)]\n",
      "    for column_key in column_keys:\n",
      "        number = column_key.split(\"_\")[-1]\n",
      "        column = special_args[column_key]\n",
      "        op = special_args.get(f\"_filter_op_{number}\") or \"exact\"\n",
      "        value = special_args.get(f\"_filter_value_{number}\") or \"\"\n",
      "        if \"__\" in op:\n",
      "            op, value = op.split(\"__\", 1)\n",
      "        if column:\n",
      "            redirect_params.append((f\"{column}__{op}\", value))\n",
      "        redirect_params.extend(\n",
      "            [\n",
      "                (f\"_filter_column_{number}\", None),\n",
      "                (f\"_filter_op_{number}\", None),\n",
      "                (f\"_filter_value_{number}\", None),\n",
      "            ]\n",
      "        )\n",
      "    return redirect_params\n",
      "filters_should_redirect(special_args=<MultiParams: {'_shape': ['array']}>)\n",
      "\n",
      "n:\n",
      "3\n",
      "Statement:\n",
      "filter_column = special_args.get(\"_filter_column\")\n",
      "State:\n",
      "None\n",
      "==================================================\n",
      "Clean Code:\n",
      "def filters_should_redirect(special_args):\n",
      "    redirect_params = []\n",
      "    filter_column = special_args.get(\"_filter_column\")\n",
      "    filter_op = special_args.get(\"_filter_op\") or \"\"\n",
      "    filter_value = special_args.get(\"_filter_value\") or \"\"\n",
      "    if \"__\" in filter_op:\n",
      "        filter_op, filter_value = filter_op.split(\"__\", 1)\n",
      "    if filter_column:\n",
      "        redirect_params.append((f\"{filter_column}__{filter_op}\", filter_value))\n",
      "    for key in (\"_filter_column\", \"_filter_op\", \"_filter_value\"):\n",
      "        if key in special_args:\n",
      "            redirect_params.append((key, None))\n",
      "    column_keys = [k for k in special_args if filter_column_re.match(k)]\n",
      "    for column_key in column_keys:\n",
      "        number = column_key.split(\"_\")[-1]\n",
      "        column = special_args[column_key]\n",
      "        op = special_args.get(f\"_filter_op_{number}\") or \"exact\"\n",
      "        value = special_args.get(f\"_filter_value_{number}\") or \"\"\n",
      "        if \"__\" in op:\n",
      "            op, value = op.split(\"__\", 1)\n",
      "        if column:\n",
      "            redirect_params.append((f\"{column}__{op}\", value))\n",
      "        redirect_params.extend(\n",
      "            [\n",
      "                (f\"_filter_column_{number}\", None),\n",
      "                (f\"_filter_op_{number}\", None),\n",
      "                (f\"_filter_value_{number}\", None),\n",
      "            ]\n",
      "        )\n",
      "    return redirect_params\n",
      "filters_should_redirect(special_args=<MultiParams: {'_shape': ['array']}>)\n",
      "\n",
      "n:\n",
      "4\n",
      "Statement:\n",
      "filter_op = special_args.get(\"_filter_op\") or \"\"\n",
      "State:\n",
      "''\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('incremental_statement_prediction.jsonl', 'r') as file:\n",
    "    for line in file:\n",
    "        json_object = json.loads(line)\n",
    "        \n",
    "        print(\"Clean Code:\")\n",
    "        print(json_object.get(\"Source Code\"))\n",
    "        print(\"\\nn:\")\n",
    "        print(json_object.get(\"Block_Size\"))\n",
    "        print(\"Statement:\")\n",
    "        print(json_object.get(\"Selected Statement\"))\n",
    "        print(\"State:\")\n",
    "        print(json_object.get(\"Value After Statement Execution\"))\n",
    "        print(\"=\"*50) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
